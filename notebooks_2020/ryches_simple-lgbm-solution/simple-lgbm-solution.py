#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.


# In[ ]:


building_df = pd.read_csv("../input/ashrae-energy-prediction/building_metadata.csv")
weather_train = pd.read_csv("../input/ashrae-energy-prediction/weather_train.csv")
train = pd.read_csv("../input/ashrae-energy-prediction/train.csv")


# Features that are likely predictive:
# 
# **Buildings**
# * primary_use
# * square_feet
# * year_built
# * floor_count (may be too sparse to use)
# 
# **Weather**
# * time of day
# * holiday
# * weekend
# * cloud_coverage + lags
# * dew_temperature + lags
# * precip_depth + lags
# * sea_level_pressure + lags
# * wind_direction + lags
# * wind_speed + lags
# 
# **Train**
# * max, mean, min, std of the specific building historically
# * number of meters
# * number of buildings at a siteid

# In[ ]:


building_df


# In[ ]:


weather_train


# In[ ]:


train


# In[ ]:


train = train.merge(building_df, left_on = "building_id", right_on = "building_id", how = "left")


# In[ ]:


weather_train


# In[ ]:


train = train.merge(weather_train, left_on = ["site_id", "timestamp"], right_on = ["site_id", "timestamp"], how = "left")


# In[ ]:


del weather_train


# In[ ]:


train["timestamp"] = pd.to_datetime(train["timestamp"])
train["hour"] = train["timestamp"].dt.hour
train["day"] = train["timestamp"].dt.day
train["weekend"] = train["timestamp"].dt.weekday
train["month"] = train["timestamp"].dt.month


# In[ ]:


#looks like there may be some errors with some of the readings
train[train["site_id"] == 0].plot("timestamp", "meter_reading")


# In[ ]:


train[train["site_id"] == 2].plot("timestamp", "meter_reading")


# In[ ]:


train[["hour", "day", "weekend", "month"]]


# In[ ]:


train = train.drop("timestamp", axis = 1)


# In[ ]:


from sklearn.preprocessing import LabelEncoder


# In[ ]:


le = LabelEncoder()
train["primary_use"] = le.fit_transform(train["primary_use"])


# In[ ]:


categoricals = ["building_id", "primary_use", "hour", "day", "weekend", "month", "meter"]


# In[ ]:


train


# In[ ]:


drop_cols = ["precip_depth_1_hr", "sea_level_pressure", "wind_direction", "wind_speed"]


# In[ ]:


numericals = ["square_feet", "year_built", "air_temperature", "cloud_coverage",
              "dew_temperature"]


# In[ ]:


train[categoricals + numericals]


# In[ ]:


feat_cols = categoricals + numericals


# In[ ]:


train["meter_reading"].value_counts()


# In[ ]:


#maybe remove some of the high outliers because of sensor error
# train["meter_reading"] = train["meter_reading"].clip(upper = train["meter_reading"].quantile(.999))


# In[ ]:





# In[ ]:


#uncomment to plot 100 highest consuming buildings
# import matplotlib.pyplot as plt
# top_buildings = train.groupby("building_id")["meter_reading"].mean().sort_values(ascending = False).iloc[:100]
# for value in top_buildings.index:
#     train[train["building_id"] == value]["meter_reading"].rolling(window = 24).mean().plot()
#     plt.show()


# In[ ]:


target = np.log1p(train["meter_reading"])


# In[ ]:


del train["meter_reading"]


# In[ ]:


train = train.drop(drop_cols + ["site_id", "floor_count"], axis = 1)


# In[ ]:


train


# In[ ]:


#Based on this great kernel https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65
def reduce_mem_usage(df):
    start_mem_usg = df.memory_usage().sum() / 1024**2 
    print("Memory usage of properties dataframe is :",start_mem_usg," MB")
    NAlist = [] # Keeps track of columns that have missing values filled in. 
    for col in df.columns:
        if df[col].dtype != object:  # Exclude strings            
            # Print current column type
            print("******************************")
            print("Column: ",col)
            print("dtype before: ",df[col].dtype)            
            # make variables for Int, max and min
            IsInt = False
            mx = df[col].max()
            mn = df[col].min()
            print("min for this col: ",mn)
            print("max for this col: ",mx)
            # Integer does not support NA, therefore, NA needs to be filled
            if not np.isfinite(df[col]).all(): 
                NAlist.append(col)
                df[col].fillna(mn-1,inplace=True)  
                   
            # test if column can be converted to an integer
            asint = df[col].fillna(0).astype(np.int64)
            result = (df[col] - asint)
            result = result.sum()
            if result > -0.01 and result < 0.01:
                IsInt = True            
            # Make Integer/unsigned Integer datatypes
            if IsInt:
                if mn >= 0:
                    if mx < 255:
                        df[col] = df[col].astype(np.uint8)
                    elif mx < 65535:
                        df[col] = df[col].astype(np.uint16)
                    elif mx < 4294967295:
                        df[col] = df[col].astype(np.uint32)
                    else:
                        df[col] = df[col].astype(np.uint64)
                else:
                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:
                        df[col] = df[col].astype(np.int8)
                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:
                        df[col] = df[col].astype(np.int16)
                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:
                        df[col] = df[col].astype(np.int32)
                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:
                        df[col] = df[col].astype(np.int64)    
            # Make float datatypes 32 bit
            else:
                df[col] = df[col].astype(np.float32)
            
            # Print new column type
            print("dtype after: ",df[col].dtype)
            print("******************************")
    # Print final result
    print("___MEMORY USAGE AFTER COMPLETION:___")
    mem_usg = df.memory_usage().sum() / 1024**2 
    print("Memory usage is: ",mem_usg," MB")
    print("This is ",100*mem_usg/start_mem_usg,"% of the initial size")
    return df, NAlist


# In[ ]:


train, NAlist = reduce_mem_usage(train)


# In[ ]:


from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
import lightgbm as lgb
num_folds = 5
kf = KFold(n_splits = num_folds, shuffle = False, random_state = 42)
error = 0
models = []
for i, (train_index, val_index) in enumerate(kf.split(train)):
    if i + 1 < num_folds:
        continue
    print(train_index.max(), val_index.min())
    train_X = train[feat_cols].iloc[train_index]
    val_X = train[feat_cols].iloc[val_index]
    train_y = target.iloc[train_index]
    val_y = target.iloc[val_index]
    lgb_train = lgb.Dataset(train_X, train_y > 0)
    lgb_eval = lgb.Dataset(val_X, val_y > 0)
    params = {
            'boosting_type': 'gbdt',
            'objective': 'binary',
            'metric': {'binary_logloss'},
            'learning_rate': 0.1,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq' : 5
            }
    gbm_class = lgb.train(params,
                lgb_train,
                num_boost_round=2000,
                valid_sets=(lgb_train, lgb_eval),
               early_stopping_rounds=20,
               verbose_eval = 20)
    
    lgb_train = lgb.Dataset(train_X[train_y > 0], train_y[train_y > 0])
    lgb_eval = lgb.Dataset(val_X[val_y > 0] , val_y[val_y > 0])
    params = {
            'boosting_type': 'gbdt',
            'objective': 'regression',
            'metric': {'rmse'},
            'learning_rate': 0.5,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq' : 5
            }
    gbm_regress = lgb.train(params,
                lgb_train,
                num_boost_round=2000,
                valid_sets=(lgb_train, lgb_eval),
               early_stopping_rounds=20,
               verbose_eval = 20)
#     models.append(gbm)

    y_pred = (gbm_class.predict(val_X, num_iteration=gbm_class.best_iteration) > .5) *    (gbm_regress.predict(val_X, num_iteration=gbm_regress.best_iteration))
    error += np.sqrt(mean_squared_error(y_pred, (val_y)))/num_folds
    print(np.sqrt(mean_squared_error(y_pred, (val_y))))
    break
print(error)


# In[ ]:


sorted(zip(gbm_regress.feature_importance(), gbm_regress.feature_name()),reverse = True)


# In[ ]:


import gc
del train


# In[ ]:


del train_X, val_X, lgb_train, lgb_eval, train_y, val_y, y_pred, target


# In[ ]:


gc.collect()


# In[ ]:


#preparing test data
test = pd.read_csv("../input/ashrae-energy-prediction/test.csv")
# test, NAlist = reduce_mem_usage(test)
test = test.merge(building_df, left_on = "building_id", right_on = "building_id", how = "left")
del building_df
gc.collect()


# In[ ]:


test


# In[ ]:


test["primary_use"] = le.transform(test["primary_use"])


# In[ ]:


test, NAlist = reduce_mem_usage(test)


# In[ ]:


test


# In[ ]:


gc.collect()


# In[ ]:


weather_test = pd.read_csv("../input/ashrae-energy-prediction/weather_test.csv")
weather_test = weather_test.drop(drop_cols, axis = 1)


# In[ ]:


weather_test


# In[ ]:


test = test.merge(weather_test, left_on = ["site_id", "timestamp"], right_on = ["site_id", "timestamp"], how = "left")
del weather_test


# In[ ]:


test["timestamp"] = pd.to_datetime(test["timestamp"])
test["hour"] = test["timestamp"].dt.hour.astype(np.uint8)
test["day"] = test["timestamp"].dt.day.astype(np.uint8)
test["weekend"] = test["timestamp"].dt.weekday.astype(np.uint8)
test["month"] = test["timestamp"].dt.month.astype(np.uint8)
test = test[feat_cols]


# In[ ]:


from tqdm import tqdm
i=0
res=[]
step_size = 50000
for j in tqdm(range(int(np.ceil(test.shape[0]/50000)))):
    
    res.append(np.expm1((gbm_class.predict(test.iloc[i:i+step_size], num_iteration=gbm_class.best_iteration) > .5) *    (gbm_regress.predict(test.iloc[i:i+step_size], num_iteration=gbm_regress.best_iteration))))
    i+=step_size


# In[ ]:


del test


# In[ ]:


res = np.concatenate(res)


# In[ ]:


pd.DataFrame(res).describe()


# In[ ]:


res.shape


# In[ ]:


sub = pd.read_csv("../input/ashrae-energy-prediction/sample_submission.csv")


# In[ ]:


sub["meter_reading"] = res


# In[ ]:


sub.to_csv("submission.csv", index = False)


# In[ ]:




