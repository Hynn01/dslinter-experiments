#!/usr/bin/env python
# coding: utf-8

# # Intro
# 
# At the end of this lesson, you will understand how stochastic gradient descent and back-propagation are used to set the weights in a deep learning model. These topics are complex, but many experts view them as the most important ideas in deep learning.
# 
# # Lesson
# 

# In[ ]:


from IPython.display import YouTubeVideo
YouTubeVideo('kQmHaI5Jw1c', width=800, height=450)


# # Keep Going
# Now you are ready to **[train your own models from scratch](https://www.kaggle.com/dansbecker/deep-learning-from-scratch).**
# 
# ---
# **Links Mentioned**
# 
# [ReLU activation function](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning)

# ---
# 
# 
# 
# 
# *Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/161321) to chat with other Learners.*
