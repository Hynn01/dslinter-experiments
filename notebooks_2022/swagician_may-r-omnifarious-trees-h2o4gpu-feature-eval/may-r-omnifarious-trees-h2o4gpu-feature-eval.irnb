{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Loading in libraries\npacman::p_load(tidyverse, dplyr, ggplot2,  ggthemes, RColorBrewer, fastDummies, janitor, MLmetrics,\n               repr, BBmisc, devtools, reticulate, Rcpp, tictoc, data.table, corrplot, ggrepel, cvAUC, e1071, resample)\n\n\n#Loading in train data\ndf_train <- read.csv(\"../input/tabular-playground-series-may-2022/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-01T12:54:27.527151Z","iopub.execute_input":"2022-05-01T12:54:27.529173Z","iopub.status.idle":"2022-05-01T12:56:08.581253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reticulate::py_discover_config(\"h2o4gpu\")\nreticulate::conda_version(conda = \"auto\")","metadata":{"execution":{"iopub.status.busy":"2022-05-01T12:56:08.584426Z","iopub.execute_input":"2022-05-01T12:56:08.611201Z","iopub.status.idle":"2022-05-01T12:56:11.028757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading in h2o4gpu package\ndevtools::install_github(\"h2oai/h2o4gpu\", subdir = \"src/interface_r\")\n\nlibrary(h2o4gpu)\n\n#Installing python module for h2o4gpu\npy_install(packages = \"h2o4gpu\",\n           pip = TRUE)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T12:56:11.031849Z","iopub.execute_input":"2022-05-01T12:56:11.03319Z","iopub.status.idle":"2022-05-01T12:59:36.23719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What's the deal with the data?","metadata":{}},{"cell_type":"markdown","source":"**Pie maker function**","metadata":{}},{"cell_type":"code","source":"pie_maker <- function(df, names) {\n\n#Piechart\ndf_pie <- data.frame(c(as.numeric(prop.table(table(df[, length(df)])))), unique(df[, length(df)]))\n    colnames(df_pie) <- names\n\ndf_pie[, length(df_pie)] <- as.factor(df_pie[, length(df_pie)])\n\n#Label positions\ndf_pie <- df_pie %>% \n  arrange(desc(df_pie[, length(df_pie)])) %>%\n  mutate(prop = df_pie[, length(df_pie)-1] / sum(df_pie[, length(df_pie)-1]) *100) %>%\n  mutate(ypos = cumsum(prop)- 0.5*prop)\n\ndf_pie <- df_pie %>%\ncbind(roundup = round(df_pie$prop, digits = 3))\n\n\n\noptions(repr.plot.width = 12, repr.plot.height = 7)\n# Basic piechart\nggplot(df_pie, aes(x =\" \", y = prop, fill = df_pie[, 2])) +\n  geom_bar(stat = \"identity\", width = 2, color = \"white\") +\n  coord_polar(\"y\", start = 0) +\n  theme_void() + \n  geom_label_repel(data = df_pie,\n                   aes(y = ypos, label = paste0(roundup, \"%\")),\n                   size = 7.5, nudge_x = 1.75, show.legend = FALSE) +\n scale_fill_manual(values = c(brewer.pal(n = 8, name = 'Pastel2'), brewer.pal(n = 7, name = 'Pastel1')), name = names[2]) +\n theme(legend.key.size = unit(1, 'cm'), \n        legend.key.height = unit(1, 'cm'), \n        legend.key.width = unit(1, 'cm'), \n        legend.title = element_text(size = 14), \n        legend.text = element_text(size = 11))\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-01T12:59:36.240879Z","iopub.execute_input":"2022-05-01T12:59:36.242466Z","iopub.status.idle":"2022-05-01T12:59:36.256546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a pie chart\npie_maker(df_train, c(\"Target\", \"States of Target\")) + ggtitle(\"Distribution of 'target' classes\") + theme(plot.title = element_text(size = 30))","metadata":{"execution":{"iopub.status.busy":"2022-05-01T12:59:36.259931Z","iopub.execute_input":"2022-05-01T12:59:36.261551Z","iopub.status.idle":"2022-05-01T12:59:37.093025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Classes seem to be balanced with a very slight lead by '1' state.","metadata":{}},{"cell_type":"code","source":"head(df_train[, 1:18])\nhead(df_train[, 19:length(df_train)])","metadata":{"execution":{"iopub.status.busy":"2022-05-01T12:59:37.098631Z","iopub.execute_input":"2022-05-01T12:59:37.102877Z","iopub.status.idle":"2022-05-01T12:59:37.192881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing unnecessary (for this summary) features\ndf_stats <- df_train %>%\n    dplyr::select(-c(1,29,33))\n\n#Generating columns of min, max and mean values for each feature\ndf_stats <- df_stats %>%\n  pivot_longer(everything()) %>%\n  group_by(name) %>% \n  summarise_at(\"value\", list(~min(.), ~max(.), ~mean(.),  ~sd(.), ~skewness(.)))\n\ndf_stats <- t(df_stats) #Transposing changed dataframe/matrix\n\nhead(df_stats[, 1:15])\nhead(df_stats[, 16:30])","metadata":{"execution":{"iopub.status.busy":"2022-05-01T12:59:37.198452Z","iopub.execute_input":"2022-05-01T12:59:37.202872Z","iopub.status.idle":"2022-05-01T12:59:51.023908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ranges of features get different as the suffixes rise. ['f_00' - 'f06'], ['f19' - 'f26'] these two groups have vastly diffent ranges and have a symmetrical distribution (skewness is very close to 0 for 'double' type variables). Although feature f_28 stands out with its range it also has a symmetrical distribution.\n\nFirst group: ['f_00' - 'f06'] has a standard Gaussian distribution - mean = 0 and sd = 1. Second group: ['f19' - 'f26']  slightly varies.","metadata":{}},{"cell_type":"markdown","source":"**Feature interactions**\n\nSince the theme of this playground excercise is interactions between features, I'll be trying and most probably missing different combinations of statistics. Do previously established groups have a negative mean / how do integer features come into play / counting the letters and checking if feature_27 starts with an 'A' or a 'B'.","metadata":{}},{"cell_type":"markdown","source":"To start off, I'm tinkering with feature_27. Counting number of letters and also creating a sum (where A = 1 and Z = 26).","metadata":{}},{"cell_type":"code","source":"data_prep <- function(df, is_test = NULL) {\n\nif(missing(is_test) || is_test == FALSE) {\n\n#Counting how many times does each letter appear in the string\ndf <- df %>%\n    mutate(AB_start =  ifelse(substr(f_27, 1,1) == 'A', 1,0))\n\n\nfor(i in letters[1:26]) {\n    \n    df <- df %>%\n        mutate(temp = str_count(f_27, toupper(i)))\n    \n    colnames(df)[length(df)] <- paste('letter', toupper(i), sep = '_')\n}\n\n\n\nranked_letters_sum <- 0\n\nfor(j in 1:26) {\n    ranked_letters_sum <- ranked_letters_sum + j*df[, j+34] \n}\n\ndf <- df %>%\n    cbind(ranked_letters_sum)\n\n#Removing f_27 and id\n    df <- df[, -c(1,29)]\n\n    \ndf <- df %>% \n    dplyr::select(-target, target) #Putting target variable back at the end of the dataframe\n\n\n    } else if(!missing(is_test) && is_test == TRUE) {\n    \n       #Counting how many times does each letter appear in the string\n        df <- df %>%\n            mutate(AB_start =  ifelse(substr(f_27, 1,1) == 'A', 1,0))\n\n\n    for(i in letters[1:26]) {\n    \n        df <- df%>%\n            mutate(temp = str_count(f_27, toupper(i)))\n    \n        colnames(df)[length(df)] <- paste('letter', toupper(i), sep = '_')\n                            }\n\n\n\nranked_letters_sum <- 0\n\nfor(j in 1:26) {\n    ranked_letters_sum <- ranked_letters_sum + j*df[, j+32] \n}\n\ndf <- df %>%\n    cbind(ranked_letters_sum)\n\n#Removing f_27 and id\ndf <- df[, -c(1,29)]\n\n}\n\n#Scaling some numerical features to a (0,1) range\ndf <- df %>%\n   mutate_at(c(20:27,28), BBmisc::normalize, method = 'range', range = c(0,1))\n  \nreturn(df)\n    }","metadata":{"execution":{"iopub.status.busy":"2022-05-01T12:59:51.027166Z","iopub.execute_input":"2022-05-01T12:59:51.028527Z","iopub.status.idle":"2022-05-01T12:59:51.041464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sprouting trees from the ground up","metadata":{}},{"cell_type":"markdown","source":"**Grid search function**","metadata":{}},{"cell_type":"code","source":"#dataset = data.frame object, model_type = h2o4gpu classifier model, params = list of parameters*, return_n = how many observations should be returned\n#k - number of folds; if NULL, k = 10, which_fold - setting a seed to check on the same fold; if NULL, last fold is chosen\n#params = depending on model type - list of n parameter arrays\nh2o4gpu_grid_search <- function(dataset, model_type, params, return_n, k = NULL, which_fold = NULL) {\n\n#Dataset split\nif(missing(k) || (is.null(k))) {\n    k_grid <- 10\n} else if(!missing(k))\n        k_grid <- k\n\n\n\nif(missing(which_fold) || (is.null(which_fold))) {\nsampled_k <- k_grid \n} else if(!missing(which_fold))\n        sampled_k <- which_fold\n\nholdout_rows <- c(((sampled_k-1)*floor((nrow(dataset)/k_grid))+1):((floor(nrow(dataset)/k_grid))*sampled_k)+nrow(dataset)%%k_grid)\n\nif(model_type == 'rf') {\n    \ndf_grid <- data.frame(n_estimators = 0, max_depth = 0, colsample_bytree = 0, subsample = 0, AUC = 0) \n    \n\n        rf_n_estimators <- params[[1]]\n        rf_max_depth <- params[[2]]\n        rf_colsample_bytree <- params[[3]]\n        rf_subsample <- params[[4]]\n    \n#Brute grid search loops\nfor(i in rf_n_estimators){\n  for(j in rf_max_depth){\n    for(k in rf_colsample_bytree){\n      for(l in rf_subsample){\n        \n    rf_model_class <- h2o4gpu.random_forest_classifier(n_estimators = i, criterion = \"entropy\",\n    max_depth = j, colsample_bytree = k,\n    subsample = l, random_state = 1212, max_features = \"auto\",\n    verbose = 0, tree_method = \"gpu_hist\",\n    predictor = \"gpu_predictor\", backend = \"h2o4gpu\") %>% fit(dataset[-holdout_rows, -length(dataset)], dataset[-holdout_rows, length(dataset)])\n\n        \n        df_grid <- df_grid %>%\n          rbind(c(i,j,k,l, AUC(predictions = predict(rf_model_class,\n                                  dataset[holdout_rows, -length(dataset)], type = \"prob\")[, 2],\n                                  labels = dataset[holdout_rows, length(dataset)])))\n        \n                          }\n                        }\n                      }\n                    }\n    \n    \n    \n        df_grid <- df_grid %>%\n        arrange(desc(AUC)) %>%\n        head(return_n)     \n    \n\n}     else if(model_type == 'gb') {\n\ndf_grid <- data.frame(learning_rate = 0, n_estimators = 0, max_depth = 0, colsample_bytree = 0, subsample = 0, AUC = 0) \n    \n\n        gb_learning_rate <- params[[1]]\n        gb_n_estimators <- params[[2]]\n        gb_max_depth <- params[[3]]\n        gb_colsample_bytree <- params[[4]]\n        gb_subsample <- params[[5]]\n\n    \n#Brute grid search loops\nfor(i in gb_learning_rate) {\n    for(j in gb_n_estimators) {\n      for(k in gb_max_depth) {\n        for(l in gb_colsample_bytree){\n          for(n in gb_subsample) {\n        \n    gb_model_class <- h2o4gpu.gradient_boosting_classifier(loss = \"deviance\", learning_rate = i,\n                n_estimators = j, subsample = n, criterion = \"friedman_mse\",\n                min_weight_fraction_leaf = 0, max_depth = k, min_impurity_decrease = 0,random_state = 1212,\n                max_features = \"auto\", verbose = 0,\n                warm_start = FALSE, presort = \"auto\", colsample_bytree = l,\n                tree_method = \"gpu_hist\", \n                predictor = \"gpu_predictor\", objective = \"binary:logistic\",\n                booster = \"gbtree\", gamma = 0, colsample_bylevel = 1, reg_alpha = 0,\n                reg_lambda = 1, scale_pos_weight = 1, base_score = 0.5,\n                backend = \"h2o4gpu\") %>% fit(dataset[-holdout_rows, -length(dataset)], dataset[-holdout_rows, length(dataset)])\n              \n        df_grid <- df_grid %>%\n          rbind(c(i,j,k,l, n, AUC(predictions = predict(gb_model_class,\n                                  dataset[holdout_rows, -length(dataset)], type = \"prob\")[, 2],\n                                  labels = dataset[holdout_rows, length(dataset)])))\n        \n                                  }\n                                 }\n                                }\n                               }\n                              }\n\n    df_grid <- df_grid %>%\n        arrange(desc(AUC)) %>%\n        head(return_n)\n}\n\nreturn(df_grid)\n}","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-05-01T12:59:51.04469Z","iopub.execute_input":"2022-05-01T12:59:51.046008Z","iopub.status.idle":"2022-05-01T12:59:51.059494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Finding parameters**","metadata":{}},{"cell_type":"code","source":"df_train <- data_prep(df_train)\n\ngb_params <- list(c(0.05), c(400,800), c(10,15), c(1),c(1))\n\nh2o4gpu_grid_search(df_train, model_type = 'gb', params = gb_params, return_n = 5, k = 5, which_fold = 3)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T12:59:51.062549Z","iopub.execute_input":"2022-05-01T12:59:51.063983Z","iopub.status.idle":"2022-05-01T13:12:51.179029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time for predictions","metadata":{}},{"cell_type":"code","source":"#Loading in test data\ndf_test <- read.csv(\"../input/tabular-playground-series-may-2022/test.csv\")\n\n#Augmenting data\ndf_test <- data_prep(df_test, is_test = TRUE)\n\n\n#Creating and fitting a  model\n    gb_model <- h2o4gpu.gradient_boosting_classifier(loss = \"deviance\", learning_rate = 0.05,\n                n_estimators = 1200, subsample = 1, criterion = \"friedman_mse\",\n                min_samples_split = 2, min_samples_leaf = 1,\n                min_weight_fraction_leaf = 0, max_depth = 14, min_impurity_decrease = 0,random_state = 1077,\n                max_features = \"auto\", verbose = 0,\n                warm_start = FALSE, presort = \"auto\", colsample_bytree = 1,\n                tree_method = \"gpu_hist\", \n                predictor = \"gpu_predictor\", objective = \"binary:logistic\",\n                booster = \"gbtree\", gamma = 0, colsample_bylevel = 1, reg_alpha = 0,\n                reg_lambda = 1, scale_pos_weight = 1, base_score = 0.5,\n                backend = \"h2o4gpu\") %>% fit(df_train[, -length(df_train)], df_train[, length(df_train)])\n                                                                                              \n                                                                                              \n                                                                                              \n#Creating predictions                                                                                              \ngb_predictions <- predict(gb_model, df_test, type = \"prob\")[, 2]                                                                                          ","metadata":{"execution":{"iopub.status.busy":"2022-05-01T13:12:51.182173Z","iopub.execute_input":"2022-05-01T13:12:51.183517Z","iopub.status.idle":"2022-05-01T13:21:07.438655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading in the submission file\ntree_submission <- read.csv(\"../input/tabular-playground-series-may-2022/sample_submission.csv\")\n\ntree_submission$target <- gb_predictions\n\nwrite.csv(tree_submission, \"./tree_sub.csv\", quote = FALSE, row.names = FALSE)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T13:21:07.441878Z","iopub.execute_input":"2022-05-01T13:21:07.443256Z","iopub.status.idle":"2022-05-01T13:21:09.218942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Things to do / test out:\n* correlation between 'double' type variables - do aforementioned groups that are based on ranges and normality exist?\n* making dummy columns for integer features\n* sequential addition of features\n* adding statistics that are row-based","metadata":{}}]}