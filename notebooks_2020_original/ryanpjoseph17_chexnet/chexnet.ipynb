{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Necessary Dependencies\nimport numpy as np \nimport pandas as pd \n!pip install utils\nfrom utils import *\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nimport os\nfrom sklearn.model_selection import train_test_split\nimport statistics\nfrom tqdm import tqdm\n\n# DenseNet Dependencies\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.activations import sigmoid\nfrom tensorflow.keras.layers import Dense,Conv2D, Flatten, Dropout, MaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.metrics import Accuracy, Precision, Recall, AUC, BinaryAccuracy, FalsePositives, FalseNegatives, TruePositives, TrueNegatives\nfrom tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.applications import DenseNet121, DenseNet169, DenseNet201\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import regularizers\nfrom tensorflow import keras\nfrom matplotlib import pyplot as plt\n\n# Classification Metrics\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\nprint('Started')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test if GPU present\nprint(\"Num GPUs Used: \", len(tf.config.experimental.list_physical_devices('GPU')))\n\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\nmpl.rcParams['figure.figsize'] = (12, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters\n\n\n# Preprocessing \n# 224\nIMG_IND = 224\nIMG_SIZE = (IMG_IND, IMG_IND)\nIMG_SHAPE = (IMG_IND,IMG_IND,3)\n\n# Model\nOPTIMIZER = Adam(learning_rate=0.1,\n                 beta_1=0.9,\n                 beta_2=0.999)\n\nLOSS = 'binary_crossentropy'\nMETRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='BinaryAccuracy')\n]\n\n# Training \nEPOCHS = 100\nBATCH_SIZE = 64\n\n# Callbacks \nMODEL_CHECKPOINT_PERIOD = 25\nLEARNING_RATE_PATIENCE = 5\n\n# Binary Disease\nbinary_disease = ['Effusion']\nbinary_disease_str = 'Effusion'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weight = {\n    0: 1.,\n    1: 2.0\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Establish Directories \n\nif not os.path.exists('logs'):\n    os.makedirs('logs')\n    \nif not os.path.exists('callbacks'):\n    os.makedirs('callbacks')\n    \nif not os.path.exists('training_1'):\n    os.makedirs('training_1')\n    \nCALLBACKS_DIR = '/kaggle/working/callbacks/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Curating Labels Folder","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Disease Names / Class Labels \ndisease_labels = ['Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema', 'Emphysema', 'Fibrosis', 'Effusion', 'Pneumonia', 'Pleural_Thickening',\n'Cardiomegaly', 'Nodule', 'Mass', 'Hernia']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Stanford Images Distribution Files - Predetermined by the Stanford ChexNet Team\n\nlabels_train_val = pd.read_csv('/kaggle/input/train-val-images/train_val_list.txt')\nlabels_train_val.columns = ['Image_Index']\n\nlabels_test = pd.read_csv('/kaggle/input/tests-image/test_list.txt')\nlabels_test.columns = ['Image_Index']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NIH Dataset Labels CSV File \n\nlabels_df = pd.read_csv('/kaggle/input/data/Data_Entry_2017.csv')\n\nlabels_df.columns = ['Image_Index', 'Finding_Labels', 'Follow_Up_#', 'Patient_ID',\n                  'Patient_Age', 'Patient_Gender', 'View_Position',\n                  'Original_Image_Width', 'Original_Image_Height',\n                  'Original_Image_Pixel_Spacing_X',\n                  'Original_Image_Pixel_Spacing_Y', 'dfd']\n\n# Print Example Labels DataFrame\n#print(labels_df.head(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Binary Class Mapping \nlabels_df[binary_disease_str] = labels_df['Finding_Labels'].map(lambda x: binary_disease_str in x)\n\n# Print Class Mapping\nprint(labels_df[binary_disease_str].head(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combines Stanford Image Distribution csv file with NIH Label CSV\ntrain_val_merge = pd.merge(left=labels_train_val, right=labels_df, left_on='Image_Index', right_on='Image_Index')\n\ntest_merge = pd.merge(left=labels_test, right=labels_df, left_on='Image_Index', right_on='Image_Index')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting Finding Labels\ntrain_val_merge['Finding_Labels'] = train_val_merge['Finding_Labels'].apply(lambda s: [l for l in str(s).split('|')])\n\ntest_merge['Finding_Labels'] = test_merge['Finding_Labels'].apply(lambda s: [l for l in str(s).split('|')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mapping Images to the theirs paths \nnum_glob = glob('/kaggle/input/data/*/images/*.png')\nimg_path = {os.path.basename(x): x for x in num_glob}\n\n# Training + Validation Mapping\ntrain_val_merge['Paths'] = train_val_merge['Image_Index'].map(img_path.get)\n# Testing Mapping\ntest_merge['Paths'] = test_merge['Image_Index'].map(img_path.get)\n\n# Print Paths \n#train_val_merge.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No Overlap in patients between the Train and Validation Data Sets\npatients = np.unique(train_val_merge['Patient_ID'])\ntest_patients = np.unique(test_merge['Patient_ID'])\n\nprint('Number of Patients Between Train-Val Overall: ', len(patients))\nprint('Number of Patients Between Test Overall: ', len(test_patients))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and Validation Split\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train-Validation Split \ntrain_df, val_df = train_test_split(patients,\n                                   test_size = 0.0669,\n                                   random_state = 2019,\n                                    shuffle= True\n                                   )  \n\n\nprint('No. of Unique Patients in Train dataset : ',len(train_df))\ntrain_df = train_val_merge[train_val_merge['Patient_ID'].isin(train_df)]\nprint('Training Dataframe   : ', train_df.shape[0],' images')\n\nprint('\\nNo. of Unique Patients in Validtion dataset : ',len(val_df))\nval_df = train_val_merge[train_val_merge['Patient_ID'].isin(val_df)]\nprint('Validation Dataframe   : ', val_df.shape[0],' images')\n\nprint('\\nNo. of Unique Patients in Testing dataset : ',len(test_patients))\ntest_df = test_merge[test_merge['Patient_ID'].isin(test_patients)]\nprint('Testing Dataframe   : ', test_df.shape[0],' images')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of Cases in each dataframe - before Oversampling \n\nprint(f'# of {binary_disease} Cases in Training\\n', train_df[binary_disease_str].value_counts(), '\\n')\n\nprint(f'# of {binary_disease} Cases in Validation\\n', val_df[binary_disease_str].value_counts(), '\\n')\n\nprint(f'# of {binary_disease} Cases in Testing\\n', test_df[binary_disease_str].value_counts(), '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Oversampling Method \npositive_cases = np.sum(train_df[binary_disease_str]==True)//2\noversample_factor = 2 # maximum number of cases in negative group so it isn't super rare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Over and Undersampling Specific Disease Classes\ntrain_df = train_df.groupby(['Patient_Gender', binary_disease_str]).apply(lambda x: x.sample(min(oversample_factor*positive_cases, x.shape[0]), replace = False)).reset_index(drop = True)\n\npositive_cases = np.sum(val_df[binary_disease_str]==True)//2\nval_df = val_df.groupby(['Patient_Gender', binary_disease_str]).apply(lambda x: x.sample(min(oversample_factor*positive_cases, x.shape[0]), replace = False)).reset_index(drop = True)\n\npositive_cases = np.sum(test_df[binary_disease_str]==True)//2\ntest_df = test_df.groupby(['Patient_Gender', binary_disease_str]).apply(lambda x: x.sample(min(oversample_factor*positive_cases, x.shape[0]), replace = False)).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of Cases in each dataframe - AFTER OVERSAMPLING\n\nprint(f'# of {binary_disease} Cases in Training\\n', train_df[binary_disease_str].value_counts(), '\\n')\n\nprint(f'# of {binary_disease} Cases in Validation\\n', val_df[binary_disease_str].value_counts(), '\\n')\n\nprint(f'# of {binary_disease} Cases in Testing\\n', test_df[binary_disease_str].value_counts(), '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reducing the Entire Dataset Size - Training  \n\nnum = 8095 \nprint(num)\n\noriginal_train_df = train_df\ntrain_df = train_df.groupby([binary_disease_str]).apply(lambda x: x.sample(num, replace = True)\n                                                      ).reset_index(drop = True)\n\nprint('New Data Size:', train_df.shape[0], 'Old Size:', original_train_df.shape[0])\nprint(f'# of {binary_disease} Cases in Training\\n', train_df[binary_disease_str].value_counts(), '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reducing the Entire Dataset Size - Validation\n\nnum = 564 \nprint(num)\n\noriginal_val_df = val_df\nval_df = val_df.groupby([binary_disease_str]).apply(lambda x: x.sample(num, replace = True)\n                                                      ).reset_index(drop = True)\n\nprint('New Data Size:', val_df.shape[0], 'Old Size:', original_val_df.shape[0])\nprint(f'# of {binary_disease} Cases in Validation\\n', val_df[binary_disease_str].value_counts(), '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reducing the Entire Dataset Size - Testing\n\nnum = 4658 \nprint(num)\n\noriginal_testing_df = test_df\ntest_df = test_df.groupby([binary_disease_str]).apply(lambda x: x.sample(num, replace = True)\n                                                      ).reset_index(drop = True)\n\nprint('New Data Size:', test_df.shape[0], 'Old Size:', original_testing_df.shape[0])\nprint(f'# of {binary_disease} Cases in Testing\\n', test_df[binary_disease_str].value_counts(), '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of Cases in each dataframe - AFTER REDUCTION\n\nprint(f'# of {binary_disease} Cases in Training\\n', train_df[binary_disease_str].value_counts(), '\\n')\n\nprint(f'# of {binary_disease} Cases in Validation\\n', val_df[binary_disease_str].value_counts(), '\\n')\n\nprint(f'# of {binary_disease} Cases in Testing\\n', test_df[binary_disease_str].value_counts(), '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Image Data Generator - Data Augmentation \ntrain_data_gen = ImageDataGenerator(rescale=1./255,\n                                    samplewise_center=True, \n                                    samplewise_std_normalization=True, \n                                    horizontal_flip = True,\n                                    zoom_range=0.1, \n                                    height_shift_range=0.05, \n                                    width_shift_range=0.05,\n                                    rotation_range=5\n                                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Flow From DataFrame - Keras Preprocessing Pipeline \n# Training Flow From DataFrame \n\ntrain_gen = train_data_gen.flow_from_dataframe(dataframe=train_df, \n                                                directory=None,\n                                                shuffle= True,\n                                                seed = 2,\n                                                x_col = 'Paths',\n                                                y_col = binary_disease, \n                                                target_size = IMG_SIZE,\n                                                class_mode='raw',\n                                                classes = disease_labels,\n                                                color_mode = 'rgb',\n                                                batch_size = BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validation Flow From DataFrame \n\nval_gen = train_data_gen.flow_from_dataframe(\n                                            dataframe=val_df, \n                                            directory=None,\n                                            shuffle= True,\n                                            seed = 2,\n                                            x_col = 'Paths',\n                                            y_col = binary_disease, \n                                            target_size = IMG_SIZE,\n                                            classes = disease_labels,\n                                            class_mode='raw',\n                                            color_mode = 'rgb',\n                                            batch_size = BATCH_SIZE\n                                            )\n\n# Splitting of Validation Generator\nx_val, y_val = next(val_gen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF DATA API","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Declare TensorFlow Datasets for more efficient training\ntrain_data = tf.data.Dataset.from_generator(lambda: train_gen,\n                                            output_types=(tf.float32, tf.int32),\n                                           output_shapes=([None, IMG_IND, IMG_IND, 3], [None, 1]))\nval_data = tf.data.Dataset.from_generator(lambda: val_gen,\n                                          output_types=(tf.float32, tf.int32),\n                                         output_shapes=([None, IMG_IND, IMG_IND, 3], [None, 1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feed_data(dataset):\n    \"\"\"\n    This prefetches all the data for the model\n    \n    Arguments:\n        dataset = tf.keras.FlowFromDataFrame \n        \n    Returns:\n        dataset = (tf.Dataset) the prefetched dataset (smaller batches)\n    \"\"\"\n    \n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)  \n    \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_paths = np.array(train_df['Paths'])\n#print(image_paths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n\nimages_to_augment = []\n\nfor image_path in image_paths[:4]:\n    image = load_img(image_path, target_size=(IMG_IND, IMG_IND))\n    image = img_to_array(image)\n    images_to_augment.append(image)\n    \nimages_to_augment = np.array(images_to_augment)\n\nimages_augmented = next(train_data_gen.flow(x=images_to_augment,\n                                batch_size=10,\n                                shuffle=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import array_to_img\n\nfig, axes = plt.subplots(2, 2)\n\nfor i in range(2):\n    axes[i, 0].imshow(array_to_img(images_to_augment[i]), \n                      #horizontal_flip = True,\n                      interpolation='nearest')\n    \n    axes[i, 1].imshow(array_to_img(images_augmented[i]), \n                      interpolation='nearest')\n    \n    axes[i, 0].set_xticks([])\n    axes[i, 1].set_xticks([])\n    \n    axes[i, 0].set_yticks([])\n    axes[i, 1].set_yticks([])\n    \n    axes[i, 0].set_yticks([])\n    axes[i, 1].set_yticks([])\n    \n    axes[i, 0].set_xticks([])\n    axes[i, 1].set_xticks([])\n    \n    axes[i, 0].set_yticks([])\n    axes[i, 1].set_yticks([])\n    \n    axes[i, 0].set_yticks([])\n    axes[i, 1].set_yticks([])\n    \ncolumns = ['Base Image', 'Augmented Image']\nfor ax, column in zip(axes[0], columns):\n    ax.set_title(column) \n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Section of Code written by brucechou1983 - https://github.com/brucechou1983/CheXNet-Keras\n# I have no experience with class weighting, brucechou1983 provided a very thorough explanation of the topic with example code\nCLASS_NAMES = disease_labels\ndef get_class_weights(total_counts, class_positive_counts, multiply):\n    \"\"\"\n    Calculate class_weight used in training\n    Arguments:\n    total_counts - int\n    class_positive_counts - dict of int, ex: {\"Effusion\": 300, \"Infiltration\": 500 ...}\n    multiply - int, positve weighting multiply\n    use_class_balancing - boolean \n    Returns:\n    class_weight - dict of dict, ex: {\"Effusion\": { 0: 0.01, 1: 0.99 }, ... }\n    \"\"\"\n    def get_single_class_weight(pos_counts, total_counts):\n        denominator = (total_counts - pos_counts) * multiply + pos_counts\n        return {\n            0: pos_counts / denominator,\n            1: (denominator - pos_counts) / denominator,\n        }\n\n    class_names = list(class_positive_counts.keys())\n    label_counts = np.array(list(class_positive_counts.values()))\n    class_weights = []\n    for i, class_name in enumerate(class_names):\n        class_weights.append(get_single_class_weight(label_counts[i], total_counts))\n\n    return class_weights\n\ndef get_sample_counts(output_dir, dataset, class_names):\n    \"\"\"\n    Get total and class-wise positive sample count of a dataset\n    Arguments:\n    output_dir - str, folder of dataset.csv\n    dataset - str, train|dev|test\n    class_names - list of str, target classes\n    Returns:\n    total_count - int\n    class_positive_counts - dict of int, ex: {\"Effusion\": 300, \"Infiltration\": 500 ...}\n    \"\"\"\n    df = pd.read_csv(os.path.join(output_dir, f\"{dataset}.csv\"))\n    total_count = df.shape[0]\n    labels = df[class_names].as_matrix()\n    positive_counts = np.sum(labels, axis=0)\n    class_positive_counts = dict(zip(class_names, positive_counts))\n    #class_positive_counts = (class_names, positive_counts)\n\n\n    return total_count, class_positive_counts\n\nnewfds = 'newfds'\ntrain_counts, train_pos_counts = get_sample_counts(newfds, \"/kaggle/input/newfds/train\", disease_labels)\nclass_weights = get_class_weights(train_counts, train_pos_counts, multiply=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code Snippet written by https://github.com/brucechou1983/CheXNet-Keras/blob/master/callback.py \nfrom keras.callbacks import Callback\nimport keras.backend as kb\nimport shutil\n\n# Empty List for future Visualizations\nMEAN_AUROC = []\nDISEASE_AUROC = []\nDISEASE_ = []\nLR_LOG = []\n\nclass MultipleClassAUROC(Callback):\n    \"\"\"\n    Monitor mean AUROC and update model\n    \"\"\"\n    def __init__(self, sequence, class_names, weights_path, stats=None, workers=1):\n        super(Callback, self).__init__()\n        #self.steps=STEPS ############################\n        self.sequence = sequence\n        self.workers = workers\n        self.class_names = class_names\n        self.weights_path = weights_path\n        self.best_weights_path = os.path.join(\n            os.path.split(weights_path)[0],\n            f\"best_{os.path.split(weights_path)[1]}\",\n        )\n        self.best_auroc_log_path = os.path.join(\n            os.path.split(weights_path)[0],\n            \"best_auroc.log\",\n        )\n        self.stats_output_path = os.path.join(\n            os.path.split(weights_path)[0],\n            \".training_stats.json\"\n        )\n        # for resuming previous training\n        if stats:\n            self.stats = stats\n        else:\n            self.stats = {\"best_mean_auroc\": 0}\n\n        # aurocs log\n        self.aurocs = {}\n        for c in self.class_names:\n            self.aurocs[c] = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        \"\"\"\n        Calculate the average AUROC and save the best model weights according\n        to this metric.\n        \"\"\"\n        print(\"\\n*********************************\")\n        self.stats[\"lr\"] = float(kb.eval(self.model.optimizer.lr))\n        print(f\"current learning rate: {self.stats['lr']}\")\n        #LR_LOG.append(self.stats['lr'])\n\n        y_hat = model.predict(self.sequence,verbose=1)\n        \n        pred_indices = np.argmax(y_hat,axis=1)\n\n        y = y_val \n    \n        print(f\"*** epoch#{epoch + 1} dev auroc ***\")\n        current_auroc = []\n        try:\n            score = roc_auc_score(y, y_hat)\n        except ValueError:\n            score = 0\n\n        current_auroc.append(score)\n        EPOCH = epoch + 1 \n\n        print(\"*********************************\")\n\n        mean_auroc = np.mean(current_auroc)\n        MEAN_AUROC.append(mean_auroc)\n        print(f\"Effusion auroc: {mean_auroc}\\n\")\n        \n        print(\"*********************************\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saves weights every 5 Epochs\n\n# Dynamic Checkpoint File Name \ncheckpoint_path = \"training_1/cp-{epoch:04d}.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Model Checkpointing Callback\ncp_callback = tf.keras.callbacks.ModelCheckpoint(\n                                                filepath=checkpoint_path, \n                                                verbose=1, \n                                                save_weights_only=True,\n                                                period=MODEL_CHECKPOINT_PERIOD)\n\n# Dynamic Learning Rate\nreduced_lr = tf.keras.callbacks.ReduceLROnPlateau(\n                                                monitor='val_loss',\n                                                factor=.05,\n                                                patience=LEARNING_RATE_PATIENCE,\n                                                verbose=1,\n                                                mode='min',\n                                                cooldown=0,\n                                                min_lr=1e-6 \n                                                )\n\n# Custom Callback displaying AUROC score across all diseases every epoch\nauroc = MultipleClassAUROC(\n                            sequence = x_val,\n                            class_names=binary_disease,\n                            weights_path=CALLBACKS_DIR,\n                            stats={},\n                            workers=1,\n                            )\n\n\n# CSV Logger of Metrics and Loss\ncsv_logger = CSVLogger('training.log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training/Validation Steps\n\ntrain_steps = train_gen.samples // BATCH_SIZE\nval_steps = val_gen.samples // BATCH_SIZE \n\n\nprint('Training Steps: ', train_steps)\nprint('Validation Steps: ', val_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initializing GPU \nwith tf.device('/GPU:0'):\n\n    # Using Pre-trained Model (DenseNet)\n    base_model = tf.keras.applications.DenseNet121(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet',\n                                               pooling=\"avg\")\n\n    base_model.trainable = False\n\n    x = base_model.output\n    \n    x = Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n    x = Dropout(0.2)(x)\n    x = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n    x = Dropout(0.2)(x)\n    x = Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n    x = Dropout(0.2)(x)\n    x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n    x = Dropout(0.2)(x)\n    x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n    x = Dropout(0.2)(x)\n    x = Dense(32, activation='relu', name='cautious_extract',  kernel_regularizer=regularizers.l2(0.0001))(x)\n    x = Dropout(0.2)(x)\n\n    predictions = Dense(1, activation='sigmoid',name='Final')(x)\n\n    model = Model(inputs=base_model.input, outputs=predictions)\n\n    model.compile(loss = 'binary_crossentropy',    \n                  optimizer=OPTIMIZER,\n                  metrics=METRICS\n                                 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Model\nhistory = model.fit(\n                    feed_data(train_data),\n                    steps_per_epoch = train_steps, \n                    validation_data= (feed_data(val_data)),    \n                    validation_steps = val_steps, \n                    epochs=EPOCHS,\n                    #use_multiprocessing=True,\n                    #class_weight = class_weight,\n                    callbacks=[reduced_lr, cp_callback, auroc]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save Model Weights at End of Training \nmodel.save_weights('Model_finished')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Graph of Binary Accuracy \n\nacc = history.history['BinaryAccuracy']\nval_acc = history.history['val_BinaryAccuracy']\n\n\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs_range = range(EPOCHS)\n\nplt.figure(figsize=(40, 10))\nplt.subplot(1, 2, 1)\nplt.grid()\nplt.plot(epochs_range, acc, label='Training Binary Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Binary Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Binary Accuracy', color='Green')\nfig.savefig('TrainingValidationAccuracy.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Graph of Loss\nplt.figure(figsize=(40, 10))\nplt.subplot(1, 2, 2)\nplt.grid()\n\nacc = history.history['loss']\nval_acc = history.history['val_loss']\nepochs_range = range(EPOCHS)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss', color='red')\nplt.show()\nfig.savefig('TrainingValidationLoss.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Graph of Mean AUROC - Validation Data\n\nplt.figure(figsize=(40, 10))\nplt.subplot(1, 2, 2)\n#plt.grid()\nplt.plot(MEAN_AUROC, label='Validation MEAN AUC_ROC')\nplt.legend(loc='upper right')\nplt.title('Validation AUC_ROC', color='Black')\n\nplt.axhline(y=0.8638, color='r', linestyle='--')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Graph of Mean AUROC - Validation Data\n\nplt.figure(figsize=(40, 10))\nplt.subplot(1, 2, 2)\nplt.grid()\nplt.plot(MEAN_AUROC, label='Validation MEAN AUC_ROC')\nplt.legend(loc='upper right')\nplt.title('Validation AUC_ROC', color='Black')\n\nplt.axhline(y=0.8638, color='r', linestyle='--')\n\nplt.ylim([0.0, 0.99])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Image Data Generator \ntest_data_gen = ImageDataGenerator(rescale=1./255)\n\n# Test Data Flow From DataFrame\ntest_gen = test_data_gen.flow_from_dataframe(dataframe=test_df, \n                                                directory=None,\n                                                shuffle = True,\n                                                seed = 3,\n                                                x_col = 'Paths',\n                                                y_col = binary_disease_str, \n                                                target_size = IMG_SIZE, \n                                                classes = disease_labels,\n                                                class_mode='raw',\n                                                color_mode = 'rgb',\n                                                batch_size = 640\n                                                )\n\nx_test, y_test = next(test_gen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(x_test , y_test, verbose=1)\nprint(f\"Test loss: {score[0]}\")\nprint(f\"Test accuracy: {score[1]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Model to Predict \npred_Y = model.predict(x_test,\n                        steps=64,\n                        verbose = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\nplt.matshow(confusion_matrix(y_test, pred_Y>0.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Precision \nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, pred_Y>0.5, target_names = ['Healthy', 'Effusion']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Graph of True Positive/False Positive \n\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\nfpr, tpr, _ = roc_curve(y_test, pred_Y)\nfig, ax1 = plt.subplots(1,1, figsize = (5, 5), dpi = 250)\nax1.plot(fpr, tpr, 'b.-', label = 'Effusion (AUC:%2.2f)' % roc_auc_score(y_test, pred_Y))\nax1.plot(fpr, fpr, 'k-', label = 'Random Guessing')\nax1.legend(loc = 4)\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate');\nfig.savefig('ROC_IMAGE_Atel')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Single AUCROC Binary Score\n\nprint('My ROC Score (Effusion) : ', roc_auc_score(y_test, pred_Y))\nprint('Stanford ROC Score (Effusion) : 0.8638')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Notes\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"###### ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}