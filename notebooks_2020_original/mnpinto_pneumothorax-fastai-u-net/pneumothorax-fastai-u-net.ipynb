{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kernel guideline\n\n### Version 2 - start here\n* Image size 128x128\n* Unet with pretrained resnet34 encoder\n* Best threshold selection\n* Output visualization\n* Total run time of about 34 minutes\n\n### Version 3 - 5-fold ensemble\n* Example of 5-fold ensemble using sklearn KFold function, based on version 2\n* Changed learning rates\n* Total run time of about 132 minutes\n\n### Version 4 - 256x256 \n* Based on version 2 but with images of size 256x256\n* Batch size reduced to 32\n\n### Where to go next?\n* Look at as many examples as you can and try to understand why the model fails when it does;\n* There are several ways to convert a 1024x1024 to a lower resolution (e.g., bilinear, nearest), some may be more appropriate to this competition than others;\n* Progressive rescaling (start with small images like 64x64, train, save the weights, increase to 128x128, train with previous weights, and so on);"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, '../input/siim-acr-pneumothorax-segmentation')\n\nimport fastai\nfrom fastai.vision import *\nfrom mask_functions import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fastai.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SZ = 256\npath = Path(f'../input/pneumotorax{SZ}/data{SZ}/data{SZ}')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# copy pretrained weights for resnet34 to the folder fastai will search by default\nPath('/tmp/.cache/torch/checkpoints/').mkdir(exist_ok=True, parents=True)\n!cp '../input/resnet34/resnet34.pth' '/tmp/.cache/torch/checkpoints/resnet34-333f7ec4.pth'","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Setting div=True in open_mask\nclass SegmentationLabelList(SegmentationLabelList):\n    def open(self, fn): return open_mask(fn, div=True)\n    \nclass SegmentationItemList(SegmentationItemList):\n    _label_cls = SegmentationLabelList\n\n# Setting transformations on masks to False on test set\ndef transform(self, tfms:Optional[Tuple[TfmList,TfmList]]=(None,None), **kwargs):\n    if not tfms: tfms=(None,None)\n    assert is_listy(tfms) and len(tfms) == 2\n    self.train.transform(tfms[0], **kwargs)\n    self.valid.transform(tfms[1], **kwargs)\n    kwargs['tfm_y'] = False # Test data has no labels\n    if self.test: self.test.transform(tfms[1], **kwargs)\n    return self\nfastai.data_block.ItemLists.transform = transform","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create databunch\ndata = (SegmentationItemList.from_folder(path=path/'train')\n        .split_by_rand_pct(0.2)\n        .label_from_func(lambda x : str(x).replace('train', 'masks'), classes=[0, 1])\n        .add_test((path/'test').ls(), label=None)\n        .transform(get_transforms(), size=SZ, tfm_y=True)\n        .databunch(path=Path('.'), bs=32)\n        .normalize(imagenet_stats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display some images with masks\ndata.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create U-Net with a pretrained resnet34 as encoder\nlearn = unet_learner(data, models.resnet34, metrics=[dice])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit one cycle of 6 epochs with max lr of 1e-3\nlearn.fit_one_cycle(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unfreeze the encoder (resnet34)\nlearn.unfreeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit one cycle of 12 epochs\nlr = 1e-3\nlearn.fit_one_cycle(12, slice(lr/30, lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predictions for the validation set\npreds, ys = learn.get_preds()\npreds = preds[:,1,...]\nys = ys.squeeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def dice_overall(preds, targs):\n    n = preds.shape[0]\n    preds = preds.view(n, -1)\n    targs = targs.view(n, -1)\n    intersect = (preds * targs).sum(-1).float()\n    union = (preds+targs).sum(-1).float()\n    u0 = union==0\n    intersect[u0] = 1\n    union[u0] = 2\n    return (2. * intersect / union)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Find optimal threshold\ndices = []\nthrs = np.arange(0.01, 1, 0.01)\nfor i in progress_bar(thrs):\n    preds_m = (preds>i).long()\n    dices.append(dice_overall(preds_m, ys).mean())\ndices = np.array(dices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"best_dice = dices.max()\nbest_thr = thrs[dices.argmax()]\n\nplt.figure(figsize=(8,4))\nplt.plot(thrs, dices)\nplt.vlines(x=best_thr, ymin=dices.min(), ymax=dices.max())\nplt.text(best_thr+0.03, best_dice-0.01, f'DICE = {best_dice:.3f}', fontsize=14);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot some samples\nrows = 10\nplot_idx = ys.sum((1,2)).sort(descending=True).indices[:rows]\nfor idx in plot_idx:\n    fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(12, 4))\n    ax0.imshow(data.valid_ds[idx][0].data.numpy().transpose(1,2,0))\n    ax1.imshow(ys[idx], vmin=0, vmax=1)\n    ax2.imshow(preds[idx], vmin=0, vmax=1)\n    ax1.set_title('Targets')\n    ax2.set_title('Predictions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predictions for test set\npreds, _ = learn.get_preds(ds_type=DatasetType.Test)\npreds = (preds[:,1,...]>best_thr).long().numpy()\nprint(preds.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Generate rle encodings (images are first converted to the original size)\nrles = []\nfor p in progress_bar(preds):\n    im = PIL.Image.fromarray((p.T*255).astype(np.uint8)).resize((1024,1024))\n    im = np.asarray(im)\n    rles.append(mask2rle(im, 1024, 1024))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ids = [o.stem for o in data.test_ds.items]\nsub_df = pd.DataFrame({'ImageId': ids, 'EncodedPixels': rles})\nsub_df.loc[sub_df.EncodedPixels=='', 'EncodedPixels'] = '-1'\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}