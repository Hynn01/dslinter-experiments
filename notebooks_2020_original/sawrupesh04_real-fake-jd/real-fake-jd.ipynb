{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom wordcloud import WordCloud\n\nimport missingno as ms\n\nimport string\n\n# NLTK\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# spaCy\nimport spacy\nfrom spacy import displacy\nfrom spacy.lang.en.stop_words import STOP_WORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set Plot style\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load data\ndf = pd.read_csv('../input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv', index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print head of data\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Info of DataFrame\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above snippet shows that 13 object dtypes and 4 int64 dtypes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Statistical Description\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing value"},{"metadata":{},"cell_type":"markdown","source":"**Missingno** is a visualization tools that highlights the missing value in the entire dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"ms.matrix(df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* White line shows the missing record in the dataset.\n* It looks lost of missing value in the dataset.\n\n**Let's check out the missing value count**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"ms.bar(df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The value above bar shows the total count of non-missing value**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# counts of missing value for each feature and target\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Salary_range feature containt lot's of missing value.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop salary_range\ndel df['salary_range']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill null value\ndf.fillna(\"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Count value of flaudulent(target)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2)\n\nsns.countplot(x='fraudulent', data=df, ax=ax[0])\nax[1].pie(df['fraudulent'].value_counts(), labels=['Real Post', 'Fake Post'], autopct='%1.1f%%')\n\nfig.suptitle('Bar & Pie charts of Fraudulent value count', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Required Experience Real/Fake**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2)\n\nchart = sns.countplot(x = 'required_experience', data=df[df['fraudulent']==0], ax=ax[0])\nchart.set_xticklabels(chart.get_xticklabels(), rotation=90)\nax[0].set_title('Real required experience')\n\nchart = sns.countplot(x = 'required_experience', data=df[df['fraudulent']==1], ax=ax[1])\nchart.set_xticklabels(chart.get_xticklabels(), rotation=90)\nax[1].set_title('Fake required experience')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic Feature Extraction\n* Number of characters\n* Number of words\n* Average Word Length\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create features from text columns\ntext_features = df[[\"title\", \"company_profile\", \"description\", \"requirements\", \"benefits\",\"fraudulent\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print samples of the text_features\ntext_features.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['title', 'company_profile', 'description', 'requirements', 'benefits']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Number of characters"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in columns:\n    text_features[col+'_len'] = text_features[col].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real = text_features[text_features['fraudulent']==0]\nfake = text_features[text_features['fraudulent']==1]\n\nfig, ax = plt.subplots(5, 2, figsize=(15, 15))\nax[0, 0].set_title('Character Count of Real Post ')\nax[0, 1].set_title('Character Count of Fake Post ')\n\nfor i in range(5):\n    for j in range(2):\n        if j==0:\n            ax[i, j].hist(real[columns[i]+'_len'], color='g', bins=15);\n            ax[i, j].set_ylabel( columns[i] )\n        else:\n            ax[i, j].hist(fake[columns[i]+'_len'], color='r', bins=15);\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Number of words"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in columns:\n    text_features[col+'_len_word'] = text_features[col].apply(lambda x: len(x.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real = text_features[text_features['fraudulent']==0]\nfake = text_features[text_features['fraudulent']==1]\n\nfig, ax = plt.subplots(5, 2, figsize=(15, 15))\nax[0, 0].set_title('Word Count of Real Post ')\nax[0, 1].set_title('Word Count of Fake Post ')\n\nfor i in range(5):\n    for j in range(2):\n        if j==0:\n            ax[i, j].hist(real[columns[i]+'_len_word'], color='g', bins=15);\n            ax[i, j].set_ylabel( columns[i] )\n        else:\n            ax[i, j].hist(fake[columns[i]+'_len_word'], color='r', bins=15);\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Average Word Length"},{"metadata":{"trusted":true},"cell_type":"code","source":"def avg_word_ln(string):\n    words = string.split()\n    word_len = [len(word) for word in words]\n    try:\n        return sum(word_len)/len(words)\n    except:\n        return 0\n\nfor col in columns:\n    text_features[col+'_avg_word_ln'] = text_features[col].apply(avg_word_ln)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real = text_features[text_features['fraudulent']==0]\nfake = text_features[text_features['fraudulent']==1]\n\nfig, ax = plt.subplots(5, 2, figsize=(15, 15))\nax[0, 0].set_title('Average Word Count of Real Post ')\nax[0, 1].set_title('Average Word Count of Fake Post ')\n\nfor i in range(5):\n    for j in range(2):\n        if j==0:\n            ax[i, j].hist(real[columns[i]+'_avg_word_ln'], color='g', bins=15);\n            ax[i, j].set_ylabel( columns[i] )\n        else:\n            ax[i, j].hist(fake[columns[i]+'_avg_word_ln'], color='r', bins=15);\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**After compare all the basic feature such as word length, character length and avg word length, fake post has less count than the real post.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete text_features\ndel text_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\n1. Converting words into lowercase\n2. Removing leading white spaces \n3. Removing punctuations & stop words\n4. Lemamtization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new feature jd (job description)\ndf['jd'] = df['title'] + ' ' + df['location'] + ' ' + df['department'] + ' ' + df['company_profile'] + ' ' + df['description'] + ' ' + df['requirements'] + ' ' + df['benefits'] + ' ' + df['employment_type'] + ' ' + df['required_education'] + ' ' + df['industry'] + ' ' + df['function'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop features\ndel df['title']\ndel df['location']\ndel df['department']\ndel df['company_profile']\ndel df['description']\ndel df['requirements']\ndel df['benefits']\ndel df['employment_type']\ndel df['required_experience']\ndel df['required_education']\ndel df['industry']\ndel df['function']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load spacy large model\nnlp = spacy.load('en_core_web_lg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Converting word into lowercase"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['jd'] = df['jd'].apply(str.lower)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['jd'].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**All characters converted into lower case.**"},{"metadata":{},"cell_type":"markdown","source":"2. Removing extra white spaces, removing Punctuation & Stop words\n\n**Tokenization**\n* It is a process of splitting a string into the consituent tokens.\n* These tokens may be sentence, words or punctutions and is specific to a percular language(In our case: English)\n\n  I'm using word_tokenize from **nltk libarary**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuation_and_stop_words(s):\n    punctuations = list(string.punctuation)\n    \n    strings = \" \".join([token for token in word_tokenize(s) if not token in punctuations+list(STOP_WORDS)])\n    return strings\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply above function to the jd feature\ndf['jd'] = df['jd'].apply(remove_punctuation_and_stop_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After removing puctuations and stopwords\ndf['jd'].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Lemmatization\n\n    * It is the process of converting a word lowercase base form or lemma.\n    * This is extremely powerful standarization\n    * Examples:\n            am, are, is --->  be\n            n't        ---->  not\n            've        ---->  have"},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(df['jd'].iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatization(s):\n    doc = nlp(s)\n    return \" \".join([token.lemma_ for token in doc])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply above function to the jd feature\ndf['jd'] = df['jd'].apply(lemmatization)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['jd'].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NER(Named-entity recognition)**\n\n1. An NER is anything that can be denoted with the proper name or a pronoun.\n2. Indentifying and classifying named entity into predefined category.\n3. Categorization include **Person**, **Organization**, **Country**, etc."},{"metadata":{},"cell_type":"markdown","source":"**Visualize named entities**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# take first record and visualize NER\ndoc = nlp(df['jd'].iloc[0])\n\ndisplacy.render(doc, style=\"ent\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SpaCy is great library for NER and NER visualization\nRepresents: \n1. **new york** is GPE(Geopolitical entity, i.e. countries, cities, states.) \n2. **mario batali** is Person \n3. **twitter** ORG(Organizatio)\n\n\nVisualize Dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"displacy.render(doc, style=\"dep\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**WordCloud**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# WordCloud Real/Fake post\n\nreal = df[df['fraudulent']==0]['jd']\nfake = df[df['fraudulent']==1]['jd']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Real WordCloud\n\nplt.figure(figsize = (20,20))\nwc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(\" \".join(real))\nplt.imshow(wc , interpolation = 'bilinear')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fake WordCloud\n\nplt.figure(figsize = (20,20))\nwc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(\" \".join(fake))\nplt.imshow(wc , interpolation = 'bilinear')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}