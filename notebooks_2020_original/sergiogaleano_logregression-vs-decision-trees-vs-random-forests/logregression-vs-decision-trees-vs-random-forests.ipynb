{"cells":[{"metadata":{},"cell_type":"markdown","source":"# In this notebook I implemented three machine learning algorithms on loan data with the purpose of predicting which lenders would pay back or not.\n"},{"metadata":{},"cell_type":"markdown","source":"## The Machine Learning Algorithms used are:\n1. Logistic Regression\n2. Decision Trees\n3. Random Forests"},{"metadata":{},"cell_type":"markdown","source":"## The purpose of this notebook will only be to run and compare the results of the three models. Therefore, I won't be creating any visualizations. Prep work will involve cleaning the data. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Cleaning the Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"loans = pd.read_csv('/kaggle/input/predicting-who-pays-back-loans/loan_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loans.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loans.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loans.isnull().sum()\n# NO MISSING VALUES\n# with no missing values, we'll turn to the columns and see if any should be dropped or edited.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# not fully paid, with a score of 1 or 0, will be our target value and the variable we want to predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's observe the one 'object' data type\nloans['purpose'].nunique()\n# With 7 unique values that may correlate with our target value, we'll turn it into a dummy variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"purpose_ = pd.get_dummies(loans['purpose'],drop_first=True)\npublic_record  = pd.get_dummies(loans['pub.rec'],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the original columns that we're replacing with dummy variables \nloans.drop(['purpose','pub.rec'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dummy decider\n# loans['pub.rec'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loans = pd.concat([loans,purpose_,public_record],axis=1)\nloans.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 2. :::Model Selection and Execution:::"},{"metadata":{},"cell_type":"markdown","source":"# 2.0 Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(loans.drop('not.fully.paid',axis=1), \n                                                    loans['not.fully.paid'], test_size=0.30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.1 Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"### Train and Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = logmodel.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the Results "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_confusion_matrix = confusion_matrix(y_test,predictions)\nlogistic_classification_report = classification_report(y_test,predictions)\n\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.2 Decision Trees"},{"metadata":{},"cell_type":"markdown","source":"### Train and Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = dtree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_confusion_matrix = confusion_matrix(y_test,predictions)\ndecision_tree_classification_report = (classification_report(y_test,predictions))\n\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.3 Random Forests "},{"metadata":{},"cell_type":"markdown","source":"### Train and Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forests_confusion_matrix = confusion_matrix(y_test,predictions)\nrandom_forests_classification_report = classification_report(y_test,predictions)\n\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's compare the results of all three models together"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(logistic_confusion_matrix) \nprint(logistic_classification_report)\n\nprint(decision_tree_confusion_matrix) \nprint(decision_tree_classification_report)\n\nprint(random_forests_confusion_matrix) \nprint(random_forests_classification_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Winner = Random Forests "},{"metadata":{},"cell_type":"markdown","source":"###  An important point to note is that our model was heavily more balanced towards positive cases. Our target class was not balanced, which would expectedly result in a model's ability to better predidct the more highly represented instance of the variable. \n\n###  Interestingly, while Random Forests appears to be the winner, decision trees accurately identified the most amount of true positives.\n\n###  While logisitc models can be more intuitive, random forests are generally better than decision trees, at the cost of having a process more opaque to the data scientist."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### That concludes our quick three-model showdown.\n\n### - Sergio A. Galeano\n\n### #fortheloveofdatascience"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}