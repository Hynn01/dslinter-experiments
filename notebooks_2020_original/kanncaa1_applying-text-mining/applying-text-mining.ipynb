{"cells":[{"metadata":{"_uuid":"a1f8bee8ea2570bec4dab351d9c08c0398d89683"},"cell_type":"markdown","source":"# Introduction\n* In this kernel, we will learn text mining together step by step. \n* **Lets start with why we need to learn text mining?**\n    * Text is everywhere like books, facebook or twitter.\n    * Text data is growing each passing day. As I read on the internet, amount of text data will be approximately 40 zettabytes(10^21) in 2 years. \n    * We learn text mining to sentiment analysis, topic modelling, understanding, identfying, finding, classfying and extracting information.\n<br>Content:\n1. [Basic Text Mining Methods](#0)\n     * [len()](#1)\n     * [split()](#1)\n     * [istitle()](#2)\n     * [endswith()](#2)\n     * [startswith()](#2)\n     * [set()](#3)\n     * [isupper()](#4)\n     * [islower()](#4)\n     * [isdigit()](#4)\n     * [strip()](#4)\n     * [find()](#4)\n     * [rfind()](#4)\n     * [replace()](#4)\n     * [list()](#4)\n     * [readline()](#5)\n     * [read()](#5)\n     * [splitlines()](#5)\n     * [contains()](#5)\n     * [Regular Expression Package re](#6)\n     * [search()](#6)\n     * [findall()](#7)\n1. [Natural Language Process](#8)\n    * [import nltk](#8)\n    * [FreqDist](#8)\n    * [Normalization and Stemming words](#9)\n    * [Lemmatization](#10)\n    * [Tokenization](#11)\n1. [Text Classification](#12)    \n1. [Continue](#13)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"230fd1e4857bd46e4e99054cb02bed9a7193ecb8"},"cell_type":"markdown","source":"<a id=\"0\"></a> <br>\n## Basic Text Mining Methods\n * Text can be sentences, strings, words, characters and large documents\n * Now lets create a sentence to understand basics of text mining methods.\n * Our sentece is \"no woman no cry\" from Bob Marley."},{"metadata":{"_uuid":"274b69a23a2d274c7b45212f37fb952633fd0be2"},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>"},{"metadata":{"trusted":true,"_uuid":"4aaf5f3cdfe386125fabfb0abb58bd0b0ff0413e"},"cell_type":"code","source":"# lets create a text\ntext = \"No woman no cry\"\n\n# length of text ( includes spaces)\nprint(\"length of text: \",len(text))\n\n# split the text\nsplitted_text = text.split() # default split methods splits text according to spaces\nprint(\"Splitted text: \",splitted_text)    # splitted_text is a list that includes words of text sentence\n# each word is called token in text maning world.\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0410821d3cddccc7eb4a6d2e59606130e50f6088"},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>"},{"metadata":{"trusted":true,"_uuid":"97d799c7dacd503ff314ef66861d985b1db194e8"},"cell_type":"code","source":"# find specific words with list comprehension method\nspecific_words = [word for word in splitted_text if(len(word)>2)]\nprint(\"Words which are more than 3 letter: \",specific_words)\n\n# capitalized words with istitle() method that finds capitalized words\ncapital_words = [ word for word in splitted_text if word.istitle()]\nprint(\"Capitalized words: \",capital_words)\n\n# words which end with \"o\": endswith() method finds last letter of word\nwords_end_with_o =  [word for word in splitted_text if word.endswith(\"o\")]\nprint(\"words end with o: \",words_end_with_o) \n\n# words which starts with \"w\": startswith() method\nwords_start_with_w = [word for word in splitted_text if word.startswith(\"w\")]\nprint(\"words start with w: \",words_start_with_w) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c7349dcd176633090a431e7a3f760e547f66dc9"},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>"},{"metadata":{"trusted":true,"_uuid":"5de3084612dc757b017147d1e73868f552585451"},"cell_type":"code","source":"# unique with set() method\nprint(\"unique words: \",set(splitted_text))  # actually the word \"no\" is occured twice bc one word is \"no\" and others \"No\" there is a capital letter at first letter\n\n# make all letters lowercase with lower() method\nlowercase_text = [word.lower() for word in splitted_text]\n\n# then find uniques again with set() method\nprint(\"unique words: \",set(lowercase_text))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"277d130e66c1e0ff842c8bdaa59f9510b8a7b05a"},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>"},{"metadata":{"trusted":true,"_uuid":"b78ee7bd5103514ca8714cdb5df92e9960064a87"},"cell_type":"code","source":"# chech words includes or not includes particular substring or letter\nprint(\"Is w letter in woman word:\", \"w\" in \"woman\")\n\n# check words are upper case or lower case\nprint(\"Is word uppercase:\", \"WOMAN\".isupper())\nprint(\"Is word lowercase:\", \"cry\".islower())\n\n# check words are made of by digits or not\nprint(\"Is word made of by digits: \",\"12345\".isdigit())\n\n# get rid of from white space characters like spaces and tabs or from unwanted letters with strip() method\nprint(\"00000000No cry: \",\"00000000No cry\".strip(\"0\"))\n\n# find particular letter from front \nprint(\"Find particular letter from back: \",\"No cry no\".find(\"o\"))  # at index 1\n\n# find particular letter from back  rfind = reverse find\nprint(\"Find particular letter from back: \",\"No cry no\".rfind(\"o\"))  # at index 8\n\n# replace letter with letter\nprint(\"Replace o with 3 \", \"No cry no\".replace(\"o\",\"3\"))\n\n# find each letter and store them in list\nprint(\"Each letter: \",list(\"No cry\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1352213c2e932e92c6e53b37809d925210aecefa","scrolled":false},"cell_type":"code","source":"# Cleaning text\ntext1 = \"    Be fair and tolerant    \"\nprint(\"Split text: \",text1.split(\" \"))   # as you can see there are unnecessary white space in list\n\n# get rid of from these unnecassary white spaces with strip() method then split\nprint(\"Cleaned text: \",text1.strip().split(\" \"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"373c53e00457b0929e12d550ef1f680b62c4d2ff"},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>"},{"metadata":{"trusted":true,"_uuid":"3d857c1704e668528b980ef65b70d532f958ed0c"},"cell_type":"code","source":"# reading files line by line\nf = open(\"../input/religious-and-philosophical-texts/35895-0.txt\",\"r\")\n\n# read first line\nprint(f.readline())\n\n# length of text\ntext3=f.read()\nprint(\"Length of text: \",len(text3))\n\n# Number of lines with splitlines() method\nlines = text3.splitlines()\nprint(\"Number of lines: \",len(lines))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9120ded28f063e7d164909a8a8c8af4802c8353d"},"cell_type":"code","source":"# read data\ndata = pd.read_csv(r\"../input/ben-hamners-tweets/benhamner.csv\", encoding='latin-1')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59c2c50ae9bc0e78831a69efcd4be876109fbe45"},"cell_type":"code","source":"# find which entries contain the word 'appointment'\nprint(\"In his tweets, the rate of occuring kaggle word is: \",sum(data.text.str.contains('kaggle'))/len(data))\n# text\ntext = data.text[1]\nprint(text)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa0e587f8148155c401577e804c3a85f76f5e18c"},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>"},{"metadata":{"trusted":true,"_uuid":"9e7ca63da940c23e9d4098f4d8a9a489c77e0651"},"cell_type":"code","source":"# find regular expression on text\n# import regular expression package\nimport re\n# find callouts that starts with @\ncallouts = [word for word in text.split(\" \") if re.search(\"@[A-Za-z0-9_]+\",word)]\nprint(\"callouts: \",callouts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b76990b47417e0b8ed2687849763518d54131855"},"cell_type":"markdown","source":"* Lets look at this @[A-Za-z0-9_]+ expression more detailed\n    * @: we say that our searched word start with @\n    * [A-Za-z0-9_]: @ is followed by upper or lower case letters, digits or underscore\n    * +: there can be more than one @. So with \"+\" sign we say that the words which start with @ can be occured more than one times."},{"metadata":{"trusted":true,"_uuid":"6a9bed92566f8f89a528d6dae7fa868ce96029e4"},"cell_type":"code","source":"# continue finding regular expressions\n# [A-Za-z0-9_] =\\w\n# We will use \"\\w\" to find callouts and our result will be same because \\w matches with [A-Za-z0-9_] \ncallouts1 = [word for word in text.split(\" \") if re.search(\"@\\w+\",word)]\nprint(\"callouts: \",callouts1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"264778d4ff52535ef72c9e15e93c3810fb7d387a"},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>"},{"metadata":{"trusted":true,"_uuid":"76085e59be9195f2b74b5969ee38719bee33859e"},"cell_type":"code","source":"# find specific characters like \"w\"\nprint(re.findall(r\"[w]\",text))\n# \"w\"ith, \"w\"indo\"w\", sho\"w\"ing, s\"w\"itches \n\n# do not find specific character like \"w\". We will use \"^\" symbol\nprint(re.findall(r\"[^w]\",text))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b4d0b3090c0c508fd9896f1b7040c135236a204"},"cell_type":"markdown","source":"* Lets look at regular expressions for date\n* We will use \"\\d{1,2}[/-]\\d{1,2}[/-]\\d{1,4}\" expression to find dates\n    * d{1,2}: first number can be 1 or 2 digit\n    * [/-]: between digits there can be \"/\" or \"-\" symbols\n    * d{1,4}: last number can be between 1 and 4"},{"metadata":{"trusted":true,"_uuid":"53dc78ef29b7f087ed73fa18150a568e1e678860"},"cell_type":"code","source":"# Regular expressions for Dates\ndate = \"15-10-2000\\n09/10/2005\\n15-05-1999\\n05/05/99\\n\\n05/05/199\\n\\n05/05/9\"\nre.findall(r\"\\d{1,2}[/-]\\d{1,2}[/-]\\d{1,4}\",date)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e75a144ddd3ec6596c2f52a89bf7702df302feb"},"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n## Natural Language Process (NLP)\n* Natural language is any language that is used by people in everyday like English or Spanish\n* Natural language processing is that any computation and manipulation of natural language to get inside about how words mean and how sentences are contructed is natural language processing.\n* Natural languages are in change like new words tweets\n* Natural language process tasks are counting words, finding unique words and sentence boundaries, identify semantic rules and entities in a sentence\n* We will use natural language tool kit that is open source library in python.\n    * It supports most of the NLP tasks\n"},{"metadata":{"trusted":true,"_uuid":"de1458d10de38b7af7ded327d61be9225ddd56d3"},"cell_type":"code","source":"# import natural language tool kit\nimport nltk as nlp\n\n# counting vocabulary of words\ntext = data.text[1]\nsplitted = text.split(\" \")\nprint(\"number of words: \",len(splitted))\n\n# counting unique vocabulary of words\ntext = data.text[1]\nprint(\"number of unique words: \",len(set(splitted)))\n\n# print first five unique words\nprint(\"first 5 unique words: \",list(set(splitted))[:5])\n\n# frequency of words \ndist = nlp.FreqDist(splitted)\nprint(\"frequency of words: \",dist)\n\n# look at keys in dist\nprint(\"words in text: \",dist.keys())\n\n# count how many time a particalar value occurs. Lets look at \"box\"\nprint(\"the word box is occured how many times:\",dist[\"box\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d033fe87cec8adb5d7b77b3c438000aae72163d1"},"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n## Normalization and Stemming words\n* Normalization is different forms of the same word like have and having\n* Stemming is finding a root of the words like having => have\n"},{"metadata":{"trusted":true,"_uuid":"fc58b5041d135fafc5a2bff10e3cd72e4d84c895"},"cell_type":"code","source":"# normalization\nwords = \"task Tasked tasks tasking\"\nwords_list = words.lower().split(\" \")\nprint(\"normalized words: \",words_list)\n\n# stemming\nporter_stemmer = nlp.PorterStemmer()\nroots = [porter_stemmer.stem(each) for each in words_list]\nprint(\"roots of task Tasked tasks tasking: \",roots)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ff8bbe08d0b42ca72cb1e47e626527bad07d7d7a"},"cell_type":"markdown","source":"<a id=\"10\"></a> <br>\n## Lemmatization\n* It is also stemming but resulting stems are all valid words"},{"metadata":{"trusted":true,"_uuid":"902265ac9ba0a6d1ef27dfed933eb46ca8f5434c"},"cell_type":"code","source":"# stemming\nstemming_word_list = [\"Universal\",\"recognition\",\"Become\",\"being\",\"happened\"]\nporter_stemmer = nlp.PorterStemmer()\nroots = [porter_stemmer.stem(each) for each in stemming_word_list]\nprint(\"result of stemming: \",roots)\n\n# lemmatization\nlemma = nlp.WordNetLemmatizer()\nlemma_roots = [lemma.lemmatize(each) for each in stemming_word_list]\nprint(\"result of lemmatization: \",lemma_roots)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac7a394d97e0fa295e052733fd77608db20d3d41"},"cell_type":"markdown","source":"<a id=\"11\"></a> <br>\n## Tokenization\n* Splitting a sentece into words(tokes)\n* Learn tokenize with nltk"},{"metadata":{"trusted":true,"_uuid":"29582aada7b17bba4f23ab76641b7252e8b95368"},"cell_type":"code","source":"text_t = \"You’re in the right place!\"\nprint(\"split the sentece: \", text_t.split(\" \"))  # 5 words\n\n# tokenization with nltk\nprint(\"tokenize with nltk: \",nlp.word_tokenize(text_t))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35df7948a79bf3b774e2f8dcd61b209a38540bfe"},"cell_type":"markdown","source":"<a id=\"12\"></a> <br>\n# Text Classification\n*  Classify male and femala according to their tweets(description)\n* import twitter data set from \"Twitter User Gender Classification\""},{"metadata":{"_uuid":"33ffff53a5a166b92fc43f8252aaee1bd9074ce4","trusted":true},"cell_type":"code","source":"# %% import data\ndata = pd.read_csv(r\"../input/twitter-user-gender-classification/gender-classifier-DFE-791531.csv\",encoding='latin1')\n\n# concat gender and description\ndata = pd.concat([data.gender,data.description],axis=1)\n\n# drop nan values\ndata.dropna(inplace=True,axis=0)\n\n# convert genders from female and male to 1 and 0 respectively\ndata.gender = [1 if each == \"female\" else 0 for each in data.gender] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47eb761ea96d61cf38372073cbf57cc38bb839c5"},"cell_type":"code","source":"# import re # regular expression library\n# # %% remove non important word a, the, that, and, in \n# import nltk as nlp\n# import nltk\n# nltk.download(\"stopwords\")  # stopwords = (irrelavent words)\n# from nltk.corpus import stopwords \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"144258d2fc56c53d68a76b10042572063c82b521"},"cell_type":"code","source":"# %% creating bag of words model\nfrom sklearn.feature_extraction.text import CountVectorizer  # for bag of words \nmax_features = 150 # max_features dimension reduction \ncount_vectorizer = CountVectorizer(stop_words = \"english\",max_features = max_features)  \n# stop_words parameter = automatically remove all stopwords \n# lowercase parameter \n# token_pattern removing other karakters like .. !\n\nsparce_matrix = count_vectorizer.fit_transform(review_list).toarray() # sparce matrix yaratir bag of words model = sparce matrix\n\nprint(\"Most used {} words: {}\".format(max_features,count_vectorizer.get_feature_names()))\n\ny = data.iloc[:,0].values  # positive or negative comment\n\n#sparce matrix includes independent variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"627fd0ca8b9c9ebf9966d1bebf307243ca3b92e1"},"cell_type":"code","source":"# train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(sparce_matrix,y,test_size = 0.1,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d30dc0c6970ff75b329ad12ca6cd117a513e9e1a"},"cell_type":"markdown","source":"* As a classifier we use naive bayes."},{"metadata":{"trusted":true,"_uuid":"636af2788fbbf7323fa5311607fcb5fcd5c6a9d3"},"cell_type":"code","source":"# %% naive bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(sparce_matrix,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"610196f8904d8e86540388836a38203d68fe7cf7"},"cell_type":"code","source":"# %% predict\ny_pred = nb.predict(sparce_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a19eecab94635b192fece6c996d101b1906ccde5"},"cell_type":"code","source":"#%%\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y,y_pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4291363543ad57bab64708e1fe7460fd00f34dd9"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nf,ax = plt.subplots(figsize=(5, 5))\nsns.heatmap(cm, annot=True, linewidths=0.5,linecolor=\"red\", fmt= '.1f',ax=ax)\nplt.show()\nplt.savefig('graph.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"475fde7c772a3ba4466de2767340b1da9488e04c"},"cell_type":"markdown","source":"* The result is not good but you got the idea."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"<a id=\"13\"></a> <br>\n# Conclusion\n* If you have any question you can free to ask."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}