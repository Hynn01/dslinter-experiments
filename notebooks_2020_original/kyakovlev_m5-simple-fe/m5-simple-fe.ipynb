{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, time, warnings, pickle, psutil, random\n\nfrom math import ceil\n\nfrom sklearn.preprocessing import LabelEncoder\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Simple \"Memory profilers\" to see memory usage\ndef get_memory_usage():\n    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n        \ndef sizeof_fmt(num, suffix='B'):\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Memory Reducer\n# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n# :verbose                                        # type: bool\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                       df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Merging by concat to not lose dtypes\ndef merge_by_concat(df1, df2, merge_on):\n    merged_gf = df1[merge_on]\n    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n    return df1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"########################### Vars\n#################################################################################\nTARGET = 'sales'         # Our main target\nEND_TRAIN = 1913         # Last day in train set\nMAIN_INDEX = ['id','d']  # We can identify item by these columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Load Data\n#################################################################################\nprint('Load Main Data')\n\n# Here are reafing all our data \n# without any limitations and dtype modification\ntrain_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\nprices_df = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\ncalendar_df = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Make Grid\n#################################################################################\nprint('Create Grid')\n\n# We can tranform horizontal representation \n# to vertical \"view\"\n# Our \"index\" will be 'id','item_id','dept_id','cat_id','store_id','state_id'\n# and labels are 'd_' coulmns\n\nindex_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\ngrid_df = pd.melt(train_df, \n                  id_vars = index_columns, \n                  var_name = 'd', \n                  value_name = TARGET)\n\n# If we look on train_df we se that \n# we don't have a lot of traning rows\n# but each day can provide more train data\nprint('Train rows:', len(train_df), len(grid_df))\n\n# To be able to make predictions\n# we need to add \"test set\" to our grid\nadd_grid = pd.DataFrame()\nfor i in range(1,29):\n    temp_df = train_df[index_columns]\n    temp_df = temp_df.drop_duplicates()\n    temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n    temp_df[TARGET] = np.nan\n    add_grid = pd.concat([add_grid,temp_df])\n\ngrid_df = pd.concat([grid_df,add_grid])\ngrid_df = grid_df.reset_index(drop=True)\n\n# Remove some temoprary DFs\ndel temp_df, add_grid\n\n# We will not need original train_df\n# anymore and can remove it\ndel train_df\n\n# You don't have to use df = df construction\n# you can use inplace=True instead.\n# like this\n# grid_df.reset_index(drop=True, inplace=True)\n\n# Let's check our memory usage\nprint(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n\n# We can free some memory \n# by converting \"strings\" to categorical\n# it will not affect merging and \n# we will not lose any valuable data\nfor col in index_columns:\n    grid_df[col] = grid_df[col].astype('category')\n\n# Let's check again memory usage\nprint(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Product Release date\n#################################################################################\nprint('Release week')\n\n# It seems that leadings zero values\n# in each train_df item row\n# are not real 0 sales but mean\n# absence for the item in the store\n# we can safe some memory by removing\n# such zeros\n\n# Prices are set by week\n# so it we will have not very accurate release week \nrelease_df = prices_df.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\nrelease_df.columns = ['store_id','item_id','release']\n\n# Now we can merge release_df\ngrid_df = merge_by_concat(grid_df, release_df, ['store_id','item_id'])\ndel release_df\n\n# We want to remove some \"zeros\" rows\n# from grid_df \n# to do it we need wm_yr_wk column\n# let's merge partly calendar_df to have it\ngrid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk','d']], ['d'])\n                      \n# Now we can cutoff some rows \n# and safe memory \ngrid_df = grid_df[grid_df['wm_yr_wk']>=grid_df['release']]\ngrid_df = grid_df.reset_index(drop=True)\n\n# Let's check our memory usage\nprint(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n\n# Should we keep release week \n# as one of the features?\n# Only good CV can give the answer.\n# Let's minify the release values.\n# Min transformation will not help here \n# as int16 -> Integer (-32768 to 32767)\n# and our grid_df['release'].max() serves for int16\n# but we have have an idea how to transform \n# other columns in case we will need it\ngrid_df['release'] = grid_df['release'] - grid_df['release'].min()\ngrid_df['release'] = grid_df['release'].astype(np.int16)\n\n# Let's check again memory usage\nprint(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Save part 1\n#################################################################################\nprint('Save Part 1')\n\n# We have our BASE grid ready\n# and can save it as pickle file\n# for future use (model training)\ngrid_df.to_pickle('grid_part_1.pkl')\n\nprint('Size:', grid_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Prices\n#################################################################################\nprint('Prices')\n\n# We can do some basic aggregations\nprices_df['price_max'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('max')\nprices_df['price_min'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('min')\nprices_df['price_std'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('std')\nprices_df['price_mean'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n\n# and do price normalization (min/max scaling)\nprices_df['price_norm'] = prices_df['sell_price']/prices_df['price_max']\n\n# Some items are can be inflation dependent\n# and some items are very \"stable\"\nprices_df['price_nunique'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\nprices_df['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n\n# I would like some \"rolling\" aggregations\n# but would like months and years as \"window\"\ncalendar_prices = calendar_df[['wm_yr_wk','month','year']]\ncalendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\nprices_df = prices_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\ndel calendar_prices\n\n# Now we can add price \"momentum\" (some sort of)\n# Shifted by week \n# by month mean\n# by year mean\nprices_df['price_momentum'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\nprices_df['price_momentum_m'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\nprices_df['price_momentum_y'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n\ndel prices_df['month'], prices_df['year']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Merge prices and save part 2\n#################################################################################\nprint('Merge prices and save part 2')\n\n# Merge Prices\noriginal_columns = list(grid_df)\ngrid_df = grid_df.merge(prices_df, on=['store_id','item_id','wm_yr_wk'], how='left')\nkeep_columns = [col for col in list(grid_df) if col not in original_columns]\ngrid_df = grid_df[MAIN_INDEX+keep_columns]\ngrid_df = reduce_mem_usage(grid_df)\n\n# Safe part 2\ngrid_df.to_pickle('grid_part_2.pkl')\nprint('Size:', grid_df.shape)\n\n# We don't need prices_df anymore\ndel prices_df\n\n# We can remove new columns\n# or just load part_1\ngrid_df = pd.read_pickle('grid_part_1.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Merge calendar\n#################################################################################\ngrid_df = grid_df[MAIN_INDEX]\n\n# Merge calendar partly\nicols = ['date',\n         'd',\n         'event_name_1',\n         'event_type_1',\n         'event_name_2',\n         'event_type_2',\n         'snap_CA',\n         'snap_TX',\n         'snap_WI']\n\ngrid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n\n# Minify data\n# 'snap_' columns we can convert to bool or int8\nicols = ['event_name_1',\n         'event_type_1',\n         'event_name_2',\n         'event_type_2',\n         'snap_CA',\n         'snap_TX',\n         'snap_WI']\nfor col in icols:\n    grid_df[col] = grid_df[col].astype('category')\n\n# Convert to DateTime\ngrid_df['date'] = pd.to_datetime(grid_df['date'])\n\n# Make some features from date\ngrid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\ngrid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8)\ngrid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\ngrid_df['tm_y'] = grid_df['date'].dt.year\ngrid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\ngrid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x/7)).astype(np.int8)\n\ngrid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\ngrid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8)\n\n# Remove date\ndel grid_df['date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Save part 3 (Dates)\n#################################################################################\nprint('Save part 3')\n\n# Safe part 3\ngrid_df.to_pickle('grid_part_3.pkl')\nprint('Size:', grid_df.shape)\n\n# We don't need calendar_df anymore\ndel calendar_df\ndel grid_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Some additional cleaning\n#################################################################################\n\n## Part 1\n# Convert 'd' to int\ngrid_df = pd.read_pickle('grid_part_1.pkl')\ngrid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n\n# Remove 'wm_yr_wk'\n# as test values are not in train set\ndel grid_df['wm_yr_wk']\ngrid_df.to_pickle('grid_part_1.pkl')\n\ndel grid_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Summary\n#################################################################################\n\n# Now we have 3 sets of features\ngrid_df = pd.concat([pd.read_pickle('grid_part_1.pkl'),\n                     pd.read_pickle('grid_part_2.pkl').iloc[:,2:],\n                     pd.read_pickle('grid_part_3.pkl').iloc[:,2:]],\n                     axis=1)\n                     \n# Let's check again memory usage\nprint(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\nprint('Size:', grid_df.shape)\n\n# 2.5GiB + is is still too big to train our model\n# (on kaggle with its memory limits)\n# and we don't have lag features yet\n# But what if we can train by state_id or shop_id?\nstate_id = 'CA'\ngrid_df = grid_df[grid_df['state_id']==state_id]\nprint(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n#           Full Grid:   1.2GiB\n\nstore_id = 'CA_1'\ngrid_df = grid_df[grid_df['store_id']==store_id]\nprint(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n#           Full Grid: 321.2MiB\n\n# Seems its good enough now\n# In other kernel we will talk about LAGS features\n# Thank you.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Final list of features\n#################################################################################\ngrid_df.info()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}