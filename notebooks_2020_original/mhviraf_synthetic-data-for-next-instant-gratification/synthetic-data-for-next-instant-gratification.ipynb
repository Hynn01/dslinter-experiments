{"cells":[{"metadata":{},"cell_type":"markdown","source":"As of now, despite that only a few days have passed since this competition started, we know a lot about the structure of dataset. We know about ID's, splits, etc. etc.\nI tried to put everything we knew so far together and generate a synthetic `train.csv` dataset from scratch as similar as possible to Kaggle's `train.csv`.\nI then ran some EDA on it and also trained some of the public kernels on this synthetic dataset and compared the CV score to theirs. \n\nHope you enjoy this kernel. Don't forget to upvote and share your opinions.\n\n\n\nFirst, we generate the dataset from 512 different classification datasets by using sklearn's `make_calssification`. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from sklearn.datasets import make_classification \nimport numpy as np\nimport pandas as pd\nnp.random.seed(2019)\n\n# generate dataset \ntrain, target = make_classification(512, 255, n_informative=np.random.randint(33, 47), n_redundant=0, flip_y=0.08)\ntrain = np.hstack((train, np.ones((len(train), 1))*0))\n\nfor i in range(1, 512):\n    X, y = make_classification(512, 255, n_informative=np.random.randint(33, 47), n_redundant=0, flip_y=0.08)\n    X = np.hstack((X, np.ones((len(X), 1))*i))\n    train = np.vstack((train, X))\n    target = np.concatenate((target, y))\n    ","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Silly column names abound;\n\nlet's add some names to our features. It must be silly and cryptic so let's have this format of `kind-color-animal-goal`. \n\nNote that if you want it to look more cryptic you can rename some of them and replace `goal` part with `animal` part or `animal` part with `color` part so that the competitors spend some time on that too. For example make something like `slimy-seashell-cassowary-goose` (this column actually exists in Kaggle's train.csv).\n\nYou can also add some probabilities in `np.random.choice()` to make them look as such that there is pattern."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"col_names = []\nkind_arr = ['muggy', 'dorky', 'slimy', 'snazzy', 'frumpy', 'stealthy', 'chummy', 'hazy', 'nerdy', 'leaky', 'ugly', 'shaggy', 'flaky','squirrely', 'freaky', 'lousy', 'bluesy', 'baggy', 'greasy',\n       'cranky', 'snippy', 'flabby', 'goopy', 'homey', 'homely', 'hasty','blurry', 'snoopy', 'stinky', 'bumpy', 'slaphappy', 'messy','geeky', 'crabby', 'beady', 'pasty', 'snappy', 'breezy', 'sunny',\n       'cheeky', 'wiggy', 'flimsy', 'lanky', 'scanty', 'grumpy', 'chewy','crappy', 'clammy', 'tasty', 'thirsty', 'gloppy', 'gamy', 'hilly','woozy', 'squeaky', 'lovely', 'paltry', 'smelly', 'pokey','skanky', 'zippy', 'sleazy', 'queasy', 'foggy', 'wheezy', 'droopy',\n       'cozy', 'skinny', 'seedy', 'stuffy', 'jumpy', 'trippy', 'woolly','gimpy', 'randy', 'silly', 'craggy', 'skimpy', 'nippy', 'whiny','boozy', 'pretty', 'sickly', 'shabby', 'surly']\ncolor_arr = ['smalt', 'peach', 'seashell', 'harlequin', 'beige', 'cream','emerald', 'indigo', 'amaranth', 'tangerine', 'silver','chocolate', 'tan', 'plum', 'rose', 'copper', 'scarlet','cinnamon', 'cardinal', 'auburn', 'sepia', 'brass', 'eggplant',\n       'ruby', 'blue', 'wisteria', 'maroon', 'tomato', 'mauve', 'pumpkin','teal', 'goldenrod', 'aquamarine', 'gamboge', 'persimmon','mustard', 'red', 'magnolia', 'chestnut', 'champagne', 'flax',\n       'viridian', 'amber', 'zucchini', 'myrtle', 'lemon', 'pear',\n       'xanthic', 'turquoise', 'lilac', 'amethyst', 'lime', 'pink',\n       'periwinkle', 'crimson', 'burgundy', 'purple', 'rust', 'cerise',\n       'khaki', 'malachite', 'violet', 'sangria', 'magenta', 'russet',\n       'apricot', 'cobalt', 'platinum', 'denim', 'yellow', 'sapphire',\n       'bronze', 'green', 'thistle', 'buff', 'razzmatazz', 'charcoal',\n       'ultramarine', 'puce', 'carmine', 'gold', 'asparagus', 'ivory',\n       'orange', 'vermilion', 'chartreuse', 'heliotrope', 'azure', 'grey',\n       'jade', 'olive', 'coral', 'brown', 'cinnabar', 'lavender', 'aqua',\n       'firebrick', 'corn', 'bistre', 'cyan', 'ochre', 'dandelion',\n       'white']\nanimal_arr = ['axolotl', 'sheepdog', 'cassowary', 'chicken', 'mau', 'pinscher',\n       'tarantula', 'cuttlefish', 'wolfhound', 'lizard', 'chihuahua',\n       'indri', 'beetle', 'sheep', 'angelfish', 'penguin', 'wallaby',\n       'oriole', 'hound', 'bonobo', 'dogfish', 'vole', 'coral', 'fowl',\n       'bombay', 'bulldog', 'oyster', 'blue', 'armadillo', 'ragdoll',\n       'wolverine', 'moorhen', 'otter', 'bat', 'affenpinscher', 'rat',\n       'caterpillar', 'newt', 'collie', 'weasel', 'guppy', 'bullfrog',\n       'alligator', 'sloth', 'moth', 'kudu', 'wasp', 'okapi', 'quoll',\n       'shrew', 'walrus', 'schnauzer', 'termite', 'dragonfly', 'kakapo',\n       'quetzal', 'capuchin', 'eel', 'iguana', 'zonkey', 'fousek',\n       'javanese', 'leopard', 'gorilla', 'malamute', 'birman', 'donkey',\n       'lionfish', 'llama', 'emu', 'koala', 'saola', 'neanderthal',\n       'horse', 'mammoth', 'duck', 'peccary', 'hippopotamus',\n       'grasshopper', 'dolphin', 'gharial', 'frog', 'ostrich', 'akbash',\n       'bison', 'hyrax', 'capybara', 'earwig', 'cuscus', 'chinook',\n       'jackal', 'hornet', 'monkey', 'bordeaux', 'reindeer', 'squid',\n       'maltese', 'buffalo', 'hedgehog', 'octopus', 'swan', 'husky',\n       'zebu', 'olm', 'retriever', 'lemur', 'dhole', 'rabbit', 'loon',\n       'cat', 'millipede', 'flamingo', 'opossum', 'turtle', 'eagle',\n       'eleuth', 'binturong', 'uguisu', 'whippet', 'tiger', 'lobster',\n       'macaque', 'scorpion', 'goat', 'tapir', 'audemer', 'fox', 'molly',\n       'discus', 'gopher', 'gerbil', 'civet', 'gecko', 'dog', 'squirt',\n       'insect', 'tarsier', 'whale', 'paradise', 'deer', 'urchin',\n       'serval', 'rhinoceros', 'numbat', 'frigatebird', 'catfish', 'kiwi',\n       'bee', 'seahorse', 'beagle', 'tzu', 'dodo', 'mayfly', 'impala',\n       'dachshund', 'budgerigar', 'moose', 'labradoodle', 'spider',\n       'flounder', 'woodpecker', 'bobcat', 'corgi', 'buzzard', 'clam',\n       'hamster', 'bandicoot', 'mandrill', 'lemming', 'snail', 'havanese',\n       'hyena', 'monster']\ngoal_arr = ['pembus', 'ordinal', 'goose', 'distraction', 'golden', 'entropy',\n       'unsorted', 'sorted', 'important', 'fimbus', 'grandmaster',\n       'sumble', 'noise', 'discard', 'dummy', 'fepid', 'contributor',\n       'learn', 'dataset', 'master', 'expert', 'kernel', 'hint', 'novice',\n       'gaussian']\nfor _ in range(255):\n    new_col_name = np.random.choice(kind_arr) + '-' + np.random.choice(color_arr) + '-' + np.random.choice(animal_arr) + '-' + np.random.choice(goal_arr)\n    col_names.append(new_col_name)\ncol_names.append('wheezy-copper-turtle-magic')\n\n\n# build the dataframe\ntrain = pd.DataFrame(train, columns=col_names)\ntrain['target'] = target","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add mysterious IDs. \n\nAs Yirun found in https://www.kaggle.com/c/instant-gratification/discussion/92634#latest-537452, md5 hashing seems cool."},{"metadata":{"trusted":true},"cell_type":"code","source":"import hashlib\n\ndef generate_hashed_id(inp):\n    return hashlib.md5(bytes(f'{inp}train', encoding='utf-8')).hexdigest()\n    \ntrain['id'] = train.index\ntrain['id'] = train['id'].apply(generate_hashed_id)\n\n# re-arrange columns\ncols = [c for c in train.columns if c not in ['id', 'target']]\ntrain = train[['id', 'target']+cols]","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that the dataset is ready let's take a look at it from different aspects to see if it's similar enough?\nFirst we're gonna look at the head of dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have super balanced target classes in Kaggle's train.csv, Does sklearn generates balanced target classes? i.e. do we have super balanced target classes? Yes!"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.target.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How about the histograms? Are they similar to Kaggle's train.csv? Compare the shapes of this dataset to Bojan's EDA at https://www.kaggle.com/tunguz/instant-eda"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.distplot(train[train.columns[10]])\nplt.figure()\nsns.distplot(train[train.columns[210]])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What about the correlation plot? (it was taking a long time to run, so I restricted it to only the first 50 columns) Compare the results with Allunia's EDA at https://www.kaggle.com/allunia/instant-gratification-some-eda-to-go"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corr = train.iloc[:,:50].drop([\"target\", 'id'], axis=1).corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(train_corr, vmin=-0.016, vmax=0.016, cmap=\"RdYlBu_r\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Last but not least, let's filter `train.csv` by the magical feature `wheezy-copper-turtle-magic==0` and see what happens."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = [x for x in train.columns if x not in [\"target\", 'id', 'wheezy-copper-turtle-magic']]\ntrain2 = train.loc[train['wheezy-copper-turtle-magic']==0, columns]\nplt.figure(figsize=(6,6))\nplt.plot(train2.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oh cool! we have those useful/useless columns in here too. Surprisingly enough, useless cols have std about 1.0 and useful columns have an STD of about 3.7. Thanks to Chris for his discussion https://www.kaggle.com/c/instant-gratification/discussion/92930#latest-538458\n\nwhat about the distributions now? do they look normal after filtering?"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train2[train.columns[10]])\nplt.figure()\nsns.distplot(train2[train.columns[210]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes they do. "},{"metadata":{},"cell_type":"markdown","source":"EDA shows this dataset is very similar to competition's data. \nNow it's time to build some models on top of this dataset;\n\nBelow is the code from Chris' kernel https://www.kaggle.com/cdeotte/logistic-regression-0-800 which has CV score of 0.52994. \n\nLet's train it on this dataset and compare the CV score."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd, os\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\ncols = [c for c in train.columns if c not in ['id', 'target']]\noof = np.zeros(len(train))\nskf = StratifiedKFold(n_splits=5, random_state=42)\n   \nfor train_index, test_index in skf.split(train.iloc[:,1:-1], train['target']):\n    clf = LogisticRegression(solver='liblinear',penalty='l2',C=1.0)\n    clf.fit(train.loc[train_index][cols],train.loc[train_index]['target'])\n    oof[test_index] = clf.predict_proba(train.loc[test_index][cols])[:,1]\n    \nauc = roc_auc_score(train['target'],oof)\nprint('LR without interactions scores CV =',round(auc,5))","execution_count":15,"outputs":[{"output_type":"stream","text":"LR without interactions scores CV = 0.50213\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Below is the code from Chris' kernel https://www.kaggle.com/cdeotte/logistic-regression-0-800 which has CV score of 0.80549.\n\nLet's train it on this dataset and compare the CV score."},{"metadata":{"trusted":true},"cell_type":"code","source":"# INITIALIZE VARIABLES\ncols.remove('wheezy-copper-turtle-magic')\ninteractions = np.zeros((512,255))\noof = np.zeros(len(train))\n\n# BUILD 512 SEPARATE MODELS\nfor i in range(512):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    skf = StratifiedKFold(n_splits=25, random_state=42)\n    for train_index, test_index in skf.split(train2.iloc[:,1:-1], train2['target']):\n        # LOGISTIC REGRESSION MODEL\n        clf = LogisticRegression(solver='liblinear',penalty='l1',C=0.05)\n        clf.fit(train2.loc[train_index][cols],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train2.loc[test_index][cols])[:,1]\n\n        \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('LR with interactions scores CV =',round(auc,5))","execution_count":5,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-8d4857055cf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# LOGISTIC REGRESSION MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1305\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1306\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    924\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{},"cell_type":"markdown","source":"Below is the code from Chris' kernel https://www.kaggle.com/cdeotte/support-vector-machine-0-925 which has CV score of 0.9262.\n\nLet's train it on this dataset and compare the CV score."},{"metadata":{"trusted":true},"cell_type":"code","source":"# LOAD LIBRARIES\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n# INITIALIZE VARIABLES\noof = np.zeros(len(train))\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n\n# BUILD 512 SEPARATE NON-LINEAR MODELS\nfor i in range(512):\n    \n    # EXTRACT SUBSET OF DATASET WHERE WHEEZY-MAGIC EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n    train3 = sel.transform(train2[cols])\n        \n    # STRATIFIED K FOLD (Using splits=25 scores 0.002 better but is slower)\n    skf = StratifiedKFold(n_splits=11, random_state=42)\n    for train_index, test_index in skf.split(train3, train2['target']):\n        \n        # MODEL WITH SUPPORT VECTOR MACHINE\n        clf = SVC(probability=True,kernel='poly',degree=4,gamma='auto')\n        clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        \n    #if i%10==0: print(i)\n        \n# PRINT VALIDATION CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('CV score =',round(auc,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final notes\n* It is very likely that Kaggle used a similar way to generate this competition's dataset. Yet nothing's for sure and they might have tricked a few parts of it so keep searching.\n* I believe if you play with `flip_y` and `n_informative` in `make_classification` you can get closer CV scores than mine to those mentioned above.\n* I would like to express my gratitude to Chris, Bojan, and other kagglers who generously share their findings with us\n* dont forget to upvote all the kernels/discussions linked here \n* please let me know if I used parts of your work here and forgot to link"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}