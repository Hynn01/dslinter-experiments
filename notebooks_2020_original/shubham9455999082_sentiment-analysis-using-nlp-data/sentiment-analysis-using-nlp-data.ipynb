{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\n#from sentiment_utils import *\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nfrom keras.initializers import glorot_uniform\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping\nfrom nltk.corpus import stopwords\nnp.random.seed(1)\nfrom sklearn.model_selection import train_test_split\n\n#from emo_utils import *\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad9ce6fdde97ea84f88ede6d94777e4be17212db","collapsed":true},"cell_type":"code","source":"def remove_stopwords(input_text):\n    '''\n    Function to remove English stopwords from a Pandas Series.\n    \n    Parameters:\n        input_text : text to clean\n    Output:\n        cleaned Pandas Series \n    '''\n    stopwords_list = stopwords.words('english')\n    # Some words which might indicate a certain sentiment are kept via a whitelist\n    whitelist = [\"n't\", \"not\", \"no\"]\n    words = input_text.split() \n    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n    return \" \".join(clean_words)\n\ndef remove_mentions(input_text):\n    '''\n    Function to remove mentions, preceded by @, in a Pandas Series\n    \n    Parameters:\n        input_text : text to clean\n    Output:\n        cleaned Pandas Series \n    '''\n    return re.sub(r'@\\w+', '', input_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d87ec5f83dbd2b88e99009488183a25f0a3c0e42"},"cell_type":"code","source":"#Read the data\ntrain_df = pd.read_csv(\"../input/training-jan-mar-m-n/Training_Jan_mar_2019_Master.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5d53c951bc2d9110a4f30db67dda95da0645e15","collapsed":true},"cell_type":"code","source":"#check Different labels for Sentiment\nMood = train_df['Alert ID'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11f87199bbe3fd9efc432cf405f3ecd97404dc5a","collapsed":true},"cell_type":"code","source":"#index = [1,2,3]\n#plt.bar(index,Mood,color=['r','b','g'])\n#plt.xticks(index,['Negative','Neutral','Positive'])\n#plt.xlabel('Mood')\n#plt.ylabel('Mood Count')\n#plt.title('Mood Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7922bd79bb8ae24012820c369347c469ea00220d","collapsed":true},"cell_type":"code","source":"def plot_sub_sentiment(Airline):\n    pdf = train_df[train_df['airline']==Airline]\n    count = pdf['airline_sentiment'].value_counts()\n    Index = [1,2,3]\n    color = ['red','blue','green']\n    plt.bar(Index,count,width=0.5,color=color)\n    plt.xticks(Index,['Negative','Neutral','Positive'])\n    plt.title('Mood Summary of' + \" \" + Airline)\n\nairline_name = train_df['airline'].unique()\nplt.figure(1,figsize=(12,12))\nfor i in range(6):\n    plt.subplot(3,2,i+1)\n    plot_sub_sentiment(airline_name[i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8df70c80ca88edf7bbe74cbab0f554cf92efbe2c"},"cell_type":"code","source":"#cleaning Data\ntrain_df = train_df[['Summary', 'Alert ID']]\ntrain_df.text = train_df.Summary.apply(remove_mentions)\n#train_df.loc[:,'sentiment'] = train_df.airline_sentiment.map({'negative':0,'neutral':1,'positive':2})\n#train_df = train_df.drop(['airline_sentiment'], axis=1)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train_df.loc[train_df['Alert ID']>1,'Alert ID']=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train_df['Alert ID'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cd63e88796d483fbfcb5bc01895a985d64da84d","collapsed":true},"cell_type":"code","source":"#This step is to find the maximun length of the input string so as to fed the neural net with same length\nraw_docs_train = train_df[\"Summary\"].values\nsentiment_train = train_df['Alert ID'].values\n\nmaxLen = len(max(raw_docs_train, key=len).split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d9f6fadb7c4b3eb4111eefd91d1aa21a9cfaa13"},"cell_type":"code","source":"#For our model we need to split our training dataset into test dataset. This is actually dev set for getting the loss\nX_train, X_test, Y_train, Y_test = train_test_split(raw_docs_train, sentiment_train, \n                                                  stratify=sentiment_train, \n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)\nprint('# Train data samples:', X_train.shape)\nprint('# Test data samples:', X_test.shape)\nassert X_train.shape[0] == Y_train.shape[0]\nassert X_test.shape[0] == Y_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"628aab4d9e98412d84e625386ca8e29a93087b0e"},"cell_type":"code","source":"#Convert the labels to One hot encoding vector for softmax for neural network\n\n#Y_oh_train = convert_to_one_hot(Y_train, C = num_labels)\nnum_labels = len(np.unique(sentiment_train))\nY_oh_train = np_utils.to_categorical(Y_train)\nY_oh_test = np_utils.to_categorical(Y_test)\nprint(Y_oh_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_oh_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99d981e8c3cff2bf8735c84a103e7036594c8f2b","collapsed":true},"cell_type":"code","source":"#Now we need Glove Vectors for Word which is available online \n#word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('Data/glove.6B.50d.txt')\n\n# load the GloVe vectors in a dictionary:\n\ndef read_glove_vecs(glove_file):\n    with open(glove_file, encoding=\"utf8\") as f:\n        words = set()\n        word_to_vec_map = {}\n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n        \n        i = 1\n        words_to_index = {}\n        index_to_words = {}\n        for w in sorted(words):\n            words_to_index[w] = i\n            index_to_words[i] = w\n            i = i + 1\n    return words_to_index, index_to_words, word_to_vec_map\n\n\nword_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39c22d10df64e9c89fc7ad5cfe7739d688110086"},"cell_type":"code","source":"word = \"accident\"\nindex = 289846\nprint(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\nprint(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"104f5eb15b2f6dfc2e7a5b966389468b24ed8e8c","collapsed":true},"cell_type":"code","source":"# This function convert array of strings into array of Indices of word in the voacab.\n\ndef sentences_to_indices(X, word_to_index, max_len):\n    \"\"\"\n    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n    \n    Arguments:\n    X -- array of sentences (strings), of shape (m, 1)\n    word_to_index -- a dictionary containing the each word mapped to its index\n    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n    \n    Returns:\n    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n    \"\"\"\n    \n    m = X.shape[0]                                   # number of training examples\n    \n    # Initialize X_indices as a numpy matrix of zeros and the correct shape (â‰ˆ 1 line)\n    X_indices = np.zeros((m,max_len))\n    \n    for i in range(m):                               # loop over training examples\n        \n        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n        sentence_words =[word.lower().replace('\\t', '') for word in X[i].split(' ') if word.replace('\\t', '') != '']\n        \n        # Initialize j to 0\n        j = 0\n        \n        # Loop over the words of sentence_words\n        for w in sentence_words:\n            # Set the (i,j)th entry of X_indices to the index of the correct word.\n            try:\n                X_indices[i, j] = word_to_index[w]\n            except: 0\n            # Increment j to j + 1\n            j = j+1\n    \n    return X_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9df0a15f48e9bb10b7125be3c566e618db6eb7a2","collapsed":true},"cell_type":"code","source":"# Create Keras Embedding layer\ndef pretrained_embedding_layer(word_to_vec_map, word_to_index):\n    \"\"\"\n    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n    \n    Arguments:\n    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n\n    Returns:\n    embedding_layer -- pretrained layer Keras instance\n    \"\"\"\n    \n    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n    \n    ### START CODE HERE ###\n    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n    emb_matrix = np.zeros((vocab_len,emb_dim))\n    \n    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n    for word, index in word_to_index.items():\n        emb_matrix[index, :] = word_to_vec_map[word]\n\n    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)\n    ### END CODE HERE ###\n\n    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n    embedding_layer.build((None,))\n    \n    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n    embedding_layer.set_weights([emb_matrix])\n    \n    return embedding_layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9b7a82c8a291557db49cdd4cf8cdfc12ebf0549a"},"cell_type":"code","source":"\ndef ltsm_model(input_shape, word_to_vec_map, word_to_index):\n    \"\"\"\n    Function creating the ltsm_model model's graph.\n    \n    Arguments:\n    input_shape -- shape of the input, usually (max_len,)\n    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n\n    Returns:\n    model -- a model instance in Keras\n    \"\"\"\n    \n    ### START CODE HERE ###\n    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n    sentence_indices =  Input(shape=input_shape, dtype='int32')\n    \n    # Create the embedding layer pretrained with GloVe Vectors (â‰ˆ1 line)\n    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n    \n    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n    embeddings = embedding_layer(sentence_indices)   \n    \n    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n    # Be careful, the returned output should be a batch of sequences.\n    X = LSTM(128, return_sequences=True)(embeddings)\n    # Add dropout with a probability of 0.5\n    X = Dropout(0.5)(X)\n    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n    X = LSTM(128, return_sequences=False)(X)\n    # Add dropout with a probability of 0.5\n    X = Dropout(0.5)(X)\n    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n    X = Dense(2, activation=None)(X)\n    # Add a softmax activation\n    X = Activation('softmax')(X)\n    \n    # Create Model instance which converts sentence_indices into X.\n    model = Model(inputs=[sentence_indices], outputs=X)\n    \n    ### END CODE HERE ###\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cbfcb0d47cf206e920adf8865206dd74d14365f"},"cell_type":"code","source":"model = ltsm_model((maxLen,), word_to_vec_map, word_to_index)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dc66c0d3c92afdbe9a382258183377fc4cd7281d"},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b71f587e27e9d58ce1e52014fc83c4c88c75a856"},"cell_type":"code","source":"X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\nX_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\nprint(X_train_indices.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_oh_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ab27c301e845fb2e0e7bfb3445c4663701e5f33"},"cell_type":"code","source":"earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n\nmodel.fit(X_train_indices, y=Y_oh_train, batch_size=512, epochs=10, \n          verbose=1, validation_data=(X_test_indices, Y_oh_test), callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e088cbee7b111c1aa6fd099fbedfb7673d56c82"},"cell_type":"code","source":"#Check Prediction for a particular example\n# Change the sentence below to see your prediction. Make sure all the words are in the Glove embeddings.  \nx_test = np.array(['Good Airlines'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  str(np.argmax(model.predict(X_test_indices))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=model.predict(X_test_indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x[100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexes = tf.argmax(x, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nsess = tf.InteractiveSession()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=indexes.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'df':x})['df'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_oh_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexes1 = tf.argmax(Y_oh_test, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l=indexes1.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(l,x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'l':l})['l'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}