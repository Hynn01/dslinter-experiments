{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Based on https://www.kaggle.com/benhamner/d/uciml/iris/python-data-visualizations/notebook\n# First, we'll import pandas, a data processing and CSV file I/O library\nimport pandas as pd\nimport numpy as np\n\n# We'll also import seaborn, a Python graphing library\nimport warnings # current version of seaborn generates a bunch of warnings that we'll ignore\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nsns.set(style=\"white\", color_codes=True)\n\n# Next, we'll load the train and test dataset, which is in the \"../input/\" directory\ntrain = pd.read_csv(\"../input/train.csv\") # the train dataset is now a Pandas DataFrame\ntest = pd.read_csv(\"../input/test.csv\") # the train dataset is now a Pandas DataFrame\n\n# Let's see what's in the trainings data - Jupyter notebooks print the result of the last thing you do\ntrain.head()\n\n# Press shift+enter to execute this cell"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# happy customers have TARGET==0, unhappy custormers have TARGET==1\n# A little less then 4% are unhappy => unbalanced dataset\ndf = pd.DataFrame(train.TARGET.value_counts())\ndf['Percentage'] = 100*df['TARGET']/train.shape[0]\ndf"},{"cell_type":"markdown","metadata":{},"source":"# var3: nationality of the customer"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Top-10 most common values\ntrain.var3.value_counts()[:10]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# 116 values in column var3 are -999999\n# var3 is suspected to be the nationality of the customer\n# -999999 would mean that the nationality of the customer is unknown\ntrain.loc[train.var3==-999999].shape"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Replace -999999 in var3 column with most common value 2 \n# See https://www.kaggle.com/cast42/santander-customer-satisfaction/debugging-var3-999999\n# for details\ntrain = train.replace(-999999,2)\ntrain.loc[train.var3==-999999].shape"},{"cell_type":"markdown","metadata":{},"source":"# Add feature that counts the number of zeros in a row"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"X = train.iloc[:,:-1]\ny = train.TARGET\n\nX['n0'] = (X==0).sum(axis=1)\ntrain['n0'] = X['n0']"},{"cell_type":"markdown","metadata":{},"source":"# num_var4 : number of bank products"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# According to dmi3kno (see https://www.kaggle.com/cast42/santander-customer-satisfaction/exploring-features/comments#115223)\n# num_var4 is the number of products. Let's plot the distribution:\ntrain.num_var4.hist(bins=100)\nplt.xlabel('Number of bank products')\nplt.ylabel('Number of customers in train')\nplt.title('Most customers have 1 product with the bank')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Let's look at the density of the of happy/unhappy customers in function of the number of bank products\nsns.FacetGrid(train, hue=\"TARGET\", size=6) \\\n   .map(plt.hist, \"num_var4\") \\\n   .add_legend()\nplt.title('Unhappy cosutomers have less products')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train[train.TARGET==1].num_var4.hist(bins=6)\nplt.title('Amount of unhappy customers in function of the number of products');"},{"cell_type":"markdown","metadata":{},"source":"# Var38\nvar38 is important according to XGBOOST\nsee https://www.kaggle.com/cast42/santander-customer-satisfaction/xgboost-with-early-stopping/files\n\nAlso RFC thinks var38 is important\nsee https://www.kaggle.com/tks0123456789/santander-customer-satisfaction/data-exploration/notebook\n\nVar38 is suspected to be the mortage value with the bank. If the mortage is with another bank the national\naverage is used. \nSee https://www.kaggle.com/c/santander-customer-satisfaction/forums/t/19895/var38-is-mortgage-value\n\n[dmi3kno](https://www.kaggle.com/dmi3kno) says that var38 is value of the customer: [https://www.kaggle.com/cast42/santander-customer-satisfaction/exploring-features/comments#115223](https://www.kaggle.com/cast42/santander-customer-satisfaction/exploring-features/comments#115223)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train.var38.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# How is var38 looking when customer is unhappy ?\ntrain.loc[train['TARGET']==1, 'var38'].describe()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Histogram for var 38 is not normal distributed\ntrain.var38.hist(bins=1000);"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train.var38.map(np.log).hist(bins=1000);"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# where is the spike between 11 and 12  in the log plot ?\ntrain.var38.map(np.log).mode()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# What are the most common values for var38 ?\ntrain.var38.value_counts()"},{"cell_type":"markdown","metadata":{},"source":"the value 117310.979016 appears 14868 times in colum var38"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# the most common value is very close to the mean of the other values\ntrain.var38[train['var38'] != 117310.979016494].mean()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# what if we exclude the most common value\ntrain.loc[~np.isclose(train.var38, 117310.979016), 'var38'].value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Look at the distribution\ntrain.loc[~np.isclose(train.var38, 117310.979016), 'var38'].map(np.log).hist(bins=100);"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Above plot suggest we split up var38 into two variables\n# var38mc == 1 when var38 has the most common value and 0 otherwise\n# logvar38 is log transformed feature when var38mc is 0, zero otherwise\ntrain['var38mc'] = np.isclose(train.var38, 117310.979016)\ntrain['logvar38'] = train.loc[~train['var38mc'], 'var38'].map(np.log)\ntrain.loc[train['var38mc'], 'logvar38'] = 0"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Check for nan's\nprint('Number of nan in var38mc', train['var38mc'].isnull().sum())\nprint('Number of nan in logvar38',train['logvar38'].isnull().sum())"},{"cell_type":"markdown","metadata":{},"source":"# var15"},{"cell_type":"markdown","metadata":{},"source":"The most important feature for XGBoost is var15. According to [a Kaggle form post](https://www.kaggle.com/c/santander-customer-satisfaction/forums/t/19291/data-dictionary/110414#post110414)\n    var15 is the age of the customer. Let's explore var15"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train['var15'].describe()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Looks more normal, plot the histogram\ntrain['var15'].hist(bins=100);"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Let's look at the density of the age of happy/unhappy customers\nsns.FacetGrid(train, hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"var15\") \\\n   .add_legend()\nplt.title('Unhappy customers are slightly older');"},{"cell_type":"markdown","metadata":{},"source":"# saldo_var30"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train.saldo_var30.hist(bins=100)\nplt.xlim(0, train.saldo_var30.max());"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# improve the plot by making the x axis logarithmic\ntrain['log_saldo_var30'] = train.saldo_var30.map(np.log)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Let's look at the density of the age of happy/unhappy customers for saldo_var30\nsns.FacetGrid(train, hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"log_saldo_var30\") \\\n   .add_legend();"},{"cell_type":"markdown","metadata":{},"source":"# Explore the interaction between var15 (age) and var38"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.FacetGrid(train, hue=\"TARGET\", size=10) \\\n   .map(plt.scatter, \"var38\", \"var15\") \\\n   .add_legend();"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.FacetGrid(train, hue=\"TARGET\", size=10) \\\n   .map(plt.scatter, \"logvar38\", \"var15\") \\\n   .add_legend()\nplt.ylim([0,120]); # Age must be positive ;-)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Exclude most common value for var38 \nsns.FacetGrid(train[~train.var38mc], hue=\"TARGET\", size=10) \\\n   .map(plt.scatter, \"logvar38\", \"var15\") \\\n   .add_legend()\nplt.ylim([0,120]);"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# What is distribution of the age when var38 has it's most common value ?\nsns.FacetGrid(train[train.var38mc], hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"var15\") \\\n   .add_legend();"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# What is density of n0 ?\nsns.FacetGrid(train, hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"n0\") \\\n   .add_legend()\nplt.title('Unhappy customers have a lot of features that are zero');"},{"cell_type":"markdown","metadata":{},"source":"# Select the most important features"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import f_classif,chi2\nfrom sklearn.preprocessing import Binarizer, scale\n\n# First select features based on chi2 and f_classif\np = 3\n\nX_bin = Binarizer().fit_transform(scale(X))\nselectChi2 = SelectPercentile(chi2, percentile=p).fit(X_bin, y)\nselectF_classif = SelectPercentile(f_classif, percentile=p).fit(X, y)\n\nchi2_selected = selectChi2.get_support()\nchi2_selected_features = [ f for i,f in enumerate(X.columns) if chi2_selected[i]]\nprint('Chi2 selected {} features {}.'.format(chi2_selected.sum(),\n   chi2_selected_features))\nf_classif_selected = selectF_classif.get_support()\nf_classif_selected_features = [ f for i,f in enumerate(X.columns) if f_classif_selected[i]]\nprint('F_classif selected {} features {}.'.format(f_classif_selected.sum(),\n   f_classif_selected_features))\nselected = chi2_selected & f_classif_selected\nprint('Chi2 & F_classif selected {} features'.format(selected.sum()))\nfeatures = [ f for f,s in zip(X.columns, selected) if s]\nprint (features)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Make a dataframe with the selected features and the target variable\nX_sel = train[features+['TARGET']]"},{"cell_type":"markdown","metadata":{},"source":"# var36"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"X_sel['var36'].value_counts()"},{"cell_type":"markdown","metadata":{},"source":"var36 is most of the times 99 or [0,1,2,3]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Let's plot the density in function of the target variabele\nsns.FacetGrid(train, hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"var36\") \\\n   .add_legend()\nplt.title('If var36 is 0,1,2 or 3 => less unhappy customers');"},{"cell_type":"markdown","metadata":{},"source":"In above plot we see that the density of unhappy custormers is lower when var36 is not 99"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# var36 in function of var38 (most common value excluded) \nsns.FacetGrid(train[~train.var38mc], hue=\"TARGET\", size=10) \\\n   .map(plt.scatter, \"var36\", \"logvar38\") \\\n   .add_legend();"},{"cell_type":"markdown","metadata":{},"source":"Let's seperate that in two plots"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.FacetGrid(train[(~train.var38mc) & (train.var36 < 4)], hue=\"TARGET\", size=10) \\\n   .map(plt.scatter, \"var36\", \"logvar38\") \\\n   .add_legend()\nplt.title('If var36==0, only happy customers');"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Let's plot the density in function of the target variabele, when var36 = 99\nsns.FacetGrid(train[(~train.var38mc) & (train.var36 ==99)], hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"logvar38\") \\\n   .add_legend();"},{"cell_type":"markdown","metadata":{},"source":"# num_var5"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train.num_var5.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train[train.TARGET==1].num_var5.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train[train.TARGET==0].num_var5.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.FacetGrid(train, hue=\"TARGET\", size=6) \\\n   .map(plt.hist, \"num_var5\") \\\n   .add_legend();"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.FacetGrid(train, hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"num_var5\") \\\n   .add_legend();"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.pairplot(train[['var15','var36','logvar38','TARGET']], hue=\"TARGET\", size=2, diag_kind=\"kde\");"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train[['var15','var36','logvar38','TARGET']].boxplot(by=\"TARGET\", figsize=(12, 6));"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# A final multivariate visualization technique pandas has is radviz\n# Which puts each feature as a point on a 2D plane, and then simulates\n# having each sample attached to those points through a spring weighted\n# by the relative value for that feature\nfrom pandas.tools.plotting import radviz\nradviz(train[['var15','var36','logvar38','TARGET']], \"TARGET\");"},{"cell_type":"markdown","metadata":{},"source":"# now look at all 8 features together"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"features"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"radviz(train[features+['TARGET']], \"TARGET\");"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.pairplot(train[features+['TARGET']], hue=\"TARGET\", size=2, diag_kind=\"kde\");"},{"cell_type":"markdown","metadata":{},"source":"# Correlations"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"cor_mat = X.corr()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"f, ax = plt.subplots(figsize=(15, 12))\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(cor_mat,linewidths=.5, ax=ax);"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"cor_mat = X_sel.corr()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"f, ax = plt.subplots(figsize=(15, 12))\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(cor_mat,linewidths=.5, ax=ax);"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# only important correlations and not auto-correlations\nthreshold = 0.7\nimportant_corrs = (cor_mat[abs(cor_mat) > threshold][cor_mat != 1.0]) \\\n    .unstack().dropna().to_dict()\nunique_important_corrs = pd.DataFrame(\n    list(set([(tuple(sorted(key)), important_corrs[key]) \\\n    for key in important_corrs])), columns=['attribute pair', 'correlation'])\n# sorted by absolute value\nunique_important_corrs = unique_important_corrs.ix[\n    abs(unique_important_corrs['correlation']).argsort()[::-1]]\nunique_important_corrs"},{"cell_type":"markdown","metadata":{},"source":"# Clusters "},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Recipe from https://github.com/mgalardini/python_plotting_snippets/blob/master/notebooks/clusters.ipynb\nimport matplotlib.patches as patches\nfrom scipy.cluster import hierarchy\nfrom scipy.stats.mstats import mquantiles\nfrom scipy.cluster.hierarchy import dendrogram, linkage"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Correlate the data\n# also precompute the linkage\n# so we can pick up the \n# hierarchical thresholds beforehand\n\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\n\n# scale to mean 0, variance 1\ntrain_std = pd.DataFrame(scale(X_sel))\ntrain_std.columns = X_sel.columns\nm = train_std.corr()\nl = linkage(m, 'ward')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Plot the clustermap\n# Save the returned object for further plotting\nmclust = sns.clustermap(m,\n               linewidths=0,\n               cmap=plt.get_cmap('RdBu'),\n               vmax=1,\n               vmin=-1,\n               figsize=(14, 14),\n               row_linkage=l,\n               col_linkage=l)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Threshold 1: median of the\n# distance thresholds computed by scipy\nt = np.median(hierarchy.maxdists(l))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Plot the clustermap\n# Save the returned object for further plotting\nmclust = sns.clustermap(m,\n               linewidths=0,\n               cmap=plt.get_cmap('RdBu'),\n               vmax=1,\n               vmin=-1,\n               figsize=(12, 12),\n               row_linkage=l,\n               col_linkage=l)\n\n# Draw the threshold lines\nmclust.ax_col_dendrogram.hlines(t,\n                               0,\n                               m.shape[0]*10,\n                               colors='r',\n                               linewidths=2,\n                               zorder=1)\nmclust.ax_row_dendrogram.vlines(t,\n                               0,\n                               m.shape[0]*10,\n                               colors='r',\n                               linewidths=2,\n                               zorder=1)\n\n# Extract the clusters\nclusters = hierarchy.fcluster(l, t, 'distance')\nfor c in set(clusters):\n    # Retrieve the position in the clustered matrix\n    index = [x for x in range(m.shape[0])\n             if mclust.data2d.columns[x] in m.index[clusters == c]]\n    # No singletons, please\n    if len(index) == 1:\n        continue\n\n    # Draw a rectangle around the cluster\n    mclust.ax_heatmap.add_patch(\n        patches.Rectangle(\n            (min(index),\n             m.shape[0] - max(index) - 1),\n                len(index),\n                len(index),\n                facecolor='none',\n                edgecolor='r',\n                lw=3)\n        )\n\nplt.title('Cluster matrix')\n\npass"},{"cell_type":"markdown","metadata":{},"source":"For clustering with more features, have a look at: [https://www.kaggle.com/cast42/santander-customer-satisfaction/correlation-pairs](https://www.kaggle.com/cast42/santander-customer-satisfaction/correlation-pairs)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}