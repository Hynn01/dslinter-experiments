{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center>RNNs, Encoder-Decoder, 1d CNN</center>\n## <div align=right>Made by Ihor Markevych</div>"},{"metadata":{},"cell_type":"markdown","source":"**Note:** Model selection cycle was not included due to its size. Each model presented is best found one in its subclass."},{"metadata":{},"cell_type":"markdown","source":"-----------------\n-----------------"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Convolution1D, Flatten, Dropout, \\\n                                    LSTM, GRU, Input, RepeatVector, SimpleRNN\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport os\n\nimport sklearn.metrics\nfrom sklearn.preprocessing import MinMaxScaler\nimport sklearn.model_selection\nfrom sklearn.linear_model import LinearRegression\nnp.random.seed(123)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/uop-aml-hw3/admData20.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"plt.plot(data.AdmittedNum)\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoder-decoder (taking each admission date separately)"},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"grouped = data.groupby(by='ExpStartDate')\n\nX =  []\nX_aux = []\ny = []\nfor g in grouped.groups:\n    X.append(grouped.get_group(g).AppliedNum.values)\n    X_aux.append(grouped.get_group(g).Budget.iloc[0])\n    y.append(grouped.get_group(g).AdmittedNum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"X = pd.DataFrame([[i for i in j] for j in X])\ny = pd.DataFrame([[i for i in j] for j in y])\nX = np.array(X)\ny = np.array(y)\nX_aux = np.array(X_aux)\n\nX = X / X_aux[:, None]\ny = y / X_aux[:, None]\n\nX_train, X_test, X_aux_train, X_aux_test, y_train, y_test = sklearn.model_selection.train_test_split(X, X_aux, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Helper functions"},{"metadata":{},"cell_type":"markdown","source":"Generate training datasets of sequences of variable length."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def train_gen(X, y, foresight=5):\n    train = []\n    target = []\n    for i in range(len(X)):\n        X_seq = X[i, ~np.isnan(X[i,:])]\n        y_seq = y[i, ~np.isnan(y[i,:])]\n        \n        X_seq_train = X_seq[:-foresight]\n        y_seq_train = y_seq[:-foresight]\n        y_seq_test = y_seq[-foresight:]\n    \n        train.append(np.array([X_seq_train, y_seq_train]))\n        target.append(y_seq_test)\n    return train, target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Custom train cycle.  \n**NOTE:** validation is not included here due to sample size - after grouping training is 6 samples, splitting it into additional set looks inappropriate."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def train_enc_dec(model, train_gen=train_gen, X_train=X_train, y_train=y_train, epochs=50):\n    epochs = epochs\n    history = []\n#     val_history = []\n    X_train_seq, y_train_seq = train_gen(X_train, y_train)\n    \n#     X_t, X_val, y_t, y_val = sklearn.model_selection.train_test_split(X_train_seq, y_train_seq, test_size=0.1)\n    \n    for e in range(epochs):\n        for i in range(len(X_train_seq)):\n            hist = model.fit(np.array([X_train_seq[i].T]), [[y_train_seq[i]]], \n                             epochs=1, \n                             batch_size=1, \n                             validation_split=0,\n                             verbose=0)\n            history.append(hist)\n        \n#         val_temp = []\n#         for i in range(len(X_val)):\n#             val_temp.append(model.evaluate(np.array([X_val[i].T]), [[y_val[i]]], \n#                                            verbose=0))\n    return history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reporting MSE and MAE."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def print_report(y_true, y_pred):\n    print(f'MSE: {np.mean((np.array(y_true) - y_pred) ** 2) / len(y_true[0])}')\n    print(f'MAE: {np.mean(np.abs(np.array(y_true) - y_pred)) / len(y_true[0])}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LSTM-based model"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# https://stackoverflow.com/questions/43117654/many-to-many-sequence-prediction-with-different-sequence-length\n\ndef create_LSTM_model():\n\n    seq_input = Input(shape=(None, 2), name='seq_input') # unknown timespan, fixed feature size of 2\n    x = LSTM(4, return_sequences=True, activation='relu')(seq_input)\n    x = LSTM(4, return_sequences=True, activation='relu')(x)\n    x = LSTM(1, return_sequences=False, activation='relu')(x)\n    # aux_input = Input(shape=(1), name='aux_input')\n\n    # x = tf.keras.layers.concatenate([aux_input, seq_x])\n    # x = Dense(1, activation='relu')(x)\n    x = RepeatVector(5)(x)\n\n    out = LSTM(1, return_sequences=True)(x)\n\n    model = Model(seq_input, out)\n\n    model.compile(loss='mean_squared_error',\n                  optimizer='adam',\n                  metrics=['mae'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"model = create_LSTM_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"history = train_enc_dec(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"X_test_seq, y_test_seq = train_gen(X_test, y_test)\ny_pred = np.array([model.predict(np.array([X_test_seq[i].T])) for i in range(len(X_test))])\ny_pred = np.squeeze(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"print_report(y_test_seq, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"y_pred = y_pred * X_aux_test[:, None]\ny_test_seq = y_test_seq * X_aux_test[:, None]\nprint('For unscaled:')\nprint_report(y_test_seq, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"y_test_unscaled = [ts * aux for ts, aux in zip([y_t[1] for y_t in X_test_seq], X_aux_test)]\nfor i in range(len(y_test_seq)):\n    plt.plot(np.append(y_test_unscaled[i], y_test_seq[i]), label='True')\n    plt.plot(np.append(y_test_unscaled[i], y_pred[i]), label='Prediction')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### GRU-based model"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def create_GRU_model():\n\n    seq_input = Input(shape=(None, 2), name='seq_input') # unknown timespan, fixed feature size of 2\n    x = GRU(4, return_sequences=True, activation='relu')(seq_input)\n    x = GRU(4, return_sequences=True, activation='relu')(x)\n    x = GRU(1, return_sequences=False, activation='relu')(x)\n    # aux_input = Input(shape=(1), name='aux_input')\n\n    # x = tf.keras.layers.concatenate([aux_input, seq_x])\n    # x = Dense(1, activation='relu')(x)\n    x = RepeatVector(5)(x)\n\n    out = GRU(1, return_sequences=True)(x)\n\n    model = Model(seq_input, out)\n\n    model.compile(loss='mean_squared_error',\n                  optimizer='adam',\n                  metrics=['mae'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"model = create_GRU_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"history = train_enc_dec(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"X_test_seq, y_test_seq = train_gen(X_test, y_test)\ny_pred = np.array([model.predict(np.array([X_test_seq[i].T])) for i in range(len(X_test))])\ny_pred = np.squeeze(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"print_report(y_test_seq, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"y_pred = y_pred * X_aux_test[:, None]\ny_test_seq = y_test_seq * X_aux_test[:, None]\nprint('For unscaled:')\nprint_report(y_test_seq, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"y_test_unscaled = [ts * aux for ts, aux in zip([y_t[1] for y_t in X_test_seq], X_aux_test)]\nfor i in range(len(y_test_seq)):\n    plt.plot(np.append(y_test_unscaled[i], y_test_seq[i]), label='True')\n    plt.plot(np.append(y_test_unscaled[i], y_pred[i]), label='Prediction')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------------------\n----------------------\n----------------------"},{"metadata":{},"cell_type":"markdown","source":"## Taking whole timespan"},{"metadata":{},"cell_type":"markdown","source":"**NOTE:** in printed report, in predictions plots red lines correspond to predicted sequences, blue lines to true data."},{"metadata":{},"cell_type":"markdown","source":"### Helper functions"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def plot_preds(model):\n\n    plt.figure(figsize=(20, 8))\n    plt.subplot(1, 3, 1)\n    axes = plt.gca()\n    axes.set_ylim([0, 2])\n\n    for j, y in enumerate(y_train):\n        row = [None for i in range(j)]\n        row += list(y)\n        plt.plot(row, 'b', label='true')\n\n    plt.subplot(1, 3, 2)\n    axes = plt.gca()\n    axes.set_ylim([0, 2])\n    y_pred = model.predict(X_val)\n\n    plt.plot(X_val[0,:5,0], 'b')\n    \n    for j, prediction in enumerate(y_pred):\n        row = [None for i in range(j + 5)]\n        row += list(prediction)\n        plt.plot(row, 'r')\n\n\n    for j, y in enumerate(y_val):\n        row = [None for i in range(j + 5)]\n        row += list(y)\n        plt.plot(row, 'b')\n\n    plt.legend()\n\n    plt.subplot(1, 3, 3)\n    axes = plt.gca()\n    axes.set_ylim([0, 2])\n    y_pred = model.predict(X_test)\n\n    plt.plot(X_test[0,:5,0], 'b')\n    \n    for j, prediction in enumerate(y_pred):\n        row = [None for i in range(j + 5)]\n        row += list(prediction)\n        plt.plot(row, 'r')\n\n\n    for j, y in enumerate(y_test):\n        row = [None for i in range(j + 5)]\n        row += list(y)\n        plt.plot(row, 'b')\n\n    plt.legend()\n    plt.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.001)\nEPOCHS = 50\nBATCH_SIZE = 64\nLOOK_BACK = 20\nes2 = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=400, restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def train_and_report(creation_func):\n    model = creation_func()\n    print(model.summary())\n    \n    history = model.fit(X_train, y_train,\n                        batch_size=128, \n                        epochs=700, \n                        validation_split=0.0,\n                        use_multiprocessing=True,\n                        verbose=0,\n                        validation_data=(X_val, y_val), \n                        callbacks=[es2]\n                       )\n    \n    plt.title('Loss')\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()\n    \n    try:\n        y_pred_val = model.predict(X_val)[:,:,0]\n        y_pred_test = model.predict(X_test)[:,:,0]\n    except:\n        y_pred_val = model.predict(X_val)\n        y_pred_test = model.predict(X_test)\n        \n    print('For validation set:')\n    print_report(y_val, y_pred_val)\n\n    print('For test set:')\n    print_report(y_test, y_pred_test)\n    \n    plot_preds(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def inverse_transform(y, scaler, column):\n    return y * (scaler.data_max_[column] - scaler.data_min_[column]) + scaler.data_min_[column]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def create_seq(dataset, look_back=LOOK_BACK, foresight=5):\n    X, y = [], []\n    for i in range(len(dataset) - look_back - foresight):\n        X.append(np.array(dataset[i:(i + look_back)].loc[:, ['AdmittedNum', 'AppliedNum', 'WeeksBeforeStart']]) \n                 / np.array(dataset.Budget[i:(i + look_back)])[:, None])\n        y.append(dataset.iloc[(i + look_back) : (i + look_back + foresight), 0] \n                 / np.array(dataset.Budget[(i + look_back) : (i + look_back + foresight)]))\n    return np.array(X), np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"data2 = data.loc[:, ['AdmittedNum', 'AppliedNum', 'WeeksBeforeStart', 'Budget']]\n\ntrainTr = round(0.6 * len(data))\nvalTr = round((0.8 * len(data)))\ndata_train, data_val, data_test = data2[:trainTr], data2[trainTr:valTr], data2[valTr:]\n\nX_train, y_train = create_seq(data_train)\nX_val, y_val = create_seq(data_val)\nX_test, y_test = create_seq(data_test)\n\nX_train = np.reshape(np.array(X_train), (X_train.shape[0], X_train.shape[1], 3))\nX_val = np.reshape(np.array(X_val), (X_val.shape[0], X_val.shape[1], 3))\nX_test = np.reshape(np.array(X_test), (X_test.shape[0], X_test.shape[1], 3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---------\n---------"},{"metadata":{},"cell_type":"markdown","source":"## GRU-based model"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def create_GRU():\n    model = Sequential()\n    \n    seq_input = Input(shape=(LOOK_BACK, 3), name='seq_input')\n    x = GRU(4, return_sequences=False, activation='relu', dropout=0.3)(seq_input)\n#     x = GRU(16, return_sequences=True, activation='relu')(x)\n#     x = GRU(4, return_sequences=False, activation='relu')(x)\n\n    # aux_input = Input(shape=(1), name='aux_input')\n    # x = tf.keras.layers.concatenate([aux_input, seq_x])\n    # x = Dense(1, activation='relu')(x)\n    \n    x = RepeatVector(5)(x)\n\n#     x = GRU(2, return_sequences=True, activation='relu')(x)\n#     x = GRU(2, return_sequences=True, activation='relu')(x)\n    out = GRU(1, return_sequences=True)(x)\n\n    model = Model(seq_input, out)\n    \n    model.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=[\"mae\"])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train_and_report(create_GRU)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---------"},{"metadata":{},"cell_type":"markdown","source":"### LSTM-based model"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def create_LSTM():\n    model = Sequential()\n    \n    seq_input = Input(shape=(LOOK_BACK, 3), name='seq_input')\n    x = LSTM(4, return_sequences=False, activation='relu', dropout=0.3)(seq_input)\n#     x = LSTM(16, return_sequences=True, activation='relu')(x)\n#     x = LSTM(4, return_sequences=False, activation='relu')(x)\n\n    # aux_input = Input(shape=(1), name='aux_input')\n    # x = tf.keras.layers.concatenate([aux_input, seq_x])\n    # x = Dense(1, activation='relu')(x)\n    \n    x = RepeatVector(5)(x)\n\n#     x = LSTM(2, return_sequences=True, activation='relu')(x)\n#     x = LSTM(2, return_sequences=True, activation='relu')(x)\n    out = LSTM(1, return_sequences=True)(x)\n\n    model = Model(seq_input, out)\n    \n    model.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=[\"mae\"])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_and_report(create_LSTM)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----------\n-----------"},{"metadata":{},"cell_type":"markdown","source":"### 1d CNN"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def create_CNN():\n    model = Sequential()\n    \n    seq_input = Input(shape=(LOOK_BACK, 3), name='seq_input')\n    x = Convolution1D(filters=2, kernel_size=5, input_shape=(2, LOOK_BACK), activation='relu')(seq_input)\n#     x = Dropout(0.3)(x)\n#     x = Convolution1D(filters=4, kernel_size=3, activation='relu')(x)\n#     x = Dropout(0.5)(x)\n    x = Flatten()(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(8, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(4, activation='relu')(x)\n#     x = Dropout(0.3)(x)\n    \n    out = Dense(5)(x)\n\n    model = Model(seq_input, out)\n    \n    model.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=[\"mae\"])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_and_report(create_CNN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1d CNN with GRU output"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def create_CNN_GRU():\n    model = Sequential()\n    \n    seq_input = Input(shape=(LOOK_BACK, 3), name='seq_input')\n    x = Convolution1D(filters=2, kernel_size=5, input_shape=(2, LOOK_BACK), activation='relu')(seq_input)\n    x = Convolution1D(filters=4, kernel_size=3, activation='relu')(x)\n    x = Flatten()(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(8, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(4, activation='relu')(x)\n    x = Dropout(0.1)(x)\n\n    x = RepeatVector(5)(x)\n\n    out = GRU(1, return_sequences=True)(x)\n\n    model = Model(seq_input, out)\n    \n    model.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=[\"mae\"])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_and_report(create_CNN_GRU)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"--------"},{"metadata":{},"cell_type":"markdown","source":"## 1d CNN with LSTM output"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def create_CNN_LSTM():\n    model = Sequential()\n    \n    seq_input = Input(shape=(LOOK_BACK, 3), name='seq_input')\n    x = Convolution1D(filters=2, kernel_size=5, input_shape=(2, LOOK_BACK), activation='relu')(seq_input)\n    x = Convolution1D(filters=4, kernel_size=3, activation='relu')(x)\n    x = Flatten()(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(8, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(4, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    \n    x = RepeatVector(5)(x)\n\n    out = LSTM(1, return_sequences=True)(x)\n\n    model = Model(seq_input, out)\n    \n    model.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=[\"mae\"])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_and_report(create_CNN_LSTM)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----------\n-----------"},{"metadata":{},"cell_type":"markdown","source":"### Dense NN"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def create_DNN():\n    model = Sequential()\n    \n    seq_input = Input(shape=(LOOK_BACK, 3), name='seq_input')\n    x = Flatten()(seq_input)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(8, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(4, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    out = Dense(5, activation='relu')(x)\n\n    model = Model(seq_input, out)\n    \n    model.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=[\"mae\"])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_and_report(create_DNN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----------"},{"metadata":{},"cell_type":"markdown","source":"### DNN with GRU output"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def create_DNN_with_GRU_out():\n    model = Sequential()\n    \n    seq_input = Input(shape=(LOOK_BACK, 3), name='seq_input')\n    x = Flatten()(seq_input)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(8, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(4, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = RepeatVector(5)(x)\n\n    out = GRU(1, return_sequences=True)(x)\n\n    model = Model(seq_input, out)\n    \n    model.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=[\"mae\"])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_and_report(create_DNN_with_GRU_out)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------------"},{"metadata":{},"cell_type":"markdown","source":"### DNN with LSTM output"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def create_DNN_with_LSTM_out():\n    model = Sequential()\n    \n    seq_input = Input(shape=(LOOK_BACK, 3), name='seq_input')\n    x = Flatten()(seq_input)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(8, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(4, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = RepeatVector(5)(x)\n\n    out = LSTM(1, return_sequences=True)(x)\n\n    model = Model(seq_input, out)\n    \n    model.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=[\"mae\"])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_and_report(create_DNN_with_LSTM_out)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----------\n-----------\n-----------"},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Overall, all models gave comparable performance.  \nModel that fully LSTM-based model gave best performance, however, as it was said, all models had comparable error.  \nThis fact can be explained by cyclic pattern of the data and small data sample.  \n  \nStacking recurrent layers was explored, it made models worse, which may be explained by sample size.  \n  \nAuxilary input was tried. However, it appeared that best way of incorporating it is to scale each value by Budget - in this case we assume that we now Budget for future (which is usually the case).\n\nEncoder-decoder model was also explored. This model takes an input of a sequence of variable length and predicts next five weeks (limited by single admission date). This is the most accurate model and the most logical from the use case - after the start of new admission date user should wait for two-three weeks. Then model can start making predictions and adjust them by taking into account newly available data each week."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}