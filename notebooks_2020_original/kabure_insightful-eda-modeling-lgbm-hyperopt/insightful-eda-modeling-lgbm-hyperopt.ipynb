{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Welcome to my EDA Kernel\n\n### Description:\nThe dataset for this competition includes aggregate stopped vehicle information and intersection wait times. Your task is to predict congestion, based on an aggregate measure of stopping distance and waiting times, at intersections in 4 major US cities: Atlanta, Boston, Chicago & Philadelphia.\n\n<img src=\"https://cdn.citylab.com/media/img/citylab/2018/02/AP_17153592466989/facebook.jpg\" alt=\"Italian Trulli\">\n"},{"metadata":{},"cell_type":"markdown","source":"# Objective:\nIt's a first contact with the data, so I want to explore it and understand how the data is. \n\nSome important things that is standard to analyze:\n- what are the data types of the features?\n- We have missing values?\n- How many unique values we have in each feature;\n- The shape of full dataset.\n- The entropy of each feature (that show us the level of disorder on this column, it's like a \"messy metric\")\n\nAfter this first analyze we can think in other questions to explore:\n- Which distribution we have in our columns? \n- Which are the most common cities?\n- Which are the distribution of the stops, time, distances?\n- How long is our date range? \n- What are the distribution of the regions?\n\nAnd many more questions;\n\n## <font color=\"red\"> I'm near of grandmaster tier, so, if you find this kernel useful or interesting, please don't forget to upvote the kernel =)</font>"},{"metadata":{},"cell_type":"markdown","source":"### Importing the Main Libraries to work with data"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\n#import cufflinks\n#import cufflinks as cf\nimport plotly.figure_factory as ff\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\n\nfrom functools import partial\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing datasets"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/train.csv')\ndf_test = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Util functions "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the data"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"resumetable(df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice, this func give us a lot of cool and useful informations;\n- We have only two features with missing values. Entry and Exit StreetName"},{"metadata":{},"cell_type":"markdown","source":"# City's\n- I will start exploring the distribution of City's because it is a categorical with only a few categorys inside.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"resumetable(df_train)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"total = len(df_train)\nplt.figure(figsize=(15,19))\n\nplt.subplot(311)\ng = sns.countplot(x=\"City\", data=df_train)\ng.set_title(\"City Count Distribution\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"City Names\", fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=14) \ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can note that:\n- The most common value is Philadelphia and it have 45.29% of the total entries.\n- The other categories don't have a so discrepant difference between them. \n?\nLet's \n"},{"metadata":{},"cell_type":"markdown","source":"# Date Features\n- Hour Distribution\n- Month Distribution\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_hour = df_train.groupby(['City', 'Hour'])['RowId'].nunique().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,12))\n\nplt.subplot(211)\ng = sns.countplot(x=\"Hour\", data=df_train, hue='City', dodge=True)\ng.set_title(\"Hour Count Distribution by Week and Weekend Days\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"Hours of Day\", fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(212)\ng1 = sns.countplot(x=\"Month\", data=df_train, hue='City', dodge=True)\ng1.set_title(\"Hour Count Distribution by Week and Weekend Days\", fontsize=20)\ng1.set_ylabel(\"Count\",fontsize= 17)\ng1.set_xlabel(\"Hours of Day\", fontsize=17)\nsizes=[]\nfor p in g1.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng1.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplots_adjust(hspace = 0.3)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool. <br>\n\nIn the hours chart:\n- We can see that cities can have different hours patterns.\n- Philadelphia is by far the most common in all hours. Only on 5 a.m that is almost lose to Boston in total entries.\n- Atlanta is the city with less entries in all day, but after 17 p.m to 4a.m it's the second city with more rides \n\nIn the month chart:\n- We can note that the data is about only 6 months (with few values in January and May)\n- Also, the pattern of the Boston City improved througout the time and the others seem very unchanged. \n\nNow, let's explore the Entry and Exit features.\n"},{"metadata":{},"cell_type":"markdown","source":"# EntryHeading and Exit Heading"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,12))\n\ntmp = round(((df_train.groupby(['EntryHeading'])['RowId'].nunique() / total) * 100)).reset_index()\n\nplt.subplot(211)\ng = sns.countplot(x=\"EntryHeading\",\n                  data=df_train,\n                  order=list(tmp['EntryHeading'].values),\n                  hue='ExitHeading', dodge=True)\ng.set_title(\"Entry Heading by Exit Heading\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"Entry Heading Region\", fontsize=17)\ngt = g.twinx()\ngt = sns.pointplot(x='EntryHeading', y='RowId', \n                   data=tmp, order=list(tmp['EntryHeading'].values),\n                   color='black', legend=False)\ngt.set_ylim(0, tmp['RowId'].max()*1.1)\ngt.set_ylabel(\"% of Total(Black Line)\", fontsize=16)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(212)\ng1 = sns.countplot(x=\"EntryHeading\", order=list(tmp['EntryHeading'].values), \n                   data=df_train, hue='City')\ng1.set_title(\"Entry Heading Distribution By Cities\", fontsize=20)\ng1.set_ylabel(\"Count\",fontsize= 17)\ng1.set_xlabel(\"Entry Heading Region\", fontsize=17)\nsizes=[]\nfor p in g1.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng1.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplots_adjust(hspace = 0.3)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice. <br>\nIn Entry and Exit Heading chart:\n- We can note that in general the Entry and Exit Region is exactly the same. \n\nIn Entry by Cities chart:\n- We can note the difference patterns on the cities. It's a very interesting and could give us many interesting insights. "},{"metadata":{},"cell_type":"markdown","source":"## IntersectionID "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\ndf_train.IntersectionId.value_counts()[:45].plot(kind='bar')\nplt.xlabel(\"Intersection Number\", fontsize=18)\nplt.ylabel(\"Count\", fontsize=18)\nplt.title(\"TOP 45 most commmon IntersectionID's \", fontsize=22)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby(['IntersectionId', 'EntryHeading', 'ExitHeading'])['RowId'].count().reset_index().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring numerical features\nIf you readed the competition description, you know that these are the target features;\n\nThe targets are: \n- TotalTimeStopped_p20\n- TotalTimeStopped_p50\n- TotalTimeStopped_p80\n- DistanceToFirstStop_p20\n- DistanceToFirstStop_p50\n- DistanceToFirstStop_p80\n\nAnd the as the TimeFromFirstStop is an optional data, I will use it to see the correlations.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"t_stopped = ['TotalTimeStopped_p20',\n             'TotalTimeStopped_p50', \n             'TotalTimeStopped_p80']\nt_first_stopped = ['TimeFromFirstStop_p20',\n                   'TimeFromFirstStop_p50',\n                   'TimeFromFirstStop_p80']\nd_first_stopped = ['DistanceToFirstStop_p20',\n                   'DistanceToFirstStop_p50',\n                   'DistanceToFirstStop_p80']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Heatmap Target Features"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,12))\nplt.title('Correlation of Features for Train Set', fontsize=22)\nsns.heatmap(df_train[t_stopped + \n                     #t_first_stopped + \n                     d_first_stopped].astype(float).corr(),\n            vmax=1.0,  annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool!<br>\nWe can see that the best correlation between the metrics are:\n- Distance to First Stop p20 and Total Time Stopped p20 have a high correlation."},{"metadata":{},"cell_type":"markdown","source":"# Scaling the target\n- Geting the min_max transformation to get clusterization and PCA features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import minmax_scale\n\ntarget_cols = t_stopped + d_first_stopped","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in target_cols:\n    df_train[col+str(\"_minmax\")] = (minmax_scale(df_train[col], feature_range=(0,1)))\n    \nmin_max_cols = ['TotalTimeStopped_p20_minmax', 'TotalTimeStopped_p50_minmax',\n                'TotalTimeStopped_p80_minmax', 'DistanceToFirstStop_p20_minmax',\n                'DistanceToFirstStop_p50_minmax', 'DistanceToFirstStop_p80_minmax']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA\n- To better see the distribution of our metrics, lets apply PCA to reduce the dimensionality of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=3, random_state=5)\n\nprincipalComponents = pca.fit_transform(df_train[min_max_cols])\n\nprincipalDf = pd.DataFrame(principalComponents)\n\n# df.drop(cols, axis=1, inplace=True)\nprefix='Target_PCA'\nprincipalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n\ndf_train = pd.concat([df_train, principalDf], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice, now we have the PCA features... Let's see the ratio of explanation of the first two Principal Components"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_[:2].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the 2 first components we have almost 84% of the data explained. It's a very way to easiest visualize the differences between the patterns."},{"metadata":{},"cell_type":"markdown","source":"# Scatter plot of cities by the PCA "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df_train.sample(50000), col=\"City\", \n                  col_wrap=2, height=5, aspect=1.5, hue='Weekend')\n\ng.map(sns.scatterplot, \"Target_PCA0\", \"Target_PCA1\", alpha=.5 ).add_legend();\ng.set_titles('{col_name}', fontsize=17)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool. We can see differet patterns by the Cities and their weekend patterns. "},{"metadata":{},"cell_type":"markdown","source":"# KMeans Clusterization\n- First, I will apply the elbow method to find the correct number of cluster we have in our data\n- After it, we will implement the kmeans with the best quantity"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#sum of squared distances\nssd = []\n\nK = range(1,10)\nfor k in K:\n    km = KMeans(n_clusters=k, random_state=4)\n    km = km.fit(df_train[min_max_cols])\n    ssd.append(km.inertia_)\n    \nplt.plot(K, ssd, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum of squared distances')\nplt.title('Elbow Method For Optimal k')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice. <br>\nBased on Elbow Method the best number of cluster is 4. So, let's apply the K means on data."},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(n_clusters=4, random_state=4)\nkm = km.fit(df_train[min_max_cols])\ndf_train['clusters_T'] = km.predict(df_train[min_max_cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ploting Clusters\n- Understanding the cluster distribution\n- Exploring by Cities"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tmp = pd.crosstab(df_train['City'], df_train['clusters_T'], \n                  normalize='columns').unstack('City').reset_index().rename(columns={0:\"perc\"})\n\ntotal = len(df_train)\nplt.figure(figsize=(15,16))\n\nplt.subplot(311)\ng = sns.countplot(x=\"clusters_T\", data=df_train)\ng.set_title(\"Cluster Target Count Distribution\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"Target Cluster Distributions\", fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=14) \ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(312)\ng1 = sns.countplot(x=\"clusters_T\", data=df_train, hue='City')\ng1.set_title(\"CITIES - Cluster Target Distribution\", fontsize=20)\ng1.set_ylabel(\"Count\",fontsize= 17)\ng1.set_xlabel(\"Target Cluster Distributions\", fontsize=17)\nsizes=[]\nfor p in g1.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g1.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=10)\ng1.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(313)\ng1 = sns.boxplot(x=\"clusters_T\", y='Target_PCA0', \n                 data=df_train, hue='City')\ng1.set_title(\"PCA Feature - Distribution of PCA by Clusters and Cities\", \n             fontsize=20)\ng1.set_ylabel(\"PCA 0 Values\",fontsize= 17)\ng1.set_xlabel(\"Target Cluster Distributions\", fontsize=17)\n\nplt.subplots_adjust(hspace = 0.5)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice. <br>\n### In the first chart:\n- We can note that the most common cluster is the 1 that have 73% of all data.\n\n### Second chart: \n- Philadelphia is the most common in the first 3 clusters. \n- Boston is the second most common in 0,1 and the most common on Cluster 3;\n- In the second cluster, Atlanta is the second most common city.\n\n### Third Chart:\n- Is clear to understand how the algorithmn divided the data in PCA values\n\n## NOTE: EVERY TIME I RUN IT, THE VALUES CHANGES, SO SORRY BY THE WRONG \n"},{"metadata":{},"cell_type":"markdown","source":"# PCA values by CLUSTERS \n- Let's see in another way how the algorithmn have decided by the clusterization"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\n\nsns.scatterplot(x='Target_PCA0', y='Target_PCA1',\n                hue='clusters_T', data=df_train,\n                palette='Set1')\nplt.title(\"PCA 0 and PCA 1 by Clusters\", fontsize=22)\nplt.ylabel(\"Target PCA 1 values\", fontsize=18)\nplt.xlabel(\"Target PCA 0 values\", fontsize=18)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool. It gives us a good understand of the boundaries of Clusters. <br>\nI suspect that the cluster 2 is about traffic;\n\nLet's plot it by each city and try to find any pattern in the PCA dispersion."},{"metadata":{},"cell_type":"markdown","source":"# PCA Dispersion by clusters and by Each City\n- To better understand the patterns, let's plot by Cities"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df_train.sample(500000), col=\"City\", \n                  col_wrap=2, height=4, aspect=1.5, \n                  hue='clusters_T')\n\ng.map(sns.scatterplot, \"Target_PCA0\", \"Target_PCA1\", \n      alpha=.5).add_legend();\ng.set_titles('{col_name}', fontsize=50)\n\nplt.suptitle(\"CITIES \\nPrincipal Component Analysis Dispersion by Cluster\", fontsize=22)\n\nplt.subplots_adjust(hspace = 0.3, top=.85)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool! We can see that Atlanta and Philadelphia have similar pattern of the Cluster 2;<Br>\nThe other cluster seens very similar "},{"metadata":{},"cell_type":"markdown","source":"# Clusters by the Hours\nI was wondering and I had an insight that I will try to implement here. \n- I think that make a lot of sense explore the hours by the clusters\n- Let's see the distribution of PCA0 and the Clusters by the Hours"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df_train.sample(500000), col=\"City\", \n                  col_wrap=2, height=4, aspect=1.5, \n                  hue='clusters_T')\n\ng.map(sns.scatterplot, \"Hour\", \"Target_PCA0\", \n      alpha=.5).add_legend();\ng.set_titles('{col_name}', fontsize=50)\n\nplt.suptitle(\"CITIES \\nPrincipal Component Analysis Dispersion by HOURS AND CLUSTERS\", fontsize=22)\n\nplt.subplots_adjust(hspace = 0.3, top=.85)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool! We can have a best intuition about the data and how it posible clustered the data. "},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"round(pd.crosstab([df_train['clusters_T'], df_train['Weekend']], df_train['City'],\n            normalize='index' ) * 100,0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Modeling \n- As I was getting problems with my model, I decided to implement the solution of the public kernels\n- I will import the datasets again \n\nMany parts of this implementation I got on @dcaichara Kernel. <br>\nYou can see the kernel here: https://www.kaggle.com/dcaichara/feature-engineering-and-lightgbm"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/train.csv')\ndf_test = pd.read_csv('/kaggle/input/bigquery-geotab-intersection-congestion/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hour Feature \n- Let's encode the Hour Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def date_cyc_enc(df, col, max_vals):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_vals)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_vals)\n    return df\n\ndf_train = date_cyc_enc(df_train, 'Hour', 24)\ndf_test = date_cyc_enc(df_test, 'Hour', 24) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Flag - is day?\nTesting some features about the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['is_day'] = df_train['Hour'].apply(lambda x: 1 if 7 < x < 18 else 0)\ndf_test['is_day'] = df_test['Hour'].apply(lambda x: 1 if 7 < x < 18 else 0)\n\ndf_train['is_morning'] = df_train['Hour'].apply(lambda x: 1 if 6 < x < 10 else 0)\ndf_test['is_morning'] = df_test['Hour'].apply(lambda x: 1 if 6 < x < 10 else 0)\n\ndf_train['is_night'] = df_train['Hour'].apply(lambda x: 1 if 17 < x < 20 else 0)\ndf_test['is_night'] = df_test['Hour'].apply(lambda x: 1 if 17 < x < 20 else 0)\n\ndf_train['is_day_weekend'] = np.where((df_train['is_day'] == 1) & (df_train['Weekend'] == 1), 1,0)\ndf_test['is_day_weekend'] = np.where((df_test['is_day'] == 1) & (df_train['Weekend'] == 1), 1,0)\n\ndf_train['is_mor_weekend'] = np.where((df_train['is_morning'] == 1) & (df_train['Weekend'] == 1), 1,0)\ndf_test['is_mor_weekend'] = np.where((df_test['is_morning'] == 1) & (df_train['Weekend'] == 1), 1,0)\n\ndf_train['is_nig_weekend'] = np.where((df_train['is_night'] == 1) & (df_train['Weekend'] == 1), 1,0)\ndf_test['is_nig_weekend'] = np.where((df_test['is_night'] == 1) & (df_train['Weekend'] == 1), 1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Intersec - Concatenating IntersectionId and City"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"Intersec\"] = df_train[\"IntersectionId\"].astype(str) + df_train[\"City\"]\ndf_test[\"Intersec\"] = df_test[\"IntersectionId\"].astype(str) + df_test[\"City\"]\n\nprint(df_train[\"Intersec\"].sample(6).values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Label Encoder of Intersecion + City"},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\n\nle.fit(pd.concat([df_train[\"Intersec\"],df_test[\"Intersec\"]]).drop_duplicates().values)\ndf_train[\"Intersec\"] = le.transform(df_train[\"Intersec\"])\ndf_test[\"Intersec\"] = le.transform(df_test[\"Intersec\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Street Feature\n- Extracting informations from street features"},{"metadata":{"trusted":true},"cell_type":"code","source":"road_encoding = {\n    'Road': 1,\n    'Street': 2,\n    'Avenue': 2,\n    'Drive': 3,\n    'Broad': 3,\n    'Boulevard': 4\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode(x):\n    if pd.isna(x):\n        return 0\n    for road in road_encoding.keys():\n        if road in x:\n            return road_encoding[road]\n    return 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating the new feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['EntryType'] = df_train['EntryStreetName'].apply(encode)\ndf_train['ExitType'] = df_train['ExitStreetName'].apply(encode)\ndf_test['EntryType'] = df_test['EntryStreetName'].apply(encode)\ndf_test['ExitType'] = df_test['ExitStreetName'].apply(encode)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Encoding the Regions"},{"metadata":{"trusted":true},"cell_type":"code","source":"directions = {\n    'N': 0,\n    'NE': 1/4,\n    'E': 1/2,\n    'SE': 3/4,\n    'S': 1,\n    'SW': 5/4,\n    'W': 3/2,\n    'NW': 7/4\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying the transformation in Entry and Exit Heading Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['EntryHeading'] = df_train['EntryHeading'].map(directions)\ndf_train['ExitHeading'] = df_train['ExitHeading'].map(directions)\n\ndf_test['EntryHeading'] = df_test['EntryHeading'].map(directions)\ndf_test['ExitHeading'] = df_test['ExitHeading'].map(directions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Difference between the regions"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['diffHeading'] = df_train['EntryHeading']-df_train['ExitHeading']  \ndf_test['diffHeading'] = df_test['EntryHeading']-df_test['ExitHeading'] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting the binary if the entry and exit was in the same street"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"same_str\"] = (df_train[\"EntryStreetName\"] ==  df_train[\"ExitStreetName\"]).astype(int)\ndf_test[\"same_str\"] = (df_test[\"EntryStreetName\"] ==  df_test[\"ExitStreetName\"]).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## Concatenating City and Month"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenating the city and month into one variable\ndf_train['city_month'] = df_train[\"City\"] + df_train[\"Month\"].astype(str)\ndf_test['city_month'] = df_test[\"City\"] + df_test[\"Month\"].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Month rainfall ratio by city and seasons"},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_rainfall = {'Atlanta1': 5.02, 'Atlanta5': 3.95, 'Atlanta6': 3.63, 'Atlanta7': 5.12, \n                    'Atlanta8': 3.67, 'Atlanta9': 4.09,'Atlanta10': 3.11, 'Atlanta11': 4.10, \n                    'Atlanta12': 3.82, 'Boston1': 3.92, 'Boston5': 3.24, 'Boston6': 3.22,\n                    'Boston7': 3.06, 'Boston8': 3.37, 'Boston9': 3.47, 'Boston10': 3.79,\n                    'Boston11': 3.98, 'Boston12': 3.73, 'Chicago1': 1.75, 'Chicago5': 3.38,\n                    'Chicago6': 3.63, 'Chicago7': 3.51, 'Chicago8': 4.62, 'Chicago9': 3.27,\n                    'Chicago10': 2.71,  'Chicago11': 3.01, 'Chicago12': 2.43, \n                    'Philadelphia1': 3.52, 'Philadelphia5': 3.88, 'Philadelphia6': 3.29,\n                    'Philadelphia7': 4.39, 'Philadelphia8': 3.82, 'Philadelphia9':3.88 ,\n                    'Philadelphia10': 2.75, 'Philadelphia11': 3.16, 'Philadelphia12': 3.31}\n\n# Creating a new column by mapping the city_month variable to it's corresponding average monthly rainfall\ndf_train[\"average_rainfall\"] = df_train['city_month'].map(monthly_rainfall)\ndf_test[\"average_rainfall\"] = df_test['city_month'].map(monthly_rainfall)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Getting Dummies "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Shape before dummy transformation: {df_train.shape}')\ndf_train = pd.get_dummies(df_train, columns=['City' ],\\\n                          prefix=['City'], drop_first=False)\n\nprint(f'Shape after dummy transformation: {df_train.shape}')\n\ndf_test = pd.get_dummies(df_test, columns=['City' ],\\\n                          prefix=['City'], drop_first=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MinMax Scaling the lat and long"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nfor col in ['Latitude','Longitude']:\n    scaler.fit(df_train[col].values.reshape(-1, 1))\n    df_train[col] = scaler.transform(df_train[col].values.reshape(-1, 1))\n    df_test[col] = scaler.transform(df_test[col].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dropping not used features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(['RowId', 'Path','EntryStreetName','ExitStreetName'\n              ],axis=1, inplace=True)\ndf_test.drop(['RowId', 'Path',\n              'EntryStreetName','ExitStreetName'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interesting_feat = ['IntersectionId', 'Latitude', 'Longitude', 'EntryHeading',\n                    'ExitHeading', 'Hour', 'Weekend', 'Month',\n                    'is_morning', 'is_night', 'is_day_weekend', 'is_mor_weekend',\n                    'is_nig_weekend', #  'Hour_sin',\n                    'Hour', 'same_str', 'Intersec', 'EntryType',\n                    'ExitType', 'diffHeading', 'average_rainfall', 'is_day',\n                    'City_Boston', 'City_Chicago', 'City_Philadelphia', \n                    'City_Atlanta']\n\ntotal_time = ['TotalTimeStopped_p20',\n              'TotalTimeStopped_p50', \n              'TotalTimeStopped_p80']\n\ntarget_stopped = ['DistanceToFirstStop_p20',\n                  'DistanceToFirstStop_p50',\n                  'DistanceToFirstStop_p80']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting X and y"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_train[interesting_feat]\ny = df_train[total_time + target_stopped]\n\nX_test = df_test[interesting_feat]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Shape of X: {X.shape}')\nprint(f'Shape of X_test: {X_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reduce memory usage"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"X = reduce_mem_usage(X)\nX_test = reduce_mem_usage(X_test)"},{"metadata":{},"cell_type":"markdown","source":"## Spliting data into train and validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10,\n                                                  random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperopt Space\n- Here we will set all range of our hyperparameters\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define searched space\nhyper_space = {'objective': 'regression',\n               'metric':'rmse',\n               'boosting':'gbdt', 'gpu_device_id': 0,\n               #'n_estimators': hp.choice('n_estimators', [25, 40, 50, 75, 100, 250, 500]),\n               'max_depth':  hp.choice('max_depth', list(range(6, 18, 2))),\n               'num_leaves': hp.choice('num_leaves', list(range(20, 180, 20))),\n               'subsample': hp.choice('subsample', [.7, .8, .9, 1]),\n               'colsample_bytree': hp.uniform('colsample_bytree', 0.7, 1),\n               'learning_rate': hp.uniform('learning_rate', 0.03, 0.12),\n               #'reg_alpha': hp.choice('reg_alpha', [.1, .2, .3, .4, .5, .6]),\n               #'reg_lambda':  hp.choice('reg_lambda', [.1, .2, .3, .4, .5, .6]),               \n               'min_child_samples': hp.choice('min_child_samples', [20, 45, 70, 100])}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Hyperopt Function to be optimized"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feat = ['IntersectionId','Hour', 'Weekend','Month', \n            'is_day', 'is_morning', 'is_night', \n            'same_str', 'Intersec', 'City_Atlanta', 'City_Boston',\n            'City_Chicago', 'City_Philadelphia', 'EntryType', 'ExitType']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\n\ndef evaluate_metric(params):\n    \n    all_preds_test ={0:[],1:[],2:[],3:[],4:[],5:[]}\n    \n    print(f'Params: {params}')\n    FOLDS = 4    \n    \n    count=1\n    \n    for i in range(len(all_preds_test)):\n        \n        score_mean = 0\n        \n        kf = KFold(n_splits=FOLDS, shuffle=False, \n                   random_state=42)\n        \n       \n        for tr_idx, val_idx in kf.split(X, y):\n            \n            X_tr, X_vl = X.iloc[tr_idx, :], X.iloc[val_idx, :]\n            y_tr, y_vl = y.iloc[tr_idx], y.iloc[val_idx]\n\n            lgtrain = lgb.Dataset(X_tr, label=y_tr.iloc[:,i])\n            lgval = lgb.Dataset(X_vl, label=y_vl.iloc[:,i])\n\n            lgbm_reg = lgb.train(params, lgtrain, 2000, valid_sets = [lgval],\n                                 categorical_feature=cat_feat,\n                                 verbose_eval=0, \n                                 early_stopping_rounds = 300)\n                        \n        pred_lgb = lgbm_reg.predict(X_val, num_iteration=lgbm_reg.best_iteration)\n        all_preds_test[i] = pred_lgb\n        score_uni = np.sqrt(mean_squared_error(pred_lgb, y_val.iloc[:,i]))\n        print(f'Score Validation : {score_uni}')\n\n\n    pred = pd.DataFrame(all_preds_test).stack()\n    pred = pd.DataFrame(pred)\n    \n    y_val_sc = pd.DataFrame(y_val).stack()\n    y_val_sc = pd.DataFrame(y_val_sc)    \n    \n    count = count +1\n    \n    score = np.sqrt(mean_squared_error(pred[0].values, y_val_sc[0].values ))\n    #score = metric(df_val, pred)\n    \n    print(f'Full Score Run: {score}')\n \n    return {\n        'loss': score,\n        'status': STATUS_OK\n    }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Running the hyperopt Function"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Seting the number of Evals\nMAX_EVALS= 15\n\n# Fit Tree Parzen Estimator\nbest_vals = fmin(evaluate_metric, \n                 space=hyper_space,\n                 verbose=-1,\n                 algo=tpe.suggest, \n                 max_evals=MAX_EVALS)\n\n# Print best parameters\nbest_params = space_eval(hyper_space, best_vals)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_preds ={0:[],1:[],2:[],3:[],4:[],5:[]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\nimport lightgbm as lgb\n\nfor i in range(len(all_preds)):\n    print(f'## {i+1} Run')\n    X_tr,X_val,y_tr,y_val=train_test_split(X, y.iloc[:,i],\n                                           test_size=0.10, random_state=31)\n\n    xg_train = lgb.Dataset(X_tr, label = y_tr)\n    xg_valid = lgb.Dataset(X_val, label = y_val )\n    \n    lgbm_reg = lgb.train(best_params, xg_train, 10000,\n                      valid_sets = [xg_valid],\n                      verbose_eval=500, \n                      early_stopping_rounds = 250)\n    \n    all_preds[i] = lgbm_reg.predict(X_test, num_iteration=lgbm_reg.best_iteration)\n    \n    print(f\"{i+1} running done.\" )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing submission file\n- stacking all results in the same file"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub  = pd.read_csv(\"../input/bigquery-geotab-intersection-congestion/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = pd.DataFrame(all_preds).stack()\ndt = pd.DataFrame(dt)\nsub['Target'] = dt[0].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"lgbm_pred_hyperopt_test.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"Most part of the first modeling try I got from @danofer<br>\nPlase, visit the kernel with all work here: https://www.kaggle.com/danofer/baseline-feature-engineering-geotab-69-5-lb\n<br>\nThe Catboost model I got from @rohitpatil kernel, Link: https://www.kaggle.com/rohitpatil/geotab-catboost<br>\nSome ideas of modelling I saw on: https://www.kaggle.com/dcaichara/feature-engineering-and-lightgbm\n"},{"metadata":{},"cell_type":"markdown","source":"# NOTE: This Kernel is not finished. \n# Please stay tuned and votes up the kernel, please!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}