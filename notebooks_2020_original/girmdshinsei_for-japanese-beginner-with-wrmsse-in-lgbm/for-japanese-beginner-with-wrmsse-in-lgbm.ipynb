{"cells":[{"metadata":{},"cell_type":"markdown","source":"I have updated this notebook to modify the wrmsse function  at 29th Mar.  \nNew wrmsse function for LGBM metric calculate wrmsse only for last 28 days to consider non-zero demand period.  \nPlease refer comment section. I have commented the detail of my fixing.\n(note:I have also remove some variable to reduce the run-time and changed 'objective' in lgbm to 'poisson'.)\n\nThis kernel is:  \n- Based on [Very fst Model](https://www.kaggle.com/ragnar123/very-fst-model). Thanks [@ragnar123](https://www.kaggle.com/ragnar123).  \n- Based on [m5-baseline](https://www.kaggle.com/harupy/m5-baseline). Thank [@harupy](https://www.kaggle.com/harupy).  \nto explain the detail of these great notebook by Japanese especially for beginner.  \n\nAdditionaly, I have added an relatively efficient evaluation of WRSSE for LGBM metric to these kernel."},{"metadata":{},"cell_type":"markdown","source":"## module import"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport lightgbm as lgb\n#import dask_xgboost as xgb\n#import dask.dataframe as dd\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport os\nfrom tqdm import tqdm\nfrom scipy.sparse import csr_matrix\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## functionの定義"},{"metadata":{},"cell_type":"markdown","source":"reduce_mem_usageは、データのメモリを減らすためにデータ型を変更する関数です。  \n('reduce_mem_usage' is a functin which reduce memory usage by changing data type.)\nhttps://qiita.com/hiroyuki_kageyama/items/02865616811022f79754　を参照ください。"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns: #columns毎に処理\n        col_type = df[col].dtypes\n        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"read_dataはデータの読み込みと, reduce_mem_usageの適用を行う関数  \n('read data' is a function to read the files and apply the 'reduce_mem_usage'.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data():\n    print('Reading files...')\n    calendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\n    calendar = reduce_mem_usage(calendar)\n    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n    \n    sell_prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\n    sell_prices = reduce_mem_usage(sell_prices)\n    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n    \n    sales_train_val = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\n    print('Sales train validation has {} rows and {} columns'.format(sales_train_val.shape[0], sales_train_val.shape[1]))\n    \n    submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\n    \n    return calendar, sell_prices, sales_train_val, submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PandasのdataFrameをきれいに表示する関数\n(This function is to diplay a head of Pandas DataFrame.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import IPython\n\ndef display(*dfs, head=True):\n    for df in dfs:\n        IPython.display.display(df.head() if head else df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar, sell_prices, sales_train_val, submission = read_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 予測期間とitem数の定義 / number of items, and number of prediction period\nNUM_ITEMS = sales_train_val.shape[0]  # 30490\nDAYS_PRED = submission.shape[1] - 1  # 28","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## data加工 (data transform)"},{"metadata":{},"cell_type":"markdown","source":"### 1.最初にカテゴリ変数の処理(categorical variable)"},{"metadata":{},"cell_type":"markdown","source":"As [@kaushal2896](https://www.kaggle.com/kaushal2896) suggested in [this comment](https://www.kaggle.com/harupy/m5-baseline#770558), encode the categorical columns before merging to prevent the notebook from crashing even with the full dataset. [@harupy](https://www.kaggle.com/harupy) also use this encoding suggested in [m5-baseline](https://www.kaggle.com/harupy/m5-baseline).  \nメモリの効率利用のため, カテゴリ変数をあらかじめLabel encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_categorical(df, cols):\n    \n    for col in cols:\n        # Leave NaN as it is.\n        le = LabelEncoder()\n        #not_null = df[col][df[col].notnull()]\n        df[col] = df[col].fillna('nan')\n        df[col] = pd.Series(le.fit_transform(df[col]), index=df.index)\n\n    return df\n\n\ncalendar = encode_categorical(\n    calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n).pipe(reduce_mem_usage)\n\nsales_train_val = encode_categorical(\n    sales_train_val, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n).pipe(reduce_mem_usage)\n\nsell_prices = encode_categorical(sell_prices, [\"item_id\", \"store_id\"]).pipe(\n    reduce_mem_usage\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sales_train_valからidの詳細部分(itemやdepartmentなどのid)を重複なく一意に取得しておく。(extract a detail of id columns)\nproduct = sales_train_val[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.sales_train_validationのmelt処理(apply melt to sales_train_validation)  \n（時系列の特徴量が作りやすいように, id毎に横に並んだ時系列データを、（id , 時系列）で縦に変換）  \n(apply melt to sales_train_validation(time series) to make it easier to treat.)"},{"metadata":{},"cell_type":"markdown","source":"pandasのmeltを使いdemand(売上数量)を縦に並べる.  \n* pandasのmeltは https://qiita.com/ishida330/items/922caa7acb73c1540e28　を参照ください。\n* dataの行数が莫大になるので, Kaggle Notebookのmemory制限を考慮し、nrowsで直近365*2日分（2年分）のデータに限定（TODO:環境に応じて期間を変更）"},{"metadata":{"trusted":true},"cell_type":"code","source":"nrows = 365 * 3 * NUM_ITEMS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#加工前  \ndisplay(sales_train_val.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"to remove data before first non-zero demand date, replace these demand as np.nan."},{"metadata":{"trusted":true},"cell_type":"code","source":"d_name = ['d_' + str(i+1) for i in range(1913)]\nsales_train_val_values = sales_train_val[d_name].values\n\n# calculate the start position(first non-zero demand observed date) for each item / 商品の最初の売上日\n# 1-1914のdayの数列のうち, 売上が存在しない日を一旦0にし、0を9999に置換。そのうえでminimum numberを計算\ntmp = np.tile(np.arange(1,1914),(sales_train_val_values.shape[0],1))\ndf_tmp = ((sales_train_val_values>0) * tmp)\n\nstart_no = np.min(np.where(df_tmp==0,9999,df_tmp),axis=1)-1\n\nflag = np.dot(np.diag(1/(start_no+1)) , tmp)<1\n\nsales_train_val_values = np.where(flag,np.nan,sales_train_val_values)\n\nsales_train_val[d_name] = sales_train_val_values\n\ndel tmp,sales_train_val_values\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"calculate number of period after first non-zero demand date"},{"metadata":{"trusted":true},"cell_type":"code","source":"1913-np.max(start_no)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val = pd.melt(sales_train_val,\n                                     id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                                     var_name = 'day', value_name = 'demand')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#加工後  \ndisplay(sales_train_val.head(5))\nprint('Melted sales train validation has {} rows and {} columns'.format(sales_train_val.shape[0],\n                                                                            sales_train_val.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val = sales_train_val.iloc[-nrows:,:]\nsales_train_val = sales_train_val[~sales_train_val.demand.isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1と同様に予測部分(validation/evaluation部分)のmelt処理し, 学習データと結合する. 出力はdataという変数."},{"metadata":{},"cell_type":"markdown","source":"予測部分のsubmission fileを同じくmelt処理し、sales_train_valとつなげる。  \n処理の注意点:  \n* submission fileの列名を\"d_xx\"形式に変更する. submission fileで縦に結合されたvalidationとevaluationを一度分割し、それぞれことなる28日間の列名\"d_xx\"をそれぞれ付与。\n* submission fileには, idの詳細（item, department, state等）が無いためidをキーに, sales validationから取得したproductを結合\n* test2は、6/1まで不要なため削除"},{"metadata":{"trusted":true},"cell_type":"code","source":"# seperate test dataframes\n\n# submission fileのidのvalidation部分と, ealuation部分の名前を取得\ntest1_rows = [row for row in submission['id'] if 'validation' in row]\ntest2_rows = [row for row in submission['id'] if 'evaluation' in row]\n\n# submission fileのvalidation部分をtest1, ealuation部分をtest2として取得\ntest1 = submission[submission['id'].isin(test1_rows)]\ntest2 = submission[submission['id'].isin(test2_rows)]\n\n# test1, test2の列名の\"F_X\"の箇所をd_XXX\"の形式に変更\ntest1.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + DAYS_PRED)]\ntest2.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n\n# test2のidの'_evaluation'を置換\n#test1['id'] = test1['id'].str.replace('_validation','')\ntest2['id'] = test2['id'].str.replace('_evaluation','_validation')\n\n\n# idをキーにして, idの詳細部分をtest1, test2に結合する.\ntest1 = test1.merge(product, how = 'left', on = 'id')\ntest2 = test2.merge(product, how = 'left', on = 'id')\n\n# test1, test2をともにmelt処理する.（売上数量:demandは0）\ntest1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                var_name = 'day', value_name = 'demand')\n\ntest2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                var_name = 'day', value_name = 'demand')\n\n# validation部分と, evaluation部分がわかるようにpartという列を作り、 test1,test2のラベルを付ける。\nsales_train_val['part'] = 'train'\ntest1['part'] = 'test1'\ntest2['part'] = 'test2'\n\n# sales_train_valとtest1, test2の縦結合.\ndata = pd.concat([sales_train_val, test1, test2], axis = 0)\n\n# memoryの開放\ndel sales_train_val, test1, test2\n\n# delete test2 for now(6/1以前は, validation部分のみ提出のため.)\ndata = data[data['part'] != 'test2']\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 4.dataにcalendar/sell_pricesを結合"},{"metadata":{"trusted":true},"cell_type":"code","source":"#calendarの結合\n# drop some calendar features(不要な変数の削除:weekdayやwdayなどはdatetime変数から後ほど作成できる。)\ncalendar.drop(['weekday', 'wday', 'month', 'year'], \n              inplace = True, axis = 1)\n\n# notebook crash with the entire dataset (maybee use tensorflow, dask, pyspark xD)(dayとdをキーにdataに結合)\ndata = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\ndata.drop(['d', 'day'], inplace = True, axis = 1)\n\n# memoryの開放\ndel  calendar\ngc.collect()\n\n#sell priceの結合\n# get the sell price data (this feature should be very important)\ndata = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\nprint('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n\n# memoryの開放\ndel  sell_prices\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 5 dataから特徴量生成\n* groupby & transofrmの変換方法はこちらを参照:https://qiita.com/greenteabiscuit/items/132e0f9b1479926e07e0\n* shift/rollingなどの役割はこちらを参照:https://note.nkmk.me/python-pandas-rolling/ (ここでmeltがうまく効きます。)\n　ラグ変数や過去の平均値などの特徴量が生成できる。\n* 変数は, すべてlagを28以上にして, F1~F28の予測を1つのモデルで表現するのが目的。\n* TODO：特徴量の生成方法は色々変更可能. ShiftやRollingの値の変更などなど"},{"metadata":{"trusted":true},"cell_type":"code","source":"def simple_fe(data):\n    \n    # demand features(過去の数量から変数生成)\n    \n    for diff in [0, 1, 2]:\n        shift = DAYS_PRED + diff\n        data[f\"shift_t{shift}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(shift)\n        )\n    '''\n    for size in [7, 30, 60, 90, 180]:\n        data[f\"rolling_std_t{size}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(size).std()\n        )\n    '''\n    for size in [7, 30, 60, 90, 180]:\n        data[f\"rolling_mean_t{size}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(size).mean()\n        )\n    '''\n    data[\"rolling_skew_t30\"] = data.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(30).skew()\n    )\n    data[\"rolling_kurt_t30\"] = data.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(30).kurt()\n    )\n    '''\n    # price features\n    # priceの動きと特徴量化（価格の変化率、過去1年間の最大価格との比など）\n    \n    data[\"shift_price_t1\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.shift(1)\n    )\n    data[\"price_change_t1\"] = (data[\"shift_price_t1\"] - data[\"sell_price\"]) / (\n        data[\"shift_price_t1\"]\n    )\n    data[\"rolling_price_max_t365\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.shift(1).rolling(365).max()\n    )\n    data[\"price_change_t365\"] = (data[\"rolling_price_max_t365\"] - data[\"sell_price\"]) / (\n        data[\"rolling_price_max_t365\"]\n    )\n\n    data[\"rolling_price_std_t7\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(7).std()\n    )\n    data[\"rolling_price_std_t30\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(30).std()\n    )\n    \n    # time features\n    # 日付に関するデータ\n    dt_col = \"date\"\n    data[dt_col] = pd.to_datetime(data[dt_col])\n    \n    attrs = [\n        \"year\",\n        \"quarter\",\n        \"month\",\n        \"week\",\n        \"day\",\n        \"dayofweek\",\n        \"is_year_end\",\n        \"is_year_start\",\n        \"is_quarter_end\",\n        \"is_quarter_start\",\n        \"is_month_end\",\n        \"is_month_start\",\n    ]\n\n    for attr in attrs:\n        dtype = np.int16 if attr == \"year\" else np.int8\n        data[attr] = getattr(data[dt_col].dt, attr).astype(dtype)\n\n    data[\"is_weekend\"] = data[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n    \n    return data\n\ndata = simple_fe(data)\ndata = reduce_mem_usage(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## train/testの分割とmodelの推定"},{"metadata":{},"cell_type":"markdown","source":"2016/3/27より前を学習用、2016/3/27~2016/4/24（28day）を検証用として分割  \n（LightGBMのEarly stoppingの対象）\n* 交差検証の方法はいろいろと検討余地あり。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# going to evaluate with the last 28 days\nx_train = data[data['date'] <= '2016-03-27']\ny_train = x_train['demand']\nx_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\ny_val = x_val['demand']\ntest = data[(data['date'] > '2016-04-24')]\n\n#dataの削除（メモリの削除）\n#del data\n#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"modelのLGBMでの推定　　\n* early stoppingのmetricに全体のRMSEを使っているため, コンペの指標のWRMSSEとは異なる."},{"metadata":{"trusted":true},"cell_type":"code","source":"# define random hyperparammeters for LGBM\nfeatures = [\n    \"item_id\",\n    \"dept_id\",\n    \"cat_id\",\n    \"store_id\",\n    \"state_id\",\n    \"event_name_1\",\n    \"event_type_1\",\n    \"event_name_2\",\n    \"event_type_2\",\n    \"snap_CA\",\n    \"snap_TX\",\n    \"snap_WI\",\n    \"sell_price\",\n    # demand features.\n    \"shift_t28\",\n    \"shift_t29\",\n    \"shift_t30\",\n    \"rolling_mean_t7\",\n    \"rolling_mean_t30\",\n    \"rolling_mean_t60\",\n    \"rolling_mean_t90\",\n    \"rolling_mean_t180\",\n    # price features\n    \"price_change_t1\",\n    \"price_change_t365\",\n    \"rolling_price_std_t7\",\n    \"rolling_price_std_t30\",\n    # time features.\n    \"year\",\n    \"month\",\n    \"week\",\n    \"day\",\n    \"dayofweek\",\n    \"is_year_end\",\n    \"is_year_start\",\n    \"is_quarter_end\",\n    \"is_quarter_start\",\n    \"is_month_end\",\n    \"is_month_start\",\n    \"is_weekend\",\n]\n\nparams = {\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'objective': 'regression',\n    'n_jobs': -1,\n    'seed': 236,\n    'learning_rate': 0.1,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 10, \n    'colsample_bytree': 0.75}\n\ntrain_set = lgb.Dataset(x_train[features], y_train)\nval_set = lgb.Dataset(x_val[features], y_val)\n\ndel x_train, y_train\n\n'''\n# model estimation\nmodel = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, valid_sets = [train_set, val_set], verbose_eval = 100)\nval_pred = model.predict(x_val[features])\nval_score = np.sqrt(metrics.mean_squared_error(val_pred, y_val))\nprint(f'Our val rmse score is {val_score}')\ny_pred = model.predict(test[features])\ntest['demand'] = y_pred\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## submission fileの出力"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\npredictions = test[['id', 'date', 'demand']]\npredictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\npredictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\nevaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \nevaluation = submission[submission['id'].isin(evaluation_rows)]\n\nvalidation = submission[['id']].merge(predictions, on = 'id')\nfinal = pd.concat([validation, evaluation])\nfinal.to_csv('submission.csv', index = False)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## WRMSSE calculation"},{"metadata":{},"cell_type":"markdown","source":"LightGBMのMetricとして, WRMSSEの効率的な計算を行う。あくまで, 28day-lagで1つのモデルの予測するときにLGBMで効率的なWRMSSEの計算を行う場合である。\n* weight_matという0 or 1の疎行列で、効率的にaggregation levelを行列積で計算出来るようにしている\n* LightGBMのMetricを効率的に計算するためにGroupby fucntionを使うことを避けているが、そのため、non-rezo demandのデータを除くと効率的な計算ができない。そのためすべてのitemでnon-zero demand dataとなっている最後の28日分のみで検証するコードとなっている.\n* Sparce matrixは順序がProductのItem通りになっていないといけないので注意。"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_mat = np.c_[np.ones([NUM_ITEMS,1]).astype(np.int8), # level 1\n                   pd.get_dummies(product.state_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.store_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.store_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.store_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.item_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.item_id.astype(str),drop_first=False).astype('int8').values,\n                   np.identity(NUM_ITEMS).astype(np.int8) #item :level 12\n                   ].T\n\nweight_mat_csr = csr_matrix(weight_mat)\ndel weight_mat; gc.collect()\n\ndef weight_calc(data,product):\n    \n    # calculate the denominator of RMSSE, and calculate the weight base on sales amount\n\n    sales_train_val = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\n\n    d_name = ['d_' + str(i+1) for i in range(1913)]\n\n    sales_train_val = weight_mat_csr * sales_train_val[d_name].values\n\n    # calculate the start position(first non-zero demand observed date) for each item / 商品の最初の売上日\n    # 1-1914のdayの数列のうち, 売上が存在しない日を一旦0にし、0を9999に置換。そのうえでminimum numberを計算\n    df_tmp = ((sales_train_val>0) * np.tile(np.arange(1,1914),(weight_mat_csr.shape[0],1)))\n\n    start_no = np.min(np.where(df_tmp==0,9999,df_tmp),axis=1)-1\n\n    flag = np.dot(np.diag(1/(start_no+1)) , np.tile(np.arange(1,1914),(weight_mat_csr.shape[0],1)))<1\n\n    sales_train_val = np.where(flag,np.nan,sales_train_val)\n\n    # denominator of RMSSE / RMSSEの分母\n    weight1 = np.nansum(np.diff(sales_train_val,axis=1)**2,axis=1)/(1913-start_no)\n\n    # calculate the sales amount for each item/level\n    df_tmp = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n    df_tmp['amount'] = df_tmp['demand'] * df_tmp['sell_price']\n    df_tmp =df_tmp.groupby(['id'])['amount'].apply(np.sum)\n    df_tmp = df_tmp[product.id].values\n    \n    weight2 = weight_mat_csr * df_tmp \n\n    weight2 = weight2/np.sum(weight2)\n\n    del sales_train_val\n    gc.collect()\n    \n    return weight1, weight2\n\nweight1, weight2 = weight_calc(data,product)\n\ndef wrmsse(preds, data):\n    \n    # this function is calculate for last 28 days to consider the non-zero demand period\n    \n    # actual obserbed values / 正解ラベル\n    y_true = data.get_label()\n    \n    y_true = y_true[-(NUM_ITEMS * DAYS_PRED):]\n    preds = preds[-(NUM_ITEMS * DAYS_PRED):]\n    # number of columns\n    num_col = DAYS_PRED\n    \n    # reshape data to original array((NUM_ITEMS*num_col,1)->(NUM_ITEMS, num_col) ) / 推論の結果が 1 次元の配列になっているので直す\n    reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n    reshaped_true = y_true.reshape(num_col, NUM_ITEMS).T\n    \n          \n    train = weight_mat_csr*np.c_[reshaped_preds, reshaped_true]\n    \n    score = np.sum(\n                np.sqrt(\n                    np.mean(\n                        np.square(\n                            train[:,:num_col] - train[:,num_col:])\n                        ,axis=1) / weight1) * weight2)\n    \n    return 'wrmsse', score, False\n\ndef wrmsse_simple(preds, data):\n    \n    # actual obserbed values / 正解ラベル\n    y_true = data.get_label()\n    \n    y_true = y_true[-(NUM_ITEMS * DAYS_PRED):]\n    preds = preds[-(NUM_ITEMS * DAYS_PRED):]\n    # number of columns\n    num_col = DAYS_PRED\n    \n    # reshape data to original array((NUM_ITEMS*num_col,1)->(NUM_ITEMS, num_col) ) / 推論の結果が 1 次元の配列になっているので直す\n    reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n    reshaped_true = y_true.reshape(num_col, NUM_ITEMS).T\n          \n    train = np.c_[reshaped_preds, reshaped_true]\n    \n    weight2_2 = weight2[:NUM_ITEMS]\n    weight2_2 = weight2_2/np.sum(weight2_2)\n    \n    score = np.sum(\n                np.sqrt(\n                    np.mean(\n                        np.square(\n                            train[:,:num_col] - train[:,num_col:])\n                        ,axis=1) /  weight1[:NUM_ITEMS])*weight2_2)\n    \n    return 'wrmsse', score, False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'boosting_type': 'gbdt',\n    'metric': 'custom',\n    'objective': 'poisson',\n    'n_jobs': -1,\n    'seed': 236,\n    'learning_rate': 0.1,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 10, \n    'colsample_bytree': 0.75}\n\n# model estimation\nmodel = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, \n                  valid_sets = [train_set, val_set], verbose_eval = 100, feval= wrmsse)\nval_pred = model.predict(x_val[features])\nval_score = np.sqrt(metrics.mean_squared_error(val_pred, y_val))\nprint(f'Our val wrmsse score is {val_score}')\ny_pred = model.predict(test[features])\ntest['demand'] = y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = test[['id', 'date', 'demand']]\npredictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\npredictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\nevaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \nevaluation = submission[submission['id'].isin(evaluation_rows)]\n\nvalidation = submission[['id']].merge(predictions, on = 'id')\nfinal = pd.concat([validation, evaluation])\nfinal.to_csv('submission2.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}