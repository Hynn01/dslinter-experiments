{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# regular expressions\nimport re \n\n# math and data utilities\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as ss\nimport itertools\n\n# data and statistics libraries\nimport sklearn.preprocessing as pre\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# visualization libraries\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\n\n# Set-up default visualization parameters\nmpl.rcParams['figure.figsize'] = [10,6]\nviz_dict = {\n    'axes.titlesize':18,\n    'axes.labelsize':16,\n}\nsns.set_context(\"notebook\", rc=viz_dict)\nsns.set_style(\"whitegrid\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initial Setup\nWe can download the data from Kaggle to our data folder using the command line:\n\n`kaggle competitions download -c titanic`\n\n`unzip titanic.zip`\n\nAfter that, let's get the data into some Pandas dataframes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/titanic/train.csv', index_col='PassengerId')\ntest_df = pd.read_csv('../input/titanic/test.csv', index_col='PassengerId')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis:\n\nOur next step will be to ask and answer the following questions:\n\n1. _Are we missing any data?_ \n2. _What form does our data take?_\n3. _What additional information can we garner from what we already have?_\n4. _What relationships can we find between our variables, especially between the input and output variables?_ \n5. _How can we use the answers to the first two question to add value to our data and the models that will use it?_\n\n\n## Question 1: Missing Data\nLet's take a look at the number of entries in our training data, as well as those variables contain significant missing data. Below, we see that the training data contains 891 passenger samples, with 11 total variables describing each passenger. We see that there is a significant amount of missing data for the variables __Age__ and __Cabin__. We will have to deal with this missing data by either finding an intelligent way to fill the gaps, or perhaps dropping the features entirely. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Question 1: Are we missing any data?\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question 2: What is the form of our data:\nTaking a look at our `.info()` print out as well as the first few entries of our data frame below, we see that our data comes primarily in the form of categorical data, with the exception of __Age__ and __Fare__. These categories are described by Python strings, which is why the data type above is listed as 'object'. This is how Pandas deals with unidentified data types. We will later tell Pandas that these variables are strings. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at the first few entries\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question 3: What additional information can we garner from what we already have?\n\n### Passenger Title\n \nA quick glance at the __Name__ variable shows us that each name comes with a title. A title is useful in telling us things like social status, marriage status career, and even rank within a specific career. Therefore, it may be useful to have this information on hand. Let's parse it out:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Title'] = train_df['Name'].str.extract(r'([A-Za-z]+)\\.')\ntrain_df.Title.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we might notice that many of these titles are synonymous. For example, _Mme_ is the French equivalent to  'Mrs' and _Mlle_ is the equivalent to  'Miss'. Other titles imply varying levels of nobility like 'Sir', 'Countess' and 'Don'. Some titles infer a profession. Let's reduce our titles to their common denominators:"},{"metadata":{"trusted":true},"cell_type":"code","source":"title_dict = {\n    'Mrs': 'Mrs', 'Lady': 'Lady', 'Countess': 'Lady',\n    'Jonkheer': 'Lord', 'Col': 'Officer', 'Rev': 'Rev',\n    'Miss': 'Miss', 'Mlle': 'Miss', 'Mme': 'Mrs', 'Ms': 'Miss', 'Dona': 'Lady',\n    'Mr': 'Mr', 'Dr': 'Dr', 'Major': 'Officer', 'Capt': 'Officer', 'Sir': 'Lord', 'Don': 'Lord', 'Master': 'Master'\n}\n\ntrain_df.Title = train_df.Title.map(title_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_df.Title).set_title(\"Histogram of Categorical Data: Title\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By again looking at our data set, we might notice the variables __SibSp__ and __Parch__. The first is the number of siblings and/or spouses that a passenger traveled with. The second is the number of parents and/or children a passenger traveled with. Combining these two variables we can get total family size. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['FamilySize'] = 1 + train_df.SibSp + train_df.Parch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_df.FamilySize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears that the Titanic's voyage was not necessarily a couple's or family affair. The majority of passengers traveled alone, and perhaps that is valuable information. Let's add the category __Alone__."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Alone'] = train_df.FamilySize.apply(lambda x: 1 if x==1 else 0)\nplt.figure(figsize=(8,5))\nsns.countplot(train_df.Alone)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Last Name\n\nA last name is a group identity. While we know that many passengers traveled alone, there were still a significant number of families onboard the Titanic. Perhaps survival among specific families was more common than others. This is all speculation, but perhaps worth a look."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['LName'] = train_df.Name.str.extract(r'([A-Za-z]+),')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Name Length\n\nThis one has a very simple explanation: While reviewing notebooks on Kaggle, I saw that one competitor found that the length of a person's name added to the performance of the model. So, why not try it out?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['NameLength'] = train_df.Name.apply(len)\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question 4: What statistical relationships does our data contain?\n\nWe now have a more robust data set that includes (possibly) valuable new insights into our passengers lives. But how helpful is this data, really? One way to find out is to look at the statistical relationships between our variables, especially between each input variable and our single output variable __Survived__. \n\nCorrelation is a common go-to tool we would use to determine such relationships. However, it is important to note that we have mostly categorical data in our data set, and that throws a small wrench in our gears. \n\nFirst, our categorical data needs to be encoded into numeric format before we can do calculations of any kind. \n\nNext, we need to consider the types categorical we are studying:\n\n- __Ordinal__ variables imply an underlying rank, or order. The classifications mild, moderate, severe would be an example. A common method of calculating the correlation is called _Kendall's Tau ($\\tau$)_. \n- __Nominal__ variables have no such rank or order. An example would be Male or Female. In this case we will use _Cramer's V_ correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nominal variables (use Cramer's V)\nnom_vars = ['Survived', 'Title', 'Embarked', 'Sex', 'Alone', 'LName']\n\n# ordinal variables (nominal-ordinal, use Rank Biserial or Kendall's Tau)\nord_vars = ['Survived', 'Pclass', 'FamilySize', 'Parch', 'SibSp', 'NameLength']\n\n# continuous variables (use Pearson's r)\ncont_vars = ['Survived', 'Fare', 'Age']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the cell above, we separate our variables by their data types. The reason for this is that when considering the underlying associations between variables, there is not a \"one size fits all\" method. The most common mathematical method of calculating correlation is _Pearson's r_, which should typically only be used on continuous variables. In our case, the vast majority of variables are actually discrete/categorical. \n\nIn order to perform calculations, we must convert any non numeric data into numbers. Let's get started:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert all string 'object' types to numeric categories\nfor i in train_df.columns:\n    if train_df[i].dtype == 'object':\n        train_df[i], _ = pd.factorize(train_df[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cramers_v_matrix(dataframe, variables):\n    \n    df = pd.DataFrame(index=dataframe[variables].columns, columns=dataframe[variables].columns, dtype=\"float64\")\n    \n    for v1, v2 in itertools.combinations(variables, 2):\n        \n        # generate contingency table:\n        table = pd.crosstab(dataframe[v1], dataframe[v2])\n        n     = len(dataframe.index)\n        r, k  = table.shape\n        \n        # calculate chi squared and phi\n        chi2  = ss.chi2_contingency(table)[0]\n        phi2  = chi2/n\n        \n        # bias corrections:\n        r = r - ((r - 1)**2)/(n - 1)\n        k = k - ((k - 1)**2)/(n - 1)\n        phi2 = max(0, phi2 - (k - 1)*(r - 1)/(n - 1))\n        \n        # fill correlation matrix\n        df.loc[v1, v2] = np.sqrt(phi2/min(k - 1, r - 1))\n        df.loc[v2, v1] = np.sqrt(phi2/min(k - 1, r - 1))\n        np.fill_diagonal(df.values, np.ones(len(df)))\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(20,6))\n\n# nominal variable correlation\nax1 = sns.heatmap(cramers_v_matrix(train_df, nom_vars), annot=True, ax=axes[0], vmin=0)\n\n# ordinal variable correlation: \nax2 = sns.heatmap(train_df[ord_vars].corr(method='kendall'), annot=True, ax=axes[1], vmin=-1)\n\n# Pearson's correlation:\nax3 = sns.heatmap(train_df[cont_vars].corr(), annot=True, ax=axes[2], vmin=-1)\n\nax1.set_title(\"Cramer's V Correlation\")\nax2.set_title(\"Kendall's Tau Correlation\")\nax3.set_title(\"Pearson's R Correlation\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above heatmaps show our strength of association between each variable. While there is no rigid standard for \"Highly Associated\" or \"Weakly Associated\", we will use a cut-off value of |0.1| between our independent variables and survival. We will likely drop features whose association is lower than |0.1|. This is an entirely arbitrary guess, and I may return to raise or lower the bar later. \n\nFor now, the features that meet the criteria for dropping are __SibSp__ and __Age__ (I actually end up leaving __Age__ in, as the model performed better with it). Additionally, I am choosing to drop __Name__, __Ticket__ and __Cabin__, mostly on a hunch that they don't add much. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"todrop = ['SibSp', 'Ticket', 'Cabin', 'Name']\ntrain_df = train_df.drop(todrop, axis=1)\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setup for Machine Learning:\n\nDuring this phase, we will begin to format our data for feeding into a machine learning algorithm. We will then use this formatted data to get a picture of what a few different models can do for us, and pick the best one. This phase is broken into the following parts:\n\n1. __Train/Test Split__\n2. __Normalize Data of each split__ \n3. __Impute missing values__ (I feel that __Cabin__ is a lost cause, as so many entries are missing, but we will look at __Age__.)\n\nLet's go.\n\n## Train/Test Split\n\nWe will split our data once into training and testing sets. Within the training set, we will use stratified k-fold cross validation to find average performance of our models. \n\nThe test set will not be touched until after we have fully tuned each of our candidate models using the training data and k-fold cross validation. Once training and tuning is complete, we will compare the results of each model on the held-out test set. The one that performs the best will be used for the competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_df.drop(['Survived'], axis = 1)\nY = train_df.loc[:, 'Survived']\n\nx_train, x_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=0.2, random_state=333)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalizing the Data\n\nSome Machine Learning models require all of our predictors to be on the same scale, while others do not. Most notably, models like Logistic Regression and SVM will probably benefit from scaling, while decision trees will simply ignore scaling. Because we are going to be looking at a mixed bag of algorithms, I'm going to go ahead and scale our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We normalize the training and testing data separately so as to avoid data leaks.\n\nx_train = pd.DataFrame(pre.scale(x_train), columns=x_train.columns, index=x_train.index)\nx_test = pd.DataFrame(pre.scale(x_test), columns=x_test.columns, index=x_test.index)\nx_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Imputing Missing Data\n\nYou might recall that there were a significant amount of missing __Age__ values in our data. Let's fill this in with the median age:"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.loc[x_train.Age.isnull(), 'Age'] = x_train.loc[:, 'Age'].median()\nx_test.loc[x_test.Age.isnull(), 'Age'] = x_test.loc[:, 'Age'].median()\nx_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, all of the data in our training `DataFrame` is non-null. Later we will have to repeat this process on the test data."},{"metadata":{},"cell_type":"markdown","source":"# Model Selection\n\nNow that we have prepared our data, we want to look at different options available to us for solving classification problems. Some common ones are:\n\n- K-Nearest Neighbors\n- Support Vector Machines\n- Decision Trees\n- Logistic Regression\n\nWe will train and tune each of these models on our training data by way of k-fold cross-validation. When complete, we will compare the tuned models' performance on a held out test set. \n\n## Training and Comparing Base Models:\n\nFirst, we want to get a feel model's performance before tuning. We will write two functions to help us describe our results. The first will evaluate the model several times over random splits in the data, and return the average performance as a dictionary. The second will simply nicely print our dictionary."},{"metadata":{"trusted":true},"cell_type":"code","source":"def kfold_evaluate(model, folds=5):\n    eval_dict = {}\n    accuracy = 0\n    f1       = 0\n    AUC      = 0\n    \n    skf = model_selection.StratifiedKFold(n_splits=folds)\n    \n    # perform k splits on the training data. Gather performance results.\n    for train_idx, test_idx in skf.split(x_train, y_train):\n        xk_train, xk_test = x_train.iloc[train_idx], x_train.iloc[test_idx]\n        yk_train, yk_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n    \n        model.fit(xk_train, yk_train)\n        y_pred = model.predict(xk_test)\n        report = metrics.classification_report(yk_test, y_pred, output_dict=True)\n        \n        prob_array = model.predict_proba(xk_test)\n    \n        fpr, tpr, huh = metrics.roc_curve(yk_test, model.predict_proba(xk_test)[:,1])\n        auc = metrics.auc(fpr, tpr)\n        accuracy   += report['accuracy']\n        f1         += report['macro avg']['f1-score']\n        AUC        += auc\n        \n    # Average performance metrics over the k folds\n    measures = np.array([accuracy, f1, AUC])\n    measures = measures/folds\n\n    # Add metric averages to dictionary and return.\n    eval_dict['Accuracy']  = measures[0]\n    eval_dict['F1 Score']  = measures[1]\n    eval_dict['AUC']       = measures[2]  \n    eval_dict['Model']     = model\n    \n    return eval_dict\n\n# a function to pretty print our dictionary of dictionaries:\ndef pprint(web, level):\n    for k,v in web.items():\n        if isinstance(v, dict):\n            print('\\t'*level, f'{k}: ')\n            level += 1\n            pprint(v, level)\n            level -= 1\n        else:\n            print('\\t'*level, k, \": \", v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evals = {}\nevals['KNN'] = kfold_evaluate(KNeighborsClassifier())\nevals['Logistic Regression'] = kfold_evaluate(LogisticRegression(max_iter=1000))\nevals['Random Forest'] = kfold_evaluate(RandomForestClassifier())\nevals['SVC'] = kfold_evaluate(SVC(probability=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df = pd.DataFrame(evals)\nresult_df.drop('Model', axis=0).plot(kind='bar', ylim=(0.7, 0.9)).set_title(\"Base Model Performance\")\nplt.xticks(rotation=0)\nplt.show()\nresult_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Base Model Summary\n\nIt appears that we have a clear winner in our Random Forest classifier.  \n\n## Hyper-parameter Tuning: \n\nLet's tune up our current champion's hyper-parameters in hopes of eking out a little bit more performance. We will use scikit-learn's `RandomizedSearchCV` which has some speed advantages over using an exhaustive `GridSearchCV`. Our first step is to create our grid of parameters over which we will randomly search for the best settings:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators, \n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\npprint(random_grid, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we want to create our `RandomizedSearchCV` object which will use the grid we just created above. It will randomly sample 10 combinations of parameters, test them over 3 folds and return the set of parameters that performed the best on our training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create RandomizedSearchCV object\nsearcher = model_selection.RandomizedSearchCV(estimator = RandomForestClassifier(),\n                                            param_distributions = random_grid,\n                                            n_iter = 10, # Number of parameter settings to sample (this could take a while)\n                                            cv     = 3,  # Number of folds for k-fold validation \n                                            n_jobs = -1, # Use all processors to compute in parallel\n                                            random_state=0) \nsearch = searcher.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = search.best_params_\nparams","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After performing our parameter tuning, we can verify whether or not the parameters provided by the search actually improve the base model or not. Let's compare the performance of the two models before and after tuning."},{"metadata":{"trusted":true},"cell_type":"code","source":"tuning_eval = {}\ntuned_rf = RandomForestClassifier(**params)\nbasic_rf = RandomForestClassifier()\n\ntuning_eval['Tuned'] = kfold_evaluate(tuned_rf)\ntuning_eval['Basic'] = kfold_evaluate(basic_rf)\n\nresult_df = pd.DataFrame(tuning_eval)\nresult_df.drop('Model', axis=0).plot(kind='bar', ylim=(0.7, 0.9)).set_title(\"Tuning Performance\")\nplt.xticks(rotation=0)\nplt.show()\nresult_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Steps: \n\nNow that we have chosen and tuned a Random Forest classifier, we want to test it on data it has never before seen.  This will tell us how we might expect the model to perform in the future, on new data. It's time to use that held out test set. \n\nThen, we will combine the test and training data, and re-fit our model to the combined data set, hopefully giving it the greatest chance of success on the unlabeled data from the competition. \n\nFinally, we will make our predictions on the unlabeled data for submission to the competition. \n\n### Final Test on Held Out Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = tuned_rf.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = metrics.classification_report(y_test, y_pred,\n                                        labels = [0, 1],\n                                        target_names = ['Died', 'Survived'],\n                                        output_dict = True)\n\npprint(results, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like we may have experienced some overfitting. Our model's performance on the test data is roughly 8-9% lower across the board. "},{"metadata":{},"cell_type":"markdown","source":"### Combine Training and Testing Datasets for Final Model Fit\n\nNow that we have ascertained that our tuned model performs with about 76% accuracy and has an f1-score of 0.74 on new data, we can proceed to train our model on the entire labeled training set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.concat([x_train, x_test], axis=0).sort_index()\nY = pd.concat([y_train, y_test], axis=0).sort_index()\ntuned_rf.fit(X, Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Format and Standardize Unlabeled Data\n\nNext we need to transform our unlabeled data in the same manner as when we were formatting our training data. This includes encoding categorical variables, dropping the same features and normalization. This should ensure consistent results on the never before seen competition data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Engineering:\ntest_df['Title'] = test_df.Name.str.extract(r'([A-Za-z]+)\\.')\ntest_df['LName'] = test_df.Name.str.extract(r'([A-Za-z]+),')\ntest_df['NameLength'] = test_df.Name.apply(len)\ntest_df['FamilySize'] = 1 + test_df.SibSp + test_df.Parch\ntest_df['Alone'] = test_df.FamilySize.apply(lambda x: 1 if x==1 else 0)\ntest_df.Title = test_df.Title.map(title_dict)\n\n# Feature Selection\ntest_df = test_df.drop(todrop, axis=1)\n\n# Imputation of missing age and fare data\ntest_df.loc[test_df.Age.isna(), 'Age'] = test_df.Age.median()\ntest_df.loc[test_df.Fare.isna(), 'Fare'] = test_df.Fare.median()\n\n# encode categorical data\nfor i in test_df.columns:\n    if test_df[i].dtype == 'object':\n        test_df[i], _ = pd.factorize(test_df[i])\n        \n# center and scale data \ntest_df = pd.DataFrame(pre.scale(test_df), columns=test_df.columns, index=test_df.index)\n\n# ensure columns of unlabeled data are in same order as training data.\ntest_df = test_df[x_test.columns]\ntest_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make Final Predictions\n\nRoughly 32 percent of the passengers aboard the Titanic died. We will do a last, common sense check to see if our algorithm predicts roughly the same distribution of survivals."},{"metadata":{"trusted":true},"cell_type":"code","source":"final = tuned_rf.predict(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.sum()/len(final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'PassengerId':test_df.index,\n                           'Survived':final})\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission2.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}