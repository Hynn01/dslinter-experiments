{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Update \n- Increased dropout rate.\n- Trained for more epochs.\n- Added RandomSizedCrop augmentation\n\n<hr>\n\nIn my [previous kernel](https://www.kaggle.com/meaninglesslives/unet-xception-keras-for-pneumothorax-segmentation) I used pretrained Xception Model as encoder. In this kernel I use pretrained imagenet EfficientNet B4 model with ResNet decoder. I initially tried EfficientNet B3 at the start of the competition but results were not good. I saw  [Yury Dzerin's post](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/discussion/99440#latest-573567) and decided to give EfficientNet B4 a try.\n\n- I use BCE Dice Loss here. There are many other losses available like Lovasz loss, focal loss etc.\n- I use albumentations library for image augmentations. \n- Cosine annealing and Stochastic Weight Averaging to converge to a better optima.\n\nThe model's performance can be definitely be improved by using some other tricks, one obvious way is to use KFold Cross Validation. I will keep updating it as I experiment more."},{"metadata":{"_uuid":"1c5ef627f3327353ea3df4223e85d9eebd1e598c"},"cell_type":"markdown","source":"# Loading Libraries"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"!pip install albumentations > /dev/null\n!git clone https://github.com/qubvel/efficientnet.git\nimport numpy as np\nimport pandas as pd\nimport gc\nimport keras\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\n\nfrom skimage.transform import resize\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.losses import binary_crossentropy\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import  ModelCheckpoint\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom tqdm import tqdm_notebook\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.utils import conv_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras.engine import InputSpec\nfrom keras import backend as K\nfrom keras.layers import LeakyReLU\nfrom keras.layers import ZeroPadding2D\nfrom keras.losses import binary_crossentropy\nimport keras.callbacks as callbacks\nfrom keras.callbacks import Callback\nfrom keras.applications.xception import Xception\nfrom keras.layers import multiply\n\n\nfrom keras import optimizers\nfrom keras.legacy import interfaces\nfrom keras.utils.generic_utils import get_custom_objects\n\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add\nfrom keras.regularizers import l2\nfrom keras.layers.core import Dense, Lambda\nfrom keras.layers.merge import concatenate, add\nfrom keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\nfrom keras.optimizers import SGD\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport glob\nimport shutil\nimport os\nimport random\nfrom PIL import Image\n\nseed = 10\nnp.random.seed(seed)\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n    \n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Training Set Data\nAs mentioned earlier, I don't have international credit card so, I am using the data from this [kernel](https://www.kaggle.com/iafoss/data-repack-and-image-statistics)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir masks\n!unzip -q ../input/data-repack-and-image-statistics/masks.zip -d masks \n!mkdir train\n!unzip -q ../input/data-repack-and-image-statistics/train.zip -d train \n!mkdir test\n!unzip -q ../input/data-repack-and-image-statistics/test.zip -d test ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pneumothorax as percentage of mask"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_mask_fn = glob.glob('./masks/*')\nmask_df = pd.DataFrame()\nmask_df['file_names'] = all_mask_fn\nmask_df['mask_percentage'] = 0\nmask_df.set_index('file_names',inplace=True)\nfor fn in all_mask_fn:\n    mask_df.loc[fn,'mask_percentage'] = np.array(Image.open(fn)).sum()/(256*256*255) #255 is bcz img range is 255\n    \nmask_df.reset_index(inplace=True)\nsns.distplot(mask_df.mask_percentage)\nmask_df['labels'] = 0\nmask_df.loc[mask_df.mask_percentage>0,'labels'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_train_fn = glob.glob('./train/*')\ntotal_samples = len(all_train_fn)\nidx = np.arange(total_samples)\ntrain_fn,val_fn = train_test_split(all_train_fn,stratify=mask_df.labels,test_size=0.1,random_state=10)\n\nprint('No. of train files:', len(train_fn))\nprint('No. of val files:', len(val_fn))\n\nmasks_train_fn = [fn.replace('./train','./masks') for fn in train_fn]    \nmasks_val_fn = [fn.replace('./train','./masks') for fn in val_fn]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!mkdir ./keras_im_train\ntrain_dir = './keras_im_train'\nfor full_fn in train_fn:\n    fn = full_fn.split('/')[-1]\n    shutil.move(full_fn,os.path.join(train_dir,fn))\n    \n!mkdir ./keras_mask_train\ntrain_dir = './keras_mask_train'\nfor full_fn in masks_train_fn:\n    fn = full_fn.split('/')[-1]\n    shutil.move(full_fn,os.path.join(train_dir,fn))\n    \n!mkdir ./keras_im_val\ntrain_dir = './keras_im_val'\nfor full_fn in val_fn:\n    fn = full_fn.split('/')[-1]\n    shutil.move(full_fn,os.path.join(train_dir,fn))\n    \n!mkdir ./keras_mask_val\ntrain_dir = './keras_mask_val'\nfor full_fn in masks_val_fn:\n    fn = full_fn.split('/')[-1]\n    shutil.move(full_fn,os.path.join(train_dir,fn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_im_path,train_mask_path = './keras_im_train','./keras_mask_train'\nh,w,batch_size = 256,256,16\n\nval_im_path,val_mask_path = './keras_im_val','./keras_mask_val'\n\nclass DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, train_im_path=train_im_path,train_mask_path=train_mask_path,\n                 augmentations=None, batch_size=batch_size,img_size=256, n_channels=3, shuffle=True):\n        'Initialization'\n        self.batch_size = batch_size\n        self.train_im_paths = glob.glob(train_im_path+'/*')\n        \n        self.train_im_path = train_im_path\n        self.train_mask_path = train_mask_path\n\n        self.img_size = img_size\n        \n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.augment = augmentations\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.ceil(len(self.train_im_paths) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:min((index+1)*self.batch_size,len(self.train_im_paths))]\n\n        # Find list of IDs\n        list_IDs_im = [self.train_im_paths[k] for k in indexes]\n\n        # Generate data\n        X, y = self.data_generation(list_IDs_im)\n\n        if self.augment is None:\n            return X,np.array(y)/255\n        else:            \n            im,mask = [],[]   \n            for x,y in zip(X,y):\n                augmented = self.augment(image=x, mask=y)\n                im.append(augmented['image'])\n                mask.append(augmented['mask'])\n            return np.array(im),np.array(mask)/255\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.train_im_paths))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def data_generation(self, list_IDs_im):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((len(list_IDs_im),self.img_size,self.img_size, self.n_channels))\n        y = np.empty((len(list_IDs_im),self.img_size,self.img_size, 1))\n\n        # Generate data\n        for i, im_path in enumerate(list_IDs_im):\n            \n            im = np.array(Image.open(im_path))\n            mask_path = im_path.replace(self.train_im_path,self.train_mask_path)\n            \n            mask = np.array(Image.open(mask_path))\n            \n            \n            if len(im.shape)==2:\n                im = np.repeat(im[...,None],3,2)\n\n#             # Resize sample\n            X[i,] = cv2.resize(im,(self.img_size,self.img_size))\n\n            # Store class\n            y[i,] = cv2.resize(mask,(self.img_size,self.img_size))[..., np.newaxis]\n            y[y>0] = 255\n\n        return np.uint8(X),np.uint8(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nfrom albumentations import (\n    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n    RandomBrightness, RandomContrast, RandomGamma,OneOf,\n    ToFloat, ShiftScaleRotate,GridDistortion, ElasticTransform, JpegCompression, HueSaturationValue,\n    RGBShift, RandomBrightness, RandomContrast, Blur, MotionBlur, MedianBlur, GaussNoise,CenterCrop,\n    IAAAdditiveGaussianNoise,GaussNoise,OpticalDistortion,RandomSizedCrop\n)\n\nAUGMENTATIONS_TRAIN = Compose([\n    HorizontalFlip(p=0.5),\n    OneOf([\n        RandomContrast(),\n        RandomGamma(),\n        RandomBrightness(),\n         ], p=0.3),\n    OneOf([\n        ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n        GridDistortion(),\n        OpticalDistortion(distort_limit=2, shift_limit=0.5),\n        ], p=0.3),\n    RandomSizedCrop(min_max_height=(128, 256), height=h, width=w,p=0.5),\n    ToFloat(max_value=1)\n],p=1)\n\n\nAUGMENTATIONS_TEST = Compose([\n    ToFloat(max_value=1)\n],p=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Set Images with Masks"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = DataGenerator(batch_size=64,shuffle=False)\nimages,masks = a.__getitem__(0)\nmax_images = 64\ngrid_width = 16\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n\nfor i,(im, mask) in enumerate(zip(images,masks)):\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(im.squeeze(), cmap=\"bone\")\n    ax.imshow(mask.squeeze(), alpha=0.5, cmap=\"Reds\")    \n    ax.axis('off')\nplt.suptitle(\"Chest X-rays, Red: Pneumothorax.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Images after Augmentations"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = DataGenerator(batch_size=64,augmentations=AUGMENTATIONS_TRAIN,shuffle=False)\nimages,masks = a.__getitem__(0)\nmax_images = 64\ngrid_width = 16\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n\nfor i,(im, mask) in enumerate(zip(images,masks)):\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(im[:,:,0], cmap=\"bone\")\n    ax.imshow(mask.squeeze(), alpha=0.5, cmap=\"Reds\")    \n    ax.axis('off')\nplt.suptitle(\"Chest X-rays, Red: Pneumothorax.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e07a9adca04095f690733343ab30cdc8f3f95a2"},"cell_type":"markdown","source":"# Calculating IOU"},{"metadata":{"_uuid":"45410fdbd533d546a578d9dc29982a1657b8dfa9","trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/cpmpml/fast-iou-metric-in-numpy-and-tensorflow\ndef get_iou_vector(A, B):\n    # Numpy version    \n    batch_size = A.shape[0]\n    metric = 0.0\n    for batch in range(batch_size):\n        t, p = A[batch], B[batch]\n        true = np.sum(t)\n        pred = np.sum(p)\n        \n        # deal with empty mask first\n        if true == 0:\n            metric += (pred == 0)\n            continue\n        \n        # non empty mask case.  Union is never empty \n        # hence it is safe to divide by its number of pixels\n        intersection = np.sum(t * p)\n        union = true + pred - intersection\n        iou = intersection / union\n        \n        # iou metrric is a stepwise approximation of the real iou over 0.5\n        iou = np.floor(max(0, (iou - 0.45)*20)) / 10\n        \n        metric += iou\n        \n    # teake the average over all images in batch\n    metric /= batch_size\n    return metric\n\n\ndef my_iou_metric(label, pred):\n    # Tensorflow version\n    return tf.py_func(get_iou_vector, [label, pred > 0.5], tf.float64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining Dice Loss"},{"metadata":{"_uuid":"d88d952ca7c27fd0e2411eb63ec16fd25cb8ebec","trusted":true},"cell_type":"code","source":"def dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\ndef bce_logdice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Learning Rate Scheduler\nusing cosine annealing for learning rate"},{"metadata":{"_uuid":"98c8b0aff0989293ce43592a82eb3c5dcf8bbee9","trusted":true},"cell_type":"code","source":"class SnapshotCallbackBuilder:\n    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.1):\n        self.T = nb_epochs\n        self.M = nb_snapshots\n        self.alpha_zero = init_lr\n\n    def get_callbacks(self, model_prefix='Model'):\n\n        callback_list = [\n            callbacks.ModelCheckpoint(\"./keras.model\",monitor='val_loss', \n                                   mode = 'min', save_best_only=True, verbose=1),\n            swa,\n            callbacks.LearningRateScheduler(schedule=self._cosine_anneal_schedule)\n        ]\n\n        return callback_list\n\n    def _cosine_anneal_schedule(self, t):\n        cos_inner = np.pi * (t % (self.T // self.M))  # t - 1 is used when t has 1-based indexing.\n        cos_inner /= self.T // self.M\n        cos_out = np.cos(cos_inner) + 1\n        return float(self.alpha_zero / 2 * cos_out)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc9e6a9ab02ceeb1cb2491568870ab1bd881b5f2"},"cell_type":"markdown","source":"# Useful Model Blocks"},{"metadata":{"_uuid":"a8e87341f77e48a6c54de11f8dc72f50f43b8637","trusted":true},"cell_type":"code","source":"def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n    x = BatchNormalization()(x)\n    if activation == True:\n        x = LeakyReLU(alpha=0.1)(x)\n    return x\n\ndef residual_block(blockInput, num_filters=16):\n    x = LeakyReLU(alpha=0.1)(blockInput)\n    x = BatchNormalization()(x)\n    blockInput = BatchNormalization()(blockInput)\n    x = convolution_block(x, num_filters, (3,3) )\n    x = convolution_block(x, num_filters, (3,3), activation=False)\n    x = Add()([x, blockInput])\n    return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f464edae7c4c308bc70dbc50140ee1c6dee6470"},"cell_type":"markdown","source":"# Defining UEfficientNet Model"},{"metadata":{"_uuid":"dd410e24ea4348c42c272e9bcbda0ffee316092c"},"cell_type":"markdown","source":"As mentioned above, this model uses pretrained EfficientNetB4 model as encoder. I use Residual blocks in the decoder part."},{"metadata":{"_uuid":"95fee2ea26eb7d84ebbf325a2de0128814675e7c","trusted":true},"cell_type":"code","source":"from efficientnet import EfficientNetB4\n\ndef UEfficientNet(input_shape=(None, None, 3),dropout_rate=0.1):\n\n    backbone = EfficientNetB4(weights='imagenet',\n                            include_top=False,\n                            input_shape=input_shape)\n    input = backbone.input\n    start_neurons = 16\n\n    conv4 = backbone.layers[342].output\n    conv4 = LeakyReLU(alpha=0.1)(conv4)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(dropout_rate)(pool4)\n    \n     # Middle\n    convm = Conv2D(start_neurons * 32, (3, 3), activation=None, padding=\"same\")(pool4)\n    convm = residual_block(convm,start_neurons * 32)\n    convm = residual_block(convm,start_neurons * 32)\n    convm = LeakyReLU(alpha=0.1)(convm)\n    \n    deconv4 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(dropout_rate)(uconv4)\n    \n    uconv4 = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(uconv4)\n    uconv4 = residual_block(uconv4,start_neurons * 16)\n    uconv4 = residual_block(uconv4,start_neurons * 16)\n    uconv4 = LeakyReLU(alpha=0.1)(uconv4)\n    \n    deconv3 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    conv3 = backbone.layers[154].output\n    uconv3 = concatenate([deconv3, conv3])    \n    uconv3 = Dropout(dropout_rate)(uconv3)\n    \n    uconv3 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv3)\n    uconv3 = residual_block(uconv3,start_neurons * 8)\n    uconv3 = residual_block(uconv3,start_neurons * 8)\n    uconv3 = LeakyReLU(alpha=0.1)(uconv3)\n\n    deconv2 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    conv2 = backbone.layers[92].output\n    uconv2 = concatenate([deconv2, conv2])\n        \n    uconv2 = Dropout(0.1)(uconv2)\n    uconv2 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv2)\n    uconv2 = residual_block(uconv2,start_neurons * 4)\n    uconv2 = residual_block(uconv2,start_neurons * 4)\n    uconv2 = LeakyReLU(alpha=0.1)(uconv2)\n    \n    deconv1 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    conv1 = backbone.layers[30].output\n    uconv1 = concatenate([deconv1, conv1])\n    \n    uconv1 = Dropout(0.1)(uconv1)\n    uconv1 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv1)\n    uconv1 = residual_block(uconv1,start_neurons * 2)\n    uconv1 = residual_block(uconv1,start_neurons * 2)\n    uconv1 = LeakyReLU(alpha=0.1)(uconv1)\n    \n    uconv0 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv1)   \n    uconv0 = Dropout(0.1)(uconv0)\n    uconv0 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv0)\n    uconv0 = residual_block(uconv0,start_neurons * 1)\n    uconv0 = residual_block(uconv0,start_neurons * 1)\n    uconv0 = LeakyReLU(alpha=0.1)(uconv0)\n    \n    uconv0 = Dropout(dropout_rate/2)(uconv0)\n    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv0)    \n    \n    model = Model(input, output_layer)\n    model.name = 'u-xception'\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2382b088c8a6be16490354ebd386120a9ced414d","trusted":true},"cell_type":"code","source":"K.clear_session()\nimg_size = 256\nmodel = UEfficientNet(input_shape=(img_size,img_size,3),dropout_rate=0.25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stochastic Weight Averaging\nI have found SWA to give better results. Please check out the paper for more info."},{"metadata":{"_uuid":"3d7a02b2dca5074cbd7025947d031b134a17ff54","trusted":true},"cell_type":"code","source":"class SWA(keras.callbacks.Callback):\n    \n    def __init__(self, filepath, swa_epoch):\n        super(SWA, self).__init__()\n        self.filepath = filepath\n        self.swa_epoch = swa_epoch \n    \n    def on_train_begin(self, logs=None):\n        self.nb_epoch = self.params['epochs']\n        print('Stochastic weight averaging selected for last {} epochs.'\n              .format(self.nb_epoch - self.swa_epoch))\n        \n    def on_epoch_end(self, epoch, logs=None):\n        \n        if epoch == self.swa_epoch:\n            self.swa_weights = self.model.get_weights()\n            \n        elif epoch > self.swa_epoch:    \n            for i in range(len(self.swa_weights)):\n                self.swa_weights[i] = (self.swa_weights[i] * \n                    (epoch - self.swa_epoch) + self.model.get_weights()[i])/((epoch - self.swa_epoch)  + 1)  \n\n        else:\n            pass\n        \n    def on_train_end(self, logs=None):\n        self.model.set_weights(self.swa_weights)\n        print('Final model parameters set to stochastic weight average.')\n        self.model.save_weights(self.filepath)\n        print('Final stochastic averaged weights saved to file.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=bce_dice_loss, optimizer='adam', metrics=[my_iou_metric])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Begins"},{"metadata":{"_uuid":"f1773642758da7b4480e0e48c045bd01ea3684ae","scrolled":true,"trusted":true},"cell_type":"code","source":"epochs = 70\nsnapshot = SnapshotCallbackBuilder(nb_epochs=epochs,nb_snapshots=1,init_lr=1e-3)\nbatch_size = 16\nswa = SWA('./keras_swa.model',67)\nvalid_im_path,valid_mask_path = './keras_im_val','./keras_mask_val'\n# Generators\ntraining_generator = DataGenerator(augmentations=AUGMENTATIONS_TRAIN,img_size=img_size)\nvalidation_generator = DataGenerator(train_im_path = valid_im_path ,\n                                     train_mask_path=valid_mask_path,augmentations=AUGMENTATIONS_TEST,\n                                     img_size=img_size)\n\nhistory = model.fit_generator(generator=training_generator,\n                            validation_data=validation_generator,                            \n                            use_multiprocessing=False,\n                            epochs=epochs,verbose=2,\n                            callbacks=snapshot.get_callbacks())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42e9ef3c4e0a2bb2539e5e51740ba6bfc092d37c","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nplt.subplot(1,2,1)\nplt.plot(history.history['my_iou_metric'][1:])\nplt.plot(history.history['val_my_iou_metric'][1:])\nplt.ylabel('iou')\nplt.xlabel('epoch')\nplt.legend(['train','Validation'], loc='upper left')\n\nplt.title('model IOU')\n\nplt.subplot(1,2,2)\nplt.plot(history.history['loss'][1:])\nplt.plot(history.history['val_loss'][1:])\nplt.ylabel('val_loss')\nplt.xlabel('epoch')\nplt.legend(['train','Validation'], loc='upper left')\nplt.title('model loss')\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c824f6bca47f051500966c433ce7fb5a9528f6d7","trusted":true},"cell_type":"code","source":"# Load best model or swa model if not available\n# try:\n#     print('using swa weight model')\n#     model.load_weights('./keras_swa.model')\n# except Exception as e:\n#     print(e)\n#     model.load_weights('./keras.model')\n    \n# load model with least validation loss\nmodel.load_weights('./keras.model')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f168318eadb324daa8c020f0e3e0a24d82a464f"},"cell_type":"markdown","source":"# Predict the validation set to do a sanity check\nAgain plot some sample images including the predictions."},{"metadata":{"_uuid":"fc4d63ca6c7e6c13e4cfb988a554199712486af2","trusted":true},"cell_type":"code","source":"def predict_result(model,validation_generator,img_size): \n    # TBD predict both orginal and reflect x\n    preds_test1 = model.predict_generator(validation_generator).reshape(-1, img_size, img_size)\n    return preds_test1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16cbfe2fee11a8b13b96ce78161ce19b5e5a0c46","trusted":true},"cell_type":"code","source":"validation_generator = DataGenerator(train_im_path = valid_im_path ,\n                                     train_mask_path=valid_mask_path,augmentations=AUGMENTATIONS_TEST,\n                                     img_size=img_size,shuffle=False)\npreds_valid = predict_result(model,validation_generator,img_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_fn = glob.glob('./keras_mask_val/*')\ny_valid_ori = np.array([cv2.resize(np.array(Image.open(fn)),(img_size,img_size)) for fn in valid_fn])\nassert y_valid_ori.shape == preds_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"423b3268c580dc1eae84f54deeeb0f691eff6028"},"cell_type":"markdown","source":"# Plot some predictions for validation set images"},{"metadata":{"_uuid":"40c263765ac6d53a8c0c1361ff1e6f061eecf825","trusted":true},"cell_type":"code","source":"threshold_best = 0.5\nmax_images = 64\ngrid_width = 16\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n\nvalidation_generator = DataGenerator(train_im_path = valid_im_path ,\n                                     train_mask_path=valid_mask_path,augmentations=AUGMENTATIONS_TEST,\n                                     img_size=img_size,batch_size=64,shuffle=False)\n\nimages,masks = validation_generator.__getitem__(0)\nfor i,(im, mask) in enumerate(zip(images,masks)):\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(im[...,0], cmap=\"bone\")\n    ax.imshow(mask.squeeze(), alpha=0.5, cmap=\"Reds\")    \n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.5, cmap=\"Greens\")\n    ax.axis('off')\nplt.suptitle(\"Green:Prediction , Red: Pneumothorax.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)\n\nvalid_fn = glob.glob('./keras_mask_val/*')\ny_valid_ori = np.array([cv2.resize(np.array(Image.open(fn)),(img_size,img_size)) for fn in valid_fn])\nassert y_valid_ori.shape == preds_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Scoring for last model\nthresholds = np.linspace(0.2, 0.9, 31)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold_best_index = np.argmax(ious) \niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]\n\nplt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_images = 64\ngrid_width = 16\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n\nvalidation_generator = DataGenerator(train_im_path = valid_im_path ,\n                                     train_mask_path=valid_mask_path,augmentations=AUGMENTATIONS_TEST,\n                                     img_size=img_size,batch_size=64,shuffle=False)\n\nimages,masks = validation_generator.__getitem__(0)\nfor i,(im, mask) in enumerate(zip(images,masks)):\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(im[...,0], cmap=\"bone\")\n    ax.imshow(mask.squeeze(), alpha=0.5, cmap=\"Reds\")    \n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.5, cmap=\"Greens\")\n    ax.axis('off')\nplt.suptitle(\"Green:Prediction , Red: Pneumothorax.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a0123d4cbd90c49c822cf5dc545a30f4a9eb456"},"cell_type":"markdown","source":"# Test Set Prediction"},{"metadata":{"_uuid":"bd6ce9b4d5fc80a2502a43e80299d628fb5ffc42","trusted":true},"cell_type":"code","source":"test_fn = glob.glob('./test/*')\nx_test = [cv2.resize(np.array(Image.open(fn)),(img_size,img_size)) for fn in test_fn]\nx_test = np.array(x_test)\nx_test = np.array([np.repeat(im[...,None],3,2) for im in x_test])\nprint(x_test.shape)\npreds_test = model.predict(x_test,batch_size=batch_size)\n# del x_test; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"834f8f03bccb46931ce8e8afb0f26af1658a3985"},"cell_type":"markdown","source":"# Some Test Set Predictions"},{"metadata":{"_uuid":"b63b8f23f51fb7030cbe6f10d38b186044dc7d4e","trusted":true},"cell_type":"code","source":"max_images = 64\ngrid_width = 16\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n# for i, idx in enumerate(index_val[:max_images]):\nfor i, idx in enumerate(test_fn[:max_images]):\n    img = x_test[i]\n    pred = preds_test[i].squeeze()\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.5, cmap=\"Reds\")\n    ax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"388d2d738bd15cc4b7259d1ec41df6e4eede94e7","trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, '../input/siim-acr-pneumothorax-segmentation')\n\nfrom mask_functions import rle2mask,mask2rle\nimport pdb\n\n# Generate rle encodings (images are first converted to the original size)\nrles = []\ni,max_img = 1,10\nplt.figure(figsize=(16,4))\nfor p in tqdm_notebook(preds_test):\n    p = p.squeeze()\n    im = cv2.resize(p,(1024,1024))\n    im = im > threshold_best\n#     zero out the smaller regions.\n    if im.sum()<1024*2:\n        im[:] = 0\n    im = (im.T*255).astype(np.uint8)  \n    rles.append(mask2rle(im, 1024, 1024))\n    i += 1\n    if i<max_img:\n        plt.subplot(1,max_img,i)\n        plt.imshow(im)\n        plt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d85641bbb14e796c7f47a6122f2b9ed2c97a46f","trusted":true},"cell_type":"code","source":"ids = [o.split('/')[-1][:-4] for o in test_fn]\nsub_df = pd.DataFrame({'ImageId': ids, 'EncodedPixels': rles})\nsub_df.loc[sub_df.EncodedPixels=='', 'EncodedPixels'] = '-1'\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('orig_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!rm -r */","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's exploit the leaky probabilities in the current best public submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.read_csv('orig_submission.csv')\nleak_prob = pd.read_csv('../input/leak-probabilities-siim/leak_probabilities.csv')\nsub_df = sub_df.merge(leak_prob,on='ImageId')\nsub_df['pneumothorax'] = leak_prob.pred > 0.15\nsub_df['EncodedPixels'] = np.where(sub_df.pneumothorax,'-1',sub_df.EncodedPixels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('leak_exploit.csv',columns=['ImageId','EncodedPixels'],index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"009cc3a5555e45129b1db2b50ac01ba5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0524717705bd483eb83cfdc14a59c9e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"296b22c0c404487fb73154f5e26e3339":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56aaf80dbaf8402082f10e498dc38bc1","placeholder":"​","style":"IPY_MODEL_716009cd7ddf47aca1956d3673d29f7a","value":"100% 31/31 [02:32&lt;00:00,  4.95s/it]"}},"4d3cb9eec6c8421fa0e394d4eb8398d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_888316e9313442ada35fab82543d2fb3","max":1377,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0524717705bd483eb83cfdc14a59c9e7","value":1377}},"4da285cade594aaa8f105eff720cf61e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d3dcf0d968ec43508691faa65e43cf9f","IPY_MODEL_296b22c0c404487fb73154f5e26e3339"],"layout":"IPY_MODEL_e9cbbce0dac948e49b7868651cdb5943"}},"56aaf80dbaf8402082f10e498dc38bc1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"716009cd7ddf47aca1956d3673d29f7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"StyleView","description_width":""}},"765985fe19ee424886f69c63708c7765":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"87f4cff8b8984604ac8a78ead9a4968c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"StyleView","description_width":""}},"888316e9313442ada35fab82543d2fb3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0bd27b6511444299cad83bb39fbb0d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_009cc3a5555e45129b1db2b50ac01ba5","placeholder":"​","style":"IPY_MODEL_87f4cff8b8984604ac8a78ead9a4968c","value":"100% 1377/1377 [09:07&lt;00:00,  2.54it/s]"}},"d3bfe4085424427c805c8a05f5006861":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4d3cb9eec6c8421fa0e394d4eb8398d8","IPY_MODEL_b0bd27b6511444299cad83bb39fbb0d3"],"layout":"IPY_MODEL_d7fabbab3e7b4029a25978c76df8805b"}},"d3dcf0d968ec43508691faa65e43cf9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed356e98218f4a7f83ca8066063af601","max":31,"min":0,"orientation":"horizontal","style":"IPY_MODEL_765985fe19ee424886f69c63708c7765","value":31}},"d7fabbab3e7b4029a25978c76df8805b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9cbbce0dac948e49b7868651cdb5943":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed356e98218f4a7f83ca8066063af601":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}