{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"scrolled":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc, warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d38035771ffce7adad33b76d387a533b717c893"},"cell_type":"markdown","source":"# Overview\nWhat is this kernel about?\n* No predictions to make \n* No features to create\n\nWe will load competition data and look closer on it. We will try to understand what we have in our hands and how we can work with it.\n* * *"},{"metadata":{"_uuid":"0701d6cc1333560ed55d1cfe2308d7494921b6e2"},"cell_type":"markdown","source":" ## Load train data\n * * *"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"a0a1bff7d00c5a4f8f7903d2655aa25d17406e9e"},"cell_type":"code","source":"sale_train = pd.read_csv('../input/sales_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c16019f841e617154a71d6fab73eb116c2690082"},"cell_type":"markdown","source":"We can view basic DafaFrame information. \n\nAs you can see, we do not have broken and nan data that is good."},{"metadata":{"trusted":true,"_uuid":"c9c35b30b6b027f5f5730e070afc2cb21a92bf2e","scrolled":true},"cell_type":"code","source":"print(\"----------Top-5- Record----------\")\nprint(sale_train.head(5))\nprint(\"-----------Information-----------\")\nprint(sale_train.info())\nprint(\"-----------Data Types-----------\")\nprint(sale_train.dtypes)\nprint(\"----------Missing value-----------\")\nprint(sale_train.isnull().sum())\nprint(\"----------Null value-----------\")\nprint(sale_train.isna().sum())\nprint(\"----------Shape of Data----------\")\nprint(sale_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c3f9b62c0ae87d1ae99af6bf62b38e8ed167b30"},"cell_type":"markdown","source":"We have duplicated rows, but I don't think that it is a mistake.\n\nIt could be different sales methods or client type, etc.\n\nYou can remove it, but I really don't believe that 6 rows of 3m can make the difference."},{"metadata":{"trusted":true,"_uuid":"d2800fe80ac4003ef9a66c5e37eaf6fb92b4aa4f","_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"print('Number of duplicates:', len(sale_train[sale_train.duplicated()]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f55dbe8feb5c2ecf5cc40a3d592ddd5b48ea201d"},"cell_type":"markdown","source":"I can advise downcasting your DataFrame. It will save you some memory, and believe me you will need all memory possible.\n\nIn our case from 134.4+ MB, we went to 61.6+ MB\n\nNot a great deal right now but such approach works with bigger DF also.\n\n#### please see this two links for more tips (I stole that downcast basic snippet from anqitu)))\n* https://www.kaggle.com/anqitu/feature-engineer-and-model-ensemble-top-10\n* https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask"},{"metadata":{"trusted":true,"_uuid":"86b8ccd49688e11eba077c21aa638c759a10a176","scrolled":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n\nsale_train = downcast_dtypes(sale_train)\nprint(sale_train.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b95a8f4ca9e94d5260aa578efd3d410fc8bc468d"},"cell_type":"markdown","source":"## 1.1 Item_id\n* * *\n### Lets group data by item_id and date_block_num and look closer on it.\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"sales_by_item_id = sale_train.pivot_table(index=['item_id'],values=['item_cnt_day'], \n                                        columns='date_block_num', aggfunc=np.sum, fill_value=0).reset_index()\nsales_by_item_id.columns = sales_by_item_id.columns.droplevel().map(str)\nsales_by_item_id = sales_by_item_id.reset_index(drop=True).rename_axis(None, axis=1)\nsales_by_item_id.columns.values[0] = 'item_id'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb9510bf9c352a073fdcda40ec50d7dc402828ef"},"cell_type":"markdown","source":"### Simple graph\nWhat this graph is telling us. Basically nothing.)) I only see that train data has many old products (degradation line) and many 1c products are seasonal and probably release date depended.\n\n#### I'm not very good with graphs and presentations - there are better data representation examples:\n* https://www.kaggle.com/dimitreoliveira/model-stacking-feature-engineering-and-eda\n* https://www.kaggle.com/jagangupta/time-series-basics-exploring-traditional-ts"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f41fa75f4089861ffdfdc7ee551f2d99e0d7f1ac"},"cell_type":"code","source":"sales_by_item_id.sum()[1:].plot(legend=True, label=\"Monthly sum\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"183eeb505320b71e99ef429bf48324ea1d74cb90"},"cell_type":"code","source":"sales_by_item_id.mean()[1:].plot(legend=True, label=\"Monthly mean\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf5109a824d10570f1ba3eef89be7ce3759738be"},"cell_type":"markdown","source":"### Let's see how many products are outdated (no sales for the last 6 months)\n12391 of 21807 is a huge number. Probably we can set 0 for all that items and do not make any model prediction."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"69d3323d9cfcd63a11b0448fa716250ee5e51e59"},"cell_type":"code","source":"outdated_items = sales_by_item_id[sales_by_item_id.loc[:,'27':].sum(axis=1)==0]\nprint('Outdated items:', len(outdated_items))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85ff60f16dde4356ea135bc7e0572c6a335253e7"},"cell_type":"markdown","source":"### How many outdated items in test set?\n6888 - not much but we have such items"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"32886b44ba7c8fab632a1d10d9aa990f7886e8f9"},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv')\nprint('Outdated items in test set:', len(test[test['item_id'].isin(outdated_items['item_id'])]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c9e24173a4473a1cb523c054d2999558eac9d2c"},"cell_type":"markdown","source":"### Outliers by price and sales volume\nWe will get rid of them later\n\n#### please see lovely kernel made by Denis Larionov (I stole few graphs from there)\n* https://www.kaggle.com/dlarionov/feature-engineering-xgboost"},{"metadata":{"trusted":true,"_uuid":"972f035270aa822d93db73925fe3868b2c785876"},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=sale_train['item_cnt_day'])\nprint('Sale volume outliers:',sale_train['item_id'][sale_train['item_cnt_day']>500].unique())\n\nplt.figure(figsize=(10,4))\nplt.xlim(sale_train['item_price'].min(), sale_train['item_price'].max())\nsns.boxplot(x=sale_train['item_price'])\nprint('Item price outliers:',sale_train['item_id'][sale_train['item_price']>50000].unique())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df11bedc80cd9816f60c2883fa6cb4d6bcf6cd1e"},"cell_type":"markdown","source":"### Possible item_id features:\n1. Lags\n2. Release date\n3. Last month sale\n4. Days on sale\n5. Neighbors (items with id 1000 and 1001 could be somehow similar - genre, type, release date)"},{"metadata":{"_uuid":"c3ec31c7426de7bb806e0c10c6f7d95e12761d90"},"cell_type":"markdown","source":"## 1.2 shop_id\n* * *\n### Lets now group train data by shop_id.\nWe can see new shops - probably there will be a sales spike (opening event for example).\nApparently closed shops (ill call it \"outdated shops\")  - no sales for last 6 months."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"117e47f7680d6bf6ad4f13e7cb191703b74fa5a2"},"cell_type":"code","source":"sales_by_shop_id = sale_train.pivot_table(index=['shop_id'],values=['item_cnt_day'], \n                                        columns='date_block_num', aggfunc=np.sum, fill_value=0).reset_index()\nsales_by_shop_id.columns = sales_by_shop_id.columns.droplevel().map(str)\nsales_by_shop_id = sales_by_shop_id.reset_index(drop=True).rename_axis(None, axis=1)\nsales_by_shop_id.columns.values[0] = 'shop_id'\n\nfor i in range(6,34):\n    print('Not exists in month',i,sales_by_shop_id['shop_id'][sales_by_shop_id.loc[:,'0':str(i)].sum(axis=1)==0].unique())\n\nfor i in range(6,28):\n    print('Shop is outdated for month',i,sales_by_shop_id['shop_id'][sales_by_shop_id.loc[:,str(i):].sum(axis=1)==0].unique())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43d62def13e93719c56bef61263860ad473ef59c"},"cell_type":"markdown","source":"In our test set we have 5100 sales in really new shop and no \"outdated shops\" but anyway it is good feature for future."},{"metadata":{"trusted":true,"_uuid":"33dc0888c1c300114ba6fd18d4f19eea8c4ec2dc"},"cell_type":"code","source":"print('Recently opened shop items:', len(test[test['shop_id']==36]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"777c65d11da24810e2f1a6e4c8f69342b7d50c58"},"cell_type":"markdown","source":"### Possible shop_id features\n1. Lags (shop_id/shp_cnt_mth)\n2. Opening month (possible  opening sales)\n3. Closed Month (possible stock elimination)"},{"metadata":{"_uuid":"23feaefb184db48a5768eb8ff278c31abd8128e7"},"cell_type":"markdown","source":"## 1.3 Price\n* * *\n### Possible Price features:\n1. Price category (1$/10$/20$/ etc.) - obviously (or not obviously),  items with smaller price have greater volumes\n2. Discount and Discount duration\n3. Price lag (shows discount)\n4. Price correction (rubl/usd pair)\n5. Shop Revenue"},{"metadata":{"_uuid":"9630ea096fa1e19a0889c0855a2d7b16d17c7350"},"cell_type":"markdown","source":"## 1.4 Dates\n* * *\n### Possible Date features:\n1. Weekends and holidays sales (to correct monthly sales)\n2. Number of days in the month (to correct monthly sales)\n3. Month number (for seasonal items)"},{"metadata":{"_uuid":"a986daadeb23bb218191e130bbdb9dd572615b9f"},"cell_type":"markdown","source":"## 1.5 Shop info\n* * *\nThe structure of the shop information is evident.\n### Shop City | Shop type | Shop name"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"48b6d9f2dd244468b16c1a5c3a9a082f6fb21db8"},"cell_type":"code","source":"shops = pd.read_csv('../input/shops.csv')\nshops.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69702cf86fd59b74fca508963c1651177e710b82"},"cell_type":"markdown","source":"With a close look we can find out that some shops have duplicated id/name - probably it changed location (within commercial center), or it has a different type (isle sale point), but I decided to merge it.\n* 11 => 10\n* 1  => 58\n* 0  => 57\n* 40 => 39\n\nI converted train shop_id to shop_id that is in the test set"},{"metadata":{"trusted":true,"_uuid":"0dc3a7e13d879b1f1f4a3aa519800ffd256755ba"},"cell_type":"code","source":"shops['shop_name'] = shops['shop_name'].apply(lambda x: x.lower()).str.replace('[^\\w\\s]', '').str.replace('\\d+','').str.strip()\nshops['shop_city'] = shops['shop_name'].str.partition(' ')[0]\nshops['shop_type'] = shops['shop_name'].apply(lambda x: 'мтрц' if 'мтрц' in x else 'трц' if 'трц' in x else 'трк' if 'трк' in x else 'тц' if 'тц' in x else 'тк' if 'тк' in x else 'NO_DATA')\nshops.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81fc1c1fc6a1cbec542fa0b726fa982d2947ace3"},"cell_type":"markdown","source":"### Possible Shop features:\n1. Shop City\n2. Shop Type"},{"metadata":{"_uuid":"dbc33e1ebf107e1fd5f655f018e2116984dfc0be"},"cell_type":"markdown","source":"## 1.6 Item info\n* * *\nLet's see what we can get from this file."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"1ba5be08289a9bf5ff35cb407dd32545b12bf14f"},"cell_type":"code","source":"items = pd.read_csv('../input/items.csv')\nitems.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9d8671932740709147923219fcf0a1e6f280a78"},"cell_type":"markdown","source":"We can enconde \"features\" that many items have.\n\nThe structure is always the same\n### Item name [category feature] (additional feature)\nwe can split it, and \"one hot encode it.\""},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"09757b46aa8894f66a8468026d570172d250ae73"},"cell_type":"code","source":"# Ugly code to show the idea\nfrom collections import Counter\nfrom operator import itemgetter\nitems['name_1'], items['name_2'] = items['item_name'].str.split('[', 1).str\nitems['name_1'], items['name_3'] = items['item_name'].str.split('(', 1).str\n\nitems['name_2'] = items['name_2'].str.replace('[^A-Za-z0-9А-Яа-я]+', ' ').str.lower()\nitems['name_3'] = items['name_3'].str.replace('[^A-Za-z0-9А-Яа-я]+', ' ').str.lower()\nitems = items.fillna('0')\n\nresult_1 = Counter(' '.join(items['name_2'].values.tolist()).split(' ')).items()\nresult_1 = sorted(result_1, key=itemgetter(1))\nresult_1 = pd.DataFrame(result_1, columns=['feature', 'count'])\nresult_1 = result_1[(result_1['feature'].str.len() > 1) & (result_1['count'] > 200)]\n\nresult_2 = Counter(' '.join(items['name_3'].values.tolist()).split(\" \")).items()\nresult_2 = sorted(result_2, key=itemgetter(1))\nresult_2 = pd.DataFrame(result_2, columns=['feature', 'count'])\nresult_2 = result_2[(result_2['feature'].str.len() > 1) & (result_2['count'] > 200)]\n\nresult = pd.concat([result_1, result_2])\nresult = result.drop_duplicates(subset=['feature'])\n\nprint('Most common aditional features:', result)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5269123ec33e0e39a9824e64ae95602883b5c956"},"cell_type":"markdown","source":"### Item name correction\nFor our basic \"name feature\" it is enough to find identical items (not similar but identical),"},{"metadata":{"trusted":true,"_uuid":"5701d130b1589300940f588835106561efb248ec"},"cell_type":"code","source":"print('Unique item names:', len(items['item_name'].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4bf9069a41bc3333150a5eff3bfb6997c5a9d516"},"cell_type":"code","source":"import re\ndef name_correction(x):\n    x = x.lower()\n    x = x.partition('[')[0]\n    x = x.partition('(')[0]\n    x = re.sub('[^A-Za-z0-9А-Яа-я]+', ' ', x)\n    x = x.replace('  ', ' ')\n    x = x.strip()\n    return x\n\nitems['item_name'] = items['item_name'].apply(lambda x: name_correction(x))\nitems.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdab4d126288b29f669807848b6072cab09304c5"},"cell_type":"code","source":"print('Unique item names after correction:', len(items['item_name'].unique()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"beb77fb09af07912061629ed55d3f47df33355e8"},"cell_type":"markdown","source":"### Possible Item features:\n1. Item name\n2. Encoded aditional feature "},{"metadata":{"_uuid":"66a0923b3fc90f074a272a5a13a61c551114464d"},"cell_type":"markdown","source":"## 1.7 Category info\n* * *\nThe structure here is\n### Section name - subsection\nwe can split it and have two features from one"},{"metadata":{"trusted":true,"_uuid":"79468132a8d99474a585d7bb6508115da7a1f965"},"cell_type":"code","source":"categories = pd.read_csv('../input/item_categories.csv')\ncategories.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"511f796586cbffdf22586aa242cecf14b5469fb6"},"cell_type":"markdown","source":"### But I did manual feature extraction here to have four features.\nSection / Main Category name / Main SubCategory name / Secondary SubCategory name\n#### Аксессуары / PS2\t/ PS / 2"},{"metadata":{"_uuid":"865d362d26204f248b5ad64fc4465601c0d2b82c"},"cell_type":"markdown","source":"### Possible Category features\n1. Section\n2. Main Category name\n3. Main SubCategory name \n4. Secondary SubCategory name\n"},{"metadata":{"_uuid":"1cd738638ec06398323532426cdd0473a8207e5e"},"cell_type":"markdown","source":"## 1.8 Test Set\n* * *\nThe key to my success was the analysis of Test test data.\n\nWe have three groups of items:\n1. Item/shop pairs that are in train\n2. Items without any data\n3. Items that are in train"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b841aa8e71c4327c1dc96237c6a9c4a7f872d8d3"},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv')\ngood_sales = test.merge(sale_train, on=['item_id','shop_id'], how='left').dropna()\ngood_pairs = test[test['ID'].isin(good_sales['ID'])]\nno_data_items = test[~(test['item_id'].isin(sale_train['item_id']))]\n\nprint('1. Number of good pairs:', len(good_pairs))\nprint('2. No Data Items:', len(no_data_items))\nprint('3. Only Item_id Info:', len(test)-len(no_data_items)-len(good_pairs))\n  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf3bba0a38999f2b15e10b0062cd84563c105832"},"cell_type":"markdown","source":"#### Is it feature? Yes. We need to apply different prediction approach for each type of items in the test set.\n####  For example - \"No Data Items\" - it is more likely classification task."},{"metadata":{"_uuid":"ef56ab0e5fc525bf4afb7554f08d2d775076e4d6"},"cell_type":"markdown","source":"### Next part will be about data aggregation and feature preparation.\n## To be continued..."},{"metadata":{"_uuid":"e5f615f0a8dd65504e38a77b3bca1b2ec1404b6c"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}