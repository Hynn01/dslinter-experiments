{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# TensorFlow and tf.keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom keras.regularizers import l1, l2\n\n# Helper libraries\nfrom datetime import datetime\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reload the dataset into train and test\ntrain, test = tf.keras.datasets.fashion_mnist.load_data()\n\n#split train, validation sets\nx, y = train\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2)\n\nx_train = x_train.reshape(len(x_train), 28 * 28).astype('float32')/255\ny_train = to_categorical(y_train)\n\nx_valid = x_valid.reshape(len(x_valid), 28 * 28).astype('float32')/255\ny_valid = to_categorical(y_valid)\n\n#process test set\nx_test, y_test = test\nx_test = x_test.reshape(10000, 28 * 28).astype('float32')/255\n\n#construct pipelines\ntrain_set = tf.data.Dataset.from_tensor_slices((x_train, y_train))\nvalid_set = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\ntest_set = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n\n#batch the sets according to batch_size\nbatch_size = 64\ntrain_batches = train_set.shuffle(buffer_size=1024).batch(batch_size)\nvalid_batches = valid_set.batch(batch_size)\ntest_batches = test_set.batch(batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#baseline model:\n#batch the sets according to batch_size\nmodel = models.Sequential()\nmodel.add(layers.Dense(256, activation='relu', kernel_initializer = 'he_normal', input_shape = (28*28,)))\nmodel.add(layers.Dense(128, activation='relu', kernel_initializer = 'he_normal'))\nmodel.add(layers.Dense(64, activation='relu', kernel_initializer = 'he_normal'))\nmodel.add(layers.Dense(10, activation='softmax'))\n# Instantiate an optimizer\noptimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=2e-06, nesterov=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keras.utils.plot_model(model, show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate a loss function\nloss_fn = keras.losses.CategoricalCrossentropy(from_logits=False)\n# Prepare the metrics\ntrain_acc_metric = keras.metrics.CategoricalAccuracy()\ntrain_loss_avg = tf.keras.metrics.Mean()\ntrain_norm_avg = tf.keras.metrics.Mean()\nval_acc_metric = keras.metrics.CategoricalAccuracy()\nval_loss_avg = tf.keras.metrics.Mean()\n\n# Use the tf.GradientTape context to calculate the gradients used to optimize model:\ndef grad(model, x, y):\n  with tf.GradientTape() as tape:\n    probs = model(x)\n    loss_value = loss_fn(y, probs)\n    grads = tape.gradient(loss_value, model.trainable_variables)\n    norms = [tf.norm(each) for each in grads]\n  return loss_value, grads, norms\n\nnum_epochs = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keep results for plotting\ntrain_accuracy_results = []\ntrain_loss_results = []\ntrain_grad_results = []\ntrain_norm_results = []\nvalid_accuracy_results = []\nvalid_loss_results = []\nlearning_rate_records = []\n\nfor epoch in range(num_epochs):\n  # Iterate over the batches of train dataset.\n  for x_batch_train, y_batch_train in train_batches:\n    loss_value, grads, norms = grad(model, x_batch_train, y_batch_train)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    lr = optimizer._decayed_lr(var_dtype=tf.float32)\n    # Track progress\n    train_acc_metric.update_state(y_batch_train, model(x_batch_train)) # Add current batch acc\n    train_loss_avg.update_state(loss_value) # Add current batch loss\n    train_norm_avg.update_state(norms)  # Add current batch norm\n\n  # Run a validation loop over the batches of validation dataset.\n  for x_batch_val, y_batch_val in valid_batches:\n    val_loss, _, _ = grad(model, x_batch_val, y_batch_val)\n    #Track progress\n    val_acc_metric.update_state(y_batch_val, model(x_batch_val)) # Add current batch acc\n    val_loss_avg.update_state(val_loss) # Add current batch loss\n  \n  # End epoch\n  train_accuracy_results.append(train_acc_metric.result())\n  train_loss_results.append(train_loss_avg.result())\n  train_grad_results.append(grads)\n  train_norm_results.append(train_norm_avg.result())\n  valid_accuracy_results.append(val_acc_metric.result())\n  valid_loss_results.append(val_loss_avg.result())\n  learning_rate_records.append(lr)\n  #print to console\n  if epoch % 1 == 0:\n    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}, Val-Loss: {:.3f}, Val-Accuracy: {:.3%}, L2 norm: {:.3f}, Learning rate: {}\".format(epoch+1,\n                                                                train_loss_avg.result(),\n                                                                train_acc_metric.result(),\n                                                                val_loss_avg.result(),\n                                                                val_acc_metric.result(),\n                                                                train_norm_avg.result(),\n                                                                lr))\n\n#evaluate on test set\ntest_acc_metric = tf.keras.metrics.CategoricalAccuracy()\nfor (x, y) in test_batches:\n  probs = model(x)\n  prediction = tf.argmax(probs, axis=1, output_type=tf.int32)\n  test_acc_metric(prediction, y)\nprint(\"Test-Accuracy: {:.3%}\".format(test_acc_metric.result()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #log and display gradients and their l2 norms into tensorboard\n# writer = tf.summary.create_file_writer(\"./logs/gradient\")\n# with writer.as_default():\n#   with tf.name_scope(\"gradient\"):\n#     for epoch, grads in zip(range(num_epochs), train_grad_results):\n#       for weight, grad in zip(model.weights, grads):\n#         tf.summary.histogram('{}_grad'.format(weight.name), grad, step = epoch)\n#   with tf.name_scope(\"gradient_norm\"):\n#     for epoch, norm in zip(range(num_epochs), train_norm_results):\n#       tf.summary.scalar(name = \"gradient_norm\", data = norm, step = epoch)\n# writer.flush()\n\n# %reload_ext tensorboard\n# %tensorboard --logdir=logs\n\n# #log and display train-validation accuracy, loss and learning rate into tensorboard\n# root_logdir = \"logs\"\n# run_id = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n# logdir = os.path.join(root_logdir, run_id)\n# !rm -rf ./logs\n\n# writer1 = tf.summary.create_file_writer(\"./logs/train\")\n# with writer1.as_default():\n#   with tf.name_scope(\"loss\"):\n#     for epoch, train_loss in zip(range(num_epochs), train_loss_results):\n#       tf.summary.scalar(name = \"loss\", data = train_loss, step = epoch)\n#   with tf.name_scope(\"accuracy\"):\n#     for epoch, train_acc in zip(range(num_epochs), train_accuracy_results):\n#       tf.summary.scalar(name = \"accuracy\", data = train_acc, step = epoch)\n# writer1.flush()\n\n# writer2 = tf.summary.create_file_writer(\"./logs/validation\")\n# with writer2.as_default():\n#   with tf.name_scope(\"loss\"): \n#     for epoch, val_loss in zip(range(num_epochs), valid_loss_results):\n#       tf.summary.scalar(name = \"loss\", data = val_loss, step = epoch)\n#   with tf.name_scope(\"accuracy\"):\n#     for epoch, val_acc in zip(range(num_epochs), valid_accuracy_results):\n#       tf.summary.scalar(name = \"accuracy\", data = val_acc, step = epoch)\n# writer2.flush()\n\n# writer3 = tf.summary.create_file_writer(\"./logs/lr\")\n# with writer3.as_default():\n#   with tf.name_scope(\"learning rate\"):\n#     for epoch, lr in zip(range(num_epochs), learning_rate_records):\n#       tf.summary.scalar(name = \"lr\", data = lr, step = epoch)\n# writer3.flush()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropout model:\nmodel = models.Sequential()\nmodel.add(layers.Dense(256, activation='relu', kernel_initializer = 'he_normal', input_shape = (28*28,)))\nmodel.add(layers.Dropout(0.2))\nmodel.add(layers.Dense(128, activation='relu', kernel_initializer = 'he_normal'))\nmodel.add(layers.Dropout(0.2))\nmodel.add(layers.Dense(64, activation='relu', kernel_initializer = 'he_normal'))\nmodel.add(layers.Dropout(0.2))\nmodel.add(layers.Dense(10, activation='softmax'))\n# Instantiate an optimizer\noptimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=2e-06, nesterov=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keras.utils.plot_model(model, show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate a loss function\nloss_fn = keras.losses.CategoricalCrossentropy(from_logits=False)\n# Prepare the metrics\ntrain_acc_metric = keras.metrics.CategoricalAccuracy()\ntrain_loss_avg = tf.keras.metrics.Mean()\ntrain_norm_avg = tf.keras.metrics.Mean()\nval_acc_metric = keras.metrics.CategoricalAccuracy()\nval_loss_avg = tf.keras.metrics.Mean()\n\n# Use the tf.GradientTape context to calculate the gradients used to optimize model:\ndef grad(model, x, y):\n  with tf.GradientTape() as tape:\n    probs = model(x, training = True)\n    # training=True is needed only if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    loss_value = loss_fn(y, probs)\n    grads = tape.gradient(loss_value, model.trainable_variables)\n    norms = [tf.norm(each) for each in grads]\n  return loss_value, grads, norms\n\nnum_epochs = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keep results for plotting\nre_train_accuracy_results = []\nre_train_loss_results = []\nre_train_grad_results = []\nre_train_norm_results = []\nre_valid_accuracy_results = []\nre_valid_loss_results = []\nre_learning_rate_records = []\n\nfor epoch in range(num_epochs):\n  # Iterate over the batches of train dataset.\n  for x_batch_train, y_batch_train in train_batches:\n    loss_value, grads, norms = grad(model, x_batch_train, y_batch_train)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    lr = optimizer._decayed_lr(var_dtype=tf.float32)\n    weights = model.trainable_variables\n    # Track progress\n    train_acc_metric.update_state(y_batch_train, model(x_batch_train, training = True)) # Add current batch acc\n    # training=True is needed only if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    train_loss_avg.update_state(loss_value) # Add current batch loss\n    train_norm_avg.update_state(norms)  # Add current batch norm\n\n  # Run a validation loop over the batches of validation dataset.\n  for x_batch_val, y_batch_val in valid_batches:\n    val_loss, _, _ = grad(model, x_batch_val, y_batch_val)\n    #Track progress\n    val_acc_metric.update_state(y_batch_val, model(x_batch_val, training = True)) # Add current batch acc\n    # training=True is needed only if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    val_loss_avg.update_state(val_loss) # Add current batch loss\n  \n  # End epoch\n  re_train_accuracy_results.append(train_acc_metric.result())\n  re_train_loss_results.append(train_loss_avg.result())\n  re_train_grad_results.append(grads)\n  re_train_norm_results.append(train_norm_avg.result())\n  re_valid_accuracy_results.append(val_acc_metric.result())\n  re_valid_loss_results.append(val_loss_avg.result())\n  re_learning_rate_records.append(lr)\n  #print to console\n  if epoch % 1 == 0:\n    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}, Val-Loss: {:.3f}, Val-Accuracy: {:.3%}, L2 norm: {:.3f}, Learning rate: {}\".format(epoch+1,\n                                                                train_loss_avg.result(),\n                                                                train_acc_metric.result(),\n                                                                val_loss_avg.result(),\n                                                                val_acc_metric.result(),\n                                                                train_norm_avg.result(),\n                                                                lr))\n\n#evaluate on test set\ntest_acc_metric = tf.keras.metrics.CategoricalAccuracy()\nfor (x, y) in test_batches:\n  probs = model(x, training = False)\n  # training=False is needed only if there are layers with different\n  # behavior during training versus inference (e.g. Dropout).\n  prediction = tf.argmax(probs, axis=1, output_type=tf.int32)\n  test_acc_metric(prediction, y)\nprint(\"Test set accuracy: {:.3%}\".format(test_acc_metric.result()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nepochs = range(num_epochs)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, train_loss_results, 'bo', label='Training loss')\nplt.plot(epochs, re_train_loss_results, 'ro', label='Dropout Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, valid_loss_results, 'b', label='Validation loss')\nplt.plot(epochs, re_valid_loss_results, 'r', label='Dropout Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, train_accuracy_results, 'bo', label='Training accuracy')\nplt.plot(epochs, re_train_accuracy_results, 'ro', label='Dropout Training accuracy')\n# b is for \"solid blue line\"\nplt.plot(epochs, valid_accuracy_results, 'b', label='Validation accuracy')\nplt.plot(epochs, re_valid_accuracy_results, 'r', label='Dropout Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}