{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"f85adc08-915d-b0fc-4789-e7793242dceb"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dfd627dc-a88c-19e0-999c-4973d02f513a"},"outputs":[],"source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport sklearn.linear_model as linear_model\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom IPython.display import HTML, display\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\npd.options.display.max_rows = 1000\npd.options.display.max_columns = 20\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/train.csv')\n\nquantitative = [f for f in train.columns if train.dtypes[f] != 'object']\nquantitative.remove('SalePrice')\nquantitative.remove('Id')\nqualitative = [f for f in train.columns if train.dtypes[f] == 'object']"},{"cell_type":"markdown","metadata":{"_cell_guid":"3c396ec6-c89e-c6b4-3a70-c67705b196fe"},"source":"#Overview\n\nThere are 1460 instances of training data and 1460 of test data. Total number of attributes equals 81, of which 36 is quantitative, 43 categorical + Id and SalePrice.\n\n*Quantitative*: 1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold\n\n*Qualitative*: Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities, "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"478f86b7-121f-fdd3-dc5e-350cfbf56491"},"outputs":[],"source":"missing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()"},{"cell_type":"markdown","metadata":{"_cell_guid":"baf30011-463d-cc1f-95b8-97f6c7683549"},"source":"19 attributes have missing values, 5 over 50% of all data. Most of times NA means lack of subject described by attribute, like missing pool, fence, no garage and basement."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8986d183-2fe3-fc8f-19d1-c4fc98eabb82"},"outputs":[],"source":"import scipy.stats as st\ny = train['SalePrice']\nplt.figure(1); plt.title('Johnson SU')\nsns.distplot(y, kde=False, fit=st.johnsonsu)\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=False, fit=st.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y, kde=False, fit=st.lognorm)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e5ef71e3-c595-7444-98ac-f7759eba46fa"},"source":"It is apparent that SalePrice doesn't follow normal distribution, so before performing regression it has to be transformed. While log transformation does pretty good job, best fit is unbounded Johnson distribution."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b960d214-99fd-5781-5c3a-6d48c4cab160"},"outputs":[],"source":"test_normality = lambda x: stats.shapiro(x.fillna(0))[1] < 0.01\nnormal = pd.DataFrame(train[quantitative])\nnormal = normal.apply(test_normality)\nprint(not normal.any())"},{"cell_type":"markdown","metadata":{"_cell_guid":"b3c12438-3a35-d39a-c0ed-e90c958d94b6"},"source":"Also none of quantitative variables has normal distribution so these should be transformed as well."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77409ae1-6436-e002-a031-53a15e9fb4bf"},"outputs":[],"source":"f = pd.melt(train, value_vars=quantitative)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False)\ng = g.map(sns.distplot, \"value\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"dbce817c-5a69-de75-eff1-82a7d5307fd2"},"source":"Some independent variables look like good candidates for log transformation: TotalBsmtSF, KitchenAbvGr, LotFrontage, LotArea and others. While ganining on regression transformation will smooth out some irregularities which could be important like large amount of houses with 0 2ndFlrSF. Such irregularities are good candidates for feature construction."},{"cell_type":"markdown","metadata":{"_cell_guid":"9facce0e-ba89-cb9d-b2aa-8e1aeea438af"},"source":"#Categorical data\n\nWith qualitative variables we can implement two methods. First one is to check distribution of SalePrice with respect to variable values and enumerate them. Second to create dummy variable for each possible category."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"32caf301-13c5-f28c-45c4-4b84f3d09714"},"outputs":[],"source":"for c in qualitative:\n    train[c] = train[c].astype('category')\n    if train[c].isnull().any():\n        train[c] = train[c].cat.add_categories(['MISSING'])\n        train[c] = train[c].fillna('MISSING')\n\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\nf = pd.melt(train, id_vars=['SalePrice'], value_vars=qualitative)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"SalePrice\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"e95e811e-f857-65db-4cec-9f0840e85fb5"},"source":"Some categories seem to more diverse with respect to SalePrice than others. Neighborhood has big impact on house prices. Most expensive seems to be Partial SaleCondition. Having pool on property seems to improve price substantially. There are also differences in variabilities between category values."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"23b33272-5ded-73b1-25d3-61df5648efa0"},"outputs":[],"source":"def anova(frame):\n    anv = pd.DataFrame()\n    anv['feature'] = qualitative\n    pvals = []\n    for c in qualitative:\n        samples = []\n        for cls in frame[c].unique():\n            s = frame[frame[c] == cls]['SalePrice'].values\n            samples.append(s)\n        pval = stats.f_oneway(*samples)[1]\n        pvals.append(pval)\n    anv['pval'] = pvals\n    return anv.sort_values('pval')\n\na = anova(train)\na['disparity'] = np.log(1./a['pval'].values)\nsns.barplot(data=a, x='feature', y='disparity')\nx=plt.xticks(rotation=90)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0285c23b-a0e1-13b5-547e-0e9a4a5dc292"},"source":"Here is quick estimation of influence of categorical variable on SalePrice. For each variable SalePrices are partitioned to distinct sets based on category values. Then check with ANOVA test if sets have similar distributions. If variable has minor impact then set means should be equal. Decreasing pval is sign of increasing diversity in partitions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"38baab99-7e58-4042-9585-2c3a06e2800b"},"outputs":[],"source":"def encode(frame, feature):\n    ordering = pd.DataFrame()\n    ordering['val'] = frame[feature].unique()\n    ordering.index = ordering.val\n    ordering['spmean'] = frame[[feature, 'SalePrice']].groupby(feature).mean()['SalePrice']\n    ordering = ordering.sort_values('spmean')\n    ordering['ordering'] = range(1, ordering.shape[0]+1)\n    ordering = ordering['ordering'].to_dict()\n    \n    for cat, o in ordering.items():\n        frame.loc[frame[feature] == cat, feature+'_E'] = o\n    \nqual_encoded = []\nfor q in qualitative:  \n    encode(train, q)\n    qual_encoded.append(q+'_E')\nprint(qual_encoded)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1cbd8c10-44fe-73c3-6fb6-338769c1d592"},"source":"Now qualitative variables get encoded according to ordering based on mean of SalePrice."},{"cell_type":"markdown","metadata":{"_cell_guid":"f6492edc-bddf-1610-9b42-5a5f7652a691"},"source":"#Correlations\n\nGenerally to reduce confunding only variables uncorrelated with each other should be added to regression models (which are correlated with SalePrice)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d8f0872-1bcc-55e0-2f5a-b78086b8906b"},"outputs":[],"source":"def spearman(frame, features):\n    spr = pd.DataFrame()\n    spr['feature'] = features\n    spr['spearman'] = [frame[f].corr(frame['SalePrice'], 'spearman') for f in features]\n    spr = spr.sort_values('spearman')\n    plt.figure(figsize=(6, 0.25*len(features)))\n    sns.barplot(data=spr, y='feature', x='spearman', orient='h')\n    \nfeatures = quantitative + qual_encoded\nspearman(train, features)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6625d798-2dad-50a6-7b38-7e354856ed7c"},"source":"Spearman correlation is better to work with in this case because it picks up relationships between variables even when they are nonlinear. OverallQual is main criterion in establishing house price. Neighborhood has big influence, partially it has some intrisinc value in itself, but also houses in certain regions tend to share same characteristics (confunding) what causes similar valuations."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5af40eab-f9da-6939-45b0-3d14c7dc7172"},"outputs":[],"source":"plt.figure(1)\ncorr = train[quantitative+['SalePrice']].corr()\nsns.heatmap(corr)\nplt.figure(2)\ncorr = train[qual_encoded+['SalePrice']].corr()\nsns.heatmap(corr)\nplt.figure(3)\ncorr = pd.DataFrame(np.zeros([len(quantitative)+1, len(qual_encoded)+1]), index=quantitative+['SalePrice'], columns=qual_encoded+['SalePrice'])\nfor q1 in quantitative+['SalePrice']:\n    for q2 in qual_encoded+['SalePrice']:\n        corr.loc[q1, q2] = train[q1].corr(train[q2])\nsns.heatmap(corr)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3fd2ac33-3fde-2ed1-f67e-e9fdfb1a925b"},"source":"There are many strong correlations between variables. Garages seem to be built same year as houses, basements have generally same area as first floor which is pretty obvious. Garage area is strongly correlated with number of cars. Neighborhood is correlated with lots of other variables and this confirms the idea that houses in same region share same characteristics. Dwelling type is negatively correlated with kitchen above grade square feet. "},{"cell_type":"markdown","metadata":{"_cell_guid":"f25b8c63-1575-2dcf-55c1-415a560a96e2"},"source":"#Pairplots\n\nIt also would be useful to see how sale price compares to each independent variable."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4f79c9aa-19a3-534e-28df-05d9d9956f06"},"outputs":[],"source":"def pairplot(x, y, **kwargs):\n    ax = plt.gca()\n    ts = pd.DataFrame({'time': x, 'val': y})\n    ts = ts.groupby('time').mean()\n    ts.plot(ax=ax)\n    plt.xticks(rotation=90)\n    \nf = pd.melt(train, id_vars=['SalePrice'], value_vars=quantitative+qual_encoded)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(pairplot, \"value\", \"SalePrice\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"ac16a172-522e-1f47-a598-51523057ca53"},"source":"There are lots of nonlinearities this may be the cause why some variables wont be selected by Lasso/Lars. Some factors like YearBuilt, 1stFlrSF, 2ndFlrSF, Neighborhood_E look like they would benefit from adding quadratic term to regression. But on the other hand this will most probably provoke overfit."},{"cell_type":"markdown","metadata":{"_cell_guid":"721443f7-9639-569e-5c35-f78887448315"},"source":"# Price Segments\n\nIt is possible that correlations shift with change of SalePrice."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b67b0a8e-3cb3-4ad6-559e-c962423908ec"},"outputs":[],"source":"features = quantitative\n\nstandard = train[train['SalePrice'] < 200000]\npricey = train[train['SalePrice'] >= 200000]\n\ndiff = pd.DataFrame()\ndiff['feature'] = features\ndiff['difference'] = [(pricey[f].fillna(0.).mean() - standard[f].fillna(0.).mean())/(standard[f].fillna(0.).mean())\n                      for f in features]\n\nsns.barplot(data=diff, x='feature', y='difference')\nx=plt.xticks(rotation=90)"},{"cell_type":"markdown","metadata":{"_cell_guid":"39c68d9e-8085-3836-7cd2-b45aa29d9276"},"source":"Here houses are divided in two price groups: cheap (under 200000) and expensive. Then means of quantitative variables are compared. Expensive houses have pools, better overall qual and condition, open porch and increased importance of MasVnrArea."},{"cell_type":"markdown","metadata":{"_cell_guid":"e8d4811e-779d-1b65-62c1-42e3f6323269"},"source":"#Clustering"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a831c6cf-ded4-76c1-0d65-ac6d235418b5"},"outputs":[],"source":"features = quantitative + qual_encoded\nmodel = TSNE(n_components=2, random_state=0, perplexity=50)\nX = train[features].fillna(0.).values\ntsne = model.fit_transform(X)\n\nstd = StandardScaler()\ns = std.fit_transform(X)\npca = PCA(n_components=30)\npca.fit(s)\npc = pca.transform(s)\nkmeans = KMeans(n_clusters=5)\nkmeans.fit(pc)\n\nfr = pd.DataFrame({'tsne1': tsne[:,0], 'tsne2': tsne[:, 1], 'cluster': kmeans.labels_})\nsns.lmplot(data=fr, x='tsne1', y='tsne2', hue='cluster', fit_reg=False)\nprint(np.sum(pca.explained_variance_ratio_))"},{"cell_type":"markdown","metadata":{"_cell_guid":"444653bf-bc7a-9dec-e3e7-84744681520f"},"source":"30 First PCA Components explain 75% of variance. There seems to be some clustering present but it is not enough for segmented regression."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d650a15-df6b-0eec-3108-2ee32d6fb466"},"outputs":[],"source":"y = train['SalePrice'].values\ndef johnson(y):\n    gamma, eta, epsilon, lbda = stats.johnsonsu.fit(y)\n    yt = gamma + eta*np.arcsinh((y-epsilon)/lbda)\n    return yt, gamma, eta, epsilon, lbda\n\ndef johnson_inverse(y, gamma, eta, epsilon, lbda):\n    return lbda*np.sinh((y-gamma)/eta) + epsilon\n\nyt, g, et, ep, l = johnson(y)\nyt2 = johnson_inverse(yt, g, et, ep, l)\nplt.figure(1)\nsns.distplot(yt)\nplt.figure(2)\nsns.distplot(yt2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"43ca9480-a76c-0f56-f879-1d1d954a5ed0"},"source":"Here is Johnson Transform of SalePrice. I tried Box Cox, Fisher and Johnson transforms but none of these can beat log in simple linear regression. It is probably because they have tunable parameters that overfit to training data. If there were much more than 1500 instances they could be alternative to log transformation."},{"cell_type":"markdown","metadata":{"_cell_guid":"0d1a2b64-133a-e6f8-1f20-a8ec35ca5df7"},"source":"# Regression"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e6b7d52-9b37-6201-b6cf-0171fc3f9487"},"outputs":[],"source":"def error(actual, predicted):\n    actual = np.log(actual)\n    predicted = np.log(predicted)\n    return np.sqrt(np.sum(np.square(actual-predicted))/len(actual))\n\ndef log_transform(feature):\n    train[feature] = np.log1p(train[feature].values)\n\ndef quadratic(feature):\n    train[feature+'2'] = train[feature]**2\n    \nlog_transform('GrLivArea')\nlog_transform('1stFlrSF')\nlog_transform('2ndFlrSF')\nlog_transform('TotalBsmtSF')\nlog_transform('LotArea')\nlog_transform('LotFrontage')\nlog_transform('KitchenAbvGr')\nlog_transform('GarageArea')\n\nquadratic('OverallQual')\nquadratic('YearBuilt')\nquadratic('YearRemodAdd')\nquadratic('TotalBsmtSF')\nquadratic('2ndFlrSF')\nquadratic('Neighborhood_E')\nquadratic('RoofMatl_E')\nquadratic('GrLivArea')\n\nqdr = ['OverallQual2', 'YearBuilt2', 'YearRemodAdd2', 'TotalBsmtSF2',\n        '2ndFlrSF2', 'Neighborhood_E2', 'RoofMatl_E2', 'GrLivArea2']\n\ntrain['HasBasement'] = train['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntrain['HasGarage'] = train['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntrain['Has2ndFloor'] = train['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntrain['HasMasVnr'] = train['MasVnrArea'].apply(lambda x: 1 if x > 0 else 0)\ntrain['HasWoodDeck'] = train['WoodDeckSF'].apply(lambda x: 1 if x > 0 else 0)\ntrain['HasPorch'] = train['OpenPorchSF'].apply(lambda x: 1 if x > 0 else 0)\ntrain['HasPool'] = train['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntrain['IsNew'] = train['YearBuilt'].apply(lambda x: 1 if x > 2000 else 0)\n\nboolean = ['HasBasement', 'HasGarage', 'Has2ndFloor', 'HasMasVnr', 'HasWoodDeck',\n            'HasPorch', 'HasPool', 'IsNew']\n\n\nfeatures = quantitative + qual_encoded + boolean + qdr\nlasso = linear_model.LassoLarsCV(max_iter=10000)\nX = train[features].fillna(0.).values\nY = train['SalePrice'].values\nlasso.fit(X, np.log(Y))\n\nYpred = np.exp(lasso.predict(X))\nerror(Y, Ypred)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c6d5c367-4732-4056-85cc-a254dd53d439"},"source":"Adding quadratic terms improves local score but behaves unstable on LB."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db019af0-041f-5d1d-bd09-ec684f17de95"},"outputs":[],"source":"import patsy\n\nY, X = patsy.dmatrices(\n    \"SalePrice ~ \\\n        GarageCars + \\\n        np.log1p(BsmtFinSF1) + \\\n        ScreenPorch + \\\n        Condition1_E + \\\n        Condition2_E + \\\n        WoodDeckSF + \\\n        np.log1p(LotArea) + \\\n        Foundation_E + \\\n        MSZoning_E + \\\n        MasVnrType_E + \\\n        HouseStyle_E + \\\n        Fireplaces + \\\n        CentralAir_E + \\\n        BsmtFullBath + \\\n        EnclosedPorch + \\\n        PavedDrive_E + \\\n        ExterQual_E + \\\n        bs(OverallCond, df=7, degree=1) + \\\n        bs(MSSubClass, df=7, degree=1) + \\\n        bs(LotArea, df=2, degree=1) + \\\n        bs(FullBath, df=3, degree=1) + \\\n        bs(HalfBath, df=2, degree=1) + \\\n        bs(BsmtFullBath, df=3, degree=1) + \\\n        bs(TotRmsAbvGrd, df=2, degree=1) + \\\n        bs(LandSlope_E, df=2, degree=1) + \\\n        bs(LotConfig_E, df=2, degree=1) + \\\n        bs(SaleCondition_E, df=3, degree=1) + \\\n        OverallQual + np.square(OverallQual) + \\\n        GrLivArea + np.square(GrLivArea) + \\\n        Q('1stFlrSF') + np.square(Q('1stFlrSF')) + \\\n        Q('2ndFlrSF') + np.square(Q('2ndFlrSF')) +  \\\n        TotalBsmtSF + np.square(TotalBsmtSF) +  \\\n        KitchenAbvGr + np.square(KitchenAbvGr) +  \\\n        YearBuilt + np.square(YearBuilt) + \\\n        Neighborhood_E + np.square(Neighborhood_E) + \\\n        Neighborhood_E:OverallQual + \\\n        MSSubClass:BldgType_E + \\\n        ExterQual_E:OverallQual + \\\n        PoolArea:PoolQC_E + \\\n        Fireplaces:FireplaceQu_E + \\\n        OverallQual:KitchenQual_E + \\\n        GarageQual_E:GarageCond + \\\n        GarageArea:GarageCars + \\\n        Q('1stFlrSF'):TotalBsmtSF + \\\n        TotRmsAbvGrd:GrLivArea\",\n    train.to_dict('list'))\n\nridge = linear_model.RidgeCV(cv=10)\nridge.fit(X, np.log(Y))\nYpred = np.exp(ridge.predict(X))\nprint(error(Y,Ypred))"},{"cell_type":"markdown","metadata":{"_cell_guid":"3449fd3e-fe54-48fc-7dab-911b95a5dee3"},"source":"This gives better local score, but on LB is ~12.6. However it looks that dividing some variables into splines gives some improvement."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}