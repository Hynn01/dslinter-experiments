{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n<img src=\"https://i.imgur.com/x2XjjFV.jpg\" width=\"250px\">"},{"metadata":{},"cell_type":"markdown","source":"Welcome to the Abstraction and Reasoning Challenge (ARC), a potential major step towards achieving artificial general intelligence (AGI). In this competition, we are challenged to build an algorithm that can perform reasoning tasks it has never seen before. Classic machine learning problems generally involve one specific task which can be solved by training on millions of data samples. But in this challenge, we need to build an algorithm that can learn patterns from a minimal number of examples.\n\nIn this notebook, I will be demonstrating how one can use **data augmentation** and **supervised machine learning** to build a baseline model to solve this problem.\n\n<font color=\"red\" size=3>Please upvote this kernel if you like it. It motivates me to produce more quality content :)</font>"},{"metadata":{},"cell_type":"markdown","source":"# Contents\n\n* [<font size=4>Preparing the ground</font>](#preparing-the-ground)\n    * [Import libraries and define hyperparameters](#import-libraries-and-define-hyperparameters)\n    * [Load the ARC data](#load-the-arc-data)\n    \n\n* [<font size=4>Basic exploration</font>](#basic-exploration)\n    * [Look at few train/test input/output pairs](#look-at-few)\n    * [Number frequency](#number-frequency)\n    * [Matrix mean values](#matrix-mean-values)\n    * [Matrix heights](#matrix-heights)\n    * [Matrix widths](#matrix-widths)\n    * [Height vs. Width](#height-vs-width)\n    \n\n* [<font size=4>My approach</font>](#my-approach)\n    * [Data processing](#data-processing)\n    * [Modeling](#modeling)\n\n\n* [<font size=4>Training and postprocessing</font>](#training-and-postprocessing)\n    * [Loss (MSE)](#loss)\n    * [Backpropagation and optimization (Adam)](#backprop)\n\n\n* [<font size=4>Submission</font>](#submission)\n\n\n* [<font size=4>Ending note</font>](#ending-note)"},{"metadata":{},"cell_type":"markdown","source":"# Preparing the ground <a id=\"preparing-the-ground\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## Import libraries and define hyperparameters <a id=\"import-libraries-and-define-hyperparameters\"></a> "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport json\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom keras.utils import to_categorical\n\nimport seaborn as sns\nimport plotly.express as px\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt\nimport plotly.figure_factory as ff\n\nimport torch\nT = torch.Tensor\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SIZE = 1000\nEPOCHS = 50\nCONV_OUT_1 = 50\nCONV_OUT_2 = 100\nBATCH_SIZE = 128\n\nTEST_PATH = Path('../input/abstraction-and-reasoning-challenge/')\nSUBMISSION_PATH = Path('../input/abstraction-and-reasoning-challenge/')\n\nTEST_PATH = TEST_PATH / 'test'\nSUBMISSION_PATH = SUBMISSION_PATH / 'sample_submission.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load the ARC data <a id=\"load-the-arc-data\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### Get testing tasks"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_task_files = sorted(os.listdir(TEST_PATH))\n\ntest_tasks = []\nfor task_file in test_task_files:\n    with open(str(TEST_PATH / task_file), 'r') as f:\n        task = json.load(f)\n        test_tasks.append(task)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extract training and testing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xs_test, Xs_train, ys_train = [], [], []\n\nfor task in test_tasks:\n    X_test, X_train, y_train = [], [], []\n\n    for pair in task[\"test\"]:\n        X_test.append(pair[\"input\"])\n\n    for pair in task[\"train\"]:\n        X_train.append(pair[\"input\"])\n        y_train.append(pair[\"output\"])\n    \n    Xs_test.append(X_test)\n    Xs_train.append(X_train)\n    ys_train.append(y_train)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"matrices = []\nfor X_test in Xs_test:\n    for X in X_test:\n        matrices.append(X)\n        \nvalues = []\nfor matrix in matrices:\n    for row in matrix:\n        for value in row:\n            values.append(value)\n            \ndf = pd.DataFrame(values)\ndf.columns = [\"values\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic exploration <a id=\"basic-exploration\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## Look at a few train/test input/output pairs <a id=\"look-at-few\"></a>\n\nThese are some of the pairs present in the training data. I use functions from Walter's excellent starter kernel to plot these pairs."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"data_path = Path('/kaggle/input/abstraction-and-reasoning-challenge/')\ntraining_path = data_path / 'training'\ntraining_tasks = sorted(os.listdir(training_path))\n\nfor i in [1, 19, 8, 15, 9]:\n\n    task_file = str(training_path / training_tasks[i])\n\n    with open(task_file, 'r') as f:\n        task = json.load(f)\n\n    def plot_task(task):\n        \"\"\"\n        Plots the first train and test pairs of a specified task,\n        using same color scheme as the ARC app\n        \"\"\"\n        cmap = colors.ListedColormap(\n            ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n             '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n        norm = colors.Normalize(vmin=0, vmax=9)\n        fig, ax = plt.subplots(1, 4, figsize=(15,15))\n        ax[0].imshow(task['train'][0]['input'], cmap=cmap, norm=norm)\n        width = np.shape(task['train'][0]['input'])[1]\n        height = np.shape(task['train'][0]['input'])[0]\n        ax[0].set_xticks(np.arange(0,width))\n        ax[0].set_yticks(np.arange(0,height))\n        ax[0].set_xticklabels([])\n        ax[0].set_yticklabels([])\n        ax[0].tick_params(length=0)\n        ax[0].grid(True)\n        ax[0].set_title('Train Input')\n        ax[1].imshow(task['train'][0]['output'], cmap=cmap, norm=norm)\n        width = np.shape(task['train'][0]['output'])[1]\n        height = np.shape(task['train'][0]['output'])[0]\n        ax[1].set_xticks(np.arange(0,width))\n        ax[1].set_yticks(np.arange(0,height))\n        ax[1].set_xticklabels([])\n        ax[1].set_yticklabels([])\n        ax[1].tick_params(length=0)\n        ax[1].grid(True)\n        ax[1].set_title('Train Output')\n        ax[2].imshow(task['test'][0]['input'], cmap=cmap, norm=norm)\n        width = np.shape(task['test'][0]['input'])[1]\n        height = np.shape(task['test'][0]['input'])[0]\n        ax[2].set_xticks(np.arange(0,width))\n        ax[2].set_yticks(np.arange(0,height))\n        ax[2].set_xticklabels([])\n        ax[2].set_yticklabels([])\n        ax[2].tick_params(length=0)\n        ax[2].grid(True)\n        ax[2].set_title('Test Input')\n        ax[3].imshow(task['test'][0]['output'], cmap=cmap, norm=norm)\n        width = np.shape(task['test'][0]['output'])[1]\n        height = np.shape(task['test'][0]['output'])[0]\n        ax[3].set_xticks(np.arange(0,width))\n        ax[3].set_yticks(np.arange(0,height))\n        ax[3].set_xticklabels([])\n        ax[3].set_yticklabels([])\n        ax[3].tick_params(length=0)\n        ax[3].grid(True)\n        ax[3].set_title('Test Output')\n        plt.tight_layout()\n        plt.show()\n\n    plot_task(task)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Number frequency <a id=\"number-frequency\"></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"px.histogram(df, x=\"values\", title=\"Numbers present in matrices\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graph, we can clearly see that the number distribution has a string positive skew. Most numbers in the matrices are clearly 0. This is reflected by the dominance of black color in most matrices."},{"metadata":{},"cell_type":"markdown","source":"## Matrix mean values <a id=\"matrix-mean-values\"></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"means = [np.mean(X) for X in matrices]\nfig = ff.create_distplot([means], group_labels=[\"Means\"], colors=[\"green\"])\nfig.update_layout(title_text=\"Distribution of matrix mean values\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graph, we can see that lower means are more common than higher means. The graph, once again, has a strong positive skew. This is further proof that black is the most dominant color in the matrices."},{"metadata":{},"cell_type":"markdown","source":"## Matrix heights <a id=\"matrix-heights\"></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"heights = [np.shape(matrix)[0] for matrix in matrices]\nwidths = [np.shape(matrix)[1] for matrix in matrices]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = ff.create_distplot([heights], group_labels=[\"Height\"], colors=[\"magenta\"])\nfig.update_layout(title_text=\"Distribution of matrix heights\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graph, we can see that matrix heights have a much more uniform distribution (with significantly less skew). The distribution is somewhat normal with a mean of approximately 15."},{"metadata":{},"cell_type":"markdown","source":"## Matrix widths <a id=\"matrix-widths\"></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = ff.create_distplot([widths], group_labels=[\"Width\"], colors=[\"red\"])\nfig.update_layout(title_text=\"Distribution of matrix widths\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graph, we can see that matrix widths also have a uniform distribution (with significantly less skew). The distribution is also somewhat uniform with a mean of approximately 16."},{"metadata":{},"cell_type":"markdown","source":"## Height vs. Width <a id=\"height-vs-width\"></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot = sns.jointplot(widths, heights, kind=\"kde\", color=\"blueviolet\")\nplot.set_axis_labels(\"Width\", \"Height\", fontsize=14)\nplt.show(plot)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot = sns.jointplot(widths, heights, kind=\"reg\", color=\"blueviolet\")\nplot.set_axis_labels(\"Width\", \"Height\", fontsize=14)\nplt.show(plot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graphs, we can see that heights and widths have a strong positive correlation, *i.e.* greater widths generally result in greater heights. This is consistent with the fact that most matrices are square-shaped."},{"metadata":{},"cell_type":"markdown","source":"# My approach <a id=\"my-approach\"></a>"},{"metadata":{},"cell_type":"markdown","source":"My approach to this problem imvolves simple data augmentation techniques and a supervised 2D CNN model to make predictions. The model takes a 2D matrix as input and outputs the softmax probabilities of different values occuring in the output matrix. But since we have only few training examples for each task, I create new input-output pairs by randomly switching colors. The extra augmented data helps the model capture patterns more easily."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/ott07Lh.png\" width=\"750px\">"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/H96WieH.png\" width=\"300px\">"},{"metadata":{},"cell_type":"markdown","source":"It can be seen from the above diagram that the same training pairs are augmented (100s of times) to produce a large dataset. This dataset is used to train the CNN for each task. The CNN predicts a probability distribution over the \"pixels\" or values in the matrix. This probability distribution is used to generate the final output matrix.\n\nThe trained CNN model can be used to make predictions on the test samples as follows:"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/I3n0Q2k.png\" width=\"750px\">"},{"metadata":{},"cell_type":"markdown","source":"## Data processing <a id=\"data-processing\"></a>"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/pa9C1rz.png\" width=\"400px\">"},{"metadata":{},"cell_type":"markdown","source":"The basic steps in my data processing pipeline are given above. These steps can be summarized as:\n\n1. **Handle matrix (input and output) dimensions:** Ensure consistent dimensions among inputs and outputs\n2. **Randomly augment input and output matrices:** Mutate the matrix values in order to generate new data for each task\n3. **Return input-output pairs along with dimension information:** Return the X-y data with dimensions"},{"metadata":{},"cell_type":"markdown","source":"### Helper functions"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def replace_values(a, d):\n    return np.array([d.get(i, -1) for i in range(a.min(), a.max() + 1)])[a - a.min()]\n\ndef repeat_matrix(a):\n    return np.concatenate([a]*((SIZE // len(a)) + 1))[:SIZE]\n\ndef get_new_matrix(X):\n    if len(set([np.array(x).shape for x in X])) > 1:\n        X = np.array([X[0]])\n    return X\n\ndef get_outp(outp, dictionary=None, replace=True):\n    if replace:\n        outp = replace_values(outp, dictionary)\n\n    outp_matrix_dims = outp.shape\n    outp_probs_len = outp.shape[0]*outp.shape[1]*10\n    outp = to_categorical(outp.flatten(),\n                          num_classes=10).flatten()\n\n    return outp, outp_probs_len, outp_matrix_dims","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PyTorch DataLoader"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ARCDataset(Dataset):\n    def __init__(self, X, y, stage=\"train\"):\n        self.X = get_new_matrix(X)\n        self.X = repeat_matrix(self.X)\n        \n        self.stage = stage\n        if self.stage == \"train\":\n            self.y = get_new_matrix(y)\n            self.y = repeat_matrix(self.y)\n        \n    def __len__(self):\n        return SIZE\n    \n    def __getitem__(self, idx):\n        inp = self.X[idx]\n        if self.stage == \"train\":\n            outp = self.y[idx]\n\n        if idx != 0:\n            rep = np.arange(10)\n            orig = np.arange(10)\n            np.random.shuffle(rep)\n            dictionary = dict(zip(orig, rep))\n            inp = replace_values(inp, dictionary)\n            if self.stage == \"train\":\n                outp, outp_probs_len, outp_matrix_dims = get_outp(outp, dictionary)\n                \n        if idx == 0:\n            if self.stage == \"train\":\n                outp, outp_probs_len, outp_matrix_dims = get_outp(outp, None, False)\n        \n        return inp, outp, outp_probs_len, outp_matrix_dims, self.y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling <a id=\"modeling\"></a>"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/cpUtXRR.png\" width=\"600px\">"},{"metadata":{},"cell_type":"markdown","source":"\n\nI use a basic CNN model that takes 2D input and returns 2D output. The sequential architecture is follows:\n\n1. (Conv2D + ReLU) **x** 2\n2. MaxPool **x** 2\n3. Dense\n4. Softmax\n\nThe softmax probabilities are converted to the final 2D matrix through argmax and resize functions."},{"metadata":{},"cell_type":"markdown","source":"### PyTorch CNN model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BasicCNNModel(nn.Module):\n    def __init__(self, inp_dim=(10, 10), outp_dim=(10, 10)):\n        super(BasicCNNModel, self).__init__()\n        \n        CONV_IN = 3\n        KERNEL_SIZE = 3\n        DENSE_IN = CONV_OUT_2\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n        self.dense_1 = nn.Linear(DENSE_IN, outp_dim[0]*outp_dim[1]*10)\n        \n        if inp_dim[0] < 5 or inp_dim[1] < 5:\n            KERNEL_SIZE = 1\n\n        self.conv2d_1 = nn.Conv2d(CONV_IN, CONV_OUT_1, kernel_size=KERNEL_SIZE)\n        self.conv2d_2 = nn.Conv2d(CONV_OUT_1, CONV_OUT_2, kernel_size=KERNEL_SIZE)\n\n    def forward(self, x, outp_dim):\n        x = torch.cat([x.unsqueeze(0)]*3)\n        x = x.permute((1, 0, 2, 3)).float()\n        self.conv2d_1.in_features = x.shape[1]\n        conv_1_out = self.relu(self.conv2d_1(x))\n        self.conv2d_2.in_features = conv_1_out.shape[1]\n        conv_2_out = self.relu(self.conv2d_2(conv_1_out))\n        \n        self.dense_1.out_features = outp_dim\n        feature_vector, _ = torch.max(conv_2_out, 2)\n        feature_vector, _ = torch.max(feature_vector, 2)\n        logit_outputs = self.dense_1(feature_vector)\n        \n        out = []\n        for idx in range(logit_outputs.shape[1]//10):\n            out.append(self.softmax(logit_outputs[:, idx*10: (idx+1)*10]))\n        return torch.cat(out, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and postprocessing <a id=\"training-and-postprocessing\"></a>"},{"metadata":{},"cell_type":"markdown","source":"I train the model using PyTorch's autograd functionality. Specifically, I use the **Adam** optimizer and the **MSE** loss function."},{"metadata":{},"cell_type":"markdown","source":"## Loss (MSE) <a id=\"loss\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### The idea behind a loss function"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/WeQbG9M.png\" width=\"275px\">"},{"metadata":{},"cell_type":"markdown","source":"As shown above, the target vector *t* and the output vector *o* are diverging from each other. The loss function measures the degree to which these two diverge, *i.e.* the size of *o - t*. Here, *t* is the actual pixel probability and *o* is the predicted pixel probability. The mean squared error calculates the average squared error between *o* and *t*. \n\nIn the code, the line <code>train_loss = nn.MSELoss()(train_preds, train_y)</code> calculates the MSE loss."},{"metadata":{},"cell_type":"markdown","source":"## Backpropagation and optimization (Adam) <a id=\"backprop\"></a>"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/IEeg94y.png\" width=\"550px\">"},{"metadata":{},"cell_type":"markdown","source":"In the above diagram, we can see that Newton's Chain rule is used to calculate the gradient of the loss function *w.r.t.* the parameters in the model."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/yEVIBzj.png\" width=\"350px\">"},{"metadata":{},"cell_type":"markdown","source":"The general update equation above is used to optimize the parameters using the gradients calculated above. Note that more complex algorithms like Adam use more complex update equations than the ones specified above.\n\nIn the code, the line <code>train_loss.backward()</code> and <code>optimizer.step()</code> perform backpropagation and optimization respectively."},{"metadata":{},"cell_type":"markdown","source":"### Helper functions"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def transform_dim(inp_dim, outp_dim, test_dim):\n    return (test_dim[0]*outp_dim[0]/inp_dim[0],\n            test_dim[1]*outp_dim[1]/inp_dim[1])\n\ndef resize(x, test_dim, inp_dim):\n    if inp_dim == test_dim:\n        return x\n    else:\n        return cv2.resize(flt(x), inp_dim,\n                          interpolation=cv2.INTER_AREA)\n\ndef flt(x): return np.float32(x)\ndef npy(x): return x.cpu().detach().numpy()\ndef itg(x): return np.int32(np.round(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train the model and postprocess probabilties"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/rD53yoI.png\" width=\"350px\">"},{"metadata":{},"cell_type":"markdown","source":"In my postprocessing, I follow the steps given below:\n\n1. **Get output probabilites from the CNN model:** \n\n<code>npy(network.forward(T(X).unsqueeze(0), out_d))</code>\n2. **Perform argmax on probabilities to get indices of maximum prbabilities:**\n\n<code>np.argmax(test_preds.reshape((10, *outp_dim)), axis=0)</code>\n3. **Resize the output matrix to match the dimension ratios and round off:**\n\n<code>itg(resize(test_preds, np.shape(test_preds), tuple(itg(transform_dim(inp_dim, outp_dim, test_dim))))))</code>"},{"metadata":{},"cell_type":"markdown","source":"### Train the CNN model on loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = 0\nstart = time.time()\ntest_predictions = []\n\nfor X_train, y_train in zip(Xs_train, ys_train):\n    print(\"TASK \" + str(idx + 1))\n\n    train_set = ARCDataset(X_train, y_train, stage=\"train\")\n    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n\n    inp_dim = np.array(X_train[0]).shape\n    outp_dim = np.array(y_train[0]).shape\n    network = BasicCNNModel(inp_dim, outp_dim).cuda()\n    optimizer = Adam(network.parameters(), lr=0.01)\n    \n    for epoch in range(EPOCHS):\n        for train_batch in train_loader:\n            train_X, train_y, out_d, d, out = train_batch\n            train_preds = network.forward(train_X.cuda(), out_d.cuda())\n            train_loss = nn.MSELoss()(train_preds, train_y.cuda())\n            \n            optimizer.zero_grad()\n            train_loss.backward()\n            optimizer.step()\n\n    end = time.time()        \n    print(\"Train loss: \" + str(np.round(train_loss.item(), 3)) + \"   \" +\\\n          \"Total time: \" + str(np.round(end - start, 1)) + \" s\" + \"\\n\")\n    \n    X_test = np.array([resize(flt(X), np.shape(X), inp_dim) for X in Xs_test[idx-1]])\n    for X in X_test:\n        test_dim = np.array(T(X)).shape\n        test_preds = npy(network.forward(T(X).unsqueeze(0).cuda(), out_d.cuda()))\n        test_preds = np.argmax(test_preds.reshape((10, *outp_dim)), axis=0)\n        test_predictions.append(itg(resize(test_preds, np.shape(test_preds),\n                                           tuple(itg(transform_dim(inp_dim,\n                                                                   outp_dim,\n                                                                   test_dim))))))\n    idx += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission <a id=\"submission\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### Define function to flatten submission matrices"},{"metadata":{"trusted":true},"cell_type":"code","source":"def flattener(pred):\n    str_pred = str([row for row in pred])\n    str_pred = str_pred.replace(', ', '')\n    str_pred = str_pred.replace('[[', '|')\n    str_pred = str_pred.replace('][', '|')\n    str_pred = str_pred.replace(']]', '|')\n    return str_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare submission dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = [[list(pred) for pred in test_pred] for test_pred in test_predictions]\n\nfor idx, pred in enumerate(test_predictions):\n    test_predictions[idx] = flattener(pred)\n    \nsubmission = pd.read_csv(SUBMISSION_PATH)\nsubmission[\"output\"] = test_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert submission to .csv format"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ending note <a id=\"ending-note\"></a>\n\n<font color=\"red\" size=4>Please upvote this kernel if you like it. It motivates me to produce more quality content :)</font>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}