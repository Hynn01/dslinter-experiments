{"cells":[{"metadata":{"id":"H05UvYiVIVnw"},"cell_type":"markdown","source":"# RNN Prediction"},{"metadata":{"id":"FZqjN4jMIVoK"},"cell_type":"markdown","source":"## Imports"},{"metadata":{"id":"YVAE7gJIIVoP","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport os\nimport datetime\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"id":"mQsN0AwhIVob"},"cell_type":"markdown","source":"## We need to import several things from Keras."},{"metadata":{"id":"Ub5oRJWaIVoc","trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, GRU, Embedding\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\nfrom tensorflow.keras.backend import square, mean","execution_count":null,"outputs":[]},{"metadata":{"id":"ySSddPyKIVoo"},"cell_type":"markdown","source":"## This was developed using Python 3.6 (Anaconda) and package versions:"},{"metadata":{"scrolled":false,"id":"Y51Qj1IGIVot","outputId":"683369aa-893b-4061-b452-d02a8cfab2af","executionInfo":{"status":"ok","timestamp":1588056568669,"user_tz":-480,"elapsed":660,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"tf.__version__","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"id":"2EinWDrBIVo4","outputId":"e8e8cb6b-886a-4bd9-9f84-1b1890017c3f","executionInfo":{"status":"ok","timestamp":1588056570187,"user_tz":-480,"elapsed":809,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"tf.keras.__version__","execution_count":null,"outputs":[]},{"metadata":{"id":"d5tPLkVcIVpJ","outputId":"c9fffd19-0ecd-42f2-d4fe-c8260d3bda69","executionInfo":{"status":"ok","timestamp":1588056570777,"user_tz":-480,"elapsed":1052,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"pd.__version__","execution_count":null,"outputs":[]},{"metadata":{"id":"qUf8dWRyIVpa"},"cell_type":"markdown","source":"## Load the pre-processed Data"},{"metadata":{"id":"yUeI5FBHIVpb","outputId":"11aa833b-dcb5-485a-f96c-a021a3338fbb","executionInfo":{"status":"ok","timestamp":1588056578143,"user_tz":-480,"elapsed":1257,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"!ls \"../input/ca-data\"","execution_count":null,"outputs":[]},{"metadata":{"id":"aCK7H_ulIVpw","trusted":true},"cell_type":"code","source":"path = '../input/ca-data/'\nca1_data = pd.read_csv(path+\"CA1_ext.csv\")\nca2_data = pd.read_csv(path+\"CA2_ext.csv\")\nca3_data = pd.read_csv(path+\"CA_3_ext.csv\")\nca4_data = pd.read_csv(path+\"CA4_ext.csv\")\ntx1_data = pd.read_csv(path+\"TX_1_ext.csv\")\ntx2_data = pd.read_csv(path+\"TX_2_ext.csv\")\ntx3_data = pd.read_csv(path+\"TX_3_ext.csv\")\nwi1_data = pd.read_csv(path+\"WI_1_ext.csv\")\nwi2_data = pd.read_csv(path+\"WI_2_ext.csv\")\nwi3_data = pd.read_csv(path+\"WI_3_ext.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"pjS8hj2ajRS9","outputId":"4eb3af1a-816a-49ec-d2ae-76aead3adfce","executionInfo":{"status":"ok","timestamp":1588057879051,"user_tz":-480,"elapsed":1205,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"data = {}\ndata[\"CA_1\"] = ca1_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"CA_2\"] = ca2_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"CA_3\"] = ca3_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"CA_4\"] = ca4_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"TX_1\"] = tx1_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"TX_2\"] = tx2_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"TX_3\"] = tx3_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"WI_1\"] = wi1_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"WI_2\"] = wi2_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"WI_3\"] = wi3_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"CA_1\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"List of the cities used in the data-set."},{"metadata":{"trusted":true},"cell_type":"code","source":"listofstore = [\"CA_1\",\"CA_2\",\"CA_3\",\"CA_4\",\"TX_1\",\"TX_2\",\"TX_3\",\"WI_1\",\"WI_2\",\"WI_3\"]\nlistofstore","execution_count":null,"outputs":[]},{"metadata":{"id":"Tl-iZ5Tyodz1","trusted":true},"cell_type":"code","source":"data_temp = data[\"CA_1\"].join(data[\"CA_2\"], lsuffix='_CA_1', rsuffix='_CA_2')\nfor store in listofstore[2:]:\n    data_temp1 = data_temp.join(data[store], lsuffix='', rsuffix=store)\n    data_temp1 = data_temp1.rename(columns={\"Hobbie_revenue\": \"Hobbie_revenue_\"+store,\"House_revenue\": \"House_revenue_\"+store,\"Foods_revenue\": \"Foods_revenue_\"+store})\n    data_temp = data_temp1\n    \n#data_temp = data_temp1.join(data[\"TX_1\"], lsuffix='', rsuffix='_TX_1')\n\ndata_df = data_temp1.copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"vuZjO5r0IVqp"},"cell_type":"markdown","source":"These are the top rows of the data-set."},{"metadata":{"id":"w-SENxIsIVqr","trusted":true},"cell_type":"code","source":"data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"2-op7XuLIVrP"},"cell_type":"markdown","source":"There are 3*10 input-signals in the data-set. There are 1913 rows"},{"metadata":{"id":"nNvQgasiIVrQ","trusted":true},"cell_type":"code","source":"data_df.values.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"Ku_A65D4IVsX"},"cell_type":"markdown","source":"### Add Data\n\nWe can add some input-signals to the data that may help our model in making predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nnumdays = 1913\nbase = datetime.datetime(2011, 1, 29)\ndate_list = [base + datetime.timedelta(days=x) for x in range(numdays)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\ndayofyearlist = [i.timetuple().tm_yday for i in date_list]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df[\"Dayofyear\"] = dayofyearlist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df","execution_count":null,"outputs":[]},{"metadata":{"id":"ojJsyuMnIVsj"},"cell_type":"markdown","source":"### Target Data for Prediction"},{"metadata":{"id":"X5IwaoN-IVsp","trusted":true},"cell_type":"code","source":"target_store = 'CA_1'","execution_count":null,"outputs":[]},{"metadata":{"id":"GH2XDXhHIVsx"},"cell_type":"markdown","source":"We will try and predict these signals."},{"metadata":{"id":"NhnP686pIVsy","trusted":true},"cell_type":"code","source":"target_names = ['Hobbie_revenue', 'House_revenue', 'Foods_revenue']","execution_count":null,"outputs":[]},{"metadata":{"id":"K8_Wo9LFIVs5"},"cell_type":"markdown","source":"The following is the number of time-steps that we will shift the target-data. Our data-set is sampled to have an observation for each day, so there are 30 observations for a month.\n\nWe want to predict the Revenue 1 month into the future, we shift the data 30 time-steps"},{"metadata":{"id":"IaHaEoxvIVs6","trusted":true},"cell_type":"code","source":"shift_months = 1\nshift_steps = shift_months * 30  # Number of days.","execution_count":null,"outputs":[]},{"metadata":{"id":"5UbB7pQSIVtD","trusted":true},"cell_type":"code","source":"data_targets = data[target_store][target_names].shift(-shift_steps)","execution_count":null,"outputs":[]},{"metadata":{"id":"-W6hsBovIVtM","outputId":"6a785551-9993-4297-e3ac-bfcb7627a925","executionInfo":{"status":"ok","timestamp":1588057199714,"user_tz":-480,"elapsed":1930,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"data[target_store][target_names].head(shift_steps + 5)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"id":"auH7K3oqIVtc","outputId":"99f0af09-9896-4981-97e9-f35a846189ed","executionInfo":{"status":"ok","timestamp":1588057228249,"user_tz":-480,"elapsed":1103,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"data_targets.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"fyyQBOVSIVtn","outputId":"16a9866d-afbb-4746-a915-71f1c0b5ccdf","executionInfo":{"status":"ok","timestamp":1588057237507,"user_tz":-480,"elapsed":5502,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"data_targets.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"ApByqsvuIVtx"},"cell_type":"markdown","source":"### NumPy Arrays\n\nWe now convert the Pandas data-frames to NumPy arrays that can be input to the neural network. We also remove the last part of the numpy arrays, because the target-data has `NaN` for the shifted period, and we only want to have valid data and we need the same array-shapes for the input- and output-data.\n\nThese are the input-signals:"},{"metadata":{"id":"6vzqaXJ4sU3e","outputId":"567c9e88-14be-4b88-c1e7-4bcbc2bf29d1","executionInfo":{"status":"ok","timestamp":1588059067202,"user_tz":-480,"elapsed":1208,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"data_df.values","execution_count":null,"outputs":[]},{"metadata":{"id":"fqFehoGcIVty","trusted":true},"cell_type":"code","source":"x_data = data_df.values[0:-shift_steps]","execution_count":null,"outputs":[]},{"metadata":{"id":"3KWUi4byIVuA","outputId":"d1e2dda4-009e-4c59-c8e3-77122fd097ae","executionInfo":{"status":"ok","timestamp":1588059090142,"user_tz":-480,"elapsed":1268,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"print(type(x_data))\nprint(\"Shape:\", x_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"bTw9RpMBIVuS"},"cell_type":"markdown","source":"These are the output-signals (or target-signals):"},{"metadata":{"id":"9oaccYGcIVuU","outputId":"f7cfdfd9-bf8c-4758-813a-0d97ee973e61","executionInfo":{"status":"ok","timestamp":1588059097040,"user_tz":-480,"elapsed":1153,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"y_data = data_targets.values[:-shift_steps]\ny_data","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"id":"BYB0sctGIVuh","outputId":"c9249752-551f-45ae-cf5d-449d30cddcc3","executionInfo":{"status":"ok","timestamp":1588059109952,"user_tz":-480,"elapsed":3522,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"print(type(y_data))\nprint(\"Shape:\", y_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"GW2nZn8hIVuq"},"cell_type":"markdown","source":"This is the number of observations (aka. data-points or samples) in the data-set:"},{"metadata":{"id":"RbAjoulTIVur","outputId":"ecca0a6d-5ab5-451c-b6d3-50b98665faab","executionInfo":{"status":"ok","timestamp":1588059124134,"user_tz":-480,"elapsed":1979,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"num_data = len(x_data)\nnum_data","execution_count":null,"outputs":[]},{"metadata":{"id":"yCQSYVkOIVuv"},"cell_type":"markdown","source":"This is the fraction of the data-set that will be used for the training-set:"},{"metadata":{"id":"EqnoLkTtIVuw","trusted":true},"cell_type":"code","source":"train_split = 0.9","execution_count":null,"outputs":[]},{"metadata":{"id":"GEgW1i9cIVu8"},"cell_type":"markdown","source":"This is the number of observations in the training-set:"},{"metadata":{"id":"bh-BSwOdIVu9","outputId":"ecb5a8a9-4d77-4a36-c7cd-b8d190a6e330","executionInfo":{"status":"ok","timestamp":1588059133821,"user_tz":-480,"elapsed":1294,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"num_train = int(train_split * num_data)\nnum_train","execution_count":null,"outputs":[]},{"metadata":{"id":"-9udFOJcIVvE"},"cell_type":"markdown","source":"This is the number of observations in the test-set:"},{"metadata":{"id":"Fa2DJvJvIVvF","outputId":"40eb9212-7150-43db-ec3b-2e42109a4f8c","executionInfo":{"status":"ok","timestamp":1588059139601,"user_tz":-480,"elapsed":1338,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"num_test = num_data - num_train\nnum_test","execution_count":null,"outputs":[]},{"metadata":{"id":"YPrv0UtfIVvJ"},"cell_type":"markdown","source":"These are the input-signals for the training- and test-sets:"},{"metadata":{"id":"loBBgucjIVvK","outputId":"d8f81e89-4d2a-4c03-c7df-06bb991a6b45","executionInfo":{"status":"ok","timestamp":1588059147951,"user_tz":-480,"elapsed":2551,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"x_train = x_data[0:num_train]\nx_test = x_data[num_train:]\nlen(x_train) + len(x_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"duLF7f7KIVvR"},"cell_type":"markdown","source":"These are the output-signals for the training- and test-sets:"},{"metadata":{"id":"rZW5vLc-IVvS","outputId":"28958738-e68e-449a-cc2b-8f97f412698f","executionInfo":{"status":"ok","timestamp":1588059154446,"user_tz":-480,"elapsed":1537,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"y_train = y_data[0:num_train]\ny_test = y_data[num_train:]\nlen(y_train) + len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"hFgUgX_aIVvZ"},"cell_type":"markdown","source":"This is the number of input-signals:"},{"metadata":{"id":"VvIJlLOHIVva","outputId":"aee18a5f-c960-4b6b-bfc4-cfdcf8b6d1ed","executionInfo":{"status":"ok","timestamp":1588059161006,"user_tz":-480,"elapsed":1595,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"num_x_signals = x_data.shape[1]\nnum_x_signals","execution_count":null,"outputs":[]},{"metadata":{"id":"7Pj3lVM6IVvd"},"cell_type":"markdown","source":"This is the number of output-signals:"},{"metadata":{"scrolled":true,"id":"XvKpX9KkIVvd","outputId":"c5cf24d9-296d-4617-f4ea-a317c8b88aa6","executionInfo":{"status":"ok","timestamp":1588059168572,"user_tz":-480,"elapsed":1892,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"num_y_signals = y_data.shape[1]\nnum_y_signals","execution_count":null,"outputs":[]},{"metadata":{"id":"57H-vpNlIVvi"},"cell_type":"markdown","source":"### Scaled Data\n\nThe data-set contains a wide range of values:"},{"metadata":{"id":"9W60yaiDIVvj","outputId":"bedde7e4-15a5-439a-8c0e-973088335c27","executionInfo":{"status":"ok","timestamp":1588059180097,"user_tz":-480,"elapsed":3913,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"print(\"Min:\", np.min(x_train))\nprint(\"Max:\", np.max(x_train))","execution_count":null,"outputs":[]},{"metadata":{"id":"PWmy05n1IVvr"},"cell_type":"markdown","source":"The neural network works best on values roughly between -1 and 1, so we need to scale the data before it is being input to the neural network. We can use `scikit-learn` for this.\n\nWe first create a scaler-object for the input-signals."},{"metadata":{"id":"qGVl2Xp7IVvs","trusted":true},"cell_type":"code","source":"x_scaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"id":"dLQ79tlGIVvw"},"cell_type":"markdown","source":"We then detect the range of values from the training-data and scale the training-data."},{"metadata":{"id":"_rgan65YIVvx","trusted":true},"cell_type":"code","source":"x_train_scaled = x_scaler.fit_transform(x_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"YoKABEY5IVv5"},"cell_type":"markdown","source":"Apart from a small rounding-error, the data has been scaled to be between 0 and 1."},{"metadata":{"id":"GdYqh7FCIVv5","outputId":"d1527080-f1e6-4c14-9f09-ce3d54501284","executionInfo":{"status":"ok","timestamp":1588059199920,"user_tz":-480,"elapsed":1372,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"print(\"Min:\", np.min(x_train_scaled))\nprint(\"Max:\", np.max(x_train_scaled))","execution_count":null,"outputs":[]},{"metadata":{"id":"SGgj-MT6IVv9"},"cell_type":"markdown","source":"We use the same scaler-object for the input-signals in the test-set."},{"metadata":{"id":"QvlOXz5VIVv9","trusted":true},"cell_type":"code","source":"x_test_scaled = x_scaler.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"nNkGeOXHIVwC"},"cell_type":"markdown","source":"The target-data comes from the same data-set as the input-signals, because it is the weather-data for one of the cities that is merely time-shifted. But the target-data could be from a different source with different value-ranges, so we create a separate scaler-object for the target-data."},{"metadata":{"id":"SCDx6DmaIVwD","trusted":true},"cell_type":"code","source":"y_scaler = MinMaxScaler()\ny_train_scaled = y_scaler.fit_transform(y_train)\ny_test_scaled = y_scaler.transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"IN_hrZYUIVwJ"},"cell_type":"markdown","source":"## Data Generator\n\nThe data-set has now been prepared as 2-dimensional numpy arrays. The training-data has almost large observations, consisting of 20 input-signals and 3 output-signals.\n\nThese are the array-shapes of the input and output data:"},{"metadata":{"id":"54ef6t9hIVwL","outputId":"13c48e0b-bc47-4d0c-cdf0-766ba60a865b","executionInfo":{"status":"ok","timestamp":1588059216379,"user_tz":-480,"elapsed":1339,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"print(x_train_scaled.shape)\nprint(y_train_scaled.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"HfsuPt-zIVwZ"},"cell_type":"markdown","source":"Instead of training the Recurrent Neural Network on the complete sequences of large observations, we will use the following function to create a batch of shorter sub-sequences picked at random from the training-data."},{"metadata":{"id":"I0SB40NVIVwa","trusted":true},"cell_type":"code","source":"def batch_generator(batch_size, sequence_length):\n    \"\"\"\n    Generator function for creating random batches of training-data.\n    \"\"\"\n\n    # Infinite loop.\n    while True:\n        # Allocate a new array for the batch of input-signals.\n        x_shape = (batch_size, sequence_length, num_x_signals)\n        x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n\n        # Allocate a new array for the batch of output-signals.\n        y_shape = (batch_size, sequence_length, num_y_signals)\n        y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n\n        # Fill the batch with random sequences of data.\n        for i in range(batch_size):\n            # Get a random start-index.\n            # This points somewhere into the training-data.\n            idx = np.random.randint(num_train - sequence_length)\n            \n            # Copy the sequences of data starting at this index.\n            x_batch[i] = x_train_scaled[idx:idx+sequence_length]\n            y_batch[i] = y_train_scaled[idx:idx+sequence_length]\n        \n        yield (x_batch, y_batch)","execution_count":null,"outputs":[]},{"metadata":{"id":"4CuxcUvEIVwd"},"cell_type":"markdown","source":"We will use a large batch-size so as to keep the GPU near 100% work-load. You may have to adjust this number depending on your GPU, its RAM and your choice of `sequence_length` below."},{"metadata":{"id":"euU2I0T5IVwe","trusted":true},"cell_type":"code","source":"batch_size = 256","execution_count":null,"outputs":[]},{"metadata":{"id":"R4UL3BzHIVwi","outputId":"d8e362e5-77f6-4cf8-ebc6-f78fdbc88d30","executionInfo":{"status":"ok","timestamp":1588059255114,"user_tz":-480,"elapsed":1559,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"sequence_length = 30 * 6\nsequence_length","execution_count":null,"outputs":[]},{"metadata":{"id":"vXkQBlJAIVwm"},"cell_type":"markdown","source":"We then create the batch-generator."},{"metadata":{"id":"HcTwC6lpIVwn","trusted":true},"cell_type":"code","source":"generator = batch_generator(batch_size=batch_size,\n                            sequence_length=sequence_length)","execution_count":null,"outputs":[]},{"metadata":{"id":"PQhjr67FIVwx"},"cell_type":"markdown","source":"We can then test the batch-generator to see if it works."},{"metadata":{"id":"DSXw1qnAIVw8","trusted":true},"cell_type":"code","source":"x_batch, y_batch = next(generator)","execution_count":null,"outputs":[]},{"metadata":{"id":"NSGJl5M_IVxJ"},"cell_type":"markdown","source":"This gives us a random batch of 256 sequences, each sequence having 180 observations, and each observation having 31 input-signals and 3 output-signals."},{"metadata":{"id":"cBsE_Q9fIVxQ","outputId":"31074e6a-ba2e-4109-8567-1eab52fb8241","executionInfo":{"status":"ok","timestamp":1588059266231,"user_tz":-480,"elapsed":1422,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"print(x_batch.shape)\nprint(y_batch.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"0wA3-TexIVxe"},"cell_type":"markdown","source":"We can plot one of the 20 input-signals as an example."},{"metadata":{"id":"HUWslXztIVxe","outputId":"8c4333a5-f4ad-4a7d-85d0-949c0e1a8dc1","executionInfo":{"status":"ok","timestamp":1588059271198,"user_tz":-480,"elapsed":1354,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"batch = 0   # First sequence in the batch.\nsignal = 0  # First signal from the 20 input-signals.\nseq = x_batch[batch, :, signal]\nplt.plot(seq)","execution_count":null,"outputs":[]},{"metadata":{"id":"VeS3evLYIVxl"},"cell_type":"markdown","source":"We can also plot one of the output-signals that we want the model to learn how to predict given all those 20 input signals."},{"metadata":{"id":"V7EbGx_QIVxm","outputId":"4121b978-7d15-4e8e-8279-f370bb02426a","executionInfo":{"status":"ok","timestamp":1588059281718,"user_tz":-480,"elapsed":4264,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"seq = y_batch[batch, :, signal]\nplt.plot(seq)","execution_count":null,"outputs":[]},{"metadata":{"id":"xv4rzKKCIVxs"},"cell_type":"markdown","source":"### Validation Set\n\nThe neural network trains quickly so we can easily run many training epochs. But then there is a risk of overfitting the model to the training-set so it does not generalize well to unseen data. We will therefore monitor the model's performance on the test-set after each epoch and only save the model's weights if the performance is improved on the test-set.\n\nThe batch-generator randomly selects a batch of short sequences from the training-data and uses that during training. But for the validation-data we will instead run through the entire sequence from the test-set and measure the prediction accuracy on that entire sequence."},{"metadata":{"id":"3jnbtaJCIVxu","trusted":true},"cell_type":"code","source":"validation_data = (np.expand_dims(x_test_scaled, axis=0),\n                   np.expand_dims(y_test_scaled, axis=0))","execution_count":null,"outputs":[]},{"metadata":{"id":"srph06VBIVxz"},"cell_type":"markdown","source":"## Create the Recurrent Neural Network\n\nWe are now ready to create the Recurrent Neural Network (RNN). We will use the Keras API for this because of its simplicity."},{"metadata":{"id":"8ayEHHlHIVx0","trusted":true},"cell_type":"code","source":"model = Sequential()","execution_count":null,"outputs":[]},{"metadata":{"id":"l0EQkoIrIVx9"},"cell_type":"markdown","source":"We can now add a Gated Recurrent Unit (GRU) to the network. This will have 512 outputs for each time-step in the sequence.\n\nNote that because this is the first layer in the model, Keras needs to know the shape of its input, which is a batch of sequences of arbitrary length (indicated by `None`), where each observation has a number of input-signals (`num_x_signals`)."},{"metadata":{"id":"_wGB8EqNIVx-","trusted":true},"cell_type":"code","source":"model.add(GRU(units=512,\n              return_sequences=True,\n              input_shape=(None, num_x_signals,)))","execution_count":null,"outputs":[]},{"metadata":{"id":"EL3ALGj0IVyH"},"cell_type":"markdown","source":"The GRU outputs a batch of sequences of 512 values. We want to predict 3 output-signals, so we add a fully-connected (or dense) layer which maps 512 values down to only 3 values.\n\nThe output-signals in the data-set have been limited to be between 0 and 1 using a scaler-object. So we also limit the output of the neural network using the Sigmoid activation function, which squashes the output to be between 0 and 1."},{"metadata":{"id":"VJkkkBokIVyJ","trusted":true},"cell_type":"code","source":"model.add(Dense(num_y_signals, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"id":"46ZZ3BVCIVyR"},"cell_type":"markdown","source":"A problem with using the Sigmoid activation function, is that we can now only output values in the same range as the training-data.\n\nWe can use a linear activation function on the output instead. This allows for the output to take on arbitrary values. It might work with the standard initialization for a simple network architecture, but for more complicated network architectures e.g. with more layers, it might be necessary to initialize the weights with smaller values to avoid `NaN` values during training. You may need to experiment with this to get it working."},{"metadata":{"id":"-EXds2a_IVyS","trusted":true},"cell_type":"code","source":"if False:\n    from tensorflow.python.keras.initializers import RandomUniform\n\n    # Maybe use lower init-ranges.\n    init = RandomUniform(minval=-0.05, maxval=0.05)\n\n    model.add(Dense(num_y_signals,\n                    activation='linear',\n                    kernel_initializer=init))","execution_count":null,"outputs":[]},{"metadata":{"id":"acc8Ccl5IVyW"},"cell_type":"markdown","source":"### Loss Function\n\nWe will use Mean Squared Error (MSE) as the loss-function that will be minimized. This measures how closely the model's output matches the true output signals.\n\nHowever, at the beginning of a sequence, the model has only seen input-signals for a few time-steps, so its generated output may be very inaccurate. Using the loss-value for the early time-steps may cause the model to distort its later output. We therefore give the model a \"warmup-period\" of 30 time-steps where we don't use its accuracy in the loss-function, in hope of improving the accuracy for later time-steps."},{"metadata":{"id":"6mZUbI-jIVyX","trusted":true},"cell_type":"code","source":"warmup_steps = 30","execution_count":null,"outputs":[]},{"metadata":{"id":"kxUwz7DLIVyj","trusted":true},"cell_type":"code","source":"def loss_mse_warmup(y_true, y_pred):\n    \"\"\"\n    Calculate the Mean Squared Error between y_true and y_pred,\n    but ignore the beginning \"warmup\" part of the sequences.\n    \n    y_true is the desired output.\n    y_pred is the model's output.\n    \"\"\"\n\n    # The shape of both input tensors are:\n    # [batch_size, sequence_length, num_y_signals].\n\n    # Ignore the \"warmup\" parts of the sequences\n    # by taking slices of the tensors.\n    y_true_slice = y_true[:, warmup_steps:, :]\n    y_pred_slice = y_pred[:, warmup_steps:, :]\n\n    # These sliced tensors both have this shape:\n    # [batch_size, sequence_length - warmup_steps, num_y_signals]\n\n    # Calculat the Mean Squared Error and use it as loss.\n    mse = mean(square(y_true_slice - y_pred_slice))\n    \n    return mse","execution_count":null,"outputs":[]},{"metadata":{"id":"AMTG05H_IVyo"},"cell_type":"markdown","source":"### Compile Model\n\nThis is the optimizer and the beginning learning-rate that we will use."},{"metadata":{"id":"vks5fX08IVyo","trusted":true},"cell_type":"code","source":"optimizer = RMSprop(lr=1e-3)","execution_count":null,"outputs":[]},{"metadata":{"id":"sf3by5s8IVyx"},"cell_type":"markdown","source":"We then compile the Keras model so it is ready for training."},{"metadata":{"id":"XafVxiyFIVyz","trusted":true},"cell_type":"code","source":"model.compile(loss=loss_mse_warmup, optimizer=optimizer)","execution_count":null,"outputs":[]},{"metadata":{"id":"sYuT58usIVy6"},"cell_type":"markdown","source":"This is a very small model with only two layers. The output shape of `(None, None, 3)` means that the model will output a batch with an arbitrary number of sequences, each of which has an arbitrary number of observations, and each observation has 3 signals. This corresponds to the 3 target signals we want to predict."},{"metadata":{"id":"f10Iy3zGIVy7","outputId":"6fbb614a-e48a-4dd6-b2c5-2e8a90452dc9","executionInfo":{"status":"ok","timestamp":1588059335304,"user_tz":-480,"elapsed":1302,"user":{"displayName":"Joe V Akkara","photoUrl":"","userId":"12460244073478755788"}},"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"Xe2Len81IVzC"},"cell_type":"markdown","source":"### Callback Functions\n\nDuring training we want to save checkpoints and log the progress to TensorBoard so we create the appropriate callbacks for Keras.\n\nThis is the callback for writing checkpoints during training."},{"metadata":{"id":"sHONCRLWIVzD","trusted":true},"cell_type":"code","source":"path_checkpoint = '23_checkpoint.keras'\ncallback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n                                      monitor='val_loss',\n                                      verbose=1,\n                                      save_weights_only=True,\n                                      save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"w1NcERvTIVzW"},"cell_type":"markdown","source":"This is the callback for stopping the optimization when performance worsens on the validation-set."},{"metadata":{"id":"b3NdU13sIVzW","trusted":true},"cell_type":"code","source":"callback_early_stopping = EarlyStopping(monitor='val_loss',\n                                        patience=5, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"lR7iw6KzIVzZ"},"cell_type":"markdown","source":"This is the callback for writing the TensorBoard log during training."},{"metadata":{"id":"w4erTIKtIVza","trusted":true},"cell_type":"code","source":"callback_tensorboard = TensorBoard(log_dir='./23_logs/',\n                                   histogram_freq=0,\n                                   write_graph=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"17R0UUSRIVzc"},"cell_type":"markdown","source":"This callback reduces the learning-rate for the optimizer if the validation-loss has not improved since the last epoch (as indicated by `patience=0`). The learning-rate will be reduced by multiplying it with the given factor. We set a start learning-rate of 1e-3 above, so multiplying it by 0.1 gives a learning-rate of 1e-4. We don't want the learning-rate to go any lower than this."},{"metadata":{"id":"RIxtDwVMIVzd","trusted":true},"cell_type":"code","source":"callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                                       factor=0.1,\n                                       min_lr=1e-4,\n                                       patience=0,\n                                       verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"kWewcbqqIVzm","trusted":true},"cell_type":"code","source":"callbacks = [callback_early_stopping,\n             callback_checkpoint,\n             callback_tensorboard,\n             callback_reduce_lr]","execution_count":null,"outputs":[]},{"metadata":{"id":"9jLbnMB3IVzw"},"cell_type":"markdown","source":"## Train the Recurrent Neural Network\n\nWe can now train the neural network.\n"},{"metadata":{"id":"s1s6Yu4RIVzw","outputId":"87b9d82a-cbdc-40bf-8b5b-5aaee0ed8eb4","trusted":true},"cell_type":"code","source":"%%time\nmodel.fit(x=generator,\n          epochs=30,\n          steps_per_epoch=100,\n          validation_data=validation_data,\n          callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"id":"AiYnAkzjIVz3"},"cell_type":"markdown","source":"### Load Checkpoint\n\nBecause we use early-stopping when training the model, it is possible that the model's performance has worsened on the test-set for several epochs before training was stopped. We therefore reload the last saved checkpoint, which should have the best performance on the test-set."},{"metadata":{"id":"mfdj8U5XIVz5","trusted":true},"cell_type":"code","source":"try:\n    model.load_weights(path_checkpoint)\nexcept Exception as error:\n    print(\"Error trying to load checkpoint.\")\n    print(error)","execution_count":null,"outputs":[]},{"metadata":{"id":"SHiFAwHcIVz9"},"cell_type":"markdown","source":"## Performance on Test-Set\n\nWe can now evaluate the model's performance on the test-set. This function expects a batch of data, but we will just use one long time-series for the test-set, so we just expand the array-dimensionality to create a batch with that one sequence."},{"metadata":{"id":"SnY5WhZ8IV0A","trusted":true},"cell_type":"code","source":"result = model.evaluate(x=np.expand_dims(x_test_scaled, axis=0),\n                        y=np.expand_dims(y_test_scaled, axis=0))","execution_count":null,"outputs":[]},{"metadata":{"id":"UccTM0ZDIV0H","trusted":true},"cell_type":"code","source":"print(\"loss (test-set):\", result)","execution_count":null,"outputs":[]},{"metadata":{"id":"rDBqn6NTIV0L","trusted":true},"cell_type":"code","source":"# If you have several metrics you can use this instead.\nif False:\n    for res, metric in zip(result, model.metrics_names):\n        print(\"{0}: {1:.3e}\".format(metric, res))","execution_count":null,"outputs":[]},{"metadata":{"id":"jzgXCsFiIV0N"},"cell_type":"markdown","source":"## Generate Predictions\n\nThis helper-function plots the predicted and true output-signals."},{"metadata":{"id":"rFTPs67zIV0O","trusted":true},"cell_type":"code","source":"def plot_comparison(start_idx, length=100, train=True):\n    \"\"\"\n    Plot the predicted and true output-signals.\n    \n    :param start_idx: Start-index for the time-series.\n    :param length: Sequence-length to process and plot.\n    :param train: Boolean whether to use training- or test-set.\n    \"\"\"\n    \n    if train:\n        # Use training-data.\n        x = x_train_scaled\n        y_true = y_train\n    else:\n        # Use test-data.\n        x = x_test_scaled\n        y_true = y_test\n    \n    # End-index for the sequences.\n    end_idx = start_idx + length\n    \n    # Select the sequences from the given start-index and\n    # of the given length.\n    x = x[start_idx:end_idx]\n    y_true = y_true[start_idx:end_idx]\n    \n    # Input-signals for the model.\n    x = np.expand_dims(x, axis=0)\n\n    # Use the model to predict the output-signals.\n    y_pred = model.predict(x)\n    \n    # The output of the model is between 0 and 1.\n    # Do an inverse map to get it back to the scale\n    # of the original data-set.\n    y_pred_rescaled = y_scaler.inverse_transform(y_pred[0])\n    \n    #print(loss_mse_warmup(y_true, y_pred))\n    \n    # For each output-signal.\n    for signal in range(len(target_names)):\n        # Get the output-signal predicted by the model.\n        signal_pred = y_pred_rescaled[:, signal]\n        \n        # Get the true output-signal from the data-set.\n        signal_true = y_true[:, signal]\n        \n\n        # Make the plotting-canvas bigger.\n        plt.figure(figsize=(15,5))\n        \n        # Plot and compare the two signals.\n        plt.plot(signal_true, label='true')\n        plt.plot(signal_pred, label='pred')\n        \n        # Plot grey box for warmup-period.\n        p = plt.axvspan(0, warmup_steps, facecolor='black', alpha=0.15)\n        \n        # Plot labels etc.\n        plt.ylabel(target_names[signal])\n        plt.legend()\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"mD37qudaIV0Y"},"cell_type":"markdown","source":"We can now plot an example of predicted output-signals. \n\nThe prediction is not very accurate for the first 30-50 time-steps because the model has seen very little input-data at this point.\nThe model generates a single time-step of output data for each time-step of the input-data, so when the model has only run for a few time-steps, it knows very little of the history of the input-signals and cannot make an accurate prediction. The model needs to \"warm up\" by processing perhaps 30-50 time-steps before its predicted output-signals can be used.\n\nThat is why we ignore this \"warmup-period\" of 50 time-steps when calculating the mean-squared-error in the loss-function. The \"warmup-period\" is shown as a grey box in these plots."},{"metadata":{"scrolled":true,"id":"z2dNkPv1IV0b","trusted":true},"cell_type":"code","source":"plot_comparison(start_idx=1000, length=500, train=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"kj_hT8tBIV0f"},"cell_type":"markdown","source":"The model was able to predict the overall oscillations quite well but the peaks were sometimes inaccurate. "},{"metadata":{"id":"FVtxlYkVIV0p","trusted":true},"cell_type":"code","source":"data[\"CA_4\"]['Hobbie_revenue'][1000:1000+500].plot();","execution_count":null,"outputs":[]},{"metadata":{"id":"l2dZrpA8IV0t"},"cell_type":"markdown","source":"### Example from Test-Set\n\nNow consider an example from the test-set. The model has not seen this data during training.\n\nThe temperature is predicted reasonably well, although the peaks are sometimes inaccurate.\n\nThe wind-speed has not been predicted so well. The daily oscillation-frequency seems to match, but the center-level and the peaks are quite inaccurate. A guess would be that the wind-speed is difficult to predict from the given input data, so the model has merely learnt to output sinusoidal oscillations in the daily frequency and approximately at the right center-level.\n\nThe atmospheric pressure is predicted reasonably well, except for a lag and a more noisy signal than the true time-series."},{"metadata":{"scrolled":false,"id":"rhLobZHPIV0u","trusted":true},"cell_type":"code","source":"plot_comparison(start_idx=10, length=500, train=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"TCLRenZqIV0y"},"cell_type":"markdown","source":"## Conclusion\n\nUsed a Recurrent Neural Network to predict several time-series from a number of input-signals. We used revenue-data for 10 stores to predict next months revenue for one of the stores."}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"23_Time-Series-Prediction.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":4}