{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Before we begin, let's start with a \"MUST WATCH\" <br>video : "},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\n\n# Youtube\nHTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/qfdrNHqlNEk\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apology : Due to kaggle problem,somehow my kernels contents +(few graphs) getting cut off after kernel commit,i hope that kaggle will solve this problem quickly, i asked for help here : https://www.kaggle.com/product-feedback/137723"},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"The flu, the common cold, allergies, and COVID-19 ‚Äî the disease associated with the new coronavirus ‚Äî have similar symptoms, but the coronavirus has been far deadlier.Overlapping symptoms include a sore throat, fatigue, and a dry cough. That can make it challenging for doctors to diagnose COVID-19.People with COVID-19 don't typically have runny noses or sneeze a lot.The coronavirus primarily affects the lungs and commonly causes a fever, a dry cough, and shortness of breath."},{"metadata":{},"cell_type":"markdown","source":"**[Here are the symptoms associated with COVID-19 and how they compare with symptoms of the common cold, the flu, and allergies](https://www.businessinsider.com/coronavirus-symptoms-compared-to-flu-common-cold-and-allergies-2020-3):**\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.insider.com/5e6a58e684159f61963287a2?width=1000&format=jpeg&auto=webp\" width=\"600px\" align=\"left\"> "},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://ichef.bbci.co.uk/news/640/cpsprodpb/16F8F/production/_111059049_corona_virus_symptoms_short_v4_640-nc.png\" width=\"600px\" align=\"left\"> \n"},{"metadata":{},"cell_type":"markdown","source":"As a NLP beginner, In this notebook i will try to apply various TextAnalytics techniques on [COVID-19 Open Research Dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)\n"},{"metadata":{},"cell_type":"markdown","source":"# Research Goal\n\nMain goal of this research is to analyze the data and find **Risk Factors of COVID-19**"},{"metadata":{},"cell_type":"markdown","source":"**I will gradually update this kernel**"},{"metadata":{},"cell_type":"markdown","source":" <h1 align=\"left\" style=\"color:green;\">\nIf you find this kernel interesting, please drop an  <br><font color=\"red\">UPVOTE</font>. It motivates me to produce more quality <br>contents ü§ó \n</h1> "},{"metadata":{},"cell_type":"markdown","source":"**IMPORTS**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport spacy\nimport re\nimport numpy as np\nimport pandas as pd\nfrom pprint import pprint\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\nfrom nltk.stem import WordNetLemmatizer\nimport spacy\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim  \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Enable logging for gensim - optional\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nn = 1000\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!pip install  tensorflow-gpu==1.15.0","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!pip install bert-tensorflow","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n    ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!unzip cased_L-12_H-768_A-12.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.VERSION\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport bert\nfrom bert import run_classifier\nfrom bert import optimization\nfrom bert import tokenization\nfrom bert import modeling\nimport tensorflow as tf\nimport numpy as np\nimport itertools","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NLTK Stop words"},{"metadata":{},"cell_type":"markdown","source":" <h5 align=\"left\" style=\"color:purple;\">\nThe process of converting data to something a computer can understand is referred to as pre-processing. One of the major forms of pre-processing is to filter out useless data. In natural language processing, useless words (data), are referred to as stop words.\n</h5>\n\n![](http://) [What are Stop words?](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/)\n\n<h5 align=\"left\" style=\"color:blue;\">\n <font color=\"red\">Stop Words:</font>  A stop word is a commonly used word (such as ‚Äúthe‚Äù, ‚Äúa‚Äù, ‚Äúan‚Äù, ‚Äúin‚Äù) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n</h5> \n\n"},{"metadata":{},"cell_type":"markdown","source":" <img src=\"https://www.geeksforgeeks.org/wp-content/uploads/Stop-word-removal-using-NLTK.png\" width=\"600px\" align=\"left\"> "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['background', 'methods', 'introduction', 'conclusions', 'results', \n                   'purpose', 'materials', 'discussions','methodology','result analysis'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"list of files given for [COVID-19 Open Research Dataset challenge](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"os.listdir('../input/CORD-19-research-challenge/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's read the readme file first"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../input/CORD-19-research-challenge/metadata.readme', 'r') as f:\n    data = f.read()\n    print(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_dir = '../input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/'\nfilenames = os.listdir(biorxiv_dir)\nprint(\"Number of articles retrieved from biorxiv:\", len(filenames))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the cell above we can see there are 885  json files inside biorvix directory, the structure is likely too complex to directly perform analysis. we will use clean and updated dataset prepared by @xhlulu in this kernel [CORD-19: EDA, parse JSON and generate clean CSVüßπ](https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv). Thanks to the author  (ÔΩ°‚óèÃÅ‚Äø‚óèÃÄÔΩ°) **"},{"metadata":{},"cell_type":"markdown","source":"The cell below shows that the updated datasets are in CSV format now - biorxiv_clean.csv, clean_comm_use.csv, clean_noncomm_use.csv and clean_pmc.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir( '/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading The updated clean CSV files**"},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_clean = pd.read_csv('/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/biorxiv_clean.csv')\nclean_comm_use = pd.read_csv('/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/clean_comm_use.csv')\nclean_noncomm_use =  pd.read_csv('/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/clean_noncomm_use.csv')\nclean_pmc =  pd.read_csv('/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/clean_pmc.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_clean.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_comm_use.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_noncomm_use.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_pmc.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"first text of biorxiv_clean dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_clean.text[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 1 :  Working with biorxiv"},{"metadata":{},"cell_type":"markdown","source":"# biorxiv_clean papers Abstract - frequent words (400 sample)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n\nstopwords = set(STOPWORDS)\n#https://www.kaggle.com/gpreda/cord-19-solution-toolbox\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=30, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=14)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(biorxiv_clean['abstract'], title = 'biorxiv_clean - papers Abstract - frequent words (400 sample)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convert abstract to list"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ndf = biorxiv_clean\ndf = df.abstract.dropna()\ndata = df.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find similar research papers using universalsentenceencoderlarge4"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\n#ref : https://gist.github.com/gaurav5430/8d7810495ec3f914ffb151458f352c60\n\n'''import tensorflow_hub as hub\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef prepare_similarity(vectors):\n    similarity=cosine_similarity(vectors)\n    return similarity\n\ndef get_top_similar(sentence, sentence_list, similarity_matrix, topN):\n    # find the index of sentence in list\n    index = sentence_list.index(sentence)\n    # get the corresponding row in similarity matrix\n    similarity_row = np.array(similarity_matrix[index, :])\n    # get the indices of top similar\n    indices = similarity_row.argsort()[-topN:][::-1]\n    return [sentence_list[i] for i in indices]\n\n\nmeta=pd.read_csv(\"../input/CORD-19-research-challenge/metadata.csv\")\nmodule_url = \"../input/universalsentenceencoderlarge4\" \nembed = hub.load(module_url)\n\n\n# Creating an empty Dataframe with column names only\nsimsentence = pd.DataFrame()\n\ntitles=meta['title'].fillna(\"Unknown\")\nembed_vectors=embed(titles[:5000].values)['outputs'].numpy()\nsentence_list=titles.values.tolist()\nfor i in range(5):\n\n    sentences=titles.iloc[i]\n    #print(\">>>>>>>>>>>>Using title Find similar research papers for :\",sentences, \"<<<<<<<<<<<<\")\n\n    similarity_matrix=prepare_similarity(embed_vectors)\n    similar=get_top_similar(sentences,sentence_list,similarity_matrix,6)\n    for sentence in similar:\n        #print(sentence)\n        simsentence = simsentence.append({'sentence': sentences, 'similar': sentence}, ignore_index=True)\n        #print(\"\\n\") '''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The Cell above Finds similar research papers for given sentence,if you observe carefully,you can see i have stored all the sentences and corresponding similar sentences in a pandas dataframe called simsentence.let's check that file below**"},{"metadata":{},"cell_type":"markdown","source":"**IMPORTANT NOTE : I've commented out the codes above that finds similar papers to save time.i have executed that code in previous versions of this kernel to get related papers and save them as csv format**"},{"metadata":{},"cell_type":"markdown","source":"# Now let's save this new dataframe as csv file for possible further research"},{"metadata":{"trusted":true},"cell_type":"code","source":"#simsentence.to_csv('simsentence.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I have downloaded simsentence.csv from version 1's output of this kernel and now using that csv file in cell below for little analysis **"},{"metadata":{"trusted":true},"cell_type":"code","source":"simsentence = pd.read_csv('../input/simsentence/simsentence.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The function below converts sentences to words using gensim**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  \n\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build the bigram and trigram models using gensim"},{"metadata":{},"cell_type":"markdown","source":" <h5 align=\"left\" style=\"color:blue;\">\n <font color=\"red\"> Question-1 : [What is a bigram and a trigram?](https://www.quora.com/What-is-a-bigram-and-a-trigram-layman-explanation-please) </font> <br>\n \n Answer : Start with a unigram. If you put all of the words in some sentence into a box, and choose one single word randomely, it is called a unigram. A unigram is just one single word. But a bigram is a word pair. The bigrams within a sentence are all possible word pairs formed from neighboring words in the sentence. It is easier to look at an example. The bigrams in the sentence I  really love Quora are I really, and really love, and love Quora. That is a total of 3 word pairs, or bigrams. The same goes for trigrams, or triplets. This sentence would contain only two trigrams, which are I really love, and really love quora.\n</h5> "},{"metadata":{},"cell_type":"markdown","source":"<h5 align=\"left\" style=\"color:green;\">\n <font color=\"red\"> Question-2 : What is Gensim? </font> <br>\n \n Answer :  It is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.\n</h5> "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=20) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=20)  \n\n# Faster way to get a sentence clubbed as a trigram/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\nprint(trigram_mod[bigram_mod[data_words[1]]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define functions for stopwords, bigrams, trigrams and lemmatization"},{"metadata":{},"cell_type":"markdown","source":"<h5 align=\"left\" style=\"color:green;\">\n <font color=\"blue\"> Question-1 : [What is difference between stemming and lemmatization?](https://www.quora.com/What-is-difference-between-stemming-and-lemmatization) </font> <br><br>\n \n \n Answer :  <br><br> <font color=\"red\"> Stemming -  </font>  Stemming is a process of reducing words to its root form even if the root has no dictionary meaning. For eg: beautiful and beautifully will be stemmed to beauti which has no meaning in English dictionary. <br> <br>\n\n\n <font color=\"purple\"> Lemmatisation - </font> Lemmatisation is a process of reducing words into their lemma or dictionary. It takes into account the meaning of the word in the sentence. For eg: beautiful and beautifully are lemmatised to beautiful and beautifully respectively without changing the meaning of the words. But, good, better and best are lemmatised to good since all the words have similar meaning.\n</h5> "},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"\n#https://github.com/cjriggio/classifying_medical_innovation\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(data_lemmatized[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Dictionary,Corpus and Document Frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Human readable format of corpus (term-frequency)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build LDA model"},{"metadata":{},"cell_type":"markdown","source":"\n <font align=\"left\" color=\"red\"> Topic Modeling -  </font> In recent years, huge amount of data (mostly unstructured) is growing. It is difficult to extract relevant and desired information from it. At the document level, the most useful ways to understand text by its topics. The statistical process of learning and extracting these topics from huge amount of documents is called topic modeling.In Text Mining Topic Modeling is a technique to extract the hidden topics from huge amount of text.\n\nThere are so many algorithms to do topic modeling. Latent Dirichlet Allocation (LDA) is one of those popular algorithms for topic modeling. \n\n"},{"metadata":{},"cell_type":"markdown","source":"# Let‚Äôs see where topic modeling fit in machine learning spectrum.\n\n\n "},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://2.bp.blogspot.com/-V_WqO4x3MEQ/XGWhK55_S3I/AAAAAAAABoY/riaPM64fUcovdV543zphBLwYPe3MHgGBwCLcBGAs/s1600/image002.png\" width = \"600px\" align=\"left\"> "},{"metadata":{},"cell_type":"markdown","source":"\n <font color=\"purple\"> How Latent Dirichlet Allocation (LDA) Works?</font> \n \n\n"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://2.bp.blogspot.com/-UO8E6wws1Go/XGWgbLTPJnI/AAAAAAAABoQ/tGuBrjfJZ1UGmUQ112ZCv3gAu3Tg0O1FACLcBGAs/s1600/image001-min.png\" width = \"600px\" align=\"left\"> \n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=8, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True\n                                        )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Print the Keyword in the n topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#pprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compute Perplexity and Coherence Score"},{"metadata":{},"cell_type":"markdown","source":"*perplexity is a measurement of how well a probability distribution or probability model predicts a sample where The coherence score is for assessing the quality of the learned topics. *\n\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize the topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save Topics as html format"},{"metadata":{"trusted":true},"cell_type":"code","source":"pyLDAvis.save_html(vis, './lda4topics_v2.html')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_model = lda_model\n\nmodel_topics = optimal_model.show_topics(formatted=False)\npprint(optimal_model.print_topics(num_words=20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wordcloud of Top N words in each topic"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.colors as mcolors\n\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  \n\ncloud = WordCloud(stopwords=stop_words,\n                  background_color='white',\n                  width=2500,\n                  height=1800,\n                  max_words=100,\n                  colormap='tab10',\n                  color_func=lambda *args, **kwargs: cols[i],\n                  prefer_horizontal=1.0)\n\ntopics = lda_model.show_topics(formatted=False, \n                               num_words=30)\n\nfig, axes = plt.subplots(5, 1, figsize=(10,20), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    fig.add_subplot(ax)\n    topic_words = dict(topics[i][1])\n    cloud.generate_from_frequencies(topic_words, max_font_size=500)\n    plt.gca().imshow(cloud)\n    plt.gca().set_title('Topic ' + str(i + 1), fontdict=dict(size=16))\n    plt.gca().axis('off')\n\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentence Coloring of N Sentences"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom matplotlib.patches import Rectangle\n\ndef sentences_chart(lda_model=lda_model, corpus=corpus, start = 0, end = 12):\n    corp = corpus[start:end]\n    mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n\n    fig, axes = plt.subplots(end-start, 1, figsize=(22, 10))      \n    axes[0].axis('off')\n    for i, ax in enumerate(axes):\n        if i > 1:\n            #i = i+1\n            corp_cur = corp[i-1] \n            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment='center',\n                    fontsize=15, color='black', transform=ax.transAxes, fontweight=700)\n\n            # Draw Rectange\n            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n\n            word_pos = 0.06\n            for j, (word, topics) in enumerate(word_dominanttopic):\n                if j < 14:\n                    ax.text(word_pos, 0.5, word,\n                            horizontalalignment='left',\n                            verticalalignment='center',\n                            fontsize=16, color=mycolors[topics],\n                            transform=ax.transAxes, fontweight=700)\n                    word_pos += 0.009 * len(word)  # to move the word for the next iter\n                    ax.axis('off')\n            ax.text(word_pos, 0.5, '. . .',\n                    horizontalalignment='left',\n                    verticalalignment='center',\n                    fontsize=16, color='black',\n                    transform=ax.transAxes)       \n\n    plt.subplots_adjust(wspace=0, hspace=0)\n    plt.suptitle('Sentence Topic Coloring for Documents: ' + str(start) + ' to ' + str(end-2), fontsize=20, x = 0.2, y=0.95, fontweight=700)\n    plt.tight_layout()\n    plt.show()\n\nsentences_chart()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"corp = corpus[0:13]\ncorp_cur = corp[13-1] \ntopic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\nword_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics] \nword_dominanttopic","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentence Coloring of N Sentences"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef topics_per_document(model, corpus, start=0, end=1):\n    corpus_sel = corpus[start:end]\n    dominant_topics = []\n    topic_percentages = []\n    for i, corp in enumerate(corpus_sel):\n        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n        dominant_topics.append((i, dominant_topic))\n        topic_percentages.append(topic_percs)\n    return(dominant_topics, topic_percentages)\n\ndominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n\n# Distribution of Dominant Topics in Each Document\ndf = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\ndominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\ndf_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n\n# Total Topic Distribution by actual weight\ntopic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\ndf_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n\n# Top 3 Keywords for each Topic\ntopic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n\ndf_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\ndf_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\ndf_top3words.reset_index(level=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Topic Distribution Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.ticker import FuncFormatter\n\n# Plot\n\nfig, ax1  = plt.subplots(1, figsize=(10, 10))\n\n# Topic Distribution by Dominant Topics\nax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\nax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\ntick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x + 1)+ ':\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\nax1.xaxis.set_major_formatter(tick_formatter)\nax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\nax1.set_ylabel('Number of Documents')\nax1.set_ylim(0, 2000)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dominant_topic_in_each_doc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax2  = plt.subplots(1, figsize=(10, 10))\n# Topic Distribution by Topic Weights\nax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\nax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\nax2.xaxis.set_major_formatter(tick_formatter)\nax2.set_title('Number of Documents by Topic Weightage', fontdict=dict(size=10))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get topic weights and dominant topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom bokeh.models import HoverTool\nfrom sklearn.manifold import TSNE\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\n\n# Get topic weights\ntopic_weights = []\nfor i, row_list in enumerate(lda_model[corpus]):\n    topic_weights.append([w for i, w in row_list[0]])\n\n# Array of topic weights    \narr = pd.DataFrame(topic_weights).fillna(0).values\n\n# Keep the well separated points (optional)\narr = arr[np.amax(arr, axis=1) > 0.35]\n\n# Dominant topic number in each doc\ntopic_num = np.argmax(arr, axis=1)\n\n# tSNE Dimension Reduction\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\ntsne_lda = tsne_model.fit_transform(arr)\n\n# Plot the Topic Clusters using Bokeh\noutput_notebook()\nn_topics = 5\nmycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n\n\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n              plot_width=800, plot_height=600)\nplot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\nshow(plot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom pprint import pprint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n    return texts_out\n\n    # # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n    # # Run in terminal: python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n    # # Do lemmatization keeping only Noun, Adj, Verb, Adverb\ndata_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(analyzer='word', min_df=10,                        # minimum reqd occurences of a word \n                              stop_words='english',             # remove stop words\n                              lowercase=True,                   # convert all words to lowercase\n                              token_pattern='[a-zA-Z0-9]{3,}'  # num chars > 3\n                              # max_features=50000,             # max number of uniq words\n                             )\ndata_vectorized = vectorizer.fit_transform(data_lemmatized)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Search Param for GridSearch"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\nsearch_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n\n# Init the Model\nlda = LatentDirichletAllocation(n_jobs=-1)\n\n# Init Grid Search Class\nmodel = GridSearchCV(lda, param_grid=search_params)\n\n\n# Do the Grid Search\nmodel.fit(data_vectorized)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Best Model"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nbest_lda_model = model.best_estimator_\n\n# Model Parameters\nprint(\"Best Model's Params: \", model.best_params_)\n\n# Log Likelihood Score\nprint(\"Best Log Likelihood Score: \", model.best_score_)\n\n# Perplexity\nprint(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = list(df.Dominant_Topic.unique())\ncategories","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 2 : Working with clean_comm_use.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = clean_comm_use\ndf1 = df1.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# creating n-gram and fetching to HashingVectorizer to get feature vector X"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#taking from https://www.kaggle.com/maksimeren/covid-19-literature-clustering\nwords = []\nfor ii in range(0,len(df1)):\n    words.append(str(df1.iloc[ii]['text']).split(\" \"))\n    \n    \nn_gram_all = []\n\nfor word in words:\n    # get n-grams for the instance\n    n_gram = []\n    for i in range(len(word)-2+1):\n        n_gram.append(\"\".join(word[i:i+2]))\n    n_gram_all.append(n_gram)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nfrom sklearn.feature_extraction.text import HashingVectorizer\n\n# hash vectorizer instance\nhvec = HashingVectorizer(lowercase=False, analyzer=lambda l:l, n_features=2**12)\n\n# features matrix X\nX = hvec.fit_transform(n_gram_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# test set size of 20% of the data and the random seed 42 <3\nX_train, X_test = train_test_split(X.toarray(), test_size=0.2, random_state=42)\n\nprint(\"X_train size:\", len(X_train))\nprint(\"X_test size:\", len(X_test), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**K-means clustering for 15 cluster**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nk = 15 \nkmeans = KMeans(n_clusters=k, n_jobs=4, verbose= k)\ny_pred = kmeans.fit_predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"y_train = y_pred\ny_test = kmeans.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(verbose=1)\nX_embedded = tsne.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\n# sns settings\nsns.set(rc={'figure.figsize':(10,10)})\n\n# colors\npalette = sns.color_palette(\"bright\", 1)\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n\nplt.title(\"t-SNE Covid-19 Articles\")\n# plt.savefig(\"plots/t-sne_covid19.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_embedded[:,1].shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# sns settings\nsns.set(rc={'figure.figsize':(10,10)})\n\n# colors\npalette = sns.color_palette(\"bright\", len(set(y_pred)))\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\nplt.title(\"t-SNE Covid-19 Articles - Clustered\")\n# plt.savefig(\"plots/t-sne_covid19_label.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 3 : working with clean_noncomm_use.csv"},{"metadata":{},"cell_type":"markdown","source":"in this section\n* Text Summarization Approaches\n* Understanding the TextRank Algorithm\n* Understanding the Problem Statement\n* Implementation of the TextRank Algorithm o"},{"metadata":{"trusted":true},"cell_type":"code","source":"type(clean_noncomm_use.abstract.dropna().tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"gensim.summarization offers TextRank summarization"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom gensim.summarization.summarizer import summarize\nsummarize(clean_noncomm_use.abstract.dropna().to_string())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Have you come across the mobile app inshorts? It‚Äôs an innovative news app that converts news articles into a 60-word summary. And that is exactly what we are going to learn in this section ‚Äî **Automatic Text Summarization.**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nnltk.download('punkt') # one time execution\nimport re","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Understanding the Text Rank Algorithm"},{"metadata":{},"cell_type":"markdown","source":"*All the informations below are taken from this beautiful article [An Introduction to Text Summarization using the TextRank Algorithm (with Python implementation)](https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/)*"},{"metadata":{},"cell_type":"markdown","source":"Before getting started with the TextRank algorithm, there‚Äôs another algorithm which we should become familiar with ‚Äì the **PageRank algorithm**. In fact, this actually inspired TextRank! PageRank is used primarily for ranking web pages in online search results. Let‚Äôs quickly understand the basics of this algorithm with the help of an example."},{"metadata":{},"cell_type":"markdown","source":"**PageRank Algorithm**"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/Pagerank11.png\" width=\"600px\" align=\"left\"> "},{"metadata":{},"cell_type":"markdown","source":"Suppose we have 4 web pages ‚Äî w1, w2, w3, and w4. These pages contain links pointing to one another. Some pages might have no link ‚Äì these are called dangling pages."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/webpages.png\" width=\"600px\" align=\"left\"> "},{"metadata":{},"cell_type":"markdown","source":"* Web page w1 has links directing to w2 and w4\n* w2 has links for w3 and w1\n* w4 has links only for the web page w1\n* w3 has no links and hence it will be called a dangling page"},{"metadata":{},"cell_type":"markdown","source":"In order to rank these pages, we would have to compute a score called the PageRank score. This score is the probability of a user visiting that page.\n\nTo capture the probabilities of users navigating from one page to another, we will create a square matrix M, having n rows and n columns, where n is the number of web pages."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/m_matrix.png\" width=\"600px\" align=\"left\"> "},{"metadata":{},"cell_type":"markdown","source":"Each element of this matrix denotes the probability of a user transitioning from one web page to another. For example, the highlighted cell below contains the probability of transition from w1 to w2."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/transition_probability.png\" width=\"600px\" align=\"left\"> "},{"metadata":{},"cell_type":"markdown","source":"The initialization of the probabilities is explained in the steps below: \n\n* Probability of going from page i to j, i.e., M[ i ][ j ], is initialized with 1/(number of unique links in web page wi)\n* If there is no link between the page i and j, then the probability will be initialized with 0\n* If a user has landed on a dangling page, then it is assumed that he is equally likely to transition to any page. Hence, M[ i ][ j ] will be initialized with 1/(number of web pages)\n\nHence, in our case, the matrix M will be initialized as follows:"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/final_matrix.png\" width=\"600px\" align=\"left\"> "},{"metadata":{},"cell_type":"markdown","source":"Finally, the values in this matrix will be updated in an iterative fashion to arrive at the web page rankings."},{"metadata":{},"cell_type":"markdown","source":"# TextRank Algorithm"},{"metadata":{},"cell_type":"markdown","source":"* In place of web pages, we use sentences\n* Similarity between any two sentences is used as an equivalent to the web page transition probability\n* The similarity scores are stored in a square matrix, similar to the matrix M used for PageRank"},{"metadata":{},"cell_type":"markdown","source":"**TextRank is an extractive and unsupervised text summarization technique. Let‚Äôs take a look at the flow of the TextRank algorithm that we will be following:**"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/block_3.png\" width=\"600px\" align=\"left\"> "},{"metadata":{},"cell_type":"markdown","source":"1. The first step would be to concatenate all the text contained in the articles\n2. Then split the text into individual sentences\n3. In the next step, we will find vector representation (word embeddings) for each and every sentence\n4. Similarities between sentence vectors are then calculated and stored in a matrix\n5. The similarity matrix is then converted into a graph, with sentences as vertices and similarity scores as edges, for sentence rank calculation\n6. Finally, a certain number of top-ranked sentences form the final summary"},{"metadata":{},"cell_type":"markdown","source":"**We will apply the TextRank algorithm on this dataset of  articles with the aim of creating a nice and concise summary.**"},{"metadata":{},"cell_type":"markdown","source":"Now the next step is to break the text into individual sentences. We will use the sent_tokenize( ) function of the nltk library to do this."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#ref : https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\nfrom nltk.tokenize import sent_tokenize\nsentences = []\nfor s in clean_noncomm_use.abstract.dropna():\n    sentences.append(sent_tokenize(s))\n\nsentences = [y for x in sentences for y in x] # flatten list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences[:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Download GloVe Word Embeddings"},{"metadata":{},"cell_type":"markdown","source":"[GloVe](https://nlp.stanford.edu/projects/glove/) word embeddings are vector representation of words. These word embeddings will be used to create vectors for our sentences. We could have also used the Bag-of-Words or TF-IDF approaches to create features for our sentences, but these methods ignore the order of the words (and the number of features is usually pretty large).\n\nWe will be using the pre-trained Wikipedia 2014 + Gigaword 5 GloVe vectors. Heads up ‚Äì the size of these word embeddings is 822 MB."},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove*.zip","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let‚Äôs extract the words embeddings or word vectors."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Extract word vectors\nword_embeddings = {}\nf = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(word_embeddings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have word vectors for 400,000 different terms stored in the dictionary ‚Äì ‚Äòword_embeddings‚Äô."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# remove punctuations, numbers and special characters\nclean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n\n# make alphabets lowercase\nclean_sentences = [s.lower() for s in clean_sentences]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# function to remove stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef remove_stopwords(sen):\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\n    return sen_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove stopwords from the sentences\nclean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use clean_sentences to create vectors for sentences in our data with the help of the GloVe word vectors."},{"metadata":{},"cell_type":"markdown","source":"# Vector Representation of Sentences"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract word vectors\nword_embeddings = {}\nf = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let‚Äôs create vectors for our sentences. We will first fetch vectors (each of size 100 elements) for the constituent words in a sentence and then take mean/average of those vectors to arrive at a consolidated vector for the sentence."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sentence_vectors = []\nfor i in clean_sentences:\n    if len(i) != 0:\n        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n    else:\n        v = np.zeros((100,))\n    sentence_vectors.append(v)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Similarity Matrix preparation"},{"metadata":{},"cell_type":"markdown","source":"The next step is to find similarities between the sentences, and we will use the cosine similarity approach for this challenge. Let‚Äôs create an empty similarity matrix for this task and populate it with cosine similarities of the sentences.\n\nLet‚Äôs first define a zero matrix of dimensions (n * n).  We will initialize this matrix with cosine similarity scores of the sentences. Here, n is the number of sentences."},{"metadata":{"trusted":true},"cell_type":"code","source":"# similarity matrix\nsim_mat = np.zeros([len(sentences), len(sentences)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use Cosine Similarity to compute the similarity between a pair of sentences And initialize the matrix with cosine similarity scores."},{"metadata":{},"cell_type":"markdown","source":"> NOTE : For all sentences, the cell below takes a lot of time,i waited more than  8 hours for this kernel to finish commit but the cell below doesn't finish execution for len(sentences) within 9 hours(kaggle gpu limit),so to save time and computation power i will use 1000 instead"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.metrics.pairwise import cosine_similarity\nfor i in range(1000):\n    for j in range(1000):\n        if i != j:\n            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n            #print(sim_mat[i][j])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying PageRank Algorithm"},{"metadata":{},"cell_type":"markdown","source":"Before proceeding further, let‚Äôs convert the similarity matrix sim_mat into a graph. The nodes of this graph will represent the sentences and the edges will represent the similarity scores between the sentences. On this graph, we will apply the PageRank algorithm to arrive at the sentence rankings."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%time\nimport networkx as nx\n\nnx_graph = nx.from_numpy_array(sim_mat)\nscores = nx.pagerank(nx_graph)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary Extraction : Extract top 50 sentences as the summary"},{"metadata":{},"cell_type":"markdown","source":"Finally, it‚Äôs time to extract the top N sentences based on their rankings for summary generation."},{"metadata":{"trusted":true},"cell_type":"code","source":"ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\nfor i in range(50):\n    print(ranked_sentences[i][1])\n    print('\\n\\n')\n  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 4 : Analyzing clean_pmc.csv file"},{"metadata":{},"cell_type":"markdown","source":"in this section, let's try bert for Topic Modeling"},{"metadata":{},"cell_type":"markdown","source":"Ref : https://github.com/huseinzol05/NLP-Models-Tensorflow/blob/master/topic-model/2.bert-topic.ipynb"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_pmc.title","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir('cased_L-12_H-768_A-12')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_VOCAB = 'cased_L-12_H-768_A-12/vocab.txt'\nBERT_INIT_CHKPNT = 'cased_L-12_H-768_A-12/bert_model.ckpt'\nBERT_CONFIG = 'cased_L-12_H-768_A-12/bert_config.json'   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* generate ngrams.\n* Vectorize string inputs using bert attention \n* Topic modeling Using bert for 10 topics\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def generate_ngram(seq, ngram = (1, 3)):\n    g = []\n    for i in range(ngram[0], ngram[-1] + 1):\n        g.extend(list(ngrams_generator(seq, i)))\n    return g\n\ndef _pad_sequence(\n    sequence,\n    n,\n    pad_left = False,\n    pad_right = False,\n    left_pad_symbol = None,\n    right_pad_symbol = None,\n):\n    sequence = iter(sequence)\n    if pad_left:\n        sequence = itertools.chain((left_pad_symbol,) * (n - 1), sequence)\n    if pad_right:\n        sequence = itertools.chain(sequence, (right_pad_symbol,) * (n - 1))\n    return sequence\n\n\ndef ngrams_generator(\n    sequence,\n    n,\n    pad_left = False,\n    pad_right = False,\n    left_pad_symbol = None,\n    right_pad_symbol = None,\n):\n    \"\"\"\n    generate ngrams.\n\n    Parameters\n    ----------\n    sequence : list of str\n        list of tokenize words.\n    n : int\n        ngram size\n\n    Returns\n    -------\n    ngram: list\n    \"\"\"\n    sequence = _pad_sequence(\n        sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol\n    )\n\n    history = []\n    while n > 1:\n        try:\n            next_item = next(sequence)\n        except StopIteration:\n            return\n        history.append(next_item)\n        n -= 1\n    for item in sequence:\n        history.append(item)\n        yield tuple(history)\n        del history[0]\n\ndef merge_wordpiece_tokens(paired_tokens, weighted = True):\n    new_paired_tokens = []\n    n_tokens = len(paired_tokens)\n\n    i = 0\n\n    while i < n_tokens:\n        current_token, current_weight = paired_tokens[i]\n        if current_token.startswith('##'):\n            previous_token, previous_weight = new_paired_tokens.pop()\n            merged_token = previous_token\n            merged_weight = [previous_weight]\n            while current_token.startswith('##'):\n                merged_token = merged_token + current_token.replace('##', '')\n                merged_weight.append(current_weight)\n                i = i + 1\n                current_token, current_weight = paired_tokens[i]\n            merged_weight = np.mean(merged_weight)\n            new_paired_tokens.append((merged_token, merged_weight))\n\n        else:\n            new_paired_tokens.append((current_token, current_weight))\n            i = i + 1\n\n    words = [\n        i[0]\n        for i in new_paired_tokens\n        if i[0] not in ['[CLS]', '[SEP]', '[PAD]']\n    ]\n    weights = [\n        i[1]\n        for i in new_paired_tokens\n        if i[0] not in ['[CLS]', '[SEP]', '[PAD]']\n    ]\n    if weighted:\n        weights = np.array(weights)\n        weights = weights / np.sum(weights)\n    return list(zip(words, weights))\n\ndef _extract_attention_weights(num_layers, tf_graph):\n    attns = [\n        {\n            'layer_%s'\n            % i: tf_graph.get_tensor_by_name(\n                'bert/encoder/layer_%s/attention/self/Softmax:0' % i\n            )\n        }\n        for i in range(num_layers)\n    ]\n\n    return attns\n\ndef padding_sequence(seq, maxlen, padding = 'post', pad_int = 0):\n    padded_seqs = []\n    for s in seq:\n        if padding == 'post':\n            padded_seqs.append(s + [pad_int] * (maxlen - len(s)))\n        if padding == 'pre':\n            padded_seqs.append([pad_int] * (maxlen - len(s)) + s)\n    return padded_seqs\n\n\ndef bert_tokenization(tokenizer, texts, cls = '[CLS]', sep = '[SEP]'):\n\n    input_ids, input_masks, segment_ids, s_tokens = [], [], [], []\n    for text in texts:\n        tokens_a = tokenizer.tokenize(text)\n        tokens = [cls] + tokens_a + [sep]\n        segment_id = [0] * len(tokens)\n        input_id = tokenizer.convert_tokens_to_ids(tokens)\n        input_mask = [1] * len(input_id)\n\n        input_ids.append(input_id)\n        input_masks.append(input_mask)\n        segment_ids.append(segment_id)\n        s_tokens.append(tokens)\n\n    maxlen = max([len(i) for i in input_ids])\n    input_ids = padding_sequence(input_ids, maxlen)\n    input_masks = padding_sequence(input_masks, maxlen)\n    segment_ids = padding_sequence(segment_ids, maxlen)\n\n    return input_ids, input_masks, segment_ids, s_tokens\n\nclass _Model:\n    def __init__(self, bert_config, tokenizer):\n        _graph = tf.Graph()\n        with _graph.as_default():\n            self.X = tf.placeholder(tf.int32, [None, None])\n            self._tokenizer = tokenizer\n\n            self.model = modeling.BertModel(\n                config = bert_config,\n                is_training = False,\n                input_ids = self.X,\n                use_one_hot_embeddings = False,\n            )\n            self.logits = self.model.get_pooled_output()\n            self._sess = tf.InteractiveSession()\n            self._sess.run(tf.global_variables_initializer())\n            var_lists = tf.get_collection(\n                tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'bert'\n            )\n            self._saver = tf.train.Saver(var_list = var_lists)\n            attns = _extract_attention_weights(\n                bert_config.num_hidden_layers, tf.get_default_graph()\n            )\n            self.attns = attns\n\n    def vectorize(self, strings):\n\n        \"\"\"\n        Vectorize string inputs using bert attention.\n\n        Parameters\n        ----------\n        strings : str / list of str\n\n        Returns\n        -------\n        array: vectorized strings\n        \"\"\"\n\n        if isinstance(strings, list):\n            if not isinstance(strings[0], str):\n                raise ValueError('input must be a list of strings or a string')\n        else:\n            if not isinstance(strings, str):\n                raise ValueError('input must be a list of strings or a string')\n        if isinstance(strings, str):\n            strings = [strings]\n\n        batch_x, _, _, _ = bert_tokenization(self._tokenizer, strings)\n        return self._sess.run(self.logits, feed_dict = {self.X: batch_x})\n\n    def attention(self, strings, method = 'last', **kwargs):\n        \"\"\"\n        Get attention string inputs from bert attention.\n\n        Parameters\n        ----------\n        strings : str / list of str\n        method : str, optional (default='last')\n            Attention layer supported. Allowed values:\n\n            * ``'last'`` - attention from last layer.\n            * ``'first'`` - attention from first layer.\n            * ``'mean'`` - average attentions from all layers.\n\n        Returns\n        -------\n        array: attention\n        \"\"\"\n\n        if isinstance(strings, list):\n            if not isinstance(strings[0], str):\n                raise ValueError('input must be a list of strings or a string')\n        else:\n            if not isinstance(strings, str):\n                raise ValueError('input must be a list of strings or a string')\n        if isinstance(strings, str):\n            strings = [strings]\n\n        method = method.lower()\n        if method not in ['last', 'first', 'mean']:\n            raise Exception(\n                \"method not supported, only support 'last', 'first' and 'mean'\"\n            )\n\n        batch_x, _, _, s_tokens = bert_tokenization(self._tokenizer, strings)\n        maxlen = max([len(s) for s in s_tokens])\n        s_tokens = padding_sequence(s_tokens, maxlen, pad_int = '[SEP]')\n        attentions = self._sess.run(self.attns, feed_dict = {self.X: batch_x})\n        if method == 'first':\n            cls_attn = list(attentions[0].values())[0][:, :, 0, :]\n\n        if method == 'last':\n            cls_attn = list(attentions[-1].values())[0][:, :, 0, :]\n\n        if method == 'mean':\n            combined_attentions = []\n            for a in attentions:\n                combined_attentions.append(list(a.values())[0])\n            cls_attn = np.mean(combined_attentions, axis = 0).mean(axis = 2)\n\n        cls_attn = np.mean(cls_attn, axis = 1)\n        total_weights = np.sum(cls_attn, axis = -1, keepdims = True)\n        attn = cls_attn / total_weights\n        output = []\n        for i in range(attn.shape[0]):\n            output.append(\n                merge_wordpiece_tokens(list(zip(s_tokens[i], attn[i])))\n            )\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = tokenization.FullTokenizer(vocab_file=BERT_VOCAB, do_lower_case=False)\nbert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\nmodel = _Model(bert_config, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"example 1(vectorize) :"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = model.vectorize(['hello nice to meet u', 'so long sucker'])\nv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"example 2(attention) :"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.attention(['hello nice to meet u', 'so long sucker'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 10\nngram = (1, 3)\nn_topics = 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"taking all the titles from clean_pmc.csv "},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf = clean_pmc\ndf = df.title.dropna()\nnegative = df.values.tolist()\nnegative[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative = negative[:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom tqdm import tqdm\n\nrows, attentions = [], []\nfor i in (range (len(negative))):\n          #index = min(i + batch_size, len(negative))\n          rows.append(model.vectorize(negative[i]))\n          attentions.extend(model.attention(negative[i]))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = stop_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"concat = np.concatenate(rows, axis = 0)\nkmeans = KMeans(n_clusters = n_topics, random_state = 0).fit(concat)\nlabels = kmeans.labels_\n\noverall, filtered_a = [], []\nfor a in attentions:\n    #print(a)\n    f = [i for i in a if i[0] not in stopwords]\n    overall.extend(f)\n    filtered_a.append(f)\n\no_ngram = generate_ngram(overall, ngram)\nfeatures = []\nfor i in o_ngram:\n    #print(i)\n    features.append(' '.join([w[0] for w in i]))\nfeatures = list(set(features))\n\ncomponents = np.zeros((n_topics, len(features)))\nprint(n_topics)\n#print(features)\nfor no, i in enumerate(labels):\n    if (no + 1) % 500 == 0: \n        print('processed %d'%(no + 1))\n    f = generate_ngram(filtered_a[no], ngram)\n    for w in f:\n        word = ' '.join([r[0] for r in w])\n        score = np.mean([r[1] for r in w])\n        if word in features:\n            components[i, features.index(word)] += score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def print_topics_modelling(\n    topics, feature_names, sorting, n_words = 20, return_df = True\n):\n    if return_df:\n        try:\n            import pandas as pd\n        except:\n            raise Exception(\n                'pandas not installed. Please install it and try again or set `return_df = False`'\n            )\n    df = {}\n    for i in range(topics):\n        words = []\n        for k in range(n_words):\n            words.append(feature_names[sorting[i, k]])\n        df['topic %d' % (i)] = words\n    if return_df:\n        return pd.DataFrame.from_dict(df)\n    else:\n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_topics_modelling(\n    10,\n    feature_names = np.array(features),\n    sorting = np.argsort(components)[:, ::-1],\n    n_words = 10,\n    return_df = True,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summarization Task using Bart "},{"metadata":{},"cell_type":"markdown","source":"In October 2019, teams from Google and Facebook published new transformer papers: T5 and BART. Both papers achieved better downstream performance on generation tasks, like abstractive summarization and dialogue, with two changes:\n\n* add a causal decoder to BERT's bidirectional encoder architecture\n* replace BERT's fill-in-the blank cloze task with a more complicated mix of pretraining tasks.\n\n> (BART) can be seen as generalizing Bert (due to the bidirectional encoder) and GPT2 (with the left to right decoder). - bart authors\n\nBert is pretrained to try to predict masked tokens, and uses the whole sequence to get enough info to make a good guess. This is good for tasks where the prediction at position i is allowed to utilize information from positions after i, but less useful for tasks, like text generation, where the prediction for position i can only depend on previously generated words.\n\nIn code, the idea of \"what information can be used use when predicting the token at position i\" is controlled by an argument called attention_mask1. A value of 1 in the attention mask means that the model can use information for the column's word when predicting the row's word.\n\nHere is Bert's \"Fully-visible\"2 attention_mask:\n\n<img src=\"https://sshleifer.github.io/blog_v2/images/copied_from_nb/diagram_bert_v5.png\" width=\"600px\" align=\"left\"> \n"},{"metadata":{},"cell_type":"markdown","source":"\nGPT2, meanwhile, is pretrained to predict the next word using a causal mask, and is more effective for generation tasks, but less effective on downstream tasks where the whole input yields information for the output.\n\nHere is the attention_mask for GPT2:\n\n<img src=\"https://sshleifer.github.io/blog_v2/images/copied_from_nb/diagram_bartpost_gpt2.jpg\" width=\"600px\" align=\"left\"> \n\n"},{"metadata":{},"cell_type":"markdown","source":"The prediction for \"eating\", only utilizes previous words: \"<BOS> I love\".\n    \n\nfor more information please check this reference link : [Introducing BART](https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html)"},{"metadata":{},"cell_type":"markdown","source":"ref : https://github.com/renatoviolin/Bart_T5-summarization/blob/master/app.py"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install -U transformers\n!pip install -U torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import torch\nimport os\nimport json\nfrom transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BART_PATH = 'bart-large'\nbart_model = BartForConditionalGeneration.from_pretrained(BART_PATH, output_past=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bart_tokenizer = BartTokenizer.from_pretrained(BART_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bart_summarize(input_text, num_beams=4, num_words=80):\n    #input_text = str(input_text)\n    input_text = ' '.join(input_text.split())\n    input_tokenized = bart_tokenizer.encode(input_text, return_tensors='pt')\n    summary_ids = bart_model.generate(input_tokenized,\n                                      num_beams=int(num_beams),\n                                      no_repeat_ngram_size=3,\n                                      length_penalty=2.0,\n                                      min_length=100,\n                                      max_length=int(num_words),\n                                      early_stopping=True)\n    output = [bart_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n    return output[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf = clean_pmc\ndf = df.abstract.dropna()\nabstracts = df.values.tolist()\n\nlen(abstracts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"summarizing first 20 papers"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor i in range(20):\n    try:\n        print('paper  ',i + 1, \" : \\n\" )\n        print(bart_summarize(abstracts[i]))\n        print('............................................................................\\n\\n\\n\\n')\n    except:\n        print('paper ',i+1 ,\" has LONG ABSTRACT\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summarization Task using T5 model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf1 = biorxiv_clean\ndf1 = df1.abstract.dropna()\ndf1abstracts = df.values.tolist()\n\nlen(df1abstracts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"T5_PATH = 't5-base'\nt5_model = T5ForConditionalGeneration.from_pretrained(T5_PATH, output_past=True)\nt5_tokenizer = T5Tokenizer.from_pretrained(T5_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndef t5_summarize(input_text, num_beams=4, num_words=80):\n    #input_text = str(input_text).replace('\\n', '')\n    input_text = ' '.join(input_text.split())\n    input_tokenized = t5_tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n    summary_task = torch.tensor([[21603, 10]]).to(device)\n    input_tokenized = torch.cat([summary_task, input_tokenized], dim=-1).to(device)\n    summary_ids = t5_model.generate(input_tokenized,\n                                    num_beams=int(num_beams),\n                                    no_repeat_ngram_size=3,\n                                    length_penalty=2.0,\n                                    min_length=30,\n                                    max_length=int(num_words),\n                                    early_stopping=True)\n    output = [t5_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n    return output[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"summariing first 20 papers abstract of bioarvix"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor i in range(20):\n    try:\n        print('BioArvix paper  ',i + 1, \" : \\n\" )\n        print(t5_summarize(df1abstracts[i]))\n        print('............................................................................\\n\\n\\n\\n')\n    except:\n        print('paper ',i+1 ,\" has LONG ABSTRACT\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"simsentence.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install sentence-transformers\n\"\"\"\nThis is a simple application for sentence embeddings: semantic search\nWe have a corpus with various sentences. Then, for a given query sentence,\nwe want to find the most similar sentence in this corpus.\nThis script outputs for various queries the top 5 most similar sentences in the corpus.\n\"\"\"\n# taken from : https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.py\nfrom sentence_transformers import SentenceTransformer\nimport scipy.spatial\n\nembedder = SentenceTransformer('bert-base-nli-mean-tokens')\n\n# Corpus with example sentences\ncorpus = simsentence.similar.tolist()\ncorpus_embeddings = embedder.encode(corpus)\n\n# Query sentences:\nqueries = ['Range of incubation periods for the disease in humans', 'antiviral covid-19 success treatment','virus detected from animals?', 'risk of fatality among symptomatic hospitalized patients']\nquery_embeddings = embedder.encode(queries)\n\n# Find the closest  sentences of the corpus for each query sentence based on cosine similarity\nclosest_n = 5\nfor query, query_embedding in zip(queries, query_embeddings):\n    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n\n    results = zip(range(len(distances)), distances)\n    results = sorted(results, key=lambda x: x[1])\n\n    print(\"\\n\\n======================\\n\\n\")\n    print(\"Query:\", query)\n    print(\"\\nTop 5 most similar sentences from similar\")\n\n    for idx, distance in results[0:closest_n]:\n        print(corpus[idx].strip(), \"(Score: %.4f)\" % (1-distance))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nThis is a simple application for sentence embeddings: semantic search\nWe have a corpus with various sentences. Then, for a given query sentence,\nwe want to find the most similar sentence in this corpus.\nThis script outputs for various queries the top 5 most similar sentences in the corpus.\n\"\"\"\n# taken from : https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.py\nfrom sentence_transformers import SentenceTransformer\nimport scipy.spatial\n\nembedder = SentenceTransformer('bert-base-nli-mean-tokens')\n\n# Corpus with example sentences\ncorpus = df.values.tolist()\ncorpus_embeddings = embedder.encode(corpus)\n\n# Query sentences:\nqueries = ['Range of incubation periods for the disease in humans','risk factors of covid-19','cure for covid-19', 'antiviral covid-19 success treatment','Does smoking or pre-existing pulmonary disease increase risk of COVID-19?', 'risk of fatality among symptomatic hospitalized patients']\nquery_embeddings = embedder.encode(queries)\n\n# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\nclosest_n = 5\nfor query, query_embedding in zip(queries, query_embeddings):\n    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n\n    results = zip(range(len(distances)), distances)\n    results = sorted(results, key=lambda x: x[1])\n\n    print(\"\\n\\n======================\\n\\n\")\n    print(\"Query:\", query)\n    print(\"\\nTop 5 most similar sentences in corpus:\")\n\n    for idx, distance in results[0:closest_n]:\n        print(corpus[idx].strip(), \"(Score: %.4f)\" % (1-distance))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 5 : Covid Paper Browser\n\n**Browse Covid-19 & SARS-CoV-2 Scientific Papers with Transformers ü¶† üìñ**\n\ni will  use  model : bert-base-nli-stsb-mean-tokens\n\nmore details can be found here : [COVID-19 Bert Literature Search Engine](https://towardsdatascience.com/covid-19-bert-literature-search-engine-4d06cdac08bd)\n\n# ref : [Browse Covid-19](https://github.com/gsarti/covid-papers-browser)"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport os\nimport tqdm\nimport textwrap\nimport json\nimport prettytable\nimport logging\nimport pickle\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom  transformers import *\nimport pandas as pd\nimport scipy\nfrom sentence_transformers import SentenceTransformer\n\nCOVID_BROWSER_ASCII = \"\"\"\n================================================================================\n  _____           _     _      __  ___    ____                                  \n / ____|         (_)   | |    /_ |/ _ \\  |  _ \\                                 \n| |     _____   ___  __| | ___ | | (_) | | |_) |_ __ _____      _____  ___ _ __ \n| |    / _ \\ \\ / / |/ _` ||___|| |\\__, | |  _ <| '__/ _ \\ \\ /\\ / / __|/ _ \\ '__|\n| |___| (_) \\ V /| | (_| |     | |  / /  | |_) | | | (_) \\ V  V /\\__ \\  __/ |   \n \\_____\\___/ \\_/ |_|\\__,_|     |_| /_/   |____/|_|  \\___/ \\_/\\_/ |___/\\___|_|   \n=================================================================================\n\"\"\"\n\nCOVID_BROWSER_INTRO = \"\"\"\nThis demo uses a state-of-the-art language model trained on scientific papers to\nsearch passages matching user-defined queries inside the COVID-19 Open Research\nDataset. Ask something like 'Is smoking a risk factor for Covid-19?' to retrieve\nrelevant abstracts.\\n\n\"\"\"\n\nBIORXIV_PATH = '/kaggle/input/CORD-19-research-challenge//biorxiv_medrxiv/biorxiv_medrxiv/'\nCOMM_USE_PATH = '/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/'\nNONCOMM_USE_PATH = '/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/'\nMETADATA_PATH = '/kaggle/input/CORD-19-research-challenge/metadata.csv'\n\nDATA_PATH = '/kaggle/input/CORD-19-research-challenge/'\nMODELS_PATH = 'models'\nMODEL_NAME = 'scibert-nli'\nCORPUS_PATH = os.path.join(DATA_PATH, 'corpus.pkl')\nMODEL_PATH = os.path.join(MODELS_PATH, MODEL_NAME)\nEMBEDDINGS_PATH = os.path.join(DATA_PATH, f'{MODEL_NAME}-embeddings.pkl')\n\n\ndef load_json_files(dirname):\n    filenames = [file for file in os.listdir(dirname) if file.endswith('.json')]\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    print('Loaded', len(raw_files), 'files from', dirname)\n    return raw_files\n\n\ndef create_corpus_from_json(files):\n    corpus = []\n    for file in tqdm(files):\n        for item in file['abstract']:\n            corpus.append(item['text'])\n        for item in file['body_text']:\n            corpus.append(item['text'])\n    print('Corpus size', len(corpus))\n    return corpus\n\n\ndef cache_corpus(mode='CSV'):\n    corpus = []\n    if mode == 'CSV':\n        df = pd.read_csv(METADATA_PATH)\n        corpus = [a for a in df['abstract'] if type(a) == str and a != \"Unknown\"]\n        print('Corpus size', len(corpus))\n    elif mode == 'JSON':\n        biorxiv_files = load_json_files(BIORXIV_PATH)\n        comm_use_files = load_json_files(COMM_USE_PATH)\n        noncomm_use_files = load_json_files(NONCOMM_USE_PATH)\n        corpus = create_corpus_from_json(biorxiv_files + comm_use_files + noncomm_use_files)\n    else:\n        raise AttributeError('Mode should be either CSV or JSON')\n    '''with open(CORPUS_PATH, 'wb') as file:\n        pickle.dump(corpus, file)'''\n    return corpus\n\n\ndef ask_question(query, model, corpus, corpus_embed, top_k=5):\n    \"\"\"\n    Adapted from https://www.kaggle.com/dattaraj/risks-of-covid-19-ai-driven-q-a\n    \"\"\"\n    queries = [query]\n    query_embeds = model.encode(queries, show_progress_bar=False)\n    for query, query_embed in zip(queries, query_embeds):\n        distances = scipy.spatial.distance.cdist([query_embed], corpus_embed, \"cosine\")[0]\n        distances = zip(range(len(distances)), distances)\n        distances = sorted(distances, key=lambda x: x[1])\n        results = []\n        for count, (idx, distance) in enumerate(distances[0:top_k]):\n            results.append([count + 1, corpus[idx].strip(), round(1 - distance, 4)])\n    return results\n\n\ndef show_answers(results):\n    table = prettytable.PrettyTable(\n        ['Rank', 'Abstract', 'Score']\n    )\n    for res in results:\n        rank = res[0]\n        text = res[1]\n        text = textwrap.fill(text, width=75)\n        text = text + '\\n\\n'\n        score = res[2]\n        table.add_row([\n            rank,\n            text,\n            score\n        ])\n    print('\\n')\n    print(str(table))\n    print('\\n')\n\nif __name__ == '__main__':\n    os.system('cls' if os.name == 'nt' else 'clear')\n    print(COVID_BROWSER_ASCII)\n    print(COVID_BROWSER_INTRO)\n    if not os.path.exists(CORPUS_PATH):\n        print(\"Caching the corpus for future use...\")\n        corpus = cache_corpus()\n    else:\n        print(\"Loading the corpus from\", CORPUS_PATH, '...')\n        with open(CORPUS_PATH, 'rb') as corpus_pt:\n            corpus = pickle.load(corpus_pt)\n\n    model =  SentenceTransformer('bert-base-nli-stsb-mean-tokens')\n\n    if not os.path.exists(EMBEDDINGS_PATH):\n        print(\"Computing and caching model embeddings for future use...\")\n        embeddings = model.encode(corpus, show_progress_bar=True)\n        '''with open(EMBEDDINGS_PATH, 'wb') as file:\n            pickle.dump(embeddings, file)'''\n    else:\n        print(\"Loading model embeddings from\", EMBEDDINGS_PATH, '...')\n        with open(EMBEDDINGS_PATH, 'rb') as file:\n            embeddings = pickle.load(file)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questions = ['Is smoking a risk factor for Covid-19?','What has been published about medical care?','Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities','risk for  Neonates and pregnant women?','Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.']\nfor i in range(len(questions)):\n        query = questions[i]\n        print(f'Query {i+1} : {query}\\n\\n')\n        results = ask_question(query, model, corpus, embeddings)\n        show_answers(results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:purple;\">\nTo be continued..................... üë®‚Äç‚öïÔ∏è \n</h1>"},{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"left\" style=\"color:red;\">\nIf you find this kernel interesting, please drop an<br>  <font color=\"blue\">UPVOTE</font>. It motivates me to produce more quality contents ü§ó\n</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}