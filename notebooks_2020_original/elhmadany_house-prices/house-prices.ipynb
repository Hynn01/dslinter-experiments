{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem 1 :  Predicting house prices  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import using library\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport seaborn as sns\nfrom scipy import stats \nfrom scipy.stats import norm, skew ,zscore#for some statistics\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nfrom sklearn.linear_model import LinearRegression,Lasso,ElasticNet,Ridge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load our data train and test (test dat does not include the target feature)\ntrain=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest=pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see total 81 variables for train and 80 variables for test data. And we don't have *SalePrice* variable for test set because this will be our task to infer *SalePrice* for test set by learning from train set. So *SalePrice* is our target variable and rest of the variables are our predictor variables.**\n#### Here comes the description of a few variables:\n* MSSubClass — The building class\n* MSZoning — The general zoning classification\n* LotFrontage — Linear feet of street connected to property\n* LotArea — Lot size in square feet\n* Street — Type of road access\n* Alley — Type of alley access\n* LotShape — General shape of property\n* LandContour — Flatness of the property\n* Utilities — Type of utilities available\n* LotConfig — Lot configuration\n* LandSlope: Slope of property\n* Neighborhood: Physical locations within Ames city limits\n* Condition1: Proximity to main road or railroad\n* Condition2: Proximity to main road or railroad (if a second is present)\n* BldgType: Type of dwelling\n* HouseStyle: Style of dwelling\n* OverallQual: Overall material and finish quality\n* OverallCond: Overall condition rating\n* YearBuilt: Original construction date\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape,test.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**perform appropriate exploratory data analysis **"},{"metadata":{"trusted":true},"cell_type":"code","source":"#let know more info about features data\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's know the target feature by applying XOR Function between train and test\ntest.columns^train.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's select the numeric features \nnumerc_fet=train.select_dtypes(include=np.number)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerc_fet.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Measure the correlation between those numeric feaures regarding to the target**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncorr=numerc_fet.corr()\ncorr['SalePrice'].sort_values(ascending=False)[:9]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As above the most important variables is OverallQual  and GrLivArea  features   **"},{"metadata":{},"cell_type":"markdown","source":"[](http://)**heatmap of correlation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check if there any outliers\nsns.boxplot(x=train['OverallQual'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=train['GarageCars'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=train['TotRmsAbvGrd'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=train['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**visualize some data distrbution!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsix_cols = ['GrLivArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'LotArea', 'SalePrice']\nsns.pairplot(train[six_cols]) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalData=pd.concat([train,test])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating New Features\n\nWe will create a new feature named TotalSF combining TotalBsmtSF, 1stFlrSF, and 2ndFlrSF."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Feature TotalSF\ntotalData['TotalSF'] = totalData['TotalBsmtSF'] + totalData['1stFlrSF'] + totalData['2ndFlrSF']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Measure the correlation between those numeric feaures regarding to the targe after adding TotalSF feature**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=len(y)\ntrain_fea=totalData.iloc[:x,:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerc_fet2=train_fea.select_dtypes(include=np.number)\n#Measure the correlation between those numeric feaures regarding to the target\ncorr2=numerc_fet2.corr()\ncorr2['SalePrice'].sort_values(ascending=False)[:9]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As above the most important variables is OverallQual and TotalSF features"},{"metadata":{"trusted":true},"cell_type":"code","source":"totalData.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Xtrain=totalData.drop('SalePrice',axis=1)\nytrain=train['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compute the missing data\nmissing=totalData.isnull().sum().sort_values(ascending=False)\nmissing=missing[missing>0]\nmissing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dealing With Missing data\n#for catergorical variables, we replece missing data with None\nMiss_cat=['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', \n          'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', \n          'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MSSubClass']\nfor col in Miss_cat:\n    totalData[col].fillna('None',inplace=True)\n# for numerical variables, we replace missing value with 0\nMiss_num=['GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', \n          'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'] \nfor col in Miss_num:\n    totalData[col].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rest_val=['MSZoning','Functional','Utilities','Exterior1st', 'SaleType','Electrical', 'Exterior2nd','KitchenQual']\nfor col in rest_val:\n    totalData[col].fillna(totalData[col].mode()[0],inplace=True)  #fill with most frequency data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalData['LotFrontage']=totalData.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))#fill with median","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalData=totalData.drop('Id',axis=1)  #not important feature\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert the numeric values into string becuse there are many repetition \ntotalData['YrSold'] = totalData['YrSold'].astype(str)\ntotalData['MoSold'] = totalData['MoSold'].astype(str)\ntotalData['MSSubClass'] = totalData['MSSubClass'].astype(str)\ntotalData['OverallCond'] = totalData['OverallCond'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(totalData[c].values)) \n    totalData[c] = lbl.transform(list(totalData[c].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape        \nprint('Shape totalData: {}'.format(totalData.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_feats = totalData.dtypes[totalData.dtypes != \"object\"].index\nstring_feats=totalData.dtypes[totalData.dtypes == \"object\"].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"string_feats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dealing with string_feats\ndumies = pd.get_dummies(totalData[string_feats])\nprint(dumies.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalData=pd.concat([totalData,dumies],axis='columns')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalData.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalData=totalData.drop(string_feats,axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totalData.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dealing with out liers\nlen(totalData)   # number of rows befor remove the outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=len(ytrain)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feature=totalData.iloc[:x,:]\ntest_feature=totalData.iloc[x:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feature.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here we will not apply scalling for data features because there are many str features and by compare the result with and without scalling\n\n#sc_X = MinMaxScaler()\n#all_data_train_normalized = sc_X.fit_transform(train_feature)\n#all_data_test_normalized = sc_X.transform(test_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data_train_normalized=train_feature\nall_data_test_normalized=test_feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data_train_normalized.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**visualization  of traget varible distribution**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(ytrain , fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(ytrain)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(ytrain, plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytrain.skew()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**log function in python dealing with skewness**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ytrain=np.log(ytrain)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytrain.skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytrain=pd.DataFrame(ytrain)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**plot Log-transformation of the target variable**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(ytrain , fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(ytrain)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytrain.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(ytrain),len(all_data_train_normalized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(all_data_train_normalized,ytrain,test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**First Model is linear Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1= LinearRegression()\nmodel1.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypre1=model1.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = mean_squared_error(y_pred=ypre1,y_true=Y_test)\nr2_scor = r2_score(y_pred=ypre1,y_true=Y_test)\nabsloute = mean_absolute_error(y_pred=ypre1,y_true=Y_test)\nprint(mean,r2_scor,absloute)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.score(X_test,Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# note: we here achieved more than 95 percent accuracy after adding TotalSF feature it was 91 percent before adding this feature and the mean_squared_error decresed to be 0.0089  from 0.016 "},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicting on the test set\npredictions = model1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actual_values = Y_test\nplt.scatter(predictions, actual_values, alpha= 0.75, color = 'b')\n\nplt.xlabel('Predicted price')\nplt.ylabel('Actual price')\nplt.title('Linear Regression Model')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Describe the various models **"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Try more Models\n\n# Test Options and Evaluation Metrics\nnum_folds = 5\nscoring = \"neg_mean_squared_error\"\n# Spot Check Algorithms\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))\nmodels.append(('RFR', RandomForestRegressor()))\n\n\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=0)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold,    scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(),   cv_results.std())\n    print(msg)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**Note: the best pramester here is RandomForestRegressor with 0.000579 mean_squared_error**"},{"metadata":{"trusted":true},"cell_type":"code","source":"RFR=RandomForestRegressor()\nRFR.fit(X_train,Y_train)\nypreRFR=RFR.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = mean_squared_error(y_pred=ypreRFR,y_true=Y_test)\nr2_scor = r2_score(y_pred=ypreRFR,y_true=Y_test)\nabsloute = mean_absolute_error(y_pred=ypreRFR,y_true=Y_test)\nprint(mean,r2_scor,absloute)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFR.score(X_test,Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**note: we here achieved more than 99 percent accuracy in this model after adding TotalSF feature and the mean_squared_error decresed to be 0.00057 **"},{"metadata":{},"cell_type":"markdown","source":"**#ensemble methods (gradient boosting)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn import ensemble\n# Fit regression model\nparams = {'n_estimators': 1000, 'max_depth': 2, 'min_samples_split': 2,\n          'learning_rate': 0.01, 'loss': 'ls'}\nmodel2 = ensemble.GradientBoostingRegressor(**params)\n\nmodel2.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypre2=model2.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = mean_squared_error(y_pred=ypre2,y_true=Y_test)\nr2_scor = r2_score(y_pred=ypre2,y_true=Y_test)\nabsloute = mean_absolute_error(y_pred=ypre2,y_true=Y_test)\nprint(mean,r2_scor,absloute)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.score(X_test,Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: More better Accuracy 99.9 percent"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions2 = model2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actual_values = Y_test\nplt.scatter(predictions2, actual_values, alpha= 0.75, color = 'b')\n\nplt.xlabel('Predicted price')\nplt.ylabel('Actual price')\nplt.title('gradient boosting Model')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear regression with L2 regularization kind of optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor i in range(-2, 3):\n    alpha = 10**i\n    rm = Ridge(alpha = alpha)\n    ridge_model = rm.fit(X_train, Y_train)\n    preds_ridge = ridge_model.predict(X_test)\n    \n    plt.scatter(preds_ridge,Y_test, alpha= 0.75, c= 'b')\n    plt.xlabel('Predicted price')\n    plt.ylabel('Actual price')\n    plt.title('Ridge redularization with alpha {}'.format(alpha))\n    overlay = 'R square: {} \\nMSE: {}'.format(ridge_model.score(X_test, Y_test), mean_squared_error(Y_test, preds_ridge))\n    plt.annotate(s = overlay, xy = (12.1, 10.6), size = 'x-large')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So After trying more than model we found the linear regression is the best one in terms in mean_squared_error so we can  select linear regrision Model or Ridge Regression **"},{"metadata":{},"cell_type":"markdown","source":"**here we try more value for alpha in Ridge Regression Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nalphas = np.linspace(0.0002, 100, num=50)\nscores = [\n     np.sqrt(-cross_val_score(Ridge(alpha), X_train,Y_train, \n       scoring=\"neg_mean_squared_error\")).mean()\n     for alpha in alphas\n]\nscores = pd.Series(scores, index=alphas)\nscores.plot(title = \"Alphas vs error (Lowest error is best)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rm = Ridge(alpha = 18)\nridge_model = rm.fit(X_train, Y_train)\npreds_ridge = ridge_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = mean_squared_error(y_pred=preds_ridge,y_true=Y_test)\nr2_scor = r2_score(y_pred=preds_ridge,y_true=Y_test)\nabsloute = mean_absolute_error(y_pred=preds_ridge,y_true=Y_test)\nprint(mean,r2_scor,absloute)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finally we found gradient boosting give best result compare to Random forest**"},{"metadata":{},"cell_type":"markdown","source":"**The predictor variables for gradient boosting**[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.get_params\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Build MLP for Regression and see the result**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Build MLP Model\nNN_model = Sequential()\n\n# The Input Layer :\nNN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n\n# The Hidden Layers :\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n#NN_model.add(Dropout(.2))   # Add Dropout layer\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n#NN_model.add(Dropout(.2)) # Add another Dropout layer\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n\n# The Output Layer :\nNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\n\ncheckpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MLPR_model=NN_model.fit(X_train,Y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss = NN_model.evaluate(X_test, Y_test)\ntest_loss[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**not better accuracy in terms of val_loss**"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":4}