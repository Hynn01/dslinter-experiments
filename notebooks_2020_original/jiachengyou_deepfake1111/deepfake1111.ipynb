{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport sys\nimport torch\nimport torch.nn as nn\nimport matplotlib.patches as patches\nimport math\nsys.path.insert(0, \"/kaggle/input/blazeface-pytorch\")\nfrom blazeface import BlazeFace\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport PIL\ngpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\npath = '/kaggle/input/faceforensics'\nos.listdir(path)\n!pip install --upgrade efficientnet-pytorch\nfrom efficientnet_pytorch import EfficientNet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_detections(img, detections, with_keypoints=True):\n    fig, ax = plt.subplots(1, figsize=(10, 10))\n    ax.grid(False)\n    ax.imshow(img)\n    \n    if isinstance(detections, torch.Tensor):\n        detections = detections.cpu().numpy()\n\n    if detections.ndim == 1:\n        detections = np.expand_dims(detections, axis=0)\n\n    print(\"Found %d faces\" % detections.shape[0])\n        \n    for i in range(detections.shape[0]):\n        ymin = detections[i, 0] * img.shape[0]\n        xmin = detections[i, 1] * img.shape[1]\n        ymax = detections[i, 2] * img.shape[0]\n        xmax = detections[i, 3] * img.shape[1]\n        print(xmin,ymin)\n\n        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                 linewidth=1, edgecolor=\"r\", facecolor=\"none\", \n                                 alpha=detections[i, 16])\n        ax.add_patch(rect)\n\n        if with_keypoints:\n            for k in range(6):\n                kp_x = detections[i, 4 + k*2    ] * img.shape[1]\n                kp_y = detections[i, 4 + k*2 + 1] * img.shape[0]\n                circle = patches.Circle((kp_x, kp_y), radius=0.5, linewidth=1, \n                                        edgecolor=\"lightskyblue\", facecolor=\"none\", \n                                        alpha=detections[i, 16])\n                ax.add_patch(circle)\n        \n    plt.show()\n\ndef findFace(img, detections):\n#         print(\"detections nums\", len(detections))\n    i = 0\n    ymin = math.floor(detections[i, 0] * img.shape[0])\n    xmin = math.floor(detections[i, 1] * img.shape[1])\n    ymax = math.ceil(detections[i, 2] * img.shape[0])\n    xmax = math.ceil(detections[i, 3] * img.shape[1])\n#         print(detections[i, 0], detections[i, 1],detections[i, 2], detections[i, 3])\n#         print(img.shape)\n#         print(xmin, xmax, ymin, ymax)\n    face = img[ymin:ymax, xmin:xmax+1]\n#         print(face.shape)\n\n    return face","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = {\n    \"Deepfakes\" : 429 / 500,    \n    \"Face2Face\" : 276 / 500,\n    \"FaceSwap\" : 287 / 500,\n    \"NeuralTextures\" : 209 / 500,\n    \"origin\" : 302 / 500 \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class VideoProcess():\n    \n    def __init__(self):\n        pass\n    \n    def getFrame(self, videos_dir, file_num=\"all\", frame_num=5, type=0):\n        \n        file_list = os.listdir(videos_dir)\n        file_list.sort()\n        \n        if len(file_list) == 0 : return None\n        \n        if file_num == 'all' :\n            file_list = file_list[:]\n        elif len(file_list) > file_num:\n            file_list = file_list[:file_num]\n        else:\n            file_list = file_list[:]\n            \n        \n        frame_num = 5\n        pic_num = len(file_list)\n        img_arr = [[] for i in range(pic_num)]\n        for (index1,file) in enumerate(file_list):\n            filename = os.path.join(videos_dir, file)\n            cap=cv2.VideoCapture(filename)\n            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            if frame_count <= 0 : continue\n            \n            # type : linspace\n            if type == 0:\n                frame_idxs = np.linspace(0, frame_count - 1, frame_num, endpoint=True, dtype=np.int)\n                frame_idxs = np.unique(frame_idxs)\n\n            for (index2,idx) in enumerate(frame_idxs):\n                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n                ret, frame  = cap.read()\n                img_arr[index1].append(frame)\n            \n            if index1 % 100 == 0:\n                print(\"getFrame:\", index1)\n            \n        return img_arr\n    \n        \n\n    def blazeFace(self, imgs):\n\n        gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n        net = BlazeFace().to(gpu)\n        net.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\")\n        net.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\")\n\n        # Optionally change the thresholds:\n        net.min_score_thresh = 0.75\n        net.min_suppression_threshold = 0.3\n        pic_num = len(imgs)\n\n        if pic_num == 0: return None\n\n        frame_num = len(imgs[0])\n\n        res = [[] for i in range(pic_num)]\n        for i in range(pic_num):\n            for j in range(frame_num):\n                img = imgs[i][j]\n                detections = net.predict_on_image(cv2.resize(img,(128,128)))\n                if len(detections) == 0 : continue\n                res[i].append(self.findFace(img, detections))\n        return res     \n\n    def findFace(self, img, detections):\n#         print(\"detections nums\", len(detections))\n        i = 0\n        ymin = math.floor(detections[i, 0] * img.shape[0])\n        xmin = math.floor(detections[i, 1] * img.shape[1])\n        ymax = math.ceil(detections[i, 2] * img.shape[0])\n        xmax = math.ceil(detections[i, 3] * img.shape[1])\n#         print(detections[i, 0], detections[i, 1],detections[i, 2], detections[i, 3])\n#         print(img.shape)\n#         print(xmin, xmax, ymin, ymax)\n        face = img[ymin:ymax, xmin:xmax+1]\n#         print(face.shape)\n\n        return face\n\n\nclass EfficientNetModel():\n    def __init__(self,model_name, device, tfms=0):\n        self.model = EfficientNet.from_name(model_name).to(device)\n        self.model._fc = nn.Linear(self.model._fc.in_features, 1)\n        self.device = device\n        if tfms == 0:\n            self.tfms = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),])\n    \n    def load_stat_dict(self, stat_file):\n        stat_dict = torch.load(stat_file, map_location=self.device)\n        self.model.load_state_dict(stat_dict)\n    \n    def transfrom(self, img):\n        face = PIL.Image.fromarray(np.uint8(img))\n        return self.tfms(face).unsqueeze(0)\n    \n    def train(self, trainX, batch_size, epoch, device, label, state_dict_file, optimizer=None,scheduler=None):\n        trainX_arr = [img for k in range(len(trainX)) for img in trainX[k]]\n#         print(len(trainX_arr))\n#         print(trainX_arr)\n        trainData = []\n        for img in trainX_arr:\n#             print(img.shape)\n            try:\n                img = PIL.Image.fromarray(img)\n            except:\n                continue\n            img = self.tfms(img)\n            trainData.append(img)\n        trainX = DataLoader(trainData, batch_size=batch_size)\n        self.model = self.model.to(device)\n        self.model.train()\n        for i in range(epoch):\n            total_loss = 0\n            for imgs in trainX:\n#                 print(imgs.shape)\n                imgs = imgs.to(device)\n        #         print(imgs.shape)\n                if label:\n                    y = torch.ones((len(imgs),1)).to(device)\n                else:\n                    y = torch.zeros((len(imgs),1)).to(device)\n                y_predict = self.model(imgs)\n                loss = F.binary_cross_entropy(F.sigmoid(y_predict), y)\n                optimizer.zero_grad() #判别器D的梯度归零\n        #         loss.backward(retain_graph=True) #反向传播\n                loss.backward()\n                optimizer.step()\n                total_loss += loss\n            print(\"setp:\", i, \"loss:\", total_loss)\n            torch.save(self.model.state_dict(),state_dict_file+\"epoch-{0}.pth\".format(i))\n        torch.save(self.model.state_dict(),state_dict_file+'.pth')\n        \n    def validate(self, validateX, state_dict_file):\n        \n        result = []\n        num = len(validateX)\n        for i in range(num):\n            true_label = 0\n            false_label = 0\n            for img in validateX[i]:\n                try:\n                    img = PIL.Image.fromarray(np.uint8(img))\n                except Exception as e:\n#                     print(e)\n                    continue\n                img_p = self.tfms(img).unsqueeze(0)\n                label = self.getLabelResult(img_p)\n                if label == 0:\n                    true_label += 1\n                else:\n                    false_label += 1\n            if true_label > false_label:\n                result.append(0)\n            else:\n                result.append(1)\n        print(\"true: \", result.count(0), \"false:\", result.count(1), \"total:\", len(result))\n            \n        \n        \n        \n    def getPResult(self, input_):\n        input_ = input_.to(self.device)\n        self.model = self.model.to(self.device)\n        output = self.model(input_)\n        output = torch.sigmoid(output)\n#         print(output)\n        return output\n    \n    def getLabelResult(self, input_):\n        output = self.getPResult(input_)\n        label = 0 if output < 0.5 else 1\n        return label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nvideo_process = VideoProcess()\nvideos_dir = \"/kaggle/input/faceforensics/original_sequences/youtube/c23/videos\"\ndeepfake_dir = \"/kaggle/input/faceforensics/manipulated_sequences/Deepfakes/c23/videos\"\nfaceswap_dir = \"/kaggle/input/faceforensics/manipulated_sequences/FaceSwap/c23/videos\"\nface2face_dir = \"/kaggle/input/faceforensics/manipulated_sequences/Face2Face/c23/videos\"\nneuralTextures_dir = \"/kaggle/input/faceforensics/manipulated_sequences/NeuralTextures/c23/videos\"\n\n\n#     print(os.listdir(videos_dir))\nprint(\"origin getFrame start:\")\nimgs_origin = video_process.getFrame(videos_dir, file_num=500, frame_num=5, type=0)\nprint(\"origin GetFrame Over!\")\nprint(\"origin Blaze start:\")\nres_origin = video_process.blazeFace(imgs_origin)\nprint(\"origin Blaze Over!\")\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"deepfake getFrame start:\")\nimgs_deepfake = video_process.getFrame(deepfake_dir, file_num=500, frame_num=5, type=0)\nprint(\"deepfake GetFrame Over!\")\nprint(\"deepfake Blaze start:\")\nres_deepfake = video_process.blazeFace(imgs_deepfake)\nprint(\"deepfake Blaze Over!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'efficientnet-b0'\nmodel = EfficientNetModel(model_name, gpu)\nstat_file = '/kaggle/input/deepfake-detection-model-20k/EfficientNetb0 t2 0.8616966359803837 0.3698434531609828.pth'\n# stat_file = \"/kaggle/input/deepfake-detection-model-20k/EfficientNetb1 t2 0.8410909403768391 0.36058002083572327.pth\"\nstat_file = \"origin_videos.pth\"\nmodel.load_stat_dict(stat_file)\nmodel.model.eval()\noptimizer = torch.optim.Adam(model.model.parameters(), lr=0.0001, weight_decay=0.) \nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\nmodel.validate(res_origin, state_dict_file=stat_file)\nmodel.validate(res_deepfake, state_dict_file=stat_file)\n\nmodel.train(res_origin[:400], batch_size=32, epoch=20, device=gpu, label=0, state_dict_file=\"origin_videos\", optimizer=optimizer,scheduler=None)\nmodel.train(res_deepfake[:400], batch_size=32, epoch=20, device=gpu, label=1, state_dict_file=\"origin_videos\", optimizer=optimizer,scheduler=None)\n# model.load_stat_dict(\"origin_videos.pth\")\n\nmodel.validate(res_origin[400:], state_dict_file=\"origin_videos.pth\")\nmodel.validate(res_deepfake[400:], state_dict_file=\"origin_videos.pth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"{\"origin\" : 264/236,\n \"train\"  : 89/100,\n \"deepfake\":  ,\n}","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}