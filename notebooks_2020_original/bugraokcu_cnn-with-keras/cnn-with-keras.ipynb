{"cells":[{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"2c3412a4-039b-4229-9387-9321169f16eb","_uuid":"8def9627e9d48b26de3159fc9a2ec38e854ab16e"},"execution_count":149},{"source":"# Brief Info\n\nIn this work, we will train a CNN classifier using Keras with the guidelines described in [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python).\n\nOur strategy will be using 20% of the train data (12000 data rows) as a validation set to optimize the classifier, while keeping test data to finally evaluate the accuracy of the model on the data it has never seen.\n\n#### Note\nSince I was not sure if the data was already shuffled, I didn't pass `validation_split=0.2` to _fit()_ and instead explicitly shuffled and split the validation data, as `validation_split` [would](https://keras.io/getting-started/faq/#how-is-the-validation-split-computed) use last 20% of the data in that case.","cell_type":"markdown","metadata":{"_cell_guid":"8c56a778-ba24-4d51-a89d-a16b7da3f47c","_uuid":"a04a2a1268aabc722a26e5aada6921afd8b19e2f"}},{"source":"from keras.utils import to_categorical\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndata_train = pd.read_csv('../input/fashion-mnist_train.csv')\ndata_test = pd.read_csv('../input/fashion-mnist_test.csv')\n\nimg_rows, img_cols = 28, 28\ninput_shape = (img_rows, img_cols, 1)\n\nX = np.array(data_train.iloc[:, 1:])\ny = to_categorical(np.array(data_train.iloc[:, 0]))\n\n#Here we split validation data to optimiza classifier during training\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=13)\n\n#Test data\nX_test = np.array(data_test.iloc[:, 1:])\ny_test = to_categorical(np.array(data_test.iloc[:, 0]))\n\n\n\nX_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\nX_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\nX_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_val = X_val.astype('float32')\nX_train /= 255\nX_test /= 255\nX_val /= 255","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"dec05004-ccb3-490e-b588-27c0f4f06d1e","_uuid":"41907ec74cae883fa8d56f6556cade5c67c8f3e0","collapsed":true},"execution_count":150},{"source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\n\nbatch_size = 256\nnum_classes = 10\nepochs = 50\n\n#input image dimensions\nimg_rows, img_cols = 28, 28\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 kernel_initializer='he_normal',\n                 input_shape=input_shape))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adam(),\n              metrics=['accuracy'])","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"0599166d-b975-4c88-8b91-071a8f4fb0cd","_uuid":"0e9db73157e2c0e481bf3d73892d8d29263aa56f","collapsed":true},"execution_count":151},{"source":"model.summary()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"4543df54-7373-49f8-a23a-6d8062eefe38","_uuid":"9de535d8a446b7168ac3790445610425527c8a47"},"execution_count":152},{"source":"### Training\nLet's `fit()`! Note that `fit()` will return a _History_ object which we can use to plot training vs. validation accuracy and loss.","cell_type":"markdown","metadata":{"_cell_guid":"c9d072ca-23e7-4608-a9d8-dc818715d6e3","_uuid":"1e85d4b1d5fb567f2aad6bd82da969629e585f14"}},{"source":"history = model.fit(X_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(X_val, y_val))\nscore = model.evaluate(X_test, y_test, verbose=0)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"f24bf149-70e5-43f2-8574-117606493c85","_uuid":"71b754f661ab2bba5e0d2c280d033a57300ef7e2","scrolled":false},"execution_count":153},{"source":"print('Test loss:', score[0])\nprint('Test accuracy:', score[1])","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"f4b6cb78-1ef8-46ba-b72d-49a49c81bff0","_uuid":"48787728c57ad402547540b5d528aabbd8b20c0d"},"execution_count":154},{"source":"### Results\nIt turns out our classifier does better then the best baseline reported [here](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/), which is an SVM classifier with mean accuracy of 0.897.\n","cell_type":"markdown","metadata":{"_cell_guid":"f860c207-af4d-4c32-b4d2-4ea2d39902fe","_uuid":"d18bc5dd0ec23cda8ea5ad846b9877704f484951"}},{"source":"\nLet's plot training and validation accuracy as well as loss.","cell_type":"markdown","metadata":{"_cell_guid":"00dc5434-a008-490d-bb89-a394cf95c4bc","_uuid":"adbb96b3d90dcab280f06341a8483f246f583a03"}},{"source":"import matplotlib.pyplot as plt\n%matplotlib inline\naccuracy = history.history['acc']\nval_accuracy = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'bo', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"324c3a68-6381-4a39-a5a1-8fac766150c0","_uuid":"042117c73a10d7cbda6bc42ab60f9b0407cdf2c1"},"execution_count":155},{"source":"### Classification Report\nWe can summarize the performance of our classifier as follows","cell_type":"markdown","metadata":{"_cell_guid":"1311601b-9a53-4e59-adcd-4365c811f85e","_uuid":"28e5331842de7baaffb3a1e74f6a4c03e140759a"}},{"source":"#get the predictions for the test data\npredicted_classes = model.predict_classes(X_test)\n\n#get the indices to be plotted\ny_true = data_test.iloc[:, 0]\ncorrect = np.nonzero(predicted_classes==y_true)[0]\nincorrect = np.nonzero(predicted_classes!=y_true)[0]","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"721514ef-520b-42a2-9d57-2f82fbae8393","_uuid":"0275d1189e6425353537fa3ebe5d97d7b6759551","collapsed":true},"execution_count":156},{"source":"from sklearn.metrics import classification_report\ntarget_names = [\"Class {}\".format(i) for i in range(num_classes)]\nprint(classification_report(y_true, predicted_classes, target_names=target_names))","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"2dfc23f6-e0be-4014-8ce1-c6a33b1eccde","_uuid":"cc30fc7238f09f2372f718f17bd0f724fe898736"},"execution_count":157},{"source":"It's apparent that our classifier is underperforming for class 6 in terms of both precision and recall. For class 2, classifier is slightly lacking precision whereas it is slightly lacking recall (i.e. missed) for class 4.\n\nPerhaps we would gain more insight after visualizing the correct and incorrect predictions.","cell_type":"markdown","metadata":{"_cell_guid":"dc36576e-c630-4581-89fe-3bed02c378bb","_uuid":"2863d680886cc22a2e6f11270b411e435410adaf"}},{"source":"Here is a subset of correctly predicted classes.","cell_type":"markdown","metadata":{"_cell_guid":"866ef2af-d4e1-4697-bf87-9dcaa7cba363","_uuid":"6a78901cefbe9a3bdf89cc1d22ee910970cfaf93"}},{"source":"for i, correct in enumerate(correct[:9]):\n    plt.subplot(3,3,i+1)\n    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_true[correct]))\n    plt.tight_layout()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"5a33d457-ff62-4e32-81d7-89a6cdc1106d","_uuid":"d8148b3557ac27d216e0137bbeae06379c829779"},"execution_count":158},{"source":"And here is a subset of incorrectly predicted classes.","cell_type":"markdown","metadata":{"_cell_guid":"9726e897-4d69-44d5-a1d5-358d316b5141","_uuid":"a22f4456fa1c09e609db1a6c0e8eb1438087fa91"}},{"source":"for i, incorrect in enumerate(incorrect[0:9]):\n    plt.subplot(3,3,i+1)\n    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_true[incorrect]))\n    plt.tight_layout()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"803d7c3f-2099-420d-9612-dcf617d6cbe0","_uuid":"fa4d360a4e77bd3d83490065351860b1d9f58f8b"},"execution_count":159},{"source":"It looks like diversity of the similar patterns present on multiple classes effect the performance of the classifier although CNN is a robust architechture. A jacket, a shirt, and a long-sleeve blouse has similar patterns: long sleeves (or not!), buttons (or not!), and so on.","cell_type":"markdown","metadata":{"_cell_guid":"cd66b1b1-db6f-47e4-aa2e-3ce516979fa3","_uuid":"89881c6e4d1169ac543b5de992832d9e423dfdde"}},{"source":"#### What do the activations look like?\n\nThe snippets are taken from _Chollet, F (2017)_. The idea is the give an input data and visualize the activations of the conv layers.","cell_type":"markdown","metadata":{"_cell_guid":"2d4e41bf-07a3-4aa1-a89b-b2b1c3c19aaf","_uuid":"04ce0bbb3160fe7750432a20a912308598cfc804"}},{"source":"test_im = X_train[154]\nplt.imshow(test_im.reshape(28,28), cmap='viridis', interpolation='none')\nplt.show()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"2f9993fe-e0b8-4850-84aa-92499afc9b60","_uuid":"382e3bd0030f728700ce0a6c3e2e9442d641c6e1"},"execution_count":163},{"source":"Let's see the activation of the 2nd channel of the first layer:","cell_type":"markdown","metadata":{"_cell_guid":"e836de6d-4378-40ce-a3b3-f01269788479","_uuid":"ca607572942ddc3080e4b8b74d6bc47779fe66ae"}},{"source":"from keras import models\nlayer_outputs = [layer.output for layer in model.layers[:8]]\nactivation_model = models.Model(input=model.input, output=layer_outputs)\nactivations = activation_model.predict(test_im.reshape(1,28,28,1))\n\nfirst_layer_activation = activations[0]\nplt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"d3952d67-8187-4a8e-9e82-8bef6482e04c","_uuid":"15a26cf258d1fd6152d201258538ce829bed1ccc"},"execution_count":164},{"source":"Let's plot the activations of the other conv layers as well.","cell_type":"markdown","metadata":{"_cell_guid":"6081331c-280d-442f-afc2-edf3c54b02bd","_uuid":"1969b931bf29620f2546454de5133e3b2df8080f"}},{"source":"layer_names = []\nfor layer in model.layers[:-1]:\n    layer_names.append(layer.name) \nimages_per_row = 16\nfor layer_name, layer_activation in zip(layer_names, activations):\n    if layer_name.startswith('conv'):\n        n_features = layer_activation.shape[-1]\n        size = layer_activation.shape[1]\n        n_cols = n_features // images_per_row\n        display_grid = np.zeros((size * n_cols, images_per_row * size))\n        for col in range(n_cols):\n            for row in range(images_per_row):\n                channel_image = layer_activation[0,:, :, col * images_per_row + row]\n                channel_image -= channel_image.mean()\n                channel_image /= channel_image.std()\n                channel_image *= 64\n                channel_image += 128\n                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n                display_grid[col * size : (col + 1) * size,\n                             row * size : (row + 1) * size] = channel_image\n        scale = 1. / size\n        plt.figure(figsize=(scale * display_grid.shape[1],\n                            scale * display_grid.shape[0]))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='viridis')","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"a9e0c8cc-6b25-4043-8f7c-d8b95ec27923","_uuid":"0ac1180af62085842f361291ff716bdb3935c98a"},"execution_count":165},{"source":"","cell_type":"code","outputs":[],"metadata":{"collapsed":true},"execution_count":null}],"metadata":{"language_info":{"version":"3.5.2","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","mimetype":"text/x-python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":1}