{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"932ebb10-ffd5-4181-52ae-7ed367a85a6e"},"source":"This notebook shows how to build a linear model on features from apps, app labels, phone brands and device models. It uses LogisticRegression classifier from sklearn. \n\nIt also shows an efficient way of constructing bag-of-apps and bag-of-labels features without concatenating a bunch of strings."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"10e55365-83e8-f7b6-a26e-aaa376cad62a"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.metrics import log_loss"},{"cell_type":"markdown","metadata":{"_cell_guid":"29ab3f35-c097-465e-e2a5-4efe1aa836aa"},"source":"## Load data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e76e779c-b6b2-d800-1150-b920c610e9ff"},"outputs":[],"source":"datadir = '../input'\ngatrain = pd.read_csv(os.path.join(datadir,'gender_age_train.csv'),\n                      index_col='device_id')\ngatest = pd.read_csv(os.path.join(datadir,'gender_age_test.csv'),\n                     index_col = 'device_id')\nphone = pd.read_csv(os.path.join(datadir,'phone_brand_device_model.csv'))\n# Get rid of duplicate device ids in phone\nphone = phone.drop_duplicates('device_id',keep='first').set_index('device_id')\nevents = pd.read_csv(os.path.join(datadir,'events.csv'),\n                     parse_dates=['timestamp'], index_col='event_id')\nappevents = pd.read_csv(os.path.join(datadir,'app_events.csv'), \n                        usecols=['event_id','app_id','is_active'],\n                        dtype={'is_active':bool})\napplabels = pd.read_csv(os.path.join(datadir,'app_labels.csv'))"},{"cell_type":"markdown","metadata":{"_cell_guid":"456cc4a1-26fc-0248-9cec-e786adcb35c7"},"source":"## Feature engineering\n\nThe features I'm going to use include:\n\n* phone brand\n* device model\n* installed apps\n* app labels\n\nI'm going to one-hot encode everything and sparse matrices will help deal with a very large number of features.\n\n### Phone brand\n\nAs preparation I create two columns that show which train or test set row a particular device_id belongs to."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f81678cf-e762-19b5-0a67-cf6430689886"},"outputs":[],"source":"gatrain['trainrow'] = np.arange(gatrain.shape[0])\ngatest['testrow'] = np.arange(gatest.shape[0])"},{"cell_type":"markdown","metadata":{"_cell_guid":"c8d7224e-7680-789a-fd8c-633ac1a9093b"},"source":"A sparse matrix of features can be constructed in various ways. I use this constructor:\n\n    csr_matrix((data, (row_ind, col_ind)), [shape=(M, N)])\n    where ``data``, ``row_ind`` and ``col_ind`` satisfy the\n    relationship ``a[row_ind[k], col_ind[k]] = data[k]``\n    \nIt lets me specify which values to put into which places in a sparse matrix. For phone brand data the `data` array will be all ones, `row_ind` will be the row number of a device and `col_ind` will be the number of brand."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1e93897e-518c-8ea3-4e55-b204f0d08985"},"outputs":[],"source":"brandencoder = LabelEncoder().fit(phone.phone_brand)\nphone['brand'] = brandencoder.transform(phone['phone_brand'])\ngatrain['brand'] = phone['brand']\ngatest['brand'] = phone['brand']\nXtr_brand = csr_matrix((np.ones(gatrain.shape[0]), \n                       (gatrain.trainrow, gatrain.brand)))\nXte_brand = csr_matrix((np.ones(gatest.shape[0]), \n                       (gatest.testrow, gatest.brand)))\nprint('Brand features: train shape {}, test shape {}'.format(Xtr_brand.shape, Xte_brand.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"7668f876-da68-06aa-d9ab-5317cfbd2329"},"source":"### Device model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"850876c4-f13a-7854-f875-9f68617a6333"},"outputs":[],"source":"m = phone.phone_brand.str.cat(phone.device_model)\nmodelencoder = LabelEncoder().fit(m)\nphone['model'] = modelencoder.transform(m)\ngatrain['model'] = phone['model']\ngatest['model'] = phone['model']\nXtr_model = csr_matrix((np.ones(gatrain.shape[0]), \n                       (gatrain.trainrow, gatrain.model)))\nXte_model = csr_matrix((np.ones(gatest.shape[0]), \n                       (gatest.testrow, gatest.model)))\nprint('Model features: train shape {}, test shape {}'.format(Xtr_model.shape, Xte_model.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"617f26d5-0bcb-1071-6cbc-02231e171c48"},"source":"### Installed apps features\n\nFor each device I want to mark which apps it has installed. So I'll have as many feature columns as there are distinct apps.\n\nApps are linked to devices through events. So I do the following:\n\n- merge `device_id` column from `events` table to `app_events`\n- group the resulting dataframe by `device_id` and `app` and aggregate\n- merge in `trainrow` and `testrow` columns to know at which row to put each device in the features matrix"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4e074f1-a1cd-c69e-f2e1-d2ea6aae8a27"},"outputs":[],"source":"appencoder = LabelEncoder().fit(appevents.app_id)\nappevents['app'] = appencoder.transform(appevents.app_id)\nnapps = len(appencoder.classes_)\ndeviceapps = (appevents.merge(events[['device_id']], how='left',left_on='event_id',right_index=True)\n                       .groupby(['device_id','app'])['app'].agg(['size'])\n                       .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n                       .merge(gatest[['testrow']], how='left', left_index=True, right_index=True)\n                       .reset_index())\ndeviceapps.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"445dc656-f877-743e-6eec-f2c217404d78"},"source":"Now I can build a feature matrix where the `data` is all ones, `row_ind` comes from `trainrow` or `testrow` and `col_ind` is the label-encoded `app_id`."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74a7f87d-6eb8-7553-addb-d32b72382506"},"outputs":[],"source":"d = deviceapps.dropna(subset=['trainrow'])\nXtr_app = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.app)), \n                      shape=(gatrain.shape[0],napps))\nd = deviceapps.dropna(subset=['testrow'])\nXte_app = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.app)), \n                      shape=(gatest.shape[0],napps))\nprint('Apps data: train shape {}, test shape {}'.format(Xtr_app.shape, Xte_app.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"c48ad7c3-e0c1-8b11-da35-11f4b7f2d6b2"},"source":"### App labels features\n\nThese are constructed in a way similar to apps features by merging `app_labels` with the `deviceapps` dataframe we created above."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b9bd3b7c-9146-ae81-ef83-e92e5bcd6868"},"outputs":[],"source":"applabels = applabels.loc[applabels.app_id.isin(appevents.app_id.unique())]\napplabels['app'] = appencoder.transform(applabels.app_id)\nlabelencoder = LabelEncoder().fit(applabels.label_id)\napplabels['label'] = labelencoder.transform(applabels.label_id)\nnlabels = len(labelencoder.classes_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a2f082a8-8e42-a914-aa0d-f1b53ca17788"},"outputs":[],"source":"devicelabels = (deviceapps[['device_id','app']]\n                .merge(applabels[['app','label']])\n                .groupby(['device_id','label'])['app'].agg(['size'])\n                .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n                .merge(gatest[['testrow']], how='left', left_index=True, right_index=True)\n                .reset_index())\ndevicelabels.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c4897b6-877a-2dc4-a546-a61dda27eb65"},"outputs":[],"source":"d = devicelabels.dropna(subset=['trainrow'])\nXtr_label = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.label)), \n                      shape=(gatrain.shape[0],nlabels))\nd = devicelabels.dropna(subset=['testrow'])\nXte_label = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.label)), \n                      shape=(gatest.shape[0],nlabels))\nprint('Labels data: train shape {}, test shape {}'.format(Xtr_label.shape, Xte_label.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"4d900504-996c-3bf1-9ff6-251166b6dc36"},"source":"### Concatenate all features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"31c19090-6cab-8935-c0dd-5dc7af47d5c9"},"outputs":[],"source":"Xtrain = hstack((Xtr_brand, Xtr_model, Xtr_app, Xtr_label), format='csr')\nXtest =  hstack((Xte_brand, Xte_model, Xte_app, Xte_label), format='csr')\nprint('All features: train shape {}, test shape {}'.format(Xtrain.shape, Xtest.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"bc5e2845-0597-4bab-18c2-75deace40f40"},"source":"## Cross-validation"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"100a586a-d05b-5c3c-fb80-3cd20f07505e"},"outputs":[],"source":"targetencoder = LabelEncoder().fit(gatrain.group)\ny = targetencoder.transform(gatrain.group)\nnclasses = len(targetencoder.classes_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f9751cdd-08c3-7e6c-c9fe-39346662a857"},"outputs":[],"source":"def score(clf, random_state = 0):\n    kf = StratifiedKFold(y, n_folds=5, shuffle=True, random_state=random_state)\n    pred = np.zeros((y.shape[0],nclasses))\n    for itrain, itest in kf:\n        Xtr, Xte = Xtrain[itrain, :], Xtrain[itest, :]\n        ytr, yte = y[itrain], y[itest]\n        clf.fit(Xtr, ytr)\n        pred[itest,:] = clf.predict_proba(Xte)\n        # Downsize to one fold only for kernels\n        return log_loss(yte, pred[itest, :])\n        print(\"{:.5f}\".format(log_loss(yte, pred[itest,:])), end=' ')\n    print('')\n    return log_loss(y, pred)"},{"cell_type":"markdown","metadata":{"_cell_guid":"84d78bff-af59-8862-8898-9ed974551e06"},"source":"In order to make a good logistic regression model we need to choose a value for regularization constant C. Smaller values of C mean stronger regularization and its default value is 1.0. We probably have a lot of mostly useless columns (rare brands, models or apps), so we'd better look at stronger regularization than default."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a1973a17-8c40-69a3-14de-1cf29605ee62"},"outputs":[],"source":"Cs = np.logspace(-3,0,4)\nres = []\nfor C in Cs:\n    res.append(score(LogisticRegression(C = C)))\nplt.semilogx(Cs, res,'-o');"},{"cell_type":"markdown","metadata":{"_cell_guid":"636bc986-feac-7785-132c-c55216b6e674"},"source":"Judging by the plot the best value for C is somewhere between 0.01 and 0.1."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f78cfac0-0da0-f74e-eb20-7b45748173ca"},"outputs":[],"source":"score(LogisticRegression(C=0.02))"},{"cell_type":"markdown","metadata":{"_cell_guid":"633066eb-4335-f824-0744-644498191872"},"source":"By default [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) classifier solves a multiclass classification problem in a one versus rest fashion. But it's also possible to fit a multinomial model that optimizes the multiclass logloss - exactly the metric we're evaluated on. Let's see if doing that improves our results:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3510577f-e668-7885-1560-16f80f9b1eb8"},"outputs":[],"source":"score(LogisticRegression(C=0.02, multi_class='multinomial',solver='lbfgs'))"},{"cell_type":"markdown","metadata":{"_cell_guid":"b6ef299f-901b-86f7-3b7c-c5b546af46d1"},"source":"Yes, it does!"},{"cell_type":"markdown","metadata":{"_cell_guid":"6cf9883e-62c3-27dd-71af-ef765c59bafb"},"source":"## Predict on test data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b9a67b06-83ec-cbce-d810-1a4ce78f3a9d"},"outputs":[],"source":"clf = LogisticRegression(C=0.02, multi_class='multinomial',solver='lbfgs')\nclf.fit(Xtrain, y)\npred = pd.DataFrame(clf.predict_proba(Xtest), index = gatest.index, columns=targetencoder.classes_)\npred.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0481f7e3-e8fc-59ce-6f02-4975fa5ef71a"},"outputs":[],"source":"pred.to_csv('logreg_subm.csv',index=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ac0f998c-0093-ce33-31dc-d7c1e60da87f"},"source":"## What to try next\n\n- use some aggregates for apps and labels features. Maybe using them instead of simple indicators shown here will improve the score. For example:\n    - calculate the proportion of events where an app appears on each device\n    - calculate the mean of `is_active` field for each app on each device\n    - create some TFIDF-like weighting for apps or labels\n- add features based on event locations and times\n- add feature interactions\n- fit a nonlinear model (neural networks seem to work well here)\n- blend in the results of the [previous script](https://www.kaggle.com/dvasyukova/talkingdata-mobile-user-demographics/brand-and-model-based-benchmarks) for devices that have no events data\n\nShare your ideas in the comments, fork and improve =)."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}