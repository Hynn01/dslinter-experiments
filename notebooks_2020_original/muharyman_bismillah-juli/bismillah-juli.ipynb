{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport gensim\nimport multiprocessing\nimport re\nimport string as str\n\n# from nltk.corpus import stopwords\n\nfrom gensim.models import word2vec\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"dt = pd.read_csv('/kaggle/input/data-final/quran_translasi_final.csv')\n# dt = pd.read_csv('/kaggle/input/indonesian-cleancsv/Indonesian_clean.csv')\ndt.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"dt['text_lower'] = dt['text'].str.replace('[^a-zA-Z]',' ').str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"!pip install PySastrawi\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n \nfactory = StopWordRemoverFactory()\nstopwords = factory.get_stop_words()\n\nstop_re = '\\\\b'+'\\\\b|\\\\b'.join(stopwords)+'\\\\b'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"dt['text_bersih'] = dt['text_lower'].str.replace(stop_re,'')\n\ntext_bersih = []\n\nfor i in dt['text_bersih']:\n    text_bersih.append(re.sub(\"\\s\\s+\", \" \", i))\n\ndt['text_bersih'] =  text_bersih","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"corpus = []\n\nfor i in dt['text_bersih']:\n    corpus.append(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TOKENISASI**","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# # Tokenize words\ndt['text_bersih'] = dt['text_bersih'].str.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"!pip install nlp-id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pos Tagging**\n\nmenghilangkan Tag yang dianggap tidak penting","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from nlp_id.postag import PosTag\npostagger = PosTag()\n\npos_tag = []\n\nfor i in corpus :\n    pos_tag.append(postagger.get_pos_tag(i))\n    \ndt['pos_tag'] = pos_tag","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"postag_aaa = []\nfor i in range(len(pos_tag)):\n    for j in range(len(pos_tag[i])):\n        postag_aaa.append(pos_tag[i][j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"postag_unique = []\nfor i in set(postag_aaa) :\n    postag_unique.append(i)\npostag_unique.sort()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"postag_uniquea = []\nfor i in set(postag_aaa) :\n    postag_uniquea.append(i[1])\npostag_uniquea.sort()\npostag_uniquea = set(postag_uniquea)\npostag_uniquea","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"postag_stopword = []\nfor i in range(len(pos_tag)):\n    for j in range(len(pos_tag[i])):\n        if pos_tag[i][j][1]!='NN' and pos_tag[i][j][1]!='VB' and pos_tag[i][j][1]!='JJ' and pos_tag[i][j][1]!='FW' and pos_tag[i][j][1]!='NNP' and pos_tag[i][j][1]!='NUM':\n                postag_stopword.append(pos_tag[i][j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"stopword_postag_unique = []\nfor i in set(postag_stopword) :\n    stopword_postag_unique.append(i[0])\n\nstopword_postag_unique.remove('mahapengasih')\nstopword_postag_unique.remove('salat')\nstopword_postag_unique.sort()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"dt['text_baru'] = dt['text_bersih'].apply(lambda x: [item for item in x if item not in stopword_postag_unique])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"dt['text_baru2'] = dt['text_baru'].str.join(\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"corpusX = []\n\nfor i in dt['text_baru2']:\n    corpusX.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"corpusY = []\n\nfor i in dt['text_lower'].str.split():\n    corpusY.append(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TFIDF**","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#instantiate CountVectorizer()\ncv=CountVectorizer(max_features=3000)\n \n# this steps generates word counts for the words in your docs\nword_count_vector=cv.fit_transform(corpusX)\n\ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(word_count_vector)\n\n# print idf values\ndf_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n \n# sort ascending\ndf_idf.sort_values(by=['idf_weights'],ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# settings that you use for count vectorizer will go here\ntfidf_vectorizer=TfidfVectorizer(max_features=250)\n# just send in all your docs here\ntfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(corpusX)\n\n# get the first vector out (for the first document)\nfirst_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n \n# place tf-idf values in a pandas data frame\ndf = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\ndf.sort_values(by=[\"tfidf\"],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"important_vocab = tfidf_vectorizer.get_feature_names()\nlen(important_vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pengurangan = ['untukmu','tuhannya','tuhanmu','tuhanku','sisi','penuh','manakah','ketahuilah','kepadamu','kepadaku','kebanyakan',\n              'kaumnya','kaumku','hatinya','engkau','dirimu','darinya','bertakwalah','berlaku','berilah','berdua','barangsiapa',\n              'barang','balasan','baginya','bagimu','alasan','adakan','allah']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hasil_pengurangan = []\n\nfor i in important_vocab:\n    if i not in pengurangan:\n        hasil_pengurangan.append(i)\n        \nimportant_vocab = hasil_pengurangan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tambahan = ['injil','sabar','angin','beruntung','gaib','adam','batu','laut','mekah','zakat','putra','iblis','yakub','sihir','iblis','jahat'\n       ,'mahateliti','tahun','keluarga','mahatinggi','samud','sapi','sulaiman','unta','sujud','kota','burung','berjihad','berserah'\n       ,'islam','keturunan','kurma','muslim','nasrani','suami','kikir','harun','ayah','bersabar','bertasbih','dawud','dusta','ishak','jibril','kubur','mahaesa','mahakaya','petang'\n       ,'syuaib''zabur']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tambahan:\n    important_vocab.append(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plot untuk word2Vec**","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Word2Vec dari seluruh kata di text**","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"test = word2vec.Word2Vec(corpusY, size=200, window=10, min_count=3, workers=4,sg=1, iter = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"tsne_plot(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"test.most_similar('kitab')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"len(test.wv.vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"filtered_vocab = []\nfiltered_vector = []\nfor e in test.wv.vocab:\n    if e in important_vocab:\n        filtered_vocab.append(e)\n        filtered_vector.append(test.wv.get_vector(e))\nprint('length:', len(filtered_vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"filtered_vector = np.array(filtered_vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nl = linkage(filtered_vector, method='average', metric='euclidean')\n\n# calculate full dendrogram\nplt.figure(figsize=(150, 60))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.ylabel('word')\nplt.xlabel('distance')\n\ndendrogram(\n    l,\n    leaf_rotation=0.,  # rotates the x axis labels\n    leaf_font_size=16.,  # font size for the x axis labels\n    orientation='right',\n    leaf_label_func=lambda v: (filtered_vocab[v])\n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\ncluster = AgglomerativeClustering(n_clusters=None, affinity='euclidean', linkage='average',distance_threshold=2.73)\n# cluster = AgglomerativeClustering(n_clusters=77, affinity='euclidean', linkage='average',compute_full_tree = False)\ncluster.fit_predict(filtered_vector)\nlabels = cluster.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df = pd.DataFrame()\ndf['word'] = [e for e in filtered_vocab]\ndf['cluster'] =  labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"groups = list(df.groupby('cluster'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in groups:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.to_csv('konsep_relasi.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}