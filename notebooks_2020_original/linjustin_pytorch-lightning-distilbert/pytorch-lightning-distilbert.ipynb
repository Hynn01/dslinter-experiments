{"cells":[{"metadata":{},"cell_type":"markdown","source":"[Reference](https://www.kaggle.com/yutanakamura/dear-pytorch-lovers-bert-transformers-lightning#3.-Postprocessing)"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install pytorch-lightning","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\n\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport time\nimport string\nimport re\nimport bs4\nfrom bs4 import BeautifulSoup\n\nimport transformers\nfrom transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n\nSEED = 1234\nnp.random.seed(SEED)\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nprint(\"PyTorch version: \", torch.__version__)\nprint('Huggingface version: ', transformers.__version__)\nprint('PyTorch Lightning version: ', pl.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\n\nRead in the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"PATH = '../input/tweet-sentiment-extraction/'\n\ntrain_df = pd.read_csv(PATH + 'train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(PATH + 'test.csv')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Helper functions for text preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ndef remove_html(text):\n    return BeautifulSoup(text, \"lxml\").text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text'] = train_df['text'].apply(lambda x: str(x))\ntest_df['text'] = test_df['text'].apply(lambda x: str(x))\n\ntrain_df['text'] = train_df['text'].apply(lambda text: remove_urls(text))\ntest_df['text'] = test_df['text'].apply(lambda text: remove_urls(text))\n\ntrain_df['text'] = train_df['text'].apply(lambda text: remove_html(text))\ntest_df['text'] = test_df['text'].apply(lambda text: remove_html(text))\n\ntrain_df['text_lower'] = train_df['text'].apply(lambda x: x.lower())\ntest_df['text_lower'] = test_df['text'].apply(lambda x: x.lower())\n\ntrain_df['selected_text'] = train_df['selected_text'].apply(lambda x: str(x).lower())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\n\ntrain_df['text_tokenized'] = train_df['text_lower'].apply(tokenizer.tokenize)\ntest_df['text_tokenized'] = test_df['text_lower'].apply(tokenizer.tokenize)\n\ntrain_df['selected_text_tokenized'] = train_df['selected_text'].apply(tokenizer.tokenize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start and end positions\n\nstart_positions = []\nend_positions = []\n\ntrain_df['select_length'] = train_df['selected_text_tokenized'].map(len)\n\nfor i in range(len(train_df)):\n    start_position = [j for j, token in enumerate(train_df['text_tokenized'].iloc[i]) if token == train_df['selected_text_tokenized'].iloc[i][0]]\n    end_position = [j for j, token in enumerate(train_df['text_tokenized'].iloc[i]) if token == train_df['selected_text_tokenized'].iloc[i][-1]]\n    \n    start_position = [idx for idx in start_position if idx + train_df['select_length'].iloc[i] - 1 in end_position]\n    end_position = [idx for idx in end_position if idx - train_df['select_length'].iloc[i] + 1 in start_position]\n    \n    start_positions.append(start_position)\n    end_positions.append(end_position)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_positions = [l[0] if len(l) > 0 else -1 for l in start_positions]\nend_positions = [l[0] if len(l) > 0 else -1 for l in end_positions]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['start_position'] = start_positions\ntrain_df['end_position'] = end_positions\n\ntest_df['start_position'] = -1\ntest_df['end_position'] = -1\n\ntrain_df = train_df.query('start_position!=-1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, val_df = train_test_split(train_df, test_size = 0.2, random_state = SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Positive/Negative/Neutral Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_train = train_df.query('sentiment==\"positive\"')\nneg_train = train_df.query('sentiment==\"negative\"')\nneu_train = train_df.query('sentiment==\"neutral\"')\n\npos_val = val_df.query('sentiment==\"positive\"')\nneg_val = val_df.query('sentiment==\"negative\"')\nneu_val = val_df.query('sentiment==\"neutral\"')\n\npos_test = test_df.query('sentiment==\"positive\"')\nneg_test = test_df.query('sentiment==\"negative\"')\nneu_test = test_df.query('sentiment==\"neutral\"')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DistilBERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\nneg_model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n\nMAX_LENGTH = 128\nBATCH_SIZE = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.texts = df['text_lower'].values\n        self.start_ids = df['start_position'].values\n        self.end_ids = df['end_position'].values\n        self.hash_index = df['textID'].values\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        returns = {'text': self.texts[idx],\n                   'start': self.start_ids[idx],\n                   'end': self.end_ids[idx],\n                   'idx': idx}\n        return returns\n    \nclass TestDataset(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.texts = df['text_lower'].values\n        self.hash_index = df['textID'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        returns = {'text' : self.texts[idx],\n                   'idx' : idx}\n        return returns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds_pos_train = TrainDataset(pos_train)\nds_neg_train = TrainDataset(neg_train)\n\nds_pos_val = TrainDataset(pos_val)\nds_neg_val = TrainDataset(neg_val)\n\nds_pos_test = TestDataset(pos_test)\nds_neg_test = TestDataset(neg_test)\n\n\n\ndl_pos_train = DataLoader(ds_pos_train, \n                          batch_size = BATCH_SIZE, \n                          shuffle = True,\n                          num_workers = 8)\ndl_neg_train = DataLoader(ds_neg_train, \n                          batch_size = BATCH_SIZE, \n                          shuffle = True,\n                          num_workers = 8)\n\ndl_pos_val = DataLoader(ds_pos_val, \n                        batch_size = BATCH_SIZE, \n                        shuffle = False,\n                        num_workers = 8)\ndl_neg_val = DataLoader(ds_neg_val, \n                        batch_size = BATCH_SIZE, \n                        shuffle = False,\n                        num_workers = 8)\n\ndl_pos_test = DataLoader(ds_pos_test, \n                         batch_size = BATCH_SIZE, \n                         shuffle = False,\n                         num_workers = 8)\ndl_neg_test = DataLoader(ds_neg_test, \n                         batch_size = BATCH_SIZE, \n                         shuffle = False,\n                         num_workers = 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DistilBERTModule(pl.LightningModule):\n    def __init__(self, distilbertmodel, tokenizer, prediction_save_path):\n        super().__init__()\n        self.distilbertmodel = distilbertmodel\n        self.tokenizer = tokenizer\n        self.prediction_save_path = prediction_save_path\n        \n    def get_device(self):\n        return self.distilbertmodel.state_dict()['distilbert.embeddings.word_embeddings.weight'].device\n    \n    def save_predictions(self, start_posit, end_posit):\n        df = pd.DataFrame({'start_position': start_posit,\n                           'end_position': end_posit})\n        df.to_csv(self.prediction_save_path, index = False)\n        \n    def forward(self, batch):\n        encoded_batch = tokenizer.batch_encode_plus(batch['text'],\n                                                    max_length = MAX_LENGTH,\n                                                    pad_to_max_length = True)\n        input_ids = torch.tensor(encoded_batch['input_ids']).to(self.get_device())\n        attention_mask = torch.tensor(encoded_batch['attention_mask']).to(self.get_device())\n        start_posit = batch['start'].to(self.get_device()) + 1 if 'start' in batch.keys() else None\n        end_posit = batch['end'].to(self.get_device()) + 1 if 'end' in batch.keys() else None\n        \n        model_inputs = {'input_ids': input_ids,\n                        'attention_mask': attention_mask,\n                        'start_positions': start_posit,\n                        'end_positions': end_posit}\n        \n        return self.distilbertmodel(**model_inputs)\n    \n    def training_step(self, batch, batch_nb):\n        idx = batch['idx']\n        loss = self.forward(batch)[0]\n        return {'loss': loss, 'idx': idx}\n    \n    def validation_step(self, batch, batch_nb):\n        idx = batch['idx']\n        loss = self.forward(batch)[0]\n        return {'loss': loss, 'idx': idx}\n    \n    def test_step(self, batch, batch_nb):\n        idx = batch['idx']\n        start_scores = self.forward(batch)[0]\n        end_scores = self.forward(batch)[1]\n        return {'start_scores':start_scores, 'end_scores':end_scores, 'idx':idx}\n    \n    def training_end(self, outputs):\n        return {'loss':outputs['loss']}\n    \n    def validation_end(self, outputs):\n        return {'loss':torch.mean(torch.tensor([output['loss'] for output in outputs])).detach()}\n    \n    def test_end(self, outputs):\n        start_scores = torch.cat([output['start_scores'] for output in outputs]).detach().cpu().numpy()\n        start_positions = np.argmax(start_scores, axis=1) - 1\n\n        end_scores = torch.cat([output['end_scores'] for output in outputs]).detach().cpu().numpy()\n        end_positions = np.argmax(end_scores, axis=1) - 1\n        self.save_predictions(start_positions, end_positions)\n        return {}\n    \n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr = 2e-5)\n    \n    @pl.data_loader\n    def train_dataloader(self):\n        pass\n\n    @pl.data_loader\n    def val_dataloader(self):\n        pass\n\n    @pl.data_loader\n    def test_dataloader(self):\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PositiveModule(DistilBERTModule):\n    def __init__(self, distilbertmodel, tokenizer, prediction_save_path):\n        super().__init__(distilbertmodel, tokenizer, prediction_save_path)\n\n    @pl.data_loader\n    def train_dataloader(self):\n        return dl_pos_train\n\n    @pl.data_loader\n    def val_dataloader(self):\n        return dl_pos_val\n\n    @pl.data_loader\n    def test_dataloader(self):\n        return dl_pos_test\n    \n    \n\nclass NegativeModule(DistilBERTModule):\n    def __init__(self, distilbertmodel, tokenizer, prediction_save_path):\n        super().__init__(distilbertmodel, tokenizer, prediction_save_path)\n\n    @pl.data_loader\n    def train_dataloader(self):\n        return dl_neg_train\n\n    @pl.data_loader\n    def val_dataloader(self):\n        return dl_neg_val\n\n    @pl.data_loader\n    def test_dataloader(self):\n        return dl_neg_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_module = PositiveModule(pos_model, tokenizer, 'pos_pred.csv')\nneg_module = NegativeModule(neg_model, tokenizer, 'neg_pred.csv')\n\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'\n\npos_module.to(device)\nneg_module.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_trainer = pl.Trainer(max_nb_epochs = 5, fast_dev_run = False)\nneg_trainer = pl.Trainer(max_nb_epochs = 5, fast_dev_run = False)\n\npos_trainer.fit(pos_module)\nneg_trainer.fit(neg_module)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_trainer.test()\nneg_trainer.test()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Postprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_pred = pd.read_csv('pos_pred.csv')\nneg_pred = pd.read_csv('neg_pred.csv')\npos_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.index = test_df['textID']\ntest_df['selected_text'] = ''\n\ntest_df.loc[ds_pos_test.hash_index[:BATCH_SIZE if False else len(test_df)], 'start_position':'end_position'] = pos_pred.values\ntest_df.loc[ds_neg_test.hash_index[:BATCH_SIZE if False else len(test_df)], 'start_position':'end_position'] = neg_pred.values\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(test_df)):\n    if test_df['sentiment'].iloc[i] in ('positive', 'negative'):\n        tokenized_text = test_df['text_tokenized'].iloc[i]\n        start_position = max(test_df['start_position'].iloc[i], 0)\n        end_position = min(test_df['end_position'].iloc[i], len(tokenized_text) - 1)\n        \n        selected_text = tokenizer.convert_tokens_to_string(tokenized_text[start_position:end_position + 1])\n        for original_token in test_df['text'].iloc[i].split():\n            tokenized_form = tokenizer.convert_tokens_to_string(tokenizer.tokenize(original_token))\n            selected_text = selected_text.replace(tokenized_form, original_token, 1)\n            \n        test_df['selected_text'].iloc[i] = selected_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(test_df)):\n    if test_df['sentiment'].iloc[i] == 'neutral':\n        test_df['selected_text'].iloc[i] = test_df['text'].iloc[i]\n    else:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.loc[:,['textID', 'selected_text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.loc[:, ['textID', 'selected_text']].to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}