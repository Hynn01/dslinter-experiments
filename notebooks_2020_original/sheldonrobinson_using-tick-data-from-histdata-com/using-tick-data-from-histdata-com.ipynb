{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os, sys, glob\n\nimport json\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Listing of tick data files","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"tickdata = dict()\nfor root, dirs, filenames in os.walk('/kaggle/input'):\n    for dirname in dirs:\n        if dirname.startswith('histdata-forex-'):\n            files_csv_zg = glob.glob(os.path.join(root, dirname,'*.csv.zg'))\n            info=dict()\n            missing = dict()\n            with open(os.path.join(root, dirname, 'info.json')) as f:\n              info = json.load(f)\n            with open(os.path.join(root, dirname, 'missing.json')) as f:\n              missing = json.load(f)\n            provider, sectype, ticker = dirname.split('-',3)\n            tickdata[str(dirname)] = {'provider':provider, 'ticker':ticker, 'sectype':sectype, 'details':info, 'count_of_missing_days':len(missing['days']), 'missing_days': missing['days'], 'files':files_csv_zg}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating tensorflow CsvDataset for time series data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load the gzipped csv, extension set to .csv.zg to disable decompression in kaggleÂ¶\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets = dict()\nfor key, values in tickdata.items():\n    datasets[key]=tf.data.experimental.CsvDataset(values['files'], [tf.string,tf.float32,tf.float32],header=False, compression_type=\"GZIP\",select_cols=[0,1,2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ds = next(iter(datasets.values()))\ninput_ds.element_spec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in input_ds.take(5):\n    print(f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conversion function\n\n```(datetime: string, bid: float32, ask: float32) -> (dateime: string, timestamp: float64,  bid: float32, ask: float32, mid: float32, spread: float32)```","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime, timedelta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv_func(dt, bid, ask):\n    txt = lambda t : t.numpy().decode('ascii')\n    conv = lambda z : pd.Timestamp(datetime.strptime(z.numpy().decode('ascii'), '%Y%m%d %H%M%S%f')).to_datetime64()\n    return tf.py_function(txt,[dt], tf.string), tf.py_function(conv, [dt], tf.float64), bid, ask,(bid+ask)/2, ask-bid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts_data = dict()\nfor key, ds in datasets.items():\n    ts_data[key] = ds.map(conv_func)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ds = next(iter(ts_data.values()))\ntest_ds.element_spec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in test_ds.take(5):\n    print(f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Click link >> [OpenDrive, Cloud storage with Webdav support](https://www.opendrive.com/?od=5eecb28b9dda9)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Connect Opendrive","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### username and password stored in kaggle secrets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nopendrive_usr = user_secrets.get_secret(\"OPENDRIVE_USERNAME\")\nopendrive_passwd = user_secrets.get_secret(\"OPENDRIVE_PASSWORD\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Access via WebDAV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install webdavclient3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import urllib3\nfrom webdav3.client import Client\noptions = {\n 'webdav_hostname': \"https://webdav.opendrive.com\",\n 'webdav_login':    opendrive_usr,\n 'webdav_password': opendrive_passwd\n}\nclient = Client(options)\nclient.verify = False\nurllib3.disable_warnings()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make a cache directory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"client.list()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Upload dataset cache","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if not client.check('ds_cache'):\n    client.mkdir('ds_cache')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs('/kaggle/working/cache', exist_ok=True)\nfor key, value in ts_data.items():\n    cachefilepath = os.path.join('cache',\"{}.cache\".format(key))\n    it = value.cache(cachefilepath).prefetch(tf.data.experimental.AUTOTUNE)\n    # it = iter(value)\n    count = 0\n    for i in it:\n        count+=1\n    print(\"{} has {} quotes\".format(key, count))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Uncomment following to upload cache to opendrive\n# client.push('ds_cache','/kaggle/working/cache')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}