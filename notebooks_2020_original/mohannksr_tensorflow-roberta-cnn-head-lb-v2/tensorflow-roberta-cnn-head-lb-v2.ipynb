{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TensorFlow roBERTa + CNN head - LB   v2"},{"metadata":{},"cell_type":"markdown","source":"Hello everyone! \n\n1. 1. 1. This kernel is based on [Al-Kharba Kiram](https://www.kaggle.com/al0kharba/tensorflow-roberta-0-712/output).  \n\n "},{"metadata":{},"cell_type":"markdown","source":"# Load  data and libraries"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_train():\n    train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n    train['text']=train['text'].astype(str)\n    train['selected_text']=train['selected_text'].astype(str)\n    return train\n\ndef read_test():\n    test=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n    test['text']=test['text'].astype(str)\n    return test\n\ndef read_submission():\n    test=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n    return test\n    \ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard_improve(str1, str2): \n    str1=str1.lower()\n    str2=str2.lower()    \n    index=str1.find(str2) \n    text1=str1[:index]\n    #print(text1)\n    text2=str1[index:].replace(str2,'')\n    words1=text1.split()\n    words2=text2.split()\n    #print(words1[-3:])\n\n    if len(words1)>len(words2):\n        words1=words1[-3:]\n        mod_text=\" \".join(words1)+\" \"+ str2\n    else:\n        words2=words2[0:2]\n        mod_text=str2+\" \"+\" \".join(words2)\n    return mod_text ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str(str1).lower().split())  \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_df))\n#train_df1=train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['selected_text_mod']=train_df['selected_text']\ntrain_df['mod']=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df['mod']=0\n# for index,row in train_df.iterrows():\n#     #print(row['text'])\n#     #print(row['selected_text'])\n#     res1=jaccard(row['text'],row['selected_text_mod'])\n#     res2=jaccard(row['text'],row['selected_text'])\n    \n#     if res1<0.5 and row['mod']==0:\n#         mod_text=jaccard_improve(row['text'],row['selected_text'])\n#         train_df.at[index,'mod']=1\n#         train_df.at[index,'selected_text']=mod_text\n# #         print('____________1')\n# #         print(mod_text)\n# #         print(row['text'])\n# #         print(row['selected_text'])\n# #         print('____________2')\n#         res2=jaccard(row['text'],mod_text)\n#     else:\n#         train_df.at[index,'selected_text']=row['selected_text_mod']\n    \n#     train_df.at[index,'score1']=res1\n#     train_df.at[index,'score2']=res2\n    \n#     #print(res1)\n    \n#     #print(res1)\n#     #train_df.at[index,'score']=res1\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(len(train_df[train_df.score1!=train_df.score2]))\n\n# train_df[train_df.score1!=train_df.score2]\n\n# #print(len(train_df[train_df.score>0.9]))\n# train_df2=train_df[train_df.score>0.9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df = train_df2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preproccesing"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 96\nPATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = train_df.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(input_ids.shape)\nprint(attention_mask.shape)\nprint(token_type_ids.shape)\nprint(start_tokens.shape)\nprint(end_tokens.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" \n\n# for k in range(train_df.shape[0]):\n    \n#     # FIND OVERLAP\n#     text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n#     text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n#     idx = text1.find(text2)\n#     #text1='rx th as'\n#     chars = np.zeros((len(text1)))\n#     print(\"========1\")\n#     print(chars)\n#     chars[idx:idx+len(text2)]=1\n#     print(chars)\n#     print(idx)\n#     print(text1)\n#     print(text2)\n#     print(len(text2))\n#     enc = tokenizer.encode(text1) \n#     print(enc)\n#     print(text1)\n#     if text1[idx-1]==' ': chars[idx-1] = 1 \n#     print(chars)\n\n#     offsets = []; idx=0    \n#     for t in enc.ids:\n#         w = tokenizer.decode([t])\n#         #print(w)\n#         #print(len(w))\n#         offsets.append((idx,idx+len(w)))\n#         idx += len(w)\n#     #print(offsets)\n        \n        \n#     #offsets.append((idx,idx+len(w)))\n#     #idx += len(w)\n        \n#     # START END TOKENS\n#     toks = []\n#     for i,(a,b) in enumerate(offsets):\n#         #print(a,b)\n#         sm = np.sum(chars[a:b])\n#         #print(chars[a:b])\n#         #print(sm)\n#         if sm>0: toks.append(i) \n\n#     s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n#     input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n#     attention_mask[k,:len(enc.ids)+5] = 1\n#     if len(toks)>0:\n#         start_tokens[k,toks[0]+1] = 1\n#         end_tokens[k,toks[-1]+1] = 1            \n            \n#     print(\"========21\")   \n#     print(enc.ids)\n#     print(enc)    \n#     print(text1)\n#     print(text2)\n    \n#     print(offsets)    \n#     print(chars)\n#     print(len(chars))\n#     print(toks)\n#     print(len(toks))\n\n#     print(input_ids[k,:] )\n#     print(s_tok)\n#     print( start_tokens[k,])\n#     print( end_tokens[k,])\n#     print(attention_mask[k,:])\n#     print(toks)\n#     print([0] + enc.ids + [2,2] + [s_tok] + [2])\n#     print(\"========2\")\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = train_df.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train_df.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = test_df.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test_df.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def scheduler(epoch):\n    return 3e-5 * 0.2**epoch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    #x1 = tf.keras.layers.ReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    #x2 = tf.keras.layers.ReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    \n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)    \n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train\nWe will skip this stage and load already trained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #input_ids\n# #train_df.sentiment.values\n\n# print(len(train_df))\n\n# train_df1=train_df[:1000]\n# print(len(train_df1))\n\n# input_ids1=input_ids[:1000]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777)\n# for fold,(idxT,idxV) in enumerate(skf.split(input_ids1,train_df1.sentiment.values)):\n#     print(idxV)\n#     print(len(idxV))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# jac = []; VER='v6'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n# oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n# oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n\n\n# skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777)\n\n# #for fold,(idxT,idxV) in enumerate(skf.split(input_ids1,train_df1.sentiment.values)):\n# for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n\n#     print('#'*25)\n#     print('### FOLD %i'%(fold+1))\n#     print('#'*25)\n    \n#     K.clear_session()\n#     model = build_model()\n        \n#     reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n\n#     sv = tf.keras.callbacks.ModelCheckpoint(\n#         '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, \n#         save_best_only=True,\n#         save_weights_only=True, mode='auto', save_freq='epoch')\n        \n#     hist = model.fit([input_ids[idxT,], attention_mask[idxT,], \n#                       token_type_ids[idxT,]], [start_tokens[idxT,], \n#                                                end_tokens[idxT,]], \n#                         epochs=5, batch_size=8, verbose=DISPLAY, \n#                      callbacks=[sv, reduce_lr],\n#         validation_data=([input_ids[idxV,],attention_mask[idxV,],\n#                           token_type_ids[idxV,]], \n#         [start_tokens[idxV,], end_tokens[idxV,]]))\n    \n#     print('Loading model...')\n#     model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    \n#     print('Predicting OOF...')\n#     oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n     \n    \n#     # DISPLAY FOLD JACCARD\n#     all = []\n#     for k in idxV:\n#         a = np.argmax(oof_start[k,])\n#         b = np.argmax(oof_end[k,])\n#         if a>b: \n#             st = train_df.loc[k,'text'] # IMPROVE CV/LB with better choice here\n#         else:\n#             text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n#             enc = tokenizer.encode(text1)\n#             st = tokenizer.decode(enc.ids[a-1:b])\n#         all.append(jaccard(st,train_df.loc[k,'selected_text']))\n#     jac.append(np.mean(all))\n#     print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n#     print(oof_start[idxV,])\n#     print(oof_end[idxV,])\n    \n     \n   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" \n\n# # DISPLAY FOLD JACCARD\n# all = []\n# for k in idxV:\n#     a = np.argmax(oof_start[k,])\n#     b = np.argmax(oof_end[k,])\n#     if a>b: \n#         st = train_df.loc[k,'text'] # IMPROVE CV/LB with better choice here\n#     else:\n#         text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n#         enc = tokenizer.encode(text1)\n#         st = tokenizer.decode(enc.ids[a-1:b])\n#     all.append(jaccard(st,train_df.loc[k,'selected_text']))\n# jac.append(np.mean(all))\n# print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n# # print(oof_start[idxV,])\n# # print(oof_end[idxV,])\n\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df.loc[10,'text']\n\n\n#train_df.reset_index(inplace = True) \n#train_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\nDISPLAY=1\nfor i in range(5):\n    print('#'*25)\n    print('### MODEL %i'%(i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    #model.load_weights('../input/m6aprila/v6-roberta-%i.h5'%i)\n    model.load_weights('../input/model8/v8-roberta-%i.h5'%i)\n\n    #model.load_weights('v5-roberta-%i.h5'%i)\n\n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/n_splits\n    preds_end += preds[1]/n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test_df.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])          \n        st1=st.strip()\n        if st1=='****' or  st1=='****!' or st1=='****!' or st1=='****!' or st1=='****,' or  st1=='****,' or  st1=='****.' or st1=='****.':\n            #print(st1.strip())\n            #print(text1)\n            st=text1\n        elif st1=='(good':   \n            st='good'\n        elif st1=='__joy':   \n            st='joy'           \n    all.append(st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pandas as pd\n# # train_df.to_csv('train_df.csv',index=False)\n\n# test_df=pd.read_csv('../input/submission/submission_v2.csv')\n\n# # for index,row in test_df.iterrows():\n# #     row['selected_text']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# i=0\n# for index,row in test_df.iterrows():\n#     #print(row['selected_text'])\n#     if len(row['selected_text'])>100:\n#         #print(row['selected_text'])\n#         test_df.at[index,'selected_text']=''\n#         i=i+1\n# print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['selected_text'] = all\ntest_df[['textID','selected_text']].to_csv('submission.csv',index=False)\n \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# from distutils.dir_util import copy_tree\n# todir='/kaggle/working'\n# fromdirc='../input/tweet-sentiment-extraction'\n# copy_tree(fromdirc,todir)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import os\n# os.chdir('/kaggle')\n# os.getcwd()\n# os.listdir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # import os\n# # os.getcwd()\n\n# import shutil\n# source='/kaggle/test1.csv'\n# destination='/kaggle/working/test1.csv'\n# dest = shutil.copyfile(source, destination)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import os\n# import shutil\n# for subdir, dirs, files in os.walk('/kaggle/working/'):\n#     for file in files:\n#         if '.h5' in file:\n#           #print(file) #file\n#           source='/kaggle/working'+str('/')+file\n#           destination='../input/model4'+str('/')+file\n#           destination='/kaggle'+str('/')+file\n#           print(source)  \n#           #print(destination)  \n#           dest = shutil.copyfile(source, destination) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import os\n# for subdir, dirs, files in os.walk('../input/model4/'):\n#     for file in files:\n#       print(file) #file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from IPython.display import FileLink, FileLinks\n# FileLinks('.') #lists all downloadable files on server","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}