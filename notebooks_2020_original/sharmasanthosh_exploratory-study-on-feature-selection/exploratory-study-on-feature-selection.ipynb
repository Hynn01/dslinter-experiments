{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"2e4d99e7-96b9-eeaa-1d48-0128e264ae87"},"source":"Thank you for opening this script!\n\nI have made all efforts to document each and every step involved in the prediction process so that this notebook acts as a good starting point for new Kagglers and new machine learning enthusiasts.\n\nPlease **upvote** this kernel so that it reaches the top of the chart and is easily locatable by new users. Your comments on how we can improve this kernel is welcome. Thanks.\n\nMy other exploratory studies can be accessed here :\nhttps://www.kaggle.com/sharmasanthosh/kernels\n***\n## Layout of the document\nThe prediction process is divided into two notebooks.\n\nThis notebook : Covers data statistics, data visualization, and feature selection\n\nPart 2 : Covers prediction using various algorithms : https://www.kaggle.com/sharmasanthosh/forest-cover-type-prediction/exploratory-study-of-ml-algorithms\n***\n## Data statistics\n* Shape\n* Datatypes\n* Description\n* Skew\n* Class distribution\n\n## Data Interaction\n* Correlation\n* Scatter plot\n\n## Data Visualization\n* Box and density plots\n* Grouping of one hot encoded attributes\n\n## Data Cleaning\n* Remove unnecessary columns\n\n## Data Preparation\n* Original\n* Delete rows or impute values in case of missing\n* StandardScaler\n* MinMaxScaler\n* Normalizer\n\n## Feature selection\n* ExtraTreesClassifier\n* GradientBoostingClassifier\n* RandomForestClassifier\n* XGBClassifier\n* RFE\n* SelectPercentile\n* PCA\n* PCA + SelectPercentile\n* Feature Engineering\n\n## Evaluation, prediction, and analysis\n* LDA (Linear algo)\n* LR (Linear algo)\n* KNN (Non-linear algo)\n* CART (Non-linear algo)\n* Naive Bayes (Non-linear algo)\n* SVC (Non-linear algo)\n* Bagged Decision Trees (Bagging)\n* Random Forest (Bagging)\n* Extra Trees (Bagging)\n* AdaBoost (Boosting)\n* Stochastic Gradient Boosting (Boosting)\n* Voting Classifier (Voting)\n* MLP (Deep Learning)\n* XGBoost\n\n***"},{"cell_type":"markdown","metadata":{"_cell_guid":"6087060c-c61f-fc0c-ea98-1cf2f0119342"},"source":"## Load raw data:\n\nInformation about all the attributes can be found here:\n\nhttps://www.kaggle.com/c/forest-cover-type-prediction/data\n\nLearning: \nWe need to predict the 'Cover_Type' based on the other attributes. Hence, this is a classification problem where the target could belong to any of the seven classes."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0fb020dd-109a-03fa-e695-0e428f2e56e4"},"outputs":[],"source":"# Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Read raw data from the file\n\nimport pandas #provides data structures to quickly analyze data\n#Since this code runs on Kaggle server, train data can be accessed directly in the 'input' folder\ndataset = pandas.read_csv(\"../input/train.csv\") \n\n#Drop the first column 'Id' since it just has serial numbers. Not useful in the prediction process.\ndataset = dataset.iloc[:,1:]"},{"cell_type":"markdown","metadata":{"_cell_guid":"344effe1-667f-191b-b0ad-95ba660c828d"},"source":"## Data statistics\n* Shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"560f0ea7-2105-e9d1-ea3d-85481fb197a7"},"outputs":[],"source":"# Size of the dataframe\n\nprint(dataset.shape)\n\n# We can see that there are 15120 instances having 55 attributes\n\n#Learning : Data is loaded successfully as dimensions match the data description"},{"cell_type":"markdown","metadata":{"_cell_guid":"2cdf5f1a-a053-39a9-c1c4-52d0093c91a9"},"source":"## Data statistics\n* Datatypes"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"565e62a6-e22a-ce6f-675e-3b91decf4b55"},"outputs":[],"source":"# Datatypes of the attributes\n\nprint(dataset.dtypes)\n\n# Learning : Data types of all attributes has been inferred as int64"},{"cell_type":"markdown","metadata":{"_cell_guid":"2fabe0fd-8b99-b060-b743-1ece51195ee7"},"source":"## Data statistics\n* Description"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8bdce59-27c1-1e33-b66e-f776dc758593"},"outputs":[],"source":"# Statistical description\n\npandas.set_option('display.max_columns', None)\nprint(dataset.describe())\n\n# Learning :\n# No attribute is missing as count is 15120 for all attributes. Hence, all rows can be used\n# Negative value(s) present in Vertical_Distance_To_Hydrology. Hence, some tests such as chi-sq cant be used.\n# Wilderness_Area and Soil_Type are one hot encoded. Hence, they could be converted back for some analysis\n# Attributes Soil_Type7 and Soil_Type15 can be removed as they are constant\n# Scales are not the same for all. Hence, rescaling and standardization may be necessary for some algos"},{"cell_type":"markdown","metadata":{"_cell_guid":"18041b8a-3596-969d-546a-7f5b14b2ed1a"},"source":"## Data statistics\n* Skew"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"954ec36b-9cff-bc6f-50d4-8ae7c2b0496f"},"outputs":[],"source":"# Skewness of the distribution\n\nprint(dataset.skew())\n\n# Values close to 0 show less skew\n# Several attributes in Soil_Type show a large skew. Hence, some algos may benefit if skew is corrected"},{"cell_type":"markdown","metadata":{"_cell_guid":"a6c9aedb-7cdb-b000-1ab9-67e28d8bb6cb"},"source":"## Data statistics\n* Class distribution"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc9782df-38c2-60cc-357c-7e90e8689143"},"outputs":[],"source":"# Number of instances belonging to each class\n\ndataset.groupby('Cover_Type').size()\n\n# We see that all classes have an equal presence. No class re-balancing is necessary"},{"cell_type":"markdown","metadata":{"_cell_guid":"c444a383-5d22-34bb-3ba2-d8982b558dee"},"source":"## Data Interaction\n* Correlation"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3af19d83-75bb-43cb-4c1c-77b421617548"},"outputs":[],"source":"import numpy\n\n# Correlation tells relation between two attributes.\n# Correlation requires continous data. Hence, ignore Wilderness_Area and Soil_Type as they are binary\n\n#sets the number of features considered\nsize = 10 \n\n#create a dataframe with only 'size' features\ndata=dataset.iloc[:,:size] \n\n#get the names of all the columns\ncols=data.columns \n\n# Calculates pearson co-efficient for all combinations\ndata_corr = data.corr()\n\n# Set the threshold to select only only highly correlated attributes\nthreshold = 0.5\n\n# List of pairs along with correlation above threshold\ncorr_list = []\n\n#Search for the highly correlated pairs\nfor i in range(0,size): #for 'size' features\n    for j in range(i+1,size): #avoid repetition\n        if (data_corr.iloc[i,j] >= threshold and data_corr.iloc[i,j] < 1) or (data_corr.iloc[i,j] < 0 and data_corr.iloc[i,j] <= -threshold):\n            corr_list.append([data_corr.iloc[i,j],i,j]) #store correlation and columns index\n\n#Sort to show higher ones first            \ns_corr_list = sorted(corr_list,key=lambda x: -abs(x[0]))\n\n#Print correlations and column names\nfor v,i,j in s_corr_list:\n    print (\"%s and %s = %.2f\" % (cols[i],cols[j],v))\n\n# Strong correlation is observed between the following pairs\n# This represents an opportunity to reduce the feature set through transformations such as PCA"},{"cell_type":"markdown","metadata":{"_cell_guid":"b0c017f4-5ccd-9d4d-dd38-3249af55e001"},"source":"## Data Interaction\n* Scatter plot"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"daaf8866-4901-6fc4-a80d-ea4912164221"},"outputs":[],"source":"#import plotting libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Scatter plot of only the highly correlated pairs\nfor v,i,j in s_corr_list:\n    sns.pairplot(dataset, hue=\"Cover_Type\", size=6, x_vars=cols[i],y_vars=cols[j] )\n    plt.show()\n\n#The plots show to which class does a point belong to. The class distribution overlaps in the plots.    \n#Hillshade patterns give a nice ellipsoid patterns with each other\n#Aspect and Hillshades attributes form a sigmoid pattern\n#Horizontal and vertical distance to hydrology give an almost linear pattern."},{"cell_type":"markdown","metadata":{"_cell_guid":"ffa3f34f-5daf-c917-1c07-8bf6045ff4e9"},"source":"## Data Visualization\n* Box and density plots"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c09f4be1-319b-6eb4-e2e1-dbdc3b759674"},"outputs":[],"source":"# We will visualize all the attributes using Violin Plot - a combination of box and density plots\n\n#names of all the attributes \ncols = dataset.columns\n\n#number of attributes (exclude target)\nsize = len(cols)-1\n\n#x-axis has target attribute to distinguish between classes\nx = cols[size]\n\n#y-axis shows values of an attribute\ny = cols[0:size]\n\n#Plot violin for all attributes\nfor i in range(0,size):\n    sns.violinplot(data=dataset,x=x,y=y[i])  \n    plt.show()\n\n#Elevation is has a separate distribution for most classes. Highly correlated with the target and hence an important attribute\n#Aspect contains a couple of normal distribution for several classes\n#Horizontal distance to road and hydrology have similar distribution\n#Hillshade 9am and 12pm display left skew\n#Hillshade 3pm is normal\n#Lots of 0s in vertical distance to hydrology\n#Wilderness_Area3 gives no class distinction. As values are not present, others gives some scope to distinguish\n#Soil_Type, 1,5,8,9,12,14,18-22, 25-30 and 35-40 offer class distinction as values are not present for many classes"},{"cell_type":"markdown","metadata":{"_cell_guid":"fd5dd31b-f482-c0cb-561b-338fac2a9454"},"source":"## Data Visualization\n* Grouping of One hot encoded attributes"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"043db5f0-4ee9-ac07-a5e4-651c7998f2a3"},"outputs":[],"source":"# Group one-hot encoded variables of a category into one single variable\n\n#names of all the columns\ncols = dataset.columns\n\n#number of rows=r , number of columns=c\nr,c = dataset.shape\n\n#Create a new dataframe with r rows, one column for each encoded category, and target in the end\ndata = pandas.DataFrame(index=numpy.arange(0, r),columns=['Wilderness_Area','Soil_Type','Cover_Type'])\n\n#Make an entry in 'data' for each r as category_id, target value\nfor i in range(0,r):\n    w=0;\n    s=0;\n    # Category1 range\n    for j in range(10,14):\n        if (dataset.iloc[i,j] == 1):\n            w=j-9  #category class\n            break\n    # Category2 range        \n    for k in range(14,54):\n        if (dataset.iloc[i,k] == 1):\n            s=k-13 #category class\n            break\n    #Make an entry in 'data' for each r as category_id, target value        \n    data.iloc[i]=[w,s,dataset.iloc[i,c-1]]\n\n#Plot for Category1    \nsns.countplot(x=\"Wilderness_Area\", hue=\"Cover_Type\", data=data)\nplt.show()\n#Plot for Category2\nplt.rc(\"figure\", figsize=(25, 10))\nsns.countplot(x=\"Soil_Type\", hue=\"Cover_Type\", data=data)\nplt.show()\n\n#(right-click and open the image in a new window for larger size)\n#WildernessArea_4 has a lot of presence for cover_type 4. Good class distinction\n#WildernessArea_3 has not much class distinction\n#SoilType 1-6,10-14,17, 22-23, 29-33,35,38-40 offer lot of class distinction as counts for some are very high"},{"cell_type":"markdown","metadata":{"_cell_guid":"c6867f25-b888-d5b3-1251-02f4d7f51539"},"source":"## Data Cleaning\n* Remove unnecessary columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"71fd2956-f08c-6eb0-6b7b-d329c8776a11"},"outputs":[],"source":"#Removal list initialize\nrem = []\n\n#Add constant columns as they don't help in prediction process\nfor c in dataset.columns:\n    if dataset[c].std() == 0: #standard deviation is zero\n        rem.append(c)\n\n#drop the columns        \ndataset.drop(rem,axis=1,inplace=True)\n\nprint(rem)\n\n#Following columns are dropped"},{"cell_type":"markdown","metadata":{"_cell_guid":"e26ecdad-505b-772f-e054-5c7aa28cee68"},"source":"## Data Preparation\n* Original\n* Delete rows or impute values in case of missing\n* StandardScaler\n* MinMaxScaler\n* Normalizer"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd44ff46-7b56-a7b6-8e69-5886ab3f42de"},"outputs":[],"source":"#get the number of rows and columns\nr, c = dataset.shape\n\n#get the list of columns\ncols = dataset.columns\n#create an array which has indexes of columns\ni_cols = []\nfor i in range(0,c-1):\n    i_cols.append(i)\n#array of importance rank of all features  \nranks = []\n\n#Extract only the values\narray = dataset.values\n\n#Y is the target column, X has the rest\nX = array[:,0:(c-1)]\nY = array[:,(c-1)]\n\n#Validation chunk size\nval_size = 0.1\n\n#Use a common seed in all experiments so that same chunk is used for validation\nseed = 0\n\n#Split the data into chunks\nfrom sklearn import cross_validation\nX_train, X_val, Y_train, Y_val = cross_validation.train_test_split(X, Y, test_size=val_size, random_state=seed)\n\n#Import libraries for data transformations\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import Normalizer\n\n#All features\nX_all = []\n#Additionally we will make a list of subsets\nX_all_add =[]\n\n#columns to be dropped\nrem = []\n#indexes of columns to be dropped\ni_rem = []\n\n#List of combinations\ncomb = []\ncomb.append(\"All+1.0\")\n\n#Add this version of X to the list \nX_all.append(['Orig','All', X_train,X_val,1.0,cols[:c-1],rem,ranks,i_cols,i_rem])\n\n#point where categorical data begins\nsize=10\n\n#Standardized\n#Apply transform only for non-categorical data\nX_temp = StandardScaler().fit_transform(X_train[:,0:size])\nX_val_temp = StandardScaler().fit_transform(X_val[:,0:size])\n#Concatenate non-categorical data and categorical\nX_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\nX_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n#Add this version of X to the list \nX_all.append(['StdSca','All', X_con,X_val_con,1.0,cols,rem,ranks,i_cols,i_rem])\n\n#MinMax\n#Apply transform only for non-categorical data\nX_temp = MinMaxScaler().fit_transform(X_train[:,0:size])\nX_val_temp = MinMaxScaler().fit_transform(X_val[:,0:size])\n#Concatenate non-categorical data and categorical\nX_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\nX_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n#Add this version of X to the list \nX_all.append(['MinMax', 'All', X_con,X_val_con,1.0,cols,rem,ranks,i_cols,i_rem])\n\n#Normalize\n#Apply transform only for non-categorical data\nX_temp = Normalizer().fit_transform(X_train[:,0:size])\nX_val_temp = Normalizer().fit_transform(X_val[:,0:size])\n#Concatenate non-categorical data and categorical\nX_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\nX_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n#Add this version of X to the list \nX_all.append(['Norm', 'All', X_con,X_val_con,1.0,cols,rem,ranks,i_cols,i_rem])\n\n#Impute\n#Imputer is not used as no data is missing\n\n#List of transformations\ntrans_list = []\n\nfor trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n    trans_list.append(trans)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c504c142-4603-644a-2387-1b1946d79c4e"},"source":"## Feature selection\n* ExtraTreesClassifier\n* GradientBoostingClassifier\n* RandomForestClassifier\n* XGBoostClassifier"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"238d6b4e-b47d-7a5c-b6bb-ad770e55724f"},"outputs":[],"source":"#Select top 75%,50%,25%\nratio_list = [0.75,0.50,0.25]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"17b48b0b-5fc2-c971-d432-ed701283e1ec"},"outputs":[],"source":"#List of feature selection models\nfeat = []\n\n#List of names of feature selection models\nfeat_list =[]\n\n#Import the libraries\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n#Add ExtraTreeClassifiers to the list\nn = 'ExTree'\nfeat_list.append(n)\nfor val in ratio_list:\n    comb.append(\"%s+%s\" % (n,val))\n    feat.append([n,val,ExtraTreesClassifier(n_estimators=c-1,max_features=val,n_jobs=-1,random_state=seed)])      \n\n#Add GradientBoostingClassifiers to the list \nn = 'GraBst'\nfeat_list.append(n)\nfor val in ratio_list:\n    comb.append(\"%s+%s\" % (n,val))\n    feat.append([n,val,GradientBoostingClassifier(n_estimators=c-1,max_features=val,random_state=seed)])   \n\n#Add RandomForestClassifiers to the list \nn = 'RndFst'\nfeat_list.append(n)\nfor val in ratio_list:\n    comb.append(\"%s+%s\" % (n,val))\n    feat.append([n,val,RandomForestClassifier(n_estimators=c-1,max_features=val,n_jobs=-1,random_state=seed)])   \n\n#Add XGBClassifier to the list \nn = 'XGB'\nfeat_list.append(n)\nfor val in ratio_list:\n    comb.append(\"%s+%s\" % (n,val))\n    feat.append([n,val,XGBClassifier(n_estimators=c-1,seed=seed)])   \n        \n#For all transformations of X\nfor trans,s, X, X_val, d, cols, rem, ra, i_cols, i_rem in X_all:\n    #For all feature selection models\n    for name,v, model in feat:\n        #Train the model against Y\n        model.fit(X,Y_train)\n        #Combine importance and index of the column in the array joined\n        joined = []\n        for i, pred in enumerate(list(model.feature_importances_)):\n            joined.append([i,cols[i],pred])\n        #Sort in descending order    \n        joined_sorted = sorted(joined, key=lambda x: -x[2])\n        #Starting point of the columns to be dropped\n        rem_start = int((v*(c-1)))\n        #List of names of columns selected\n        cols_list = []\n        #Indexes of columns selected\n        i_cols_list = []\n        #Ranking of all the columns\n        rank_list =[]\n        #List of columns not selected\n        rem_list = []\n        #Indexes of columns not selected\n        i_rem_list = []\n        #Split the array. Store selected columns in cols_list and removed in rem_list\n        for j, (i, col, x) in enumerate(list(joined_sorted)):\n            #Store the rank\n            rank_list.append([i,j])\n            #Store selected columns in cols_list and indexes in i_cols_list\n            if(j < rem_start):\n                cols_list.append(col)\n                i_cols_list.append(i)\n            #Store not selected columns in rem_list and indexes in i_rem_list    \n            else:\n                rem_list.append(col)\n                i_rem_list.append(i)    \n        #Sort the rank_list and store only the ranks. Drop the index \n        #Append model name, array, columns selected and columns to be removed to the additional list        \n        X_all_add.append([trans,name,X,X_val,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])    \n\n#Set figure size\nplt.rc(\"figure\", figsize=(25, 10))\n\n#Plot a graph for different feature selectors        \nfor f_name in feat_list:\n    #Array to store the list of combinations\n    leg=[]\n    fig, ax = plt.subplots()\n    #Plot each combination\n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n        if(name==f_name):\n            plt.plot(rank_list)\n            leg.append(trans+\"+\"+name+\"+%s\"% v)\n    #Set the tick names to names of columns\n    ax.set_xticks(range(c-1))\n    ax.set_xticklabels(cols[:c-1],rotation='vertical')\n    #Display the plot\n    plt.legend(leg,loc='best')    \n    #Plot the rankings of all the features for all combinations\n    plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"add852b7-1546-571b-7ff6-10b9c7a53845"},"source":"## Feature selection\n* RFE"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8d3d8c9-a977-9aa8-ed20-d6c59fc1c4df"},"outputs":[],"source":"#List of feature selection models\nfeat = []\n\n#List of names of feature selection models\nfeat_list =[]\n\n#Libraries for feature selection\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n#Add RFE to the list \nmodel = LogisticRegression(random_state=seed,n_jobs=-1)\nn = 'RFE'\nfeat_list.append(n)\nfor val in ratio_list:\n    comb.append(\"%s+%s\" % (n,val))\n    feat.append([n,val,RFE(model,val*(c-1))])   \n        \n#For all transformations of X\nfor trans,s, X, X_val, d, cols, rem, ra, i_cols, i_rem in X_all:\n    #For all feature selection models\n    for name,v, model in feat:\n        #Train the model against Y\n        model.fit(X,Y_train)\n        #Combine importance and index of the column in the array joined\n        joined = []\n        for i, pred in enumerate(list(model.ranking_)):\n            joined.append([i,cols[i],pred])\n        #Sort in ascending order    \n        joined_sorted = sorted(joined, key=lambda x: x[2])\n        #Starting point of the columns to be dropped\n        rem_start = int((v*(c-1)))\n        #List of names of columns selected\n        cols_list = []\n        #Indexes of columns selected\n        i_cols_list = []\n        #Ranking of all the columns\n        rank_list =[]\n        #List of columns not selected\n        rem_list = []\n        #Indexes of columns not selected\n        i_rem_list = []\n        #Split the array. Store selected columns in cols_list and removed in rem_list\n        for i, col, j in joined_sorted:\n            #Store the rank\n            rank_list.append([i,j-1])\n            #Store selected columns in cols_list and indexes in i_cols_list\n            if((j-1) < rem_start):\n                cols_list.append(col)\n                i_cols_list.append(i)\n            #Store not selected columns in rem_list and indexes in i_rem_list    \n            else:\n                rem_list.append(col)\n                i_rem_list.append(i)    \n        #Sort the rank_list and store only the ranks. Drop the index \n        #Append model name, array, columns selected and columns to be removed to the additional list        \n        X_all_add.append([trans,name,X,X_val,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])    \n\n#Set figure size\nplt.rc(\"figure\", figsize=(25, 10))\n\n#Plot a graph for different feature selectors        \nfor f_name in feat_list:\n    #Array to store the list of combinations\n    leg=[]\n    fig, ax = plt.subplots()\n    #Plot each combination\n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n        if(name==f_name):\n            plt.plot(rank_list)\n            leg.append(trans+\"+\"+name+\"+%s\"% v)\n    #Set the tick names to names of columns\n    ax.set_xticks(range(c-1))\n    ax.set_xticklabels(cols[:c-1],rotation='vertical')\n    #Display the plot\n    plt.legend(leg,loc='best')    \n    #Plot the rankings of all the features for all combinations\n    plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"3b40ca5a-d0c0-545c-e4f5-79d9e6ae3925"},"source":"#Feature Selection\n* SelectPercentile"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b563403-cc1b-5b25-eac0-3d7249e78324"},"outputs":[],"source":"#List of feature selection models\nfeat = []\n\n#List of names of feature selection models\nfeat_list =[]\n\n#Libraries for SelectPercentile    \nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import f_classif        \n\nn = 'SelK'\nfeat_list.append(n)\nfor val in ratio_list:\n    comb.append(\"%s+%s\" % (n,val))\n    feat.append([n,val,SelectPercentile(score_func=f_classif,percentile=val*100)])   \n\n#For all transformations of X\nfor trans,s, X, X_val, d, cols, rem, ra, i_cols, i_rem in X_all:\n    #For all feature selection models\n    for name,v, model in feat:\n        #Train the model against Y\n        model.fit(X,Y_train)\n        #Combine importance and index of the column in the array joined\n        joined = []\n        for i, pred in enumerate(list(model.scores_)):\n            joined.append([i,cols[i],pred])\n        #Sort in descending order    \n        joined_sorted = sorted(joined, key=lambda x: -x[2])\n        #Starting point of the columns to be dropped\n        rem_start = int((v*(c-1)))\n        #List of names of columns selected\n        cols_list = []\n        #Indexes of columns selected\n        i_cols_list = []\n        #Ranking of all the columns\n        rank_list =[]\n        #List of columns not selected\n        rem_list = []\n        #Indexes of columns not selected\n        i_rem_list = []\n        #Split the array. Store selected columns in cols_list and removed in rem_list\n        for j, (i, col, x) in enumerate(list(joined_sorted)):\n            #Store the rank\n            rank_list.append([i,j])\n            #Store selected columns in cols_list and indexes in i_cols_list\n            if(j < rem_start):\n                cols_list.append(col)\n                i_cols_list.append(i)\n            #Store not selected columns in rem_list and indexes in i_rem_list    \n            else:\n                rem_list.append(col)\n                i_rem_list.append(i)    \n        #Sort the rank_list and store only the ranks. Drop the index \n        #Append model name, array, columns selected and columns to be removed to the additional list        \n        X_all_add.append([trans,name,X,X_val,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])    \n\n\n#Set figure size\nplt.rc(\"figure\", figsize=(25, 10))\n\n#Plot a graph for different feature selectors        \nfor f_name in feat_list:\n    #Array to store the list of combinations\n    leg=[]\n    fig, ax = plt.subplots()\n    #Plot each combination\n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n        if(name==f_name):\n            plt.plot(rank_list)\n            leg.append(trans+\"+\"+name+\"+%s\"% v)\n    #Set the tick names to names of columns\n    ax.set_xticks(range(c-1))\n    ax.set_xticklabels(cols[:c-1],rotation='vertical')\n    #Display the plot\n    plt.legend(leg,loc='best')    \n    #Plot the rankings of all the features for all combinations\n    plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"fc34ac9a-fa7e-5661-75ab-7e0d74440dea"},"source":"#Feature Selection\nRanking summary"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"647e5c70-f844-fc4a-44ae-76b195916494"},"outputs":[],"source":"rank_df = pandas.DataFrame(data=[x[7] for x in X_all_add],columns=cols[:c-1])\n_ = rank_df.boxplot(rot=90)\n#Below plot summarizes the rankings according to the standard feature selection techniques\n#Top ranked attributes are ... first 10 attributes, Wilderness_Area1,4 ...Soil_Type 3,4,10,38-40"},{"cell_type":"markdown","metadata":{"_cell_guid":"37812a8c-4ca4-3864-6b08-f8b53c72c4dd"},"source":"#Feature Selection\nRank features based on median"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ba42507c-b8f4-ef1a-c51f-713b696d9f28"},"outputs":[],"source":"rank_df = pandas.DataFrame(data=[x[7] for x in X_all_add],columns=cols[:c-1])\nmed = rank_df.median()\nprint(med)\n#Write medians to output file for exploratory study on ML algorithms\nwith open(\"median.csv\", \"w\") as subfile:\n       subfile.write(\"Column,Median\\n\")\n       subfile.write(med.to_string())"},{"cell_type":"markdown","metadata":{"_cell_guid":"9e90c3e6-b37d-a3a3-4f66-02b5c6cfd364"},"source":"##Part 2 of the Notebook:\nhttps://www.kaggle.com/sharmasanthosh/forest-cover-type-prediction/exploratory-study-of-ml-algorithms"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}