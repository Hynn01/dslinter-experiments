{"cells":[{"metadata":{"id":"pBmrpVj0VJG8"},"cell_type":"markdown","source":"# Spooky Author Identification\n\nThis kernel is entirely inspired from [Approaching (Almost) Any NLP Problem on Kaggle](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle/notebook) - Abhishek Thakur\n\nI have added some extra points and steps for better understanding at more beginner level.\n\nFor better readablity- Download it and view on [Colab](https://colab.research.google.com/)\n\nThere might be many cases where libraries are imported repeatidly, please ignore that."},{"metadata":{"id":"xzSDr8-aVJG9"},"cell_type":"markdown","source":"# Import Data"},{"metadata":{"hidden":true,"id":"iB_d4bXZVJG-","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport zipfile\n\ntrain_archive = zipfile.ZipFile('../input/spooky-author-identification/train.zip', 'r')\ntest_archive = zipfile.ZipFile('../input/spooky-author-identification/test.zip', 'r')\ntrain = pd.read_csv(train_archive.open(\"train.csv\"))\ntest = pd.read_csv(test_archive.open(\"test.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"0MirQa84VJHC","outputId":"d4d9ecba-13bd-4765-e7d5-20738fb43702","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"aUpuaDVQ8qPc","trusted":true},"cell_type":"code","source":"test_id = test['id']","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"9n37ONEvjogK"},"cell_type":"markdown","source":"# Preprocessing Data"},{"metadata":{"heading_collapsed":true,"id":"oeCAsHcTVJHI"},"cell_type":"markdown","source":"#### Check for missing values"},{"metadata":{"hidden":true,"id":"5M1eliYpVJHJ","outputId":"be10158d-60e8-47bf-ccb1-968258378f3b","trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"5bYVvgOqVJHN"},"cell_type":"markdown","source":"No missing values found"},{"metadata":{"heading_collapsed":true,"id":"PoUwSflzVJHN"},"cell_type":"markdown","source":"#### Unique Authors "},{"metadata":{"hidden":true,"id":"fasoVU3JVJHO","outputId":"1b8299aa-4033-48fd-8af3-0d8d72d4cea9","trusted":true},"cell_type":"code","source":"train['author'].unique()","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true,"id":"3LS2d_tKVJHR"},"cell_type":"markdown","source":"#### Docs per Author count"},{"metadata":{"hidden":true,"id":"DbLiiy91VJHS","outputId":"f40a7d0b-168c-4012-a22b-65bc54059e7e","trusted":true},"cell_type":"code","source":"print(\"EAP \",len(train[train['author']=='EAP']))\nprint(\"HPL \",len(train[train['author']=='HPL']))\nprint(\"MWS \",len(train[train['author']=='MWS']))","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true,"id":"cfOdKBq3VJHV"},"cell_type":"markdown","source":"#### Label Encoding Target Values"},{"metadata":{"hidden":true,"id":"h2HhNqU_VJHW","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n\n# tfidf = TfidfVectorizer(tokenizer=tokenize_and_stem, stop_words='english',decode_error='ignore')\n\nlabel_enc = LabelEncoder()\ny = label_enc.fit_transform(train['author'])\n","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true,"id":"xDxWdLvhVJHb"},"cell_type":"markdown","source":"### Train Test Split"},{"metadata":{"hidden":true,"id":"cGo1vEVhVJHb","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nxtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, \n                                                  stratify=y, \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"CZz1KBF0VJHe"},"cell_type":"markdown","source":"### Categorical value handling - TF-IDF \n\nWe are converting all text values into numbers on the basis of their no. of occurance in the text.\n\nThere are 2 approaches for converting into count-equivalent version of text\n1. CountVectorizer\n2. TF-IDF\n\n**CountVectorizer** - This approach counts how many time a word occured for 1 document(Every row is 1 document in our case.) and replace it with its word count.\n\n**TF-IDF** - This approach also count the no. of time each word occured in document but also performs a normalisation on it. Means, if the same word is also found in other documnets a lot of time, that word-float value is reduced as it is considered as a common word. \n\nIt follows a formula of:\nWord-value = No.of time a word occured in target Documnet / No. of time same word occured in all documents.\n\n\nHere, we will work with both, Count-Vectorizer & TF-IDF Vectorizer"},{"metadata":{"id":"sti7coaTVJHe","trusted":true},"cell_type":"code","source":"tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\ntfv.fit(list(xtrain) + list(xvalid))\nxtrain_tfv = tfv.transform(xtrain)\nxvalid_tfv = tfv.transform(xvalid)\n\n\nxtest_tfv = tfv.transform(test['text'])","execution_count":null,"outputs":[]},{"metadata":{"id":"g3wUII6wVJHh","trusted":true},"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n                    ngram_range=(1,3),stop_words='english')\n\nctv.fit(list(xtrain)+list(xvalid))\nxtrain_ctv = ctv.transform(xtrain)\nxvalid_ctv = ctv.transform(xvalid)","execution_count":null,"outputs":[]},{"metadata":{"id":"Q5ZGc3a3VJHk"},"cell_type":"markdown","source":"# Fitting Models\n1. Logistic Regression - TF-IDF\n2. Logistic Regression - Count-Vec\n3. Naive Bayes\n4. XGBoost\n5. Dimensionality Reduction - SVD\n6. Grid Search\n7. Glove Word Embeddings\n8. Basic Neural Network Model\n9. Basic LSTM\n10. Bi-directional LSTM\n12. GRU"},{"metadata":{"id":"VslJvmSbVJHk","trusted":true},"cell_type":"code","source":"# Accuracy Mertic\ndef multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows * vsota","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dummy Classifier\n\nWe will aim to get better results than Dummy result"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.dummy import DummyClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nmodel = DummyClassifier()\nmodel.fit(xtrain_tfv,ytrain)\npred = model.predict_proba(xvalid_tfv)\n\nprint (\"Dummy Classifier TF-IDF logloss: %0.3f \" % multiclass_logloss(yvalid, pred))","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true,"id":"37LHAeTpVJHn"},"cell_type":"markdown","source":"## Logistic Regression on TF-IDF"},{"metadata":{"hidden":true,"id":"Nr7-vAxdVJHn","outputId":"568926a3-e1fe-4167-d762-10fea4c5359a","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression(C=1.0,max_iter=500)\nlr_model.fit(xtrain_tfv, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"FD810uEaVJHq","trusted":true},"cell_type":"code","source":"prediction = lr_model.predict(xvalid_tfv)\npredictions = lr_model.predict_proba(xvalid_tfv) # Results probabilty percentage for each target value.","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"HQXiPQlaVJHs","outputId":"2727d74d-fb67-4cd6-f522-2e4003d11dbd","trusted":true},"cell_type":"code","source":"predictions[:5,:]","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"IQmzZVG_VJHv","outputId":"040aae75-ec9f-4f59-b914-7b01b154356e","trusted":true},"cell_type":"code","source":"print (\"Logistic Regression TF-IDF logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true,"id":"pZnu0Ob-VJHx"},"cell_type":"markdown","source":"## Logistic Regression on CountVec"},{"metadata":{"hidden":true,"id":"avxrk1eSVJHy","outputId":"c2bdb2bd-125a-4795-e865-1e80ae54336d","trusted":true},"cell_type":"code","source":"lr_model.fit(xtrain_ctv,ytrain)\n\nctv_prediction = lr_model.predict_proba(xvalid_ctv)\nprint (\"Logistic Regression CountVec logloss: %0.3f \" % multiclass_logloss(yvalid, ctv_prediction))","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true,"id":"8yMFV2nwVJH0"},"cell_type":"markdown","source":"## Naive Bayes"},{"metadata":{"hidden":true,"id":"3XNbYEzxVJH1"},"cell_type":"markdown","source":"**TF-IDF**"},{"metadata":{"hidden":true,"id":"rWmMHldMVJH1","outputId":"c69b5884-dddf-4b13-d763-8af60f9886cf","trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnb_model = MultinomialNB()\n\nnb_model.fit(xtrain_tfv,ytrain)\ntfv_nb_predictions = nb_model.predict_proba(xvalid_tfv)\nprint (\"Naive Bayes TF-IDF logloss: %0.3f \" % multiclass_logloss(yvalid, tfv_nb_predictions))","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"YnXGX_ESVJH4"},"cell_type":"markdown","source":"**Count Vec**"},{"metadata":{"hidden":true,"id":"RZT_EExHVJH5","outputId":"2128c999-5ad3-4a74-ddca-6611d1bf5e89","trusted":true},"cell_type":"code","source":"nb_model.fit(xtrain_ctv, ytrain)\nctv_nb_predictions = nb_model.predict_proba(xvalid_ctv)\nprint (\"Naive Bayes CountVec logloss: %0.3f \" % multiclass_logloss(yvalid, ctv_nb_predictions))","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"qQdWuNLsVJH7"},"cell_type":"markdown","source":"Naive Bayes with CountVectorizer has given us best result so far. And to keep in mind, they both are pretty simple model. \n\nNaive Bayes is on of the most basic and most used classification model.\n\nCountVec - just count of word freq."},{"metadata":{"heading_collapsed":true,"id":"XDcS3ecHVJH7"},"cell_type":"markdown","source":"## XGBOOST"},{"metadata":{"hidden":true,"id":"uwI81wQ3VJH8","trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb_model = XGBClassifier(n_estimators=500, max_depth=7, colsample_bytree=0.8,\n                         sumbsample=0.8, n_thread=10, learning_rate=0.1)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"jaV_A_G8VJH-"},"cell_type":"markdown","source":"**TF-IDF**"},{"metadata":{"hidden":true,"id":"DGPWR_9vVJH-","outputId":"c67473b5-a113-41ff-ef66-2bc6b2718694","trusted":true},"cell_type":"code","source":"xgb_model.fit(xtrain_tfv.tocsc(), ytrain)\nxbg_tfv_predictions = xgb_model.predict_proba(xvalid_tfv.tocsc())\nprint (\"XGBoost TF-IDF logloss: %0.3f \" % multiclass_logloss(yvalid, xbg_tfv_predictions))","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"Y7FY59t4VJIA"},"cell_type":"markdown","source":"**CountVec**\n\nHeads Up !!!\nThis model will takes longer time to execute."},{"metadata":{"hidden":true,"id":"7R7grTMpVJIB","trusted":true,"collapsed":true},"cell_type":"code","source":"xgb_model.fit(xtrain_ctv.tocsc(), ytrain)\nxbg_ctv_predictions = xgb_model.predict_proba(xvalid_ctv.tocsc())\nprint (\"XGBoost CountVec logloss: %0.3f \" % multiclass_logloss(yvalid, xbg_ctv_predictions))","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"r9DVj3P5VJID"},"cell_type":"markdown","source":"XGBoost didnt perform much better on this.\n\nNote: xgb_ctv took more than 30min to run."},{"metadata":{"heading_collapsed":true,"id":"kNyGQ2iknzVW"},"cell_type":"markdown","source":"## Basic Submission - Naive Bayes\n\nAs Naive bayes gave the best result so far, lets create our first submission file."},{"metadata":{"hidden":true,"id":"umPSfmXXnzVX","outputId":"25a2605d-f80f-421f-9d96-8f9a06012da4","trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnb_model = MultinomialNB(alpha=0.1)\n\nnb_model.fit(xtrain_tfv,ytrain)\ntfv_nb_predictions = nb_model.predict_proba(xvalid_tfv)\nprint (\"Naive Bayes TF-IDF logloss: %0.3f \" % multiclass_logloss(yvalid, tfv_nb_predictions))","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"gTTxLaZDnzVZ","trusted":true},"cell_type":"code","source":"final_pred = nb_model.predict_proba(xtest_tfv)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"J1i9tSqJnzVb","trusted":true},"cell_type":"code","source":"x = {'id':test_id, 'EAP':final_pred[:,0], 'HPL':final_pred[:,1], 'MWS':final_pred[:,2]}\ndd = pd.DataFrame(x)\ndd.to_csv('spooky_author_NB_submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true,"id":"VCxtnurdVJID"},"cell_type":"markdown","source":"## Dimension Reduction\n\n"},{"metadata":{"hidden":true,"id":"vpeKOG75VJIE"},"cell_type":"markdown","source":"We will reduce Dimension using Single Value Decomposition (SVD).\n\nRead google doc about it."},{"metadata":{"hidden":true,"id":"K0UVSi8YVJIE","trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=120)\nsvd.fit(xtrain_tfv) # Truncated SVD works on TF-IDF\n\nxtrain_tfv_svd = svd.transform(xtrain_tfv)\nxvalid_tfv_svd = svd.transform(xvalid_tfv)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"v6-14yX6VJIK","trusted":true},"cell_type":"code","source":"# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\nfrom sklearn.preprocessing import StandardScaler\nscl = StandardScaler()\n\nscl.fit(xtrain_tfv_svd)\nxtrain_svd_scl = scl.transform(xtrain_tfv_svd)\nxtrain_svd_scl = scl.transform(xvalid_tfv_svd)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"bMVv8jAMXqoq"},"cell_type":"markdown","source":"Applying SVD train data on XGBOOST"},{"metadata":{"hidden":true,"id":"cnN-Sz1RX0n0","outputId":"c82f0ad2-516f-4082-aca8-a5faa2bc01e8","trusted":true},"cell_type":"code","source":"xgb_model.fit(xtrain_tfv_svd,ytrain)\npredictions = xgb_model.predict_proba(xvalid_tfv_svd)\nprint (\"XGBoost SVD logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(xtrain_svd_scl.shape)\nprint(ytrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"aqZFucfSYVXi","trusted":true},"cell_type":"code","source":"# On scaler datail\nxgb_model.fit(xtrain_svd_scl,ytrain)\npredictions = xgb_model.predict_proba(xtrain_svd_scl)\nprint (\"XGBoost SVD-SCL logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true,"hidden":true,"id":"kzlpGf3HYSiz"},"cell_type":"markdown","source":"### HyperParameter Tunning - Grid Search\n\nWe will use Grid Serach to find the right set of parameters for our model and then make predictions.\n\nWe will also use **PIPELINE** to fit our models for different,\n\n* SVD n_components - 120,180\n* And different model HyperParams\n"},{"metadata":{"hidden":true,"id":"BaGDsWhlZQB0","trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer\nfrom sklearn.model_selection import GridSearchCV\n\nmodel_scorer = make_scorer(score_func = multiclass_logloss,\n                           greater_is_better = False,\n                           needs_proba=True)\n","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true,"hidden":true,"id":"QMT5f_eSbTqa"},"cell_type":"markdown","source":"#### Grid Search for Logistic Regression"},{"metadata":{"hidden":true,"id":"QJ4JLFQkbfpR","outputId":"4b61273c-1a22-4fc3-aab6-03d42deafd78","trusted":true},"cell_type":"code","source":"# Defining Pipeline\nfrom sklearn.pipeline import Pipeline\n\nsvd = TruncatedSVD()\nscl = StandardScaler() \nlr_model = LogisticRegression()\n\nmodel = Pipeline(steps=[('svd',svd),\n                        ('scl',scl),\n                         ('model',lr_model)])\n\nhyper_parameters = {\n                    'svd__n_components':[120,180],\n                    'model__C':[0.1, 1.0, 10],\n                    'model__penalty':['l1','l2']\n                    }\n                \nmodel = GridSearchCV(estimator=model,\n                     param_grid=hyper_parameters,\n                     scoring=model_scorer,\n                     verbose=10,\n                     n_jobs= -1,\n                     iid = True,\n                     refit=True,\n                     cv = 2) # Cv = Cross fit split strategy\n\nmodel.fit(xtrain_tfv, ytrain)\n","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"MT9WFAIhfhXL","outputId":"ed53a69f-26a1-42f6-9d17-252ee3909ff7","trusted":true},"cell_type":"code","source":"print(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")                     \nbest_hyper_params = model.best_estimator_.get_params()\nfor param_name in model.param_grid.keys():\n  print(f\"\\t{param_name}: {best_hyper_params[param_name]} \")","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"woI3qV6ggFZd"},"cell_type":"markdown","source":"#### Grid Search on Naive Bayes"},{"metadata":{"hidden":true,"id":"IOl42t14gHAJ","outputId":"90c5d7c6-ee2a-4c71-b12b-24155f4d462d","trusted":true},"cell_type":"code","source":"nb_model = MultinomialNB()\nmodel = Pipeline([('model',nb_model)])\n\nhyper_parameters = {\n                    'model__alpha':[0.001, 0.01, 0.1, 1.0, 10, 100],\n                    }\n                \ngrid_nb_model = GridSearchCV(estimator=model,\n                     param_grid=hyper_parameters,\n                     scoring=model_scorer,\n                     verbose=10,\n                     n_jobs= -1,\n                     iid = True,\n                     refit=True,\n                     cv = 2) # Cv = Cross fit split strategy\n\ngrid_nb_model.fit(xtrain_tfv, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"rp_-2uzngJXD","outputId":"1c4a935e-508e-4078-ec0b-ca18e30278e7","trusted":true},"cell_type":"code","source":"print(\"Best score: %0.3f\" % grid_nb_model.best_score_)\nprint(\"Best parameters set:\")                     \nbest_hyper_params = grid_nb_model.best_estimator_.get_params()\nfor param_name in grid_nb_model.param_grid.keys():\n  print(f\"\\t{param_name}: {best_hyper_params[param_name]} \")","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"Nr3wUMx5jmfH"},"cell_type":"markdown","source":"**0.492**\n\nYESS !! We have got our best Score so far with Grid search on Naive Bayes."},{"metadata":{"heading_collapsed":true,"hidden":true,"id":"Qn4WJn_6jwT8"},"cell_type":"markdown","source":"#### Grid Search on XGB\n\nSo far, XGB didn't gave good results. So not expecting much"},{"metadata":{"hidden":true,"id":"S2ks751Kj0Yu","outputId":"ee013090-4e70-4ed4-d530-5be1d575c7f1","trusted":true},"cell_type":"code","source":"xg_model = XGBClassifier()\n\nmodel = Pipeline(steps=[('svd',svd),\n                        ('scl',scl),\n                         ('model',xg_model)])\n\nhyper_parameters = {\n                    'svd__n_components':[180],\n                    'model__max_depth':[3,7,10],\n                    'model__learning_rate':[0.01, 0.03, 0.05],\n                    'model__n_estimators':[100,300 ,600],\n                    'model__n_jobs':[-1],\n                    'model__colsample_bytree': [0.3,0.5,0.8],\n                    # 'model__subsample':[0.3,0.5,0.8],\n                    'model__nthread':[10]\n\n                    }\n                \nmodel = GridSearchCV(estimator=model,\n                     param_grid=hyper_parameters,\n                     scoring=model_scorer,\n                     verbose=10,\n                     n_jobs= -1,\n                     iid = True,\n                     refit=True,\n                     cv = 2) # Cv = Cross fit split strategy\n\nmodel.fit(xtrain_tfv, ytrain)\n","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true,"id":"jl5sBULDmDfq"},"cell_type":"markdown","source":"## Word Embeddings\n\nWe will use Glove vector embeddings\n\n"},{"metadata":{"hidden":true,"id":"xvhHIja9wHR0","outputId":"15d22290-083c-4fc3-caa8-7c4df6884404","trusted":true},"cell_type":"code","source":"# Load Glove embedding matrix\nimport pickle\nwith open(r'../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', 'rb') as handle:\n    glove_embeddings = pickle.load(handle)\n\nprint('Found %s word vectors.' % len(glove_embeddings))\n","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"B4FIERn0waxO"},"cell_type":"markdown","source":"**Sentence Vectors**\n\nConverting training data into [x1,x2,...,xn] word vectors and then taking average of enitre row to form **Sentence Vectors**"},{"metadata":{"hidden":true,"id":"T6A7zNXNxr6o","trusted":true},"cell_type":"code","source":"# this function creates a normalized vector for the whole sentence\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(glove_embeddings[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v / np.sqrt((v ** 2).sum())","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"445SvPh5xuFX","trusted":true},"cell_type":"code","source":"import nltk\n# nltk.download('stopwords')\n# nltk.download('punkt')\nfrom tqdm import tqdm\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\n# create sentence vectors using the above function for training and validation set\nxtrain_glove_list = [sent2vec(x) for x in xtrain]\nxvalid_glove_list = [sent2vec(x) for x in xvalid]","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"HggU6rvCxzsj","outputId":"12539913-dcde-4965-9b36-b4ac1d10c58f","trusted":true},"cell_type":"code","source":"len(xtrain_glove_list)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"n0Mj03qfyb6y","trusted":true},"cell_type":"code","source":"# Converting into numpy array \nxtrain_glove = np.array(xtrain_glove_list)\nxvalid_glove = np.array(xvalid_glove_list)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"MK6tOj0DDrB0","outputId":"b9bfff4a-3d5b-47dd-eddf-3216806d9b70","trusted":true},"cell_type":"code","source":"type(xtrain_glove)","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true,"hidden":true,"id":"ZTgc-mxu2XGg"},"cell_type":"markdown","source":"#### XGBoost on Vectors"},{"metadata":{"hidden":true,"id":"Y96wwOST2jKT","outputId":"4aeb0214-5b81-4987-e142-15a8f53f4e6a","trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb_model = XGBClassifier(n_estimators=500, max_depth=7, colsample_bytree=0.8,\n                         sumbsample=0.8, n_thread=10, learning_rate=0.1)\n\nxgb_model.fit(xtrain_glove,ytrain)\nxgb_vector_pred = xgb_model.predict_proba(xvalid_glove)\nprint (\"XGBoost Vector logloss: %0.3f \" % multiclass_logloss(yvalid, xgb_vector_pred))","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"Nx0S5UtzA53_"},"cell_type":"markdown","source":"XGBoost doesn't seems to be a good fit here at all !!\n"},{"metadata":{"heading_collapsed":true,"hidden":true,"id":"Ucwv1CS0gXki"},"cell_type":"markdown","source":"#### Naive Bayes on Vectors"},{"metadata":{"hidden":true,"id":"VUlB96ukhFET","outputId":"1b5970a3-7736-4bc5-de7f-be1619383e8e","trusted":false},"cell_type":"code","source":"#nb_model = MultinomialNB(alpha=0.1)\n#nb_model.fit(xtrain_glove,ytrain)\n#nb_vector_pred = nb_model.predict_proba(xvalid_glove)\n#print (\"Naive Bayes Vector logloss: %0.3f \" % multiclass_logloss(yvalid, nb_vector_pred))","execution_count":0,"outputs":[]},{"metadata":{"hidden":true,"id":"VQRrcC3YiMBt"},"cell_type":"markdown","source":"As glove vectors contain negative values, Naive model wont work here .\n\nLets try NMF here as is NonNegative Matrix Factorization"},{"metadata":{"id":"56ICoiyFiaDM"},"cell_type":"markdown","source":"## Neural Networks\n\nApplying Neural network approach to get better results."},{"metadata":{"id":"lPFiT8sDjcrA"},"cell_type":"markdown","source":"Before applying any deep learning models we will do 2 steps:\n1. Make our train_glove data more scaler in order to have values in range of 0-1.\n2. Convert our target values into categorical features."},{"metadata":{"id":"tQq7iRnMkUX0","trusted":true},"cell_type":"code","source":"# Scaler data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nxtrain_glove_sc = sc.fit_transform(xtrain_glove)\nxvalid_glove_sc = sc.transform(xvalid_glove)","execution_count":null,"outputs":[]},{"metadata":{"id":"cQ0DNpTpk_60","outputId":"8595ce99-76ca-4bba-e6a6-f825d9b2dec7","trusted":true},"cell_type":"code","source":"# Numerical encoding target value\nfrom keras.utils import np_utils\nytrain_enc = np_utils.to_categorical(ytrain)\nyvalid_enc = np_utils.to_categorical(yvalid)","execution_count":null,"outputs":[]},{"metadata":{"id":"nbIyQ-VQlaA7","outputId":"5eff8b74-4878-45cf-9024-8cca7032273b","trusted":true},"cell_type":"code","source":"ytrain_enc[:5,:]","execution_count":null,"outputs":[]},{"metadata":{"id":"6IWta9MsmM6o","outputId":"bf42bafe-8ed8-4aa4-cce8-fb32df0c040f","trusted":true},"cell_type":"code","source":"xtrain_glove_sc.shape","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true,"id":"NLk-rXReldKP"},"cell_type":"markdown","source":"### 3 Layers sequential network"},{"metadata":{"hidden":true,"id":"FwwjsaqVl67D","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.normalization import BatchNormalization\n\n\nmodel = Sequential()\n\nmodel.add(Dense(300, input_dim=300, activation='relu')) # First Layer\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(300, activation='relu')) # Second Layer\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\n\n\nmodel.add(Dense(3)) # Final Layer\nmodel.add(Activation('softmax'))\n\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"qxwzX6bpoyIT","outputId":"b7509633-ec3a-475c-cd8b-e65e19502271","trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"Up8HHBTFnC7x","outputId":"4a13e6ce-2853-47d3-896d-5031855c22d8","trusted":true},"cell_type":"code","source":"model.fit(xtrain_glove_sc,ytrain_enc,\n          batch_size=64, epochs=20, verbose=1,use_multiprocessing=True,\n          validation_data=(xvalid_glove_sc,yvalid_enc))","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"id":"grZPS9y3oWzN"},"cell_type":"markdown","source":"Loss of 0.814, can be improved a lot with parameter tunning"},{"metadata":{"id":"18qNcRlWthI8"},"cell_type":"markdown","source":"### LSTM\n\nFor LSTM, either it can be used along with glove embeddings or direcly on the text.\n\nHere, we will fit glove embedding on text and then run LSTM model"},{"metadata":{"id":"AN__RI-fuKnh","trusted":true},"cell_type":"code","source":"from keras.preprocessing import sequence,text\n\ntokenizer = text.Tokenizer(num_words=None)\nmax_len = 70 # max length of sentences we want to keep\n\ntokenizer.fit_on_texts(list(xtrain) + list(xvalid))\n\nxtrain_seq = tokenizer.texts_to_sequences(xtrain) # convert text into numbers\nxvalid_seq = tokenizer.texts_to_sequences(xvalid)","execution_count":null,"outputs":[]},{"metadata":{"id":"agTBh0Tbu9kO","outputId":"0dfc01c1-bc39-48b8-f6e6-86c2e1226ae8","trusted":true},"cell_type":"code","source":"xtrain_seq[1]","execution_count":null,"outputs":[]},{"metadata":{"id":"RqjYHwHQvaWR"},"cell_type":"markdown","source":"Every number represent a word here.\n\nTokenizer has a vocab of a number corros. to every word, that is then fit to texts_to_sequences"},{"metadata":{"id":"X1AiZ7NNvpYo"},"cell_type":"markdown","source":"Senteces can be of different length, some can be small some can be big.\n\nTherefore, \n\n    For bigger sentences we set a sent max-len=70 .\n\n    For smaller sentences we set a extra padding of 0's to make it of length=70"},{"metadata":{"id":"lGR8vjE8wCEG","trusted":true},"cell_type":"code","source":"xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_tokens = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"id":"q6WLTRY-wiFo","outputId":"08c65908-90d0-4f13-b8e7-de5fd0f475d3","trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros( (len(word_tokens)+1,300) )\n\nfor word,i in tqdm(word_tokens.items()):\n  word_vector = glove_embeddings.get(word)\n  if word_vector is not None:\n    embedding_matrix[i] = word_vector\n","execution_count":null,"outputs":[]},{"metadata":{"id":"0PAI9b4vxezQ","trusted":true},"cell_type":"code","source":"from keras.layers.embeddings import Embedding\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers import SpatialDropout1D\nfrom keras.callbacks import EarlyStopping\n\nmodel = Sequential()\n\nmodel.add(Embedding(len(word_tokens)+1,\n                    300,\n                    weights=[embedding_matrix],\n                    input_length=max_len,\n                    trainable=False\n                    ))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, dropout=0.3,recurrent_dropout=0.3))\n\nmodel.add(Dense(1024,activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"9kRrE0du0Tx2","outputId":"5ceb64ee-2166-46e8-e059-f8075afeed04","trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"YwSl9l_Y1C85","outputId":"8a80182e-a433-4e28-e8c2-ff2b0826004c","trusted":true},"cell_type":"code","source":"earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n\nmodel.fit(xtrain_pad,y=ytrain_enc,use_multiprocessing=True ,\n          batch_size=512, epochs=100, verbose=1,\n          validation_data=(xvalid_pad,yvalid_enc),\n          callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{"id":"Z8Hbn9uI1qOf"},"cell_type":"markdown","source":"The loss is quite less = **0.516** then earlier but still not better than Naive bayes on simple train data"},{"metadata":{"id":"xOj6JDzlnzWo"},"cell_type":"markdown","source":"### Bi-Directional LSTM"},{"metadata":{"id":"Z_cnvVJ0nzWo","outputId":"c984ebe2-ce87-479a-a48e-10ef8c642a57","trusted":true},"cell_type":"code","source":"from keras.layers import Bidirectional\n\nmodel = Sequential()\n\nmodel.add(Embedding(len(word_tokens)+1,\n                    300,\n                    weights=[embedding_matrix],\n                    input_length=max_len,\n                    trainable=False\n                    ))\n\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n\nmodel.fit(xtrain_pad,y=ytrain_enc,use_multiprocessing=True ,\n          batch_size=512, epochs=100, verbose=1,\n          validation_data=(xvalid_pad,yvalid_enc),\n          callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{"id":"h8PB8ujknzWp"},"cell_type":"markdown","source":"**0.48**, Best so far"},{"metadata":{"id":"cUHvaOiJqY--"},"cell_type":"markdown","source":"### GRU\nLet's try GRU instead of LSTM\n"},{"metadata":{"id":"wfytYv_Sqh45","outputId":"759e36d7-7114-4d5b-cc7c-d1566e1ca6c5","trusted":true},"cell_type":"code","source":"from keras.layers.recurrent import GRU\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_tokens) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(300, dropout=0.8, recurrent_dropout=0.8, return_sequences=True))\nmodel.add(GRU(300, dropout=0.8, recurrent_dropout=0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n\nmodel.fit(xtrain_pad,y=ytrain_enc,use_multiprocessing=True ,\n          batch_size=512, epochs=100, verbose=1,\n          validation_data=(xvalid_pad,yvalid_enc),\n          callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model shows no learning as val_loss remains constant."},{"metadata":{"id":"E0TgHSBMr6ms","trusted":false},"cell_type":"code","source":"","execution_count":0,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"Spooky_author.ipynb","provenance":[],"collapsed_sections":["xzSDr8-aVJG9","IB-TIgJgW8JK","9n37ONEvjogK","oeCAsHcTVJHI","PoUwSflzVJHN","3LS2d_tKVJHR","cfOdKBq3VJHV","xDxWdLvhVJHb","CZz1KBF0VJHe","Q5ZGc3a3VJHk","37LHAeTpVJHn","pZnu0Ob-VJHx","8yMFV2nwVJH0","XDcS3ecHVJH7","kNyGQ2iknzVW","VCxtnurdVJID","kzlpGf3HYSiz","woI3qV6ggFZd","Qn4WJn_6jwT8","jl5sBULDmDfq","ZTgc-mxu2XGg","Ucwv1CS0gXki","56ICoiyFiaDM"],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (Spyder)","language":"python3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}