{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nimport en_core_web_sm\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense, GRU, Embedding, Bidirectional, Activation\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import SimpleRNN\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import GlobalMaxPooling1D\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class LoadingData():\n            \n    def __init__(self):\n        train_file_path = os.path.join(\"..\",\"input\",\"nlp-benchmarking-data-for-intent-and-entity\",\"benchmarking_data\",\"Train\")\n        validation_file_path = os.path.join(\"..\",\"input\",\"nlp-benchmarking-data-for-intent-and-entity\",\"benchmarking_data\",\"Validate\")\n        category_id = 0\n        self.cat_to_intent = {}\n        self.intent_to_cat = {}\n        \n        for dirname, _, filenames in os.walk(train_file_path):\n            for filename in filenames:\n                file_path = os.path.join(dirname, filename)\n                intent_id = filename.replace(\".json\",\"\")\n                self.cat_to_intent[category_id] = intent_id\n                self.intent_to_cat[intent_id] = category_id\n                category_id+=1\n        print(self.cat_to_intent)\n        print(self.intent_to_cat)\n        '''Training data'''\n        training_data = list() \n        for dirname, _, filenames in os.walk(train_file_path):\n            for filename in filenames:\n                file_path = os.path.join(dirname, filename)\n                intent_id = filename.replace(\".json\",\"\")\n                training_data+=self.make_data_for_intent_from_json(file_path,intent_id,self.intent_to_cat[intent_id])\n        self.train_data_frame = pd.DataFrame(training_data, columns =['query', 'intent','category'])   \n        \n        self.train_data_frame = self.train_data_frame.sample(frac = 1)\n\n\n        \n        '''Validation data'''\n        validation_data = list()    \n        for dirname, _, filenames in os.walk(validation_file_path):\n            for filename in filenames:\n                file_path = os.path.join(dirname, filename)\n                intent_id = filename.replace(\".json\",\"\")\n                validation_data +=self.make_data_for_intent_from_json(file_path,intent_id,self.intent_to_cat[intent_id])                \n        self.validation_data_frame = pd.DataFrame(validation_data, columns =['query', 'intent','category'])\n\n        self.validation_data_frame = self.validation_data_frame.sample(frac = 1)\n        \n        \n    def make_data_for_intent_from_json(self,json_file,intent_id,cat):\n        json_d = json.load(open(json_file))         \n        \n        json_dict = json_d[intent_id]\n\n        sent_list = list()\n        for i in json_dict:\n            each_list = i['data']\n            sent =\"\"\n            for i in each_list:\n                sent = sent + i['text']+ \" \"\n            sent =sent[:-1]\n            for i in range(3):\n                sent = sent.replace(\"  \",\" \")\n            sent_list.append((sent,intent_id,cat))\n        return sent_list\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"load_data_obj = LoadingData()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"load_data_obj.train_data_frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"load_data_obj.validation_data_frame.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Preprocessing():\n    def __init__(self):\n        self.x_train = None\n        self.y_train = None\n        self.x_valid = None\n        self.y_valid = None\n        self.spacy_model = en_core_web_sm.load()\n        self.tokenizer = None\n\n    def createData(self):\n        self.tokenizer = Tokenizer(num_words=None)\n        self.max_len = 50\n        self.x_train, self.x_valid, self.y_train, self.y_valid = train_test_split(load_data_obj.train_data_frame['query'].tolist(),load_data_obj.train_data_frame['category'].tolist(),test_size=0.1)\n        self.tokenizer.fit_on_texts(list(self.x_train) + list(self.x_valid))\n        self.x_train = self.tokenizer.texts_to_sequences(self.x_train)\n        self.x_valid = self.tokenizer.texts_to_sequences(self.x_valid)\n\n        #zero pad the sequences\n        self.x_train = pad_sequences(self.x_train, maxlen=self.max_len)\n        self.x_valid = pad_sequences(self.x_valid, maxlen=self.max_len)\n        self.y_train = to_categorical(self.y_train)\n        self.y_valid = to_categorical(self.y_valid)\n        self.word_index = self.tokenizer.word_index\n        \n    def getSpacyEmbeddings(self,sentneces):\n        sentences_vectors = list()\n        for item in sentneces:\n            query_vec = self.spacy_model(item) \n            sentences_vectors.append(query_vec.vector)\n        return sentences_vectors\n    \n    \n    \n    \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_obj = Preprocessing()\npreprocess_obj.createData()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_obj.y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_obj.y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DesignModel():\n    def __init__(self):\n        self.model = None\n        self.x_train = preprocess_obj.x_train\n        self.y_train = preprocess_obj.y_train\n        self.x_valid = preprocess_obj.x_valid\n        self.y_valid = preprocess_obj.y_valid\n        \n    def simple_rnn(self):\n        self.model = Sequential()\n        self.model.add(Embedding(len(preprocess_obj.word_index) + 1,100,input_length=preprocess_obj.max_len))\n        self.model.add(SimpleRNN(100))\n        self.model.add(Dense(len(load_data_obj.cat_to_intent), activation='sigmoid'))\n        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n        \n        \n    def model_train(self,batch_size,num_epoch):\n        print(\"Fitting to model\")\n        self.model.fit(self.x_train, self.y_train, batch_size=batch_size, epochs=num_epoch, validation_data=[self.x_valid, self.y_valid])\n        print(\"Model Training complete.\")\n\n    def save_model(self,model,model_name):    \n        self.model.save(\"intent_models/\"+model_name+\".h5\")\n        print(\"Model saved to Model folder.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_obj = DesignModel()\nmodel_obj.simple_rnn()\nmodel_obj.model_train(64,5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Evaluation():\n    def get_accuracy(self,actuals, predictions):\n        acc = accuracy_score(actuals, predictions)\n        return acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Prediction():\n    def __init__(self):\n        self.model = model_obj.model\n        self.tokenizer = preprocess_obj.tokenizer\n        self.max_len = preprocess_obj.max_len\n        \n    def predict_validation(self):\n        self.xtest = load_data_obj.validation_data_frame['query'].tolist()\n        self.ytest = load_data_obj.validation_data_frame['category'].tolist()\n        self.xtest = self.tokenizer.texts_to_sequences(self.xtest)\n        self.xtest = pad_sequences(self.xtest, maxlen=self.max_len)\n        self.ypred = self.model.predict(self.xtest)\n        self.ypred = [np.argmax(item) for item in self.ypred]\n    \n    def predict(self,query):\n        query_seq = self.tokenizer.texts_to_sequences([query])\n        query_pad = pad_sequences(query_seq, maxlen=self.max_len)\n        pred = self.model.predict(query_pad)\n        pred = np.argmax(pred)\n        result = load_data_obj.cat_to_intent[pred]\n        return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_obj = Prediction()\npred_obj.predict_validation()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"querylist = [\n    'rate The Gift: Imagination and the Erotic Life of Property five stars',\n     'table for Breadline Cafe in Minnesota next friday',\n     'Will it be hot at 13:19 in De Funiak Springs Serbia and Montenegro ?',\n     'Play some sixties songs on Google Music',\n     'rate this textbook four out of 6']\nfor query in querylist:\n    result = pred_obj.predict(query)\n    print(\"Intent: \"+str(result)+\"\\tQuery: \"+str(query))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_obj = Evaluation()\nacc = eval_obj.get_accuracy(pred_obj.ytest,pred_obj.ypred)\nprint(\"Auc: {:.2%}\".format(acc))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}