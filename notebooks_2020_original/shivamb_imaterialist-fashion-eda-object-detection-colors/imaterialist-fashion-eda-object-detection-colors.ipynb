{"cells":[{"metadata":{"_cell_guid":"5c2d4e50-6a4a-4d40-b2f4-5eb2b31a2b2a","_uuid":"ef527f58d578d6a83a361634acf38119b31f77c5"},"cell_type":"markdown","source":"# Extensive EDA with Object Detection and Color Analysis\n\nThis notebook contains the exploration of iMaterialist Challenge (Fashion) at FGVC5 [dataset](https://www.kaggle.com/c/imaterialist-challenge-fashion-2018)\n\n**Contents**\n\n**1. Descriptive Statistics**\n    - Counts of Images and Labels\n    - Top Labels in the dataset\n    - Most Common Co-occuring Labels\n    - Images with maxium Labels\n    - Images with single Label\n**2. Colors Used in the Images**\n    - Top Average Color of the images\n    - Dominant Colors present in the images\n    - Common Color Palletes\n**3. Object Detection**\n    - Top Colors Detected in the images\n    - Top Objects Detected in the images"},{"metadata":{"_cell_guid":"2b988610-89eb-4d0f-a32e-4460252360c7","_uuid":"f1f8b429f61feb7c2425f4ce78aac2d3a9fabc16"},"cell_type":"markdown","source":"## Dataset Preparation "},{"metadata":{"_cell_guid":"7973e793-46ce-47e0-bab6-4c0652dc28fa","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"bcd29715fcb4f4d134ad0314411e054e4487ec59","trusted":true},"cell_type":"code","source":"from IPython.core.display import HTML\nfrom IPython.display import Image\nfrom collections import Counter\nimport pandas as pd \nimport json\n\n\nfrom plotly.offline import init_notebook_mode, iplot\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\nfrom plotly import tools\nimport seaborn as sns\nfrom PIL import Image\n\nimport tensorflow as tf\nimport numpy as np\n\ninit_notebook_mode(connected=True)\n%matplotlib inline ","execution_count":61,"outputs":[]},{"metadata":{"_cell_guid":"1a76c5a6-b49e-47eb-a12a-37b7a8c3e137","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"1b8ee699cdf9d40b61a95a7fc6000e9687d48a51","collapsed":true,"trusted":true},"cell_type":"code","source":"## read the dataset \n\npath = '../input/imaterialist-challenge-fashion-2018/train.json'\n\ninp = open(path).read()\ninp = json.loads(inp)","execution_count":62,"outputs":[]},{"metadata":{"_cell_guid":"6b2e0af3-ffdf-4b91-999f-fd1267909a0a","_uuid":"c7452f7f723674521eb44bf4b99e00c9def6b6be"},"cell_type":"markdown","source":"## 1. Descriptive Statistics\n\n## 1.1 Total Images and Total Labels Present in the dataset"},{"metadata":{"_cell_guid":"9126a503-7931-471c-b1a9-83574aa99ee9","_kg_hide-input":true,"_uuid":"443c50e2b95f2ca514b6a389b3bc4bcf604e7a5a","trusted":true},"cell_type":"code","source":"# how many images \ntotal_images = len(inp['images'])\n\n# how many labels \nall_annotations = []\nfor each in inp['annotations']:\n    all_annotations.extend(each['labelId'])\ntotal_labels = len(set(all_annotations))\n\nprint (\"Total Images in the dataset: \", total_images)\nprint (\"Total Labels in the dataset: \", total_labels)","execution_count":63,"outputs":[]},{"metadata":{"_cell_guid":"fd59fd68-c93f-45c9-b2aa-30cb15b1e5d8","_uuid":"1f579316bda0134876400b8d0da547cfb4ae7fc4"},"cell_type":"markdown","source":"## 1.2 Top Labels present in the dataset"},{"metadata":{"_cell_guid":"e33a7250-839d-4d30-9166-65bf77a33af0","_kg_hide-input":true,"_uuid":"b4648f100090276d7aa525fee9c4dc9da5f0e056","scrolled":false,"trusted":true},"cell_type":"code","source":"# Top Labels in the dataset\nlabel_dist = Counter(all_annotations)\n\nxvalues = list(label_dist.keys())\nyvalues = list(label_dist.values())\n\ntrace1 = go.Bar(x=xvalues, y=yvalues, opacity=0.8, name=\"year count\", marker=dict(color='rgba(20, 20, 20, 1)'))\nlayout = dict(height=400, title='Distribution of different labels in the dataset', legend=dict(orientation=\"h\"));\n\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","execution_count":64,"outputs":[]},{"metadata":{"_cell_guid":"ee7ff814-a588-4651-ad4c-41d778ce11ab","_kg_hide-input":true,"_uuid":"cdc8029332c700bc30ee19a87224f0c5ed43744a","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_images_for_labels(labellist):\n    image_ids = []\n    for each in inp['annotations']:\n        if all(x in each['labelId'] for x in labellist):\n            image_ids.append(each['imageId'])\n            if len(image_ids) == 2:\n                break\n    image_urls = []\n    for each in inp['images']:\n        if each['imageId'] in image_ids:\n            image_urls.append(each['url'])\n    return image_urls","execution_count":65,"outputs":[]},{"metadata":{"_cell_guid":"b00b8167-9a2c-4c00-974a-43dc734e56bd","_kg_hide-input":true,"_uuid":"8516dc1796f7b979aaac060fedf70cc3863656f6","trusted":true},"cell_type":"code","source":"# most common labels \n\ntemps = label_dist.most_common(10)\nlabels = [\"Label: \"+str(x[0]) for x in temps]\nvalues = [x[1] for x in temps]\n\ntrace1 = go.Bar(x=labels, y=values, opacity=0.7, name=\"year count\", marker=dict(color='rgba(120, 120, 120, 0.8)'))\nlayout = dict(height=400, title='Top 10 Labels in the dataset', legend=dict(orientation=\"h\"));\n\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","execution_count":66,"outputs":[]},{"metadata":{"_cell_guid":"9c93a82a-bcbf-42e5-8104-85c4579c2256","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"b54dccf0df42f58927e22e6ddb59b1adfce2a6b9","trusted":true},"cell_type":"code","source":"for labelpair in labels:\n    labelpr = labelpair.replace(\"Label: \",\"\").split(\"-\")\n    imgs = get_images_for_labels(labelpr)\n        \n#     headerhtml = \"\"\"<b>\"\"\"+ str(labelpair) +\"\"\"</b><br>\"\"\"\n#     display(HTML(headerhtml))\n#     imghtml = \"\"\n#     for img in imgs:\n#         imghtml += \"<img src=\"+img+\" width=200px; height=200px; style='float:left'>\"\n#     display(HTML(imghtml))","execution_count":67,"outputs":[]},{"metadata":{"_cell_guid":"23160b0b-cc0b-4120-82aa-1e235623921f","_uuid":"eb7366687f30cc4ea04cc19490a8ca0545687e16"},"cell_type":"markdown","source":"## 1.3 Most Common Co-Occuring Labels in the dataset"},{"metadata":{"_cell_guid":"2a73cf6c-c8bc-454f-917d-bc6d8c40840b","_kg_hide-input":true,"_uuid":"a842fbc6fa760de7d63149861a67abf778368dc1","collapsed":true,"trusted":true},"cell_type":"code","source":"# Most Commonly Occuring Labels \n\ndef cartesian_reduct(alist):\n    results = []\n    for x in alist:\n        for y in alist:\n            if x == y:\n                continue\n            srtd = sorted([int(x),int(y)])\n            srtd = \" AND \".join([str(x) for x in srtd])\n            results.append(srtd)\n    return results \n\nco_occurance = []\nfor i, each in enumerate(inp['annotations']):\n    prods = cartesian_reduct(each['labelId'])\n    co_occurance.extend(prods)","execution_count":68,"outputs":[]},{"metadata":{"_cell_guid":"4e2115cb-42b8-493d-a778-e69f6b2957f0","_kg_hide-input":true,"_uuid":"52135727afb5e93b63e0b096eed0bc4addb0ad55","trusted":true},"cell_type":"code","source":"coocur = Counter(co_occurance).most_common(10)\nlabels = list(reversed([\"Label: \"+str(x[0]) for x in coocur]))\nvalues = list(reversed([x[1] for x in coocur]))\n\ntrace1 = go.Bar(x=values, y=labels, opacity=0.7, orientation=\"h\", name=\"year count\", marker=dict(color='rgba(130, 130, 230, 0.8)'))\nlayout = dict(height=400, title='Most Common Co-Occuring Labels in the dataset', legend=dict(orientation=\"h\"));\n\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","execution_count":69,"outputs":[]},{"metadata":{"_cell_guid":"af51d714-c1d6-4cac-a0ae-f43541fc0717","_uuid":"e126017764613c9f8f71252a0635c483abf62d51","collapsed":true},"cell_type":"markdown","source":"## 1.4 Images with Maximum Labels"},{"metadata":{"_cell_guid":"3ca70481-f224-4489-95e5-edb36f3722ad","_kg_hide-input":true,"_uuid":"1065f594debe3214532502ab9eb57bf938f7f378","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_image_url(imgid):\n    for each in inp['images']:\n        if each['imageId'] == imgid:\n            return each['url']\n\nsrtedlist = sorted(inp['annotations'], key=lambda d: len(d['labelId']), reverse=True)","execution_count":70,"outputs":[]},{"metadata":{"_cell_guid":"7fa5a40e-1445-4e53-a7f2-b0f3fc577423","_kg_hide-input":true,"_kg_hide-output":false,"_uuid":"7dd171bd3b27bf5a64ac05cdae1f7410f54718ac","trusted":true},"cell_type":"code","source":"for img in srtedlist[:5]:\n    iurl = get_image_url(img['imageId'])  \n    labelpair = \", \".join(img['labelId'])\n#     headerhtml = \n#     display(HTML(headerhtml))\n    imghtml = \"\"\"<b> Labels: \"\"\"+ str(labelpair) +\"\"\"</b><br>\"\"\" + \"<img src=\"+iurl+\" width=200px; style='float:left'>\"\n    display(HTML(imghtml))","execution_count":71,"outputs":[]},{"metadata":{"_cell_guid":"34185100-fc53-400c-b576-5f6f1d685377","_uuid":"5cd086ea03137d46973c8f85db823bdd1cdfa4e2"},"cell_type":"markdown","source":"## 1.5 Images with Single Label"},{"metadata":{"_cell_guid":"e63b3fe2-9dbc-4da4-967e-98ae058c6985","_kg_hide-input":true,"_kg_hide-output":false,"_uuid":"fe2cc9aadffba1e94557bbece36200de69531d5f","trusted":true},"cell_type":"code","source":"# How many images are labelled with only 1 label \n\nfor img in srtedlist[-5:]:\n    iurl = get_image_url(img['imageId'])  \n    labelpair = \", \".join(img['labelId'])\n#     headerhtml = \n#     display(HTML(headerhtml))\n    imghtml = \"\"\"<b> Label: \"\"\"+ str(labelpair) +\"\"\"</b><br>\"\"\" + \"<img src=\"+iurl+\" width=200px; height=200px; style='float:left'>\"\n    display(HTML(imghtml))","execution_count":72,"outputs":[]},{"metadata":{"_cell_guid":"9a44f49f-f7cf-460e-9fcf-49919074110d","_uuid":"8b1a31c52d386d31bfd26005cf0f014fde9689a3"},"cell_type":"markdown","source":"## 2. Colors Used in the Images \n\n## 2.1 Common Average Color of the Images "},{"metadata":{"_cell_guid":"5a71f987-f73d-40c8-898f-3937656006b5","_kg_hide-input":true,"_uuid":"32e37de659f609957b76bc9b535f1133c7d37d4f","collapsed":true,"trusted":true},"cell_type":"code","source":"import urllib\nfrom io import StringIO\n\ndef compute_average_image_color(img):\n    width, height = img.size\n    count, r_total, g_total, b_total = 0, 0, 0, 0\n    for x in range(0, width):\n        for y in range(0, height):\n            r, g, b = img.getpixel((x,y))\n            r_total += r\n            g_total += g\n            b_total += b\n            count += 1\n    return (r_total/count, g_total/count, b_total/count)","execution_count":14,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true,"trusted":true,"_uuid":"a73b8a640d265bce5db76499cec45dc5611950c8"},"cell_type":"code","source":"import os \nimgpath = '../input/sampleimages/top_images/top_images/'\nread_from_disk = True\n\nif read_from_disk:\n    srtedlist = os.listdir(imgpath)\nelse:\n    srtedlist = sorted(inp['annotations'], key=lambda d: len(d['labelId']), reverse=True)","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"5690268a-238c-4292-b509-17503a54cbbb","_kg_hide-input":true,"_uuid":"a237f9eb532504393411a8d6e480c993d3e8715d","trusted":true,"collapsed":true},"cell_type":"code","source":"average_colors = {}\nfor img in srtedlist[:10]:\n    if read_from_disk:\n        img = Image.open(imgpath + img)\n    else:\n        iurli = get_image_url(img['imageId'])\n\n        ## download the images \n        # filename = iurli.split(\"/\")[-1].split(\"-large\")[0]\n        # urllib.urlretrieve(iurli, \"top_images/\"+filename)\n        \n        file = cStringIO.StringIO(urllib.urlopen(iurli).read())\n        img = Image.open(img)\n           \n    average_color = compute_average_image_color(img)\n    if average_color not in average_colors:\n        average_colors[average_color] = 0\n    average_colors[average_color] += 1","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"30f53b7f-63c2-412c-987d-be3b5db06191","_kg_hide-input":true,"_uuid":"14a19c795ac1bd2303d4d94dc56cc71587d7ddca","trusted":true},"cell_type":"code","source":"for average_color in average_colors:\n    average_color1 = (int(average_color[0]),int(average_color[1]),int(average_color[2]))\n    image_url = \"<span style='display:inline-block; min-width:200px; background-color:rgb\"+str(average_color1)+\";padding:10px 10px;'>\"+str(average_color1)+\"</span>\"\n#     print (image_url)\n    display(HTML(image_url))","execution_count":59,"outputs":[]},{"metadata":{"_cell_guid":"2bb951c6-d66c-40f4-9bcd-03ba365bd2e1","_uuid":"bb2ac3a1899c1ba90f672834d28b3247e2660a70"},"cell_type":"markdown","source":"## 2.2 Most Dominant Colors Used in the Images "},{"metadata":{"_cell_guid":"975a6d0c-0ffe-46ed-af36-37ca41e44b95","_kg_hide-input":true,"_uuid":"c933fb6e9c02d906c37feb3472316755266f160a","trusted":true},"cell_type":"code","source":"## top used colors in images \nfrom colorthief import ColorThief\nimport urllib \n\npallets = []\nfor img in srtedlist[:10]:\n    \n    if read_from_disk:\n        img = imgpath + img\n    else:\n        iurli = get_image_url(img['imageId'])\n\n        ## download the images \n        # filename = iurli.split(\"/\")[-1].split(\"-large\")[0]\n        # urllib.urlretrieve(iurli, \"top_images/\"+filename)\n        \n        file = cStringIO.StringIO(urllib.urlopen(iurli).read())\n        img = Image.open(img)\n\n    color_thief = ColorThief(img)\n    dominant_color = color_thief.get_color(quality=1)\n    \n    image_url = \"<span style='display:inline-block; min-width:200px; background-color:rgb\"+str(dominant_color)+\";padding:10px 10px;'>\"+str(dominant_color)+\"</span>\"\n    display(HTML(image_url))\n    \n    palette = color_thief.get_palette(color_count=6)\n    pallets.append(palette)\n","execution_count":39,"outputs":[]},{"metadata":{"_cell_guid":"fb146bdb-ca56-44ad-8145-21302ecb513c","_uuid":"bb2f0c409ac9cbc9e40e3576e7c02c6c7a37b32b"},"cell_type":"markdown","source":"## 2.3 Common Color Pallets of the Images"},{"metadata":{"_cell_guid":"b8d21ac0-9be8-4e48-abec-82337428e039","_kg_hide-input":true,"_uuid":"58519381c0c120993b4d639eb4e759b810c85e43","trusted":true},"cell_type":"code","source":"for pallet in pallets:\n    img_url = \"\"\n    for pall in pallet:\n        img_url += \"<span style='background-color:rgb\"+str(pall)+\";padding:20px 10px;'>\"+str(pall)+\"</span>\"\n    img_url += \"<br>\"\n    display(HTML(img_url))\n    print \n    ","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"7db10b16-9dae-4abd-bba1-71a775684848","_uuid":"7b9aa71f07bddb15fddfd6bfcaaa52adbd9771a7"},"cell_type":"markdown","source":"## 3. Object Detection using TensorFlow API \n\n\nI have used tensorflow API for object detection the code is given in the following cell.\n\n\n![title](https://github.com/tensorflow/models/raw/master/research/object_detection/g3doc/img/kites_detections_output.jpg)"},{"metadata":{"_cell_guid":"4223537b-7a6e-4369-bbf3-5a46acc5ca09","_kg_hide-input":false,"_uuid":"2221c4e57ef7a494df5b1a61e8b58d8046b9d28f","collapsed":true,"trusted":true},"cell_type":"code","source":"### UNCOMMENT THE FOLLOWING LINE AFTER DOWNLOADING THE UTILS FROM THIS LINK - https://github.com/tensorflow/models/tree/master/research/object_detection/utils\n\n# from utils import label_map_util\n\ndef DOWNLOAD_MODELS():\n    MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n    MODEL_FILE = MODEL_NAME + '.tar.gz'\n    DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n    PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n    PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n    \n    opener = urllib.request.URLopener()\n    opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n    tar_file = tarfile.open(MODEL_FILE)\n    for file in tar_file.getmembers():\n        file_name = os.path.basename(file.name)\n        if 'frozen_inference_graph.pb' in file_name:\n            tar_file.extract(file, os.getcwd())\n\ndef detect_object(filename):\n\n    def img2array(img):\n        (img_width, img_height) = img.size\n        return np.array(img.getdata()).reshape((img_width, img_height, 3)).astype(np.uint8)\n\n    categories, probabilities = [], []\n    PATH_TO_CKPT = 'frozen_inference_graph.pb'\n    PATH_TO_LABELS = 'mscoco_label_map.pbtxt'\n    detection_graph = tf.Graph()\n    with detection_graph.as_default():\n        od_graph_def = tf.GraphDef()\n        with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n            serialized_graph = fid.read()\n            od_graph_def.ParseFromString(serialized_graph)\n            tf.import_graph_def(od_graph_def, name='')\n\n\n    label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n    categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=100, use_display_name=True)\n    category_index = label_map_util.create_category_index(categories)\n\n    with detection_graph.as_default():\n        with tf.Session(graph=detection_graph) as sess:\n            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n            detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n            detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n            detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n\n            image = Image.open(filename)\n            image_np = img2array(image)\n            image_np_expanded = np.expand_dims(image_np, axis=0)\n            (boxes, scores, classes, num) = sess.run([detection_boxes, detection_scores, detection_classes, num_detections], feed_dict={image_tensor: image_np_expanded})\n            for index,value in enumerate(classes[0]):\n                if float(scores[0,index]) > 0.1:\n                    temp =  category_index.get(value)['name']\n                    if temp not in categories:\n                        categories.append(temp)\n                        probabilities.append(scores[0,index])\n    return categories, probabilities\n","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"f03c9c0b-873d-41f0-8355-452c71043efc","_kg_hide-input":true,"_uuid":"47621e2304534f75d151e16182c2b16dad5c7beb","collapsed":true,"trusted":true},"cell_type":"code","source":"## UNCOMMENT THE FOLLOWING LINES TO RUN THE OBJECT DETECTION MODEL AND SAVE THE RESULTS \n\n# for img in srtedlist[:10]:\n#     iurli = get_image_url(img['imageId'])\n    \n#     file = cStringIO.StringIO(urllib.urlopen(iurli).read())\n#     objects = detect_object(file)","execution_count":28,"outputs":[]},{"metadata":{"_cell_guid":"58b0a91b-8061-4d7e-abd2-28c91bffccfe","_uuid":"7ae283e11fb78106db7724bf28d1b78124d5d7d3"},"cell_type":"markdown","source":"Reference: [TensorFlow Object Detection Notebook](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb)\n\nPre-Trained Models Reference: [PreTrained Models](https://github.com/tensorflow/models/tree/676a4f70c20020ed41b533e0c331f115eeffe9a3/research/object_detection)\n\nLink to download the Utils: https://github.com/tensorflow/models/tree/master/research/object_detection/utils"},{"metadata":{"_cell_guid":"2917cbc0-941b-4732-aadd-84e8a8b2dba6","_uuid":"5c6e3c854babec0caa3858d9bb5b2101f71f49c8"},"cell_type":"markdown","source":"Since it would have taken a lot of time on kaggle kernals, I have pre-computed the objects in my local machine."},{"metadata":{"_cell_guid":"bcff6d5e-75b7-48e9-955c-c5308f7e226c","_kg_hide-input":true,"_uuid":"3b91ab41a99253c5f65fc77511789eadfad32508","collapsed":true,"trusted":true},"cell_type":"code","source":"objpath = '../input/precomputedobjects/objects.txt'\n\nobjs = open(objpath).read().strip().split(\"\\n\")\ncolors = [_ for _ in objs if \"color\" in _]\nnon_colors = [_ for _ in objs if \"color\" not in _]","execution_count":29,"outputs":[]},{"metadata":{"_cell_guid":"53fb49db-aa92-46e0-af08-70e70eb05592","_uuid":"555c23073483ea9cbe4da0e9e677742fce55ba87"},"cell_type":"markdown","source":"## 3.1 Top Colors detected using Object detection "},{"metadata":{"_cell_guid":"a676f85c-1d81-43a0-9eba-a51ac94e44c3","_kg_hide-input":true,"_uuid":"7668da79698d18ba6b1a20027c2a8a6c215c3452","trusted":true},"cell_type":"code","source":"txt = \"\"\nfor i, color in enumerate(Counter(colors).most_common(100)):\n    txt += (color[0] + \" \")\ntxt = txt.replace(\"color\", \" \")\nwordcloud = WordCloud(max_font_size=50, width=600, height=300, background_color='white').generate(txt)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.title(\"Top Colors Used in the images\", fontsize=15)\nplt.axis(\"off\")\nplt.show() ","execution_count":30,"outputs":[]},{"metadata":{"_cell_guid":"22c818bb-376e-4249-a349-ec01b6c20222","_uuid":"26afc2fc9b893404ad56a55ebb0361f5fdf2ed7d"},"cell_type":"markdown","source":"## 3.2 Top Objects Detected in the images"},{"metadata":{"_cell_guid":"a93578c5-8d58-4c77-aeb7-87b4fa0ec4bf","_kg_hide-input":true,"_uuid":"f2d1296b9a522aeaf947df93e3e0e40f75813b6d","trusted":true},"cell_type":"code","source":"txt = \"\"\nfor i, color in enumerate(Counter(non_colors).most_common(100)):\n    txt += color[0]+\" \"\nwordcloud = WordCloud(max_font_size=50, width=600, height=300).generate(txt)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.title(\"Top Objects Detected in the images\", fontsize=15)\nplt.axis(\"off\")\nplt.show() ","execution_count":31,"outputs":[]},{"metadata":{"_cell_guid":"63c5788e-70ae-4401-bd89-473cdb5e605a","_uuid":"bc2904e7890dc7571c85eec2dae6f4191188f290"},"cell_type":"markdown","source":"Thanks for viewing the notebook. Hope You liked it, if liked it please upvote. "},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"523098c61a75d99a562de18d1c8a6eaf57366346"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}