{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport string\nimport re\nimport numpy as np\nimport pandas as pd\nimport pickle\n#import lda\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\")\n\nfrom nltk.stem.porter import *\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import stop_words\n\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n%matplotlib inline\n\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook\n#from bokeh.transform import factor_cmap\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nlogging.getLogger(\"lda\").setLevel(logging.WARNING)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nlogging.getLogger(\"lda\").setLevel(logging.WARNING)\n\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls ../input/mercari-price-suggestion-challenge/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_csv(f'../input/mercari/train.tsv', sep='\\t') #sep='\\t'でタブ区切り\ntest = pd.read_csv(f'../input/mercari/test.tsv', sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.price.describe() #eの横の数字だけ10倍","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **price**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntrain['price'].hist(bins = 30, range=(0, 250))\nplt.xlabel('price')\nplt.ylabel('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#自然対数\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n(train['price'] + 1).apply(np.log).hist(bins = 30, range=(0, 7))\nplt.xlabel('log(price + 1)')\nplt.ylabel('count')\nplt.xlim(1, 7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train.loc[train['shipping'] == 0, 'price'].dropna(),\n        range=(0, 250), bins=30, alpha=0.5, label='0')\nplt.hist(train.loc[train['shipping'] == 1, 'price'].dropna(),\n        range=(0, 250), bins=30, alpha=0.5, label='1')\nplt.xlabel('price')\nplt.ylabel('count')\nplt.legend(title='shipping')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.loc[train['shipping'] == 0, 'price'].mean()) #送料購入者\nprint(train.loc[train['shipping'] == 1, 'price'].mean()) #送料出品者","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#自然対数\nplt.hist((train.loc[train['shipping'] == 0, 'price'] + 1).apply(np.log).dropna(),\n        range=(1, 7), bins=30, alpha=0.5, label='0')\nplt.hist((train.loc[train['shipping'] == 1, 'price'] + 1).apply(np.log).dropna(),\n        range=(1, 7), bins=30, alpha=0.5, label='1')\nplt.xlabel('log(price + 1)')\nplt.ylabel('count')\nplt.legend(title='shipping')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print((train.loc[train['shipping'] == 0, 'price'] + 1).apply(np.log).mean()) #送料購入者\nprint((train.loc[train['shipping'] == 1, 'price'] + 1).apply(np.log).mean()) #送料出品者","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **category_name**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#The number of category_name\ntrain['category_name'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#top 5\ntrain['category_name'].value_counts()[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#count null.\ntrain['category_name'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#survied details of categories\ndef split_cat(text):\n    try:\n        return text.split(\"/\")\n    except:\n        return (\"No data\", \"No data\", \"No data\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#edit category_name\ntrain['general_cat'], train['subcat1'], train['subcat2'] = zip(*train['category_name'].apply(lambda x : split_cat(x)))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#repeat the same step for the test set\ntest['general_cat'], test['subcat1'], test['subcat2'] = zip(*test['category_name'].apply(lambda x : split_cat(x)))\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The number of subcat1\ntrain['subcat1'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['general_cat'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['subcat1'].value_counts()[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The number of subcat2\ntrain['subcat2'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['subcat2'].value_counts()[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Woman's products are popular."},{"metadata":{"trusted":true},"cell_type":"code","source":"#valuesにすることで配列に変換\nx = train['general_cat'].value_counts().index.values.astype(str) #大カテゴリーの配列\ny = train['general_cat'].value_counts().values #大カテゴリー別件数の配列\nunique_pct = [(\"%.2f\"%(v*100))+\"%\"for v in (y/len(train))] #各general_catラベルの出現率","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.offline as py\nimport plotly.graph_objs as go\n\ntrace1 = go.Bar(x=x, y=y, text=unique_pct)\nlayout = dict(title = 'Number of Items by general_cat',\n             yaxis = dict(title = 'Count'),\n             xaxis = dict(title = 'general_cat'))\nfig = dict(data=[trace1], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#上から15件までのサブカテゴリ1を同様に出現率計算\nx = train['subcat1'].value_counts().index.values.astype('str')[:15]\ny = train['subcat1'].value_counts().values[:15]\n\nsubcat1_pct = [(\"%.2f\"%(v*100))+\"%\" for v in (y/(len(train)))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace1 = go.Bar(x=x, y=y, text=subcat1_pct,\n               marker=dict(\n               color = y,colorscale='Portland', showscale=True,\n               reversescale = False\n               ))\nlayout = dict(title='Number of Items by subcat1(~15)',\n             yaxis = dict(title='Count'),\n             xaxis = dict(title='subcat1'))\nfig = dict(data=[trace1], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Category price distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"#とりあえず箱ひげ図作る\ngeneral_cats = train['general_cat'].unique()\ngeneral_cats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#大カテゴリー別の値段をリストでまとめる(これが横軸)\nx = [train.loc[train['general_cat'] == cat, 'price'] for cat in general_cats] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#priceはlog(price + 1)で正規化する\ndata = [go.Box(x=np.log(x[i] + 1), name=general_cats[i]) for i in range(len(general_cats))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layout = dict(title='Price distribution by general_cat',\n             yaxis = dict(title='Category'),\n             xaxis = dict(title='log(price + 1)'))\nfig = dict(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Brand Name\nほとんどが欠損値であることに注意."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['brand_name'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train['brand_name'].value_counts().index.values.astype('str')[:10]\ny = train['brand_name'].value_counts().values[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace1 = go.Bar(x=x, y=y, \n                 marker=dict(\n                 color = y,colorscale='Portland',showscale=True,\n                 reversescale = False\n                 ))\nlayout = dict(title= 'Top 10 Brand by Number of Items',\n               yaxis = dict(title='Brand Name'),\n               xaxis = dict(title='Count'))\nfig=dict(data=[trace1], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Item Description (アイテム説明)"},{"metadata":{},"cell_type":"markdown","source":"re.escape\npattern 中の特殊文字をエスケープします。これは正規表現メタ文字を含みうる任意のリテラル文字列にマッチしたい時に便利。\nstring.punctuation: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\nfrom sklearn.feature_extraction import stop_words\n\n#item_decriptionに含まれる単語長を調べる(字句解析は後ほど)\ndef wordCount(text):\n    #小文字に変換して正規表現を取り除く\n    #try:\n        text = str(text).lower()\n        #正規表現パターン文字列をコンパイルして正規表現パターンオブジェクトを作成\n        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') #[!\"#$%&'()*+,-./:;<=>?@[]^_`{|}~0-9\\\\r\\\\t\\\\n]\n        #text内の特殊文字を半角空白で置換\n        txt = regex.sub(\" \", text)\n        #tokenize\n        #remove words in stop words\n        words = [w for w in txt.split(\" \") \\\n                if not w in stop_words.ENGLISH_STOP_WORDS and len(w)>3] #内包表記(処理, ループ, 条件)\n        #wordsには文字数3以上かつstop_words以外がリストとして格納されている\n        return len(words)\n    #except:\n        #return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ストップワードを取り除いた後の単語の総数をカウントした列を追加\ntrain['desc_len'] = train['item_description'].apply(lambda x : wordCount(x))\ntest['desc_len'] = test['item_description'].apply(lambda x : wordCount(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train.groupby('desc_len')['price'].mean().reset_index()\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace1 = go.Scatter(\n    x = df['desc_len'],\n    y = np.log(df['price']), #dfで平均価格0のものが無いため,log(price)で良い(真数を+1しなくていい)\n    mode = 'lines+markers',\n    name = 'lines+markers'\n)\nlayout = dict(title= 'Average log(price) by description length',\n             yaxis = dict(title='Average log(price)'),\n             xaxis = dict(title='Description length'))\nfig = dict(data=[trace1], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#item_description内の欠損値を除く\ntrain = train[pd.notnull(train['item_description'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Pre-processing:  tokenization**\n\nMost of the time, the first steps of an NLP project is to **\"tokenize\"** your documents, which main purpose is to normalize our texts. The three fundamental stages will usually include: \n* item_descriptionをテキストとして文字列に変換し,それをtokenizerで意味のある単語を抽出するためにトークン単位で分割する.\n* 句読点とストップワードを除く\n* トークンを小文字変換する\n* 3つ以上のトークンがあるdescriptionだけを考慮する."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\n\nstopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#(準備)リストの演算\na = []\na += 't'\na += 'e'\na += 's'\na += 't'\na","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem.porter import *\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom janome.tokenizer import Tokenizer\n\nstop = set(stopwords.words('english'))\n\n#tokenizerとは字句解析の意味で,文(item_description)を意味のあるトークン単位に分割する.\ndef tokenize(text):\n    \"\"\"\n    sent_tokenize(): segment text into sentences\n    word_tokenize(): break sentences into words\n    \"\"\"\n    try:\n        #正規表現パターン文字列をコンパイルして正規表現パターンオブジェクトを作成\n        #[!\"#$%&'()*+,-./:;<=>?@[]^_`{|}~0-9\\\\r\\\\t\\\\n]\n        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') \n        #text内の特殊文字を半角空白で置換\n        txt = regex.sub(\" \", text)\n        \n        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n        tokens = []\n        for token_by_sent in tokens_:\n            tokens += token_by_sent\n        #filter(lambda式, イテラブルオブジェクト),イテラブルオブジェクトでlamda式を満たすものだけを抽出\n        tokens = list(filter(lambda t : t.lower() not in stop, tokens))\n        #数字などを含むトークンを除く\n        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n        \n        return filtered_tokens \n        \n    except TypeError as e:\n        print(text, e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom collections import Counter\n\n\n#item_descriptionをtokenizeしてみる(実行に時間かかる)\n\n\n#単語辞書の作成\ncat_dicts = dict()\nfor cat in general_cats:\n    #general_catごとにitem_descriptionの文字列を抽出\n    text = \" \".join(train.loc[train['general_cat']==cat, 'item_description'].values)\n    #sentenceからtokenに変換\n    #{大カテゴリー:tokenオブジェクト化したitemdescription文字列}\n    cat_dicts[cat] = tokenize(text)\n\n    \n# flat list of all words combined\nflat_list = [item for sublist in list(cat_dicts.values()) for item in sublist]\n#Counter()は出現回数が多い順に要素を取得\nallWordCount = Counter(flat_list)\n#上位20位まで取得\nall_top10 = allWordCount.most_common(20)\nx = [w[0] for w in all_top10]\ny = [w[1] for w in all_top10]\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ntrace1 = go.Bar(x=x, y=y, text=subcat1_pct)\nlayout = dict(title= 'Word Frequency',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='Word'))\nfig=dict(data=[trace1], layout=layout)\npy.iplot(fig)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply the tokenizer into the item descriptipn column\ntrain['token'] = train['item_description'].map(tokenize)\ntest['token'] = test['item_description'].map(tokenize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#index再振り分け\ntrain.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#一旦整形データ保存\n#train.to_csv('re_train.csv', sep='\\t')\n#test.to_csv('re_test.csv', sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = pd.read_csv('re_train.csv', sep='\\t')\n#test = pd.read_csv('re_test.csv', sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for descriptions, tokens in zip(train['item_description'].head(), train['token'].head()):\n    print('description:', descriptions)\n    print('token:', tokens)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\n\n#item_descriptionをtokenizeしてみる(実行に時間かかる)\n\n\n#単語辞書の作成\ncat_dicts = dict()\nfor cat in general_cats:\n    #general_catごとにitem_descriptionの文字列を抽出\n    text = \" \".join(train.loc[train['general_cat']==cat, 'item_description'].values)\n    #sentenceからtokenに変換\n    #{大カテゴリー:tokenオブジェクト化したitemdescription文字列}\n    cat_dicts[cat] = tokenize(text)\n\n#上位4位までのカテゴリーで最も出現回数の多い単語を見つける\nwomen100 = Counter(cat_dicts['Women']).most_common(100)\nbeauty100 = Counter(cat_dicts['Beauty']).most_common(100)\nkids100 = Counter(cat_dicts['Kids']).most_common(100)\nelectronics100 = Counter(cat_dicts['Electronics']).most_common(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n\n#wordcloudを生成する\ndef generate_wordcloud(tup):\n    wordcloud = WordCloud(background_color = 'white',\n                         max_words=50, max_font_size=40,\n                         random_state=42\n                         ).generate(str(tup))\n    return wordcloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(30, 15))\n\nax = axes[0, 0]\nax.imshow(generate_wordcloud(women100), interpolation='bilinear')\nax.axis('off')\nax.set_title('Women Top 100', fontsize=30)\n\nax = axes[0, 1]\nax.imshow(generate_wordcloud(beauty100))\nax.axis('off')\nax.set_title('Beauty Top 100', fontsize=30)\n\nax = axes[1, 0]\nax.imshow(generate_wordcloud(kids100))\nax.axis('off')\nax.set_title('Kids Top 100', fontsize=30)\n\nax = axes[1, 1]\nax.imshow(generate_wordcloud(electronics100))\nax.axis('off')\nax.set_title('Electronics Top 100', fontsize=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre_processing:tf-idf\ntf-idfを用いた重要度の高い単語の抽出を行う.\n\n* tf (term frequency) : あるテキストでのその単語の出現比率\n* idf (inverse document frequency) : その単語が存在するテキストの割合の逆数\n\nidfは特定のテキストにしか出現しない単語の重要度を高める働きをする."},{"metadata":{"trusted":true},"cell_type":"code","source":"#インスタンス化\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10,\n                            max_features=180000,\n                            tokenizer=tokenize,\n                            ngram_range=(1, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_desc = np.append(train['item_description'].values, test['item_description'].values)\n#TFidfVectorizerクラスのfit_transformメソッドで単語をベクトル化\nvz = vectorizer.fit_transform(list(all_desc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"vzは「kaggleで勝つデータ分析」p203のテーブルを行列にしたもの\n* 行の総数はdescriptionの総数\n* 列の総数は単語の種類の総数"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  create a dictionary mapping the tokens to their tfidf values\n#{単語名:idf値}のdict生成\ntfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_)) \ntfidf = pd.DataFrame(columns=['tfidf']).from_dict(\n                    dict(tfidf), orient='index')\ntfidf.columns = ['tfidf']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"idf値が低いワースト10の単語\n見て分かる通り、普段使う一般的な用語で重要度が低いといえる."},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf.sort_values(by=['tfidf'], ascending=True).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"idf値が高いトップ10の単語\nこれらの特徴的な単語からカテゴリーを推測できる."},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf.sort_values(by=['tfidf'], ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# t-SNEでtfidfベクトルのサイズを2次元圧縮する"},{"metadata":{"trusted":true},"cell_type":"code","source":"#trainとtestデータをコピー\ntrn = train.copy()\ntst = test.copy()\n\n#trainとtestでラベル分け\ntrn['is_train'] = 1\ntst['is_test'] = 0\n\nsample_sz = 15000\ncombined_df = pd.concat([trn, tst])\n#ランダムに行を抽出\ncombined_sample = combined_df.sample(n=sample_sz)\nvz_sample = vectorizer.fit_transform(list(combined_sample['item_description']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"t-SNEは計算コストが膨大なため,始めに前処理の前処理としてSVDを用いて次元を30に圧縮する"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nn_comp=30\nsvd = TruncatedSVD(n_components=n_comp, random_state=42)\nsvd_tfidf = svd.fit_transform(vz_sample)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can reduce the dimension from 30 to 2 using t-SNE!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, verbose=1, random_state=42, n_iter=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_tfidf = tsne_model.fit_transform(svd_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_tfidf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2次元に圧縮できたので可視化できる"},{"metadata":{"trusted":true},"cell_type":"code","source":"#可視化ライブラリbokeh\nimport bokeh.plotting as bp\nfrom bokeh.plotting import show, figure, output_notebook\n\noutput_notebook() #notebook出力にはこの1行が必要\n#オブジェクトの作成\nplot_tfidf = bp.figure(plot_width=700, plot_height=600,\n                      title='tfidf clustering of the item_description',\n                      tools=\"pan,wheel_zoom,box_zoom,reset,hover\",\n                      x_axis_type=None, y_axis_type=None, min_border=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_sample.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_df = pd.DataFrame(tsne_tfidf, columns=['x', 'y']) #2次元データtnse_tfidfをx, yコラムに置換\ntfidf_df['description'] = combined_sample['item_description']\ntfidf_df['tokens'] = combined_sample['token']\ntfidf_df['category'] = combined_sample['general_cat']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_tfidf.scatter(x='x', y='y', source=tfidf_df, alpha=0.7)\nhover = plot_tfidf.select(dict(type=HoverTool))\nhover.tooltips = {\"description\": \"@description\", \"tokens\": \"@tokens\", \"category\": \"@category\"}\nshow(plot_tfidf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-means Clustering\n大規模データのクラスタリングにはMiniBatchKmeansを使うと計算時間が早く済む"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\n\nnum_clusters = 30 #広くとった\n#インスタンス化\nkmeans_model = MiniBatchKMeans(n_clusters=num_clusters,\n                              init='k-means++',\n                              n_init=1,\n                              init_size=1000, batch_size=1000, verbose=0, max_iter=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = kmeans_model.fit(vz) #Compute the centroids on X by chunking it into mini-batches.\nkmeans_clusters = kmeans.predict(vz) #Compute cluster centers and predict cluster index for each sample.\nkmeans_distance = kmeans.transform(vz) #Compute clustering and transform X to cluster-distance space.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(kmeans_clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\nlen(sorted_centroids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"terms = vectorizer.get_feature_names()\n\nfor i in range(num_clusters):\n    print(\"Cluster %d:\" % i)\n    aux = ''\n    for j in range(i, 30):\n        aux += terms[j] + ' | '\n    print(aux)\n    print() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to plot these clusters, first we will need to reduce the dimension of the distances to 2 using tsne: "},{"metadata":{"trusted":true},"cell_type":"code","source":"#repeat the same step for the sample\nkmeans = kmeans_model.fit(vz_sample) #Compute the centroids on X by chunking it into mini-batches.\nkmeans_clusters = kmeans.predict(vz_sample) #Compute cluster centers and predict cluster index for each sample.\nkmeans_distance = kmeans.transform(vz_sample) #Compute clustering and transform X to cluster-distance space.\ntsne_kmeans = tsne_model.fit_transform(kmeans_distance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\",\n\"#e3be38\", \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n\"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \"#d07d3c\",\n\"#52697d\", \"#194196\", \"#d27c88\", \"#36422b\", \"#b68f79\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_df = pd.DataFrame(tsne_kmeans, columns=['x', 'y']) #2次元データtnse_kmeansをx, yコラムに置換\nkmeans_df['cluster'] = kmeans_clusters\nkmeans_df['description'] = combined_sample['item_description']\nkmeans_df['category'] = combined_sample['general_cat']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_notebook() #notebook出力にはこの1行が必要\n#オブジェクトの作成\nplot_kmeans = bp.figure(plot_width=700, plot_height=600,\n                      title='KMeans clustering of the item_description',\n                      tools=\"pan,wheel_zoom,box_zoom,reset,hover\",\n                      x_axis_type=None, y_axis_type=None, min_border=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#クラスターごとの色分けはsourceで設定した\nsource = ColumnDataSource(data=dict(x=kmeans_df['x'], y=kmeans_df['y'],\n                                   color=colormap[kmeans_clusters],\n                                   description=kmeans_df['description'],\n                                   category=kmeans_df['category'],\n                                   cluster=kmeans_df['cluster']))\n\n\nplot_kmeans.scatter(x='x', y='y', color='color', source=source)\nhover = plot_kmeans.select(dict(type=HoverTool))\nhover.tooltips = {\"cluster\": \"@cluster\", \"description\": \"@description\", \"category\": \"@category\"}\nshow(plot_kmeans)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LDA(Latent Dirichlet Allocation)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncvectorizer = CountVectorizer(min_df=4,\n                             max_features=180000,\n                             tokenizer=tokenize,\n                             ngram_range=(1, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvz = cvectorizer.fit_transform(combined_sample['item_description'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation\n\nlda_model = LatentDirichletAllocation(n_components=20,\n                                     learning_method='online',\n                                     max_iter=20,\n                                     random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_topics = lda_model.fit_transform(cvz)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_top_words = 10\ntopic_summaries = []\n\ntopic_word = lda_model.components_  # get the topic words\nvocab = cvectorizer.get_feature_names()\n\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    topic_summaries.append(' '.join(topic_words))\n    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reduce dimension to 2 using tsne\ntsne_lda = tsne_model.fit_transform(X_topics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unnormalized = np.matrix(X_topics)\ndoc_topic = unnormalized/unnormalized.sum(axis=1)\n\nlda_keys = []\nfor i, tweet in enumerate(combined_sample['item_description']):\n    lda_keys += [doc_topic[i].argmax()]\n\nlda_df = pd.DataFrame(tsne_lda, columns=['x','y'])\nlda_df['description'] = combined_sample['item_description']\nlda_df['category'] = combined_sample['general_cat']\nlda_df['topic'] = lda_keys\nlda_df['topic'] = lda_df['topic'].map(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#可視化\nplot_lda = bp.figure(plot_width=700,\n                     plot_height=600,\n                     title=\"LDA topic visualization\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover\",\n    x_axis_type=None, y_axis_type=None, min_border=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"source = ColumnDataSource(data=dict(x=lda_df['x'], y=lda_df['y'],\n                                    color=colormap[lda_keys],\n                                    description=lda_df['description'],\n                                    topic=lda_df['topic'],\n                                    category=lda_df['category']))\n\nplot_lda.scatter(source=source, x='x', y='y', color='color')\nhover = plot_kmeans.select(dict(type=HoverTool))\nhover = plot_lda.select(dict(type=HoverTool))\nhover.tooltips={\"description\":\"@description\",\n                \"topic\":\"@topic\", \"category\":\"@category\"}\nshow(plot_lda)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: It's a shame that by putting the HTML of the visualization using pyLDAvis, it will distort the layout of the kernel, I won't upload in here. But if you follow the below code, there should be an HTML file generated with very interesting interactive bubble chart that visualizes the space of your topic clusters and the term components within each topic."},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepareLDAData():\n    data = {\n        'vocab': vocab,\n        'doc_topic_dists': doc_topic,\n        'doc_lengths': list(lda_df['len_docs']),\n        'term_frequency':cvectorizer.vocabulary_,\n        'topic_term_dists': lda_model.components_\n    } \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nimport pyLDAvis\n\nlda_df['len_docs'] = combined_sample['token'].map(len)\nldadata = prepareLDAData()\npyLDAvis.enable_notebook()\nprepared_data = pyLDAvis.prepare(**ldadata)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nimport IPython.display\nfrom IPython.core.display import display, HTML, Javascript\n\nh = IPython.display.display(HTML(html_string))\nIPython.display.display_HTML(h)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#整形データ保存\ntrain.to_csv('re_train.csv', sep='\\t')\ntest.to_csv('re_test.csv', sep='\\t')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}