{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"a53fba0d-c0f6-79f9-f3b0-ba55b3bf47ee"},"source":"Perfect scores were seen on Kaggle's public leaderboards [before][1], but with a different evaluation function, and the authors never published their approach.\n\nSomeone else commented that a perfect score was obtainable\n in this competition after 198 submissions, or, since 3 submissions are allowed per day, 66 days,\n using a \"brute force\" method:\n\n\n> *\"The whole process takes about 198/3 = 66 days, which is shorter than the competition length.\"*\n\n\n\nI wanted to see how quickly this could really be done, within the rules,\n and to try to win the race to the top of the public leaderboard.\n\nI ended up using 14 submissions, or 5 days \n(Additionally,  I needlessly wasted 1 submission on the \"All 0.5 Benchmark\", \n and spent another to actually claim the 0.00000 score, both being technically uninformative)\n\n![leaderboard screenshot][2]\n\nMy approach is informed by information theory. You see, when the Kaggle server gives your submission a \nscore, it emits up to 21.7  bits of information\n about the test labels, but there are only 198 labels with even fewer bits of information in them,\n so one could learn all there is to know about the labels in 8 submissions or so. Well, that's a\n theoretical limit, and might not be achievable in practice.\n\nIf you train any two models and choose the one that has a better leaderboard score, you are already using 1 bit of information from your public scores. A generalization of this to any number of models is [the boosting attack][3]. However, it would require 4 years to get to the perfect score here. My approach is fundamentally similar, but is much more effective, as it learns more bits from each score.\n\nThe core algebraic insight needed here is that if we choose 15 probabilities to be\n\n`sigmoid(- n * epsilon * 2 ** i)` \n\nwhere n=198, 0 <= i < 15, and epsilon = 1.05e-5 for example, and choose the rest of the probabilities to be 0.5, then the 15 labels corresponding to those 15\nprobabilities are easily discoverable from the score we get, because all \n32768 possible label combinations lead to different scores.\n\nNote that the final rankings are based on the **private** labels of the second stage. Discovering all **public** labels helps with those only indirectly, by effectively increasing your training set size by 14%.\n(I believe the extra 14% are likely critical, given how close Kaggle competitions tend to be)\n\n**USAGE**\n\nTo use the script, create an empty file called \"scores.txt\", copy \"stage1_sample_submission.csv\" (used to read patient IDs) into the same directory, and create a subdirectory called \"submissions\".\n\n![tree_structure][4]\n\nThe former should contain the scores the Kaggle server gives you, one per line. It should be empty in the beginning. For example, the first line should be the score corresponding to \"submission_00.csv\". Keep any trailing 0s. There should be 5 digits after the decimal point.\n\nYou can rerun the script whenever you update \"scores.txt\", but it's not necessary. This will do some partial label inference. When that file contains 14 lines, rerunning the script should also generate \"submission_fin.csv\", which will have all the correct labels. ([Don't submit it though.][5] If you wish to verify the labels, you may want to submit *1-labels* instead and get the worst score possible: 34.54)\n\n\n\n  [1]: https://www.kaggle.com/c/restaurant-revenue-prediction/forums/t/13950/our-perfect-submission\n  [2]: http://i.imgur.com/na6J44f.png\n  [3]: http://machinelearning.wustl.edu/mlpapers/papers/icml2015_blum15\n  [4]: http://i.imgur.com/xufW4wV.png\n  [5]: https://www.kaggle.com/c/data-science-bowl-2017/discussion/28597 "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc52bbbb-082a-34c9-d56d-cad1eacd98ee"},"outputs":[],"source":"from __future__ import print_function # for Python3.x compatibility\nimport numpy as np\nfrom math import log\n\n\n# INPUT / OUTPUT\n\ndef read_patient_ids():\n    with open('stage1_sample_submission.csv') as f:\n        lines = f.readlines()[1:]\n        return [line.split(',')[0] for line in lines]\n\ndef prob_format(p):\n    return '%e' % p\n\ndef truncate(p):\n    return float(prob_format(p))\n\ndef write_submit(patient_ids, probs, file_name):\n    assert len(patient_ids) == len(probs)\n    with open(file_name, 'w') as f:\n        f.write('id,cancer\\n')\n        for i, p in zip(patient_ids, probs):\n            f.write('%s,%s\\n' % (i, prob_format(p)))\n    print('wrote %s' % file_name)\n\ndef read_scores():\n    lines = open('scores.txt').readlines()\n    return [s.strip() for s in lines]\n\n\n# PROBABILITIES\n\ndef build_template(n, chunk_size):\n    epsilon = 1.05e-5\n    return 1 / (1 + np.exp(n * epsilon * 2 ** np.arange(chunk_size)))\n\ndef build_probs(n, chunk, template):\n    assert template.shape == chunk.shape\n    probs = np.zeros((n,))\n    probs[:] = 0.5\n    probs[chunk] = template\n    return probs\n\n\n# LABEL INFERENCE\n\ndef int_to_bin(x, size):\n    s = bin(x)[2:][::-1].ljust(size, '0')\n    return np.array([int(c) for c in s])\n\ndef update_labels(labels, chunk, template, score):\n    assert template.shape == chunk.shape\n    chunk_size = len(chunk)\n    n = len(labels)\n    match_count = 0\n    for i in range(2**chunk_size):\n        b = int_to_bin(i, chunk_size)\n        score_i = ((-np.log(template) * b - np.log(1-template) * (1-b)).sum() \\\n                   - log(0.5) * (n-chunk_size))/n\n        if score == ('%.5f' % score_i):\n            match_count += 1\n            new_labels = b\n    assert match_count == 1 # no collisions\n    print('new labels: %s' % new_labels)\n    labels[chunk] = new_labels\n\n\n# MAIN\n\ndef write_submit_files():\n    n = 198\n    np.random.seed(2017)\n    idx = np.arange(n)\n    np.random.shuffle(idx) # optional\n    chunk_size = 15\n    template = build_template(n, chunk_size)\n    template = np.array([truncate(x) for x in template])\n\n    scores = read_scores()\n    labels = np.zeros((n,), dtype=np.int)\n    labels[:] = -1\n\n    patient_ids = read_patient_ids()\n    chunks = [idx[i : i + chunk_size] for i in range(0, len(idx), chunk_size)]\n    for i, chunk in enumerate(chunks):\n        t = template[:len(chunk)]\n        probs = build_probs(n, chunk, t)\n        write_submit(patient_ids, probs, 'submissions/submission_%02d.csv' % i)\n        if i < len(scores):\n            update_labels(labels, chunk, t, scores[i])\n            if i+1 == len(chunks):\n                write_submit(patient_ids, labels, 'submissions/submission_fin.csv')\n\nif __name__ == '__main__':\n    write_submit_files()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}