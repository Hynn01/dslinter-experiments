{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/doocker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"markdown","source":"#### Libraries required to preprocess the data and perform analysis"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport re\n#NLTK\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\n\n#Scikit-Learn\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\n#Spell Correction\n!pip install autocorrect\nfrom autocorrect import Speller\n\n#Tokenization\n!pip install wordninja\nimport wordninja\n\n#Necessary Libraries for plotting charts\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#plotly\n!pip install plotly\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n\n!pip install contractions\n# contractions is a library for converting words like \"I'm\" to \"I am\"\nimport contractions\n\n# Necessary Libraries to find similarity\nimport math\nfrom collections import Counter\n\n# initializing spell checker \nspell = Speller(lang='en')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Custom stop words list "},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwordslist= ['i',\n 'me',\n 'my',\n 'myself',\n 'we',\n 'our',\n 'ours',\n 'ourselves',\n 'you',\n \"you're\",\n \"you've\",\n \"you'll\",\n \"you'd\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves',\n 'he',\n 'him',\n 'his',\n 'himself',\n 'she',\n \"she's\",\n 'her',\n 'hers',\n 'herself',\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'they',\n 'them',\n 'their',\n 'theirs',\n 'themselves',\n 'what',\n 'which',\n 'who',\n 'whom',\n 'this',\n 'that',\n \"that'll\",\n 'these',\n 'those',\n 'am',\n 'is',\n 'are',\n 'was',\n 'were',\n 'be',\n 'been',\n 'being',\n 'have',\n 'has',\n 'had',\n 'having',\n 'do',\n 'does',\n 'did',\n 'doing',\n 'a',\n 'an',\n 'the',\n 'and',\n 'but',\n 'if',\n 'or',\n 'because',\n 'as',\n 'until',\n 'while',\n 'of',\n 'at',\n 'by',\n 'for',\n 'with',\n 'about',\n 'into',\n 'through',\n 'during',\n 'before',\n 'after',\n 'to',\n 'from',\n 'in',\n 'out',\n 'on',\n 'off',\n 'over',\n 'under',\n 'again',\n 'further',\n 'then',\n 'once',\n 'here',\n 'there',\n 'when',\n 'where',\n 'why',\n 'how',\n 'all',\n 'any',\n 'both',\n 'each',\n 'few',\n 'more',\n 'most',\n 'other',\n 'some',\n 'such',\n 'only',\n 'own',\n 'same',\n 'so',\n 'than',\n 'too',\n 'very',\n 's',\n 't',\n 'can',\n 'will',\n 'just',\n 'should',\n \"should've\",\n 'now',\n 'ma']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Data source"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We have scraped data from Glassdoor for 4 different companies.\n- Google\n- Microsoft\n- IBM\n- Amazon\n\nBelow github link was used for scrapping:\nhttps://github.com/sagark93/Text-Analytics/blob/master/glassdoor_scrapping.ipynb\n\nreference github code: https://github.com/MatthewChatham/glassdoor-review-scraper"},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading the data extracted form the glassdoor into a pandas dataframe\ndf=pd.read_csv('../input/raw-data/reviews_final.csv', engine = 'python')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clean/preprocess corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting all the columns name to lower case for easy access\ndf.columns = df.columns.str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting employeestatus from  authorlocation\nfor i in reviews.index:\n    if reviews.authorlocation[i].find('Former Employee') != -1:\n        reviews.loc[i,'employeestatus'] = 'Former Employee'\n    elif reviews.authorlocation[i].find('Current Employee') != -1:\n        reviews.loc[i,'employeestatus'] = 'Current Employee'\n    else:\n        reviews.loc[i,'employeestatus'] = 'Unknown'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping authorjobrole, ReviewMainText consist \n# above columns dont make any contributions to actual review from the user\n\ndf.drop(columns=['authorjobrole','reviewmaintext'],inplace=True, axis= 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding year and month \ndf['review_year'] = df.apply(lambda x:x['reviewdate'].year,axis=1)\ndf['review_month'] = df.apply(lambda x:x['reviewdate'].month,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace null values with empty string\n# as we are considering pros,cons and reviewtitle to calculate polarity, we don't want to lose data\n\ndef remove_null(text):\n    text = ''\n    return text\n\n#  replacing null values in pros and cons columns\nfor i in df[df['pros'].isnull()]['pros'].index:\n    df.loc[i,'pros'] = remove_null(df.loc[i,'pros'])\n\nfor i in df[df['cons'].isnull()]['cons'].index:\n    df.loc[i,'cons'] = remove_null(df.loc[i,'cons'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Preprocessing of text data ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n# function to extract extract noun, adjective, verb and adverbs from the text\n# and tag pos for better lemmatization\ndef get_wordnet_pos(word):\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n    return tag_dict.get(tag, wordnet.NOUN)\n\n# preprocessing of text\ndef text_preprocess(text):\n    if (len(text)>0):\n        # contractions is a library for converting words like \"I'm\" to \"I am\"\n        text=contractions.fix(text)\n        \n        #Removing all the special characters from the review text\n        for char in '!#$%&@?,.:;+-*/=<>\"\\'()[\\\\]X{|}~\\n\\t1234567890':\n            text = str(text).replace(char, ' ')\n            \n        #Converting all the words in review into lower case\n        text=text.lower()\n        \n        #splitting the words in a sentence.\n        word_list = wordninja.split(text)\n        \n        #removing stopwords from customzied stopwordlists \n        #and considering only word of length greater than 2\n        word_list=[spell(w) for w in word_list if w not in stopwordslist and len(w) > 2]\n        \n#       extract noun, adjective, verb and adverbs from the text and perform lemmatizton   \n        lemmatized_text=' '.join(([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_list]))\n        \n        return lemmatized_text\n    else:\n        return ''\n\n# cleaning the pros, cons and reviewTitle\ndf['cleanpros'] = df['pros'].apply(lambda text: text_preprocess(text))\ndf['cleancons'] = df['cons'].apply(lambda text: text_preprocess(text))\ndf['cleantitle'] = df['reviewtitle'].apply(lambda text: text_preprocess(text))\n\n#Combing all the clean text information given by the user\ndf['cleantext'] = df[['cleanpros', 'cleancons','cleantitle']].apply(lambda x: ' '.join(x), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### exporting clean data file to reviewcsv1.2.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('../output/reviewsdata1/reviewcsv1.2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading the clean data file into dataframe df\ndf=pd.read_csv('../input/reviewsdata1/reviewcsv1.2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace null values with empty string on the clean data file\ndef remove_null(text):\n    text = ''\n    return text\n\n#  repalcing null values in pros and cons columns\nfor i in df[df['cleanpros'].isnull()]['cleanpros'].index:\n    df.loc[i,'cleanpros'] = remove_null(df.loc[i,'cleanpros'])\n\nfor i in df[df['cleancons'].isnull()]['cleancons'].index:\n    df.loc[i,'cleancons'] = remove_null(df.loc[i,'cleancons'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis & Visualizations"},{"metadata":{},"cell_type":"markdown","source":"### Pie plot of Ratings distribution in reviews by company"},{"metadata":{"trusted":true},"cell_type":"code","source":"company_names = df.companyname.unique()\ndef pieplotratingsdistribution(df,companyname):\n    labels = df[df['companyname']==companyname]['overallrating'].value_counts().index\n    values = df[df['companyname']==companyname]['overallrating'].value_counts().values\n    title = 'Ratings distribution in reviews of {}'.format(companyname)\n    trace = go.Pie(labels=labels, values=values, title = title)\n    py.iplot([trace], filename='basic_pie_chart')\n    \n\nfor name in company_names:\n    pieplotratingsdistribution(df,name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stacked bar chart of employment status"},{"metadata":{"trusted":true},"cell_type":"code","source":"def stackedbarplot(df):\n    company_names=list(df['companyname'].unique())\n    Employee_categories=list(df['authoremploymentstatus'].unique())\n    count_list=[]\n    for j in Employee_categories:\n        for i in company_names:\n            count_list.append(df[(df['companyname']==i)&(df['authoremploymentstatus']==j)].shape[0])\n    temp=[]\n    for i in range(0, len(count_list),len(company_names)):\n        temp.append(list(count_list[i:i + len(company_names)]))\n    data=[] \n    k=0\n    for i in Employee_categories:\n        data.append(go.Bar(name=i,x=company_names,y=temp[k]))\n        k=k+1\n    fig = go.Figure(data=data)\n# Change the bar mode\n    fig.update_layout(barmode='stack')\n    fig.show()\n\nstackedbarplot(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Plotting Word Count distribution bar plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotwordcountdistribution(df,reviewtype):\n    #reviewtyepe is 'Pros_Modified_Text','Cons_Modified_Text' in our case\n    def comment_len(x):\n        if type(x) is str:\n            return len(x.split())\n        else:\n            return 0\n    df['review_len'] = df[reviewtype].apply(comment_len)\n    length_scale=[0,5,15,25,50,75,100,200,10000]#Change length scale according to your requirement\n    out = pd.cut(df['review_len'],length_scale)\n    ax = out.value_counts(sort=False).plot.bar(rot=0, color=\"b\", figsize=(12,8))\n    for p in ax.patches:\n        ax.annotate(str(p.get_height()), (p.get_x() * 1.040, p.get_height() * 1.015))\n    plt.title('Word Count distribution of {}'.format(reviewtype))\n    plt.show()\n    df.drop(columns=['review_len'])\n    \nplotwordcountdistribution(df,'cleanpros')\nplotwordcountdistribution(df,'cleancons')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Text analysis (Sentiment score analysis)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform sentiment analysis on cleantext\n\nanalyzer=SentimentIntensityAnalyzer()\ndef polarity_score(text):\n    if len(text)>0:\n        score=analyzer.polarity_scores(text)['compound']\n        return score\n    else:\n        return 0\ndf['polarityscore'] =df['cleantext'].apply(lambda text : polarity_score(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Sentiment score anomalies bar plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sentianamolybarplot(df):\n    polarity_scale=[0.0,0.2,0.4,0.6,0.8,1]\n    #'Review_polarity' is column name of sentiment score calculated for whole review.\n    df2=df[(df['polarityscore']>0)&(df['overallrating']<=3)]\n    out = pd.cut(df2['polarityscore'],polarity_scale)\n    ax = out.value_counts(sort=False).plot.bar(rot=0, color=\"b\", figsize=(12,8))\n    for p in ax.patches:\n        ax.annotate(str(p.get_height()), (p.get_x() * 1.040, p.get_height() * 1.015))\n    plt.show()\nsentianamolybarplot(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Distribution of polarity score"},{"metadata":{"trusted":true},"cell_type":"code","source":"def polarityscorehistplot(df,companyname):\n#     df['Review_polarity'] = df['Pros'].map(lambda text: TextBlob(text).sentiment.polarity)\n    fig = px.histogram(df, x='polarityscore')\n    fig.update_layout(title_text='Distribution of sentiment polarity in Reviews {}'.format(companyname), template=\"plotly_white\")\n    fig.show()\n\nfor name in company_names:\n    polarityscorehistplot(df,name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text vectorization"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Defining n trigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"def top_n_trigram(corpus):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Defining n bigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"def top_n_bigram(corpus):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extracting trigrams from clean pros and cons text"},{"metadata":{"trusted":true},"cell_type":"code","source":"company_names = df.companyname.unique()\n\ndict_trigrams_pros= {}\ndict_trigrams_cons= {}\n\nfor name in company_names:\n    dict_trigrams_pros[name] = top_n_trigram(df[df['companyname']==name]['cleanpros'])\n\nfor name in company_names:\n    dict_trigrams_cons[name] = top_n_trigram(df[df['companyname']==name]['cleancons'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bar plot of top 20 trigrams "},{"metadata":{"trusted":true},"cell_type":"code","source":"def bar_plot_toptrigrams(trigrams, reviewtype = 'Pros'):\n    for companyname in trigrams:\n        common_words = trigrams[name][:20]\n        df1 = pd.DataFrame(common_words, columns = ['word' , 'count'])\n        fig = px.bar(df1, x='word', y='count')\n        fig.update_layout(title_text= '{0} Review Tri-gram count top 20 for {1}'.format(reviewtype, companyname), template=\"plotly_white\")\n        fig.show()\n\nbar_plot_toptrigrams(dict_trigrams_pros)\nbar_plot_toptrigrams(dict_trigrams_cons,'Cons') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extracting Bigrams from clean pros and cons text"},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_bigrams_pros= {}\ndict_bigrams_cons= {}\n\nfor name in company_names:\n    dict_bigrams_pros[name] = top_n_bigram(df[df['companyname']==name]['cleanpros'])\n\nfor name in company_names:\n    dict_bigrams_cons[name] = top_n_bigram(df[df['companyname']==name]['cleancons']) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bar plot of top 20 Bigrams "},{"metadata":{"trusted":true},"cell_type":"code","source":"def bar_plot_topbigrams(bigrams, reviewtype = 'Pros'):\n    for companyname in bigrams:\n        common_words = bigrams[name][:20]\n        df1 = pd.DataFrame(common_words, columns = ['word' , 'count'])\n        fig = px.bar(df1, x='word', y='count')\n        fig.update_layout(title_text= '{0} Review Bi-gram count top 20 for {1}'.format(reviewtype, companyname), template=\"plotly_white\")\n        fig.show()\n\nbar_plot_topbigrams(dict_bigrams_pros)\nbar_plot_topbigrams(dict_bigrams_cons,'Cons')    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### number of negative trigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"for key in dict_trigrams_cons:\n    print(key,len(dict_trigrams_cons[key]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  number of postive trigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"for key in dict_trigrams_pros:\n    print(key,len(dict_trigrams_pros[key]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text analysis to find best and worst features for each company"},{"metadata":{},"cell_type":"markdown","source":"### Function to find similarity between two trigrams and bigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cosine_similarity_grams(a, b):\n    vec1 = Counter(a)\n    vec2 = Counter(b)\n\n    intersection = set(vec1.keys()) & set(vec2.keys())\n    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n\n    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n\n    if not denominator:\n        return 0.0\n    return float(numerator) / denominator\n# Ref : https://gist.github.com/gaulinmp/da5825de975ed0ea6a24186434c24fe4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Function to find a list of unique grams and find grams which are similar to unique grams"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create of list of 100 trigrams for each company\ndef check_similarity(dict_grams):\n    top_grams = {}\n    for key in dict_grams:\n#         consider top 100 grams for each comapny\n        top_100 = dict_grams[key][:100]\n        top_similar = {}\n        top = []\n        for i in top_100:\n            for j,val in enumerate(top_100):\n#    check similarity between two grams\n                if cosine_similarity_grams(i[0],val[0]) > 0.65:\n#     calculate total freq of similar words\n                    freq = i[1] + val[1]\n#     collect all the simialr gram in a list\n                    if i[0] not in top_similar:\n                        top_similar[i[0]] = []\n                    top_similar[i[0]].append(val[0])\n#    delete grams which is simialar \n                    del top_100[j]\n            top.append((i[0],freq))\n#       collect all the top grams into a dictionary \n        top_grams[key] = top\n#       collect all the similar grams \n        top_grams[str(key)+str('_similar')] = top_similar\n    return top_grams","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extracting top trigrams and similar trigrams for pros and cons"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_pros_trigrams = check_similarity(dict_trigrams_pros)\ntop_cons_trigrams = check_similarity(dict_trigrams_cons)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extracting top bigrams and similar trigrams for pros and cons"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_pros_bigrams = check_similarity(dict_bigrams_pros)\ntop_cons_bigrams = check_similarity(dict_bigrams_cons)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find the average sentiment score of unique grams and sort by average sentiment score"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sort the kewyword list \ndef sortlist(keywords_list):\n    keywords_list =sorted(keywords_list, key = lambda x: x[2], reverse=True)\n    return keywords_list\n\n# find the average polarity of unique grams\ndef avgpolarity(name, trigrams, col='cleancons'):\n    grp = df[df['companyname'] == name]\n    lst = []\n    for keyword in  trigrams[name]:\n#         print(keyword[0] ,grp.loc[grp['cleanpros'].str.contains(keyword[0])]['polarityscore'].mean())\n        lst.append((keyword[0],keyword[1],grp.loc[grp[col].str.contains(keyword[0])]['polarityscore'].mean()))\n#     print(sortlist(lst))\n    return sortlist(lst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### calculate average rating for bigrams and trigrams of cons"},{"metadata":{"trusted":true},"cell_type":"code","source":"top5_cons_bigrams = {}\ntop5_cons_trigrams = {}\nfor name in company_names:\n    top5_cons_bigrams[name] = avgpolarity(name,top_cons_bigrams)\n    top5_cons_trigrams[name] = avgpolarity(name,top_cons_trigrams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### calculate average sentiment score for bigrams and trigrams of pros"},{"metadata":{"trusted":true},"cell_type":"code","source":"top5_pros_bigrams = {}\ntop5_pros_trigrams = {}\nfor name in company_names:\n    top5_pros_bigrams[name] = avgpolarity(name,top_pros_bigrams,'cleanpros')\n    top5_pros_trigrams[name] = avgpolarity(name,top_pros_trigrams,'cleanpros')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Printing top bigrams in pros with frequency and average polarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"top5_pros_bigrams","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Printing top bigrams in cons with frequency and average polarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"top5_cons_bigrams","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Printing top Trigrams in cons with frequency and average polarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"top5_cons_trigrams","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Printing top Trigrams in pros with frequency and average polarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"top5_pros_trigrams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Similar words in top trigrams (pros)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for name in company_names:\n    print(name)\n    print(top_pros_trigrams[str(name) +'_similar'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Similar words in top trigrams (cons)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for name in company_names:\n    print(name)\n    print(top_cons_trigrams[str(name) +'_similar'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Similar words in top bigrams (pros)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for name in company_names:\n    print(name)\n    print(top_pros_bigrams[str(name) +'_similar'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Similar words in top bigrams (cons)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for name in company_names:\n    print(name)\n    print(top_cons_bigrams[str(name) +'_similar'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dashboard in Tableau\n\nhttps://public.tableau.com/profile/sagar.surendra.kulkarni#!/vizhome/Text_Analytics_Updated/Dashboard1"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}