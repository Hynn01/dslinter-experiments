{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install efficientnet_pytorch\n\nfrom efficientnet_pytorch import EfficientNet\n\nimport cv2\nimport torch\nimport torch.utils.data as Data\nimport torch.nn as nn\nfrom torchvision import transforms\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nfrom sklearn.metrics import accuracy_score\nfrom scipy.special import softmax\nimport cv2\nfrom transformers import get_cosine_schedule_with_warmup\nfrom transformers import AdamW\nfrom tqdm.notebook import tqdm\nfrom albumentations import *\nfrom albumentations.pytorch import ToTensor\n\nfrom PIL import Image\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parameters and Hyper Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !mkdir /kaggle/working/npy-images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !rm -rf /kaggle/working/npy-images","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#IMAGE_FOLDER = '../input/plant-pathology-npy-images/kaggle/working/image_pickles/'\nIMAGE_FOLDER = '../input/plant-pathology-2020-fgvc7/images/'\nNPY_FOLDER = 'npy-images/'\n\n# def get_image_path(filename):\n#     return (IMAGE_FOLDER + filename + '.npy')\ndef get_image_path(filename):\n    return (IMAGE_FOLDER + filename + '.jpg')\n#     return (NPY_FOLDER + filename + '.npy')\n\ndef GenerateNPY(ids):\n    for id in tqdm(ids):\n        np.save(NPY_FOLDER + id + '.npy', np.array(Image.open(IMAGE_FOLDER + id + '.jpg'), dtype='uint8'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/plant-pathology-2020-fgvc7/train.csv')\ntest = pd.read_csv('../input/plant-pathology-2020-fgvc7/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GenerateNPY(train['image_id'])\n# GenerateNPY(test['image_id'])\n    \ntrain['image_path'] = train['image_id'].apply(get_image_path)\ntest['image_path'] = test['image_id'].apply(get_image_path)\ntrain_labels = train.loc[:, 'healthy':'scab']\ntrain_paths = train.image_path\ntest_paths = test.image_path\n\ndel train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_paths, valid_paths, train_labels, valid_labels = train_test_split(train_paths, train_labels, test_size = 0.2, random_state=23, stratify = train_labels)\ntrain_paths.reset_index(drop=True,inplace=True)\ntrain_labels.reset_index(drop=True,inplace=True)\nvalid_paths.reset_index(drop=True,inplace=True)\nvalid_labels.reset_index(drop=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LeafDataset(Data.Dataset):\n    def __init__(self, image_paths, labels = None, train = True, test = False):\n        self.paths = image_paths\n        self.test = test\n        if self.test == False:\n            self.labels = labels\n        self.train = train\n        self.transform = Compose([HorizontalFlip(p=0.5),\n                                  VerticalFlip(p=0.5),\n                                  ShiftScaleRotate(rotate_limit=25.0, p=0.7),\n                                  OneOf([IAAEmboss(p=1),\n                                         IAASharpen(p=1),\n                                         Blur(p=1)], p=0.5),\n                                  IAAPiecewiseAffine(p=0.5),\n                                  Resize(545,545, always_apply=True),\n                                  Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n                                  ToTensor()])\n        self.default_transform = Compose([Resize(545,545, always_apply=True),\n                                          Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225), always_apply=True),\n                                          ToTensor()]) #normalized for pretrained network\n        \n    def __len__(self):\n        return self.paths.shape[0]\n    \n    def __getitem__(self, i):\n#         image = np.load(self.paths[i]) #load from .npy file!\n        image = cv2.imread(self.paths[i])\n        \n        if self.test==False:\n            label = torch.tensor(np.argmax(self.labels.loc[i,:].values))#loss function used later doesnt take one-hot encoded labels, so convert it using argmax\n        if self.train:\n            image = self.transform(image = image)['image']\n        else:\n            image = self.default_transform(image = image)['image']\n        \n        if self.test==False:\n            return image, label\n        return image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(net, loader):\n    \n    running_loss = 0\n    preds_for_acc = []\n    labels_for_acc = []\n    \n    pbar = tqdm(total = len(loader), desc='Training')\n    \n    for _, (images, labels) in enumerate(loader):\n        \n        images, labels = images.to(device), labels.to(device)\n        net.train()\n        optimizer.zero_grad()\n        predictions = net(images)\n        loss = loss_fn(predictions, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        running_loss += loss.item()*labels.shape[0]\n        labels_for_acc = np.concatenate((labels_for_acc, labels.cpu().numpy()), 0)\n        preds_for_acc = np.concatenate((preds_for_acc, np.argmax(predictions.cpu().detach().numpy(), 1)), 0)\n        \n        pbar.update()\n        \n    accuracy = accuracy_score(labels_for_acc, preds_for_acc)\n    \n    pbar.close()\n    return running_loss/TRAIN_SIZE, accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def valid_fn(net, loader):\n    \n    running_loss = 0\n    preds_for_acc = []\n    labels_for_acc = []\n    \n    pbar = tqdm(total = len(loader), desc='Validation')\n    \n    with torch.no_grad():       #torch.no_grad() prevents Autograd engine from storing intermediate values, saving memory\n        for _, (images, labels) in enumerate(loader):\n            \n            images, labels = images.to(device), labels.to(device)\n            net.eval()\n            predictions = net(images)\n            loss = loss_fn(predictions, labels)\n            \n            running_loss += loss.item()*labels.shape[0]\n            labels_for_acc = np.concatenate((labels_for_acc, labels.cpu().numpy()), 0)\n            preds_for_acc = np.concatenate((preds_for_acc, np.argmax(predictions.cpu().detach().numpy(), 1)), 0)\n            \n            pbar.update()\n            \n        accuracy = accuracy_score(labels_for_acc, preds_for_acc)\n    \n    pbar.close()\n    return running_loss/VALID_SIZE, accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Limit Size of Training\nLIMIT_SIZE = 500\ntrain_labels = train_labels[:LIMIT_SIZE]\ntrain_paths = train_paths[:LIMIT_SIZE]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 8\nNUM_EPOCHS = 1\nTRAIN_SIZE = train_labels.shape[0]\nVALID_SIZE = valid_labels.shape[0]\nMODEL_NAME = 'efficientnet-b5'\nIMAGE_SIZE = EfficientNet.get_image_size(MODEL_NAME)\ndevice = 'cuda'\nlr = 8e-4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train Size:\", len(train_paths))\nprint(\"Valid Size:\", len(valid_paths))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = LeafDataset(train_paths, train_labels)\ntrainloader = Data.DataLoader(train_dataset, shuffle=True, batch_size = BATCH_SIZE, num_workers = 2)\n\nvalid_dataset = LeafDataset(valid_paths, valid_labels, train = False)\nvalidloader = Data.DataLoader(valid_dataset, shuffle=False, batch_size = BATCH_SIZE, num_workers = 2)\n\ndel train_paths\ndel valid_paths\ndel train_labels\ndel valid_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = EfficientNet.from_pretrained(MODEL_NAME)\n\nnum_ftrs = model._fc.in_features\nmodel._fc = nn.Sequential(nn.Linear(num_ftrs,1000,bias=True),\n                          nn.ReLU(),\n                          nn.Dropout(p=0.5),\n                          nn.Linear(1000,4, bias = True))\n\nmodel.to(device)\noptimizer = AdamW(model.parameters(), lr = lr, weight_decay = 1e-3)\nnum_train_steps = int(len(train_dataset) / BATCH_SIZE * NUM_EPOCHS)\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(train_dataset)/BATCH_SIZE*5, num_training_steps=num_train_steps)\nloss_fn = torch.nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss = []\nvalid_loss = []\ntrain_acc = []\nval_acc = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(NUM_EPOCHS):\n    \n    tl, ta = train_fn(model, loader = trainloader)\n    vl, va = valid_fn(model, loader = validloader)\n    train_loss.append(tl)\n    valid_loss.append(vl)\n    train_acc.append(ta)\n    val_acc.append(va)\n    \n    if epoch%10==0:\n        path = 'epoch' + str(epoch) + '.pt'\n        torch.save(model.state_dict(), path)\n    \n    printstr = 'Epoch: '+ str(epoch) + ', Train loss: ' + str(tl) + ', Val loss: ' + str(vl) + ', Train acc: ' + str(ta) + ', Val acc: ' + str(va)\n    tqdm.write(printstr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualise Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.ylim(0,1.5)\nsns.lineplot(list(range(len(train_loss))), train_loss)\nsns.lineplot(list(range(len(valid_loss))), valid_loss)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train','Val'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.lineplot(list(range(len(train_acc))), train_acc)\nsns.lineplot(list(range(len(val_acc))), val_acc)\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train','Val'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = LeafDataset(test_paths, train = False, test = True)\ntestloader = Data.DataLoader(test_dataset, shuffle=False, batch_size = BATCH_SIZE, num_workers = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_fn(net, loader):\n\n    preds_for_output = np.zeros((1,4))\n    \n    with torch.no_grad():\n        pbar = tqdm(total = len(loader))\n        for _, images in enumerate(loader):\n            images = images.to(device)\n            net.eval()\n            predictions = net(images)\n            preds_for_output = np.concatenate((preds_for_output, predictions.cpu().detach().numpy()), 0)\n            pbar.update()\n    \n    pbar.close()\n    return preds_for_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out = test_fn(model, testloader)\noutput = pd.DataFrame(softmax(out,1), columns = ['healthy','multiple_diseases','rust','scab']) #the submission expects probability scores for each class\noutput.drop(0, inplace = True)\noutput.reset_index(drop=True,inplace=True)\noutput['image_id'] = test.image_id\noutput = output[['image_id','healthy','multiple_diseases','rust','scab']]\n\noutput.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}