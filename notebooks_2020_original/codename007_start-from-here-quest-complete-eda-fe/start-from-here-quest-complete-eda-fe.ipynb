{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n# More To Come. Stay Tuned. !!\nIf there are any suggestions/changes you would like to see in the Kernel please let me know :). Appreciate every ounce of help!\n\n**This notebook will always be a work in progress.** Please leave any comments about further improvements to the notebook! Any feedback or constructive criticism is greatly appreciated!. **If you like it or it helps you , you can upvote and/or leave a comment :).**\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-competitions/kaggle/7968/logos/thumb76_76.png?t=2017-12-01-22-32-31)"},{"metadata":{},"cell_type":"markdown","source":"\n- <a href='#1'>1. Introduction</a>  \n- <a href='#2'>2. Retrieving the Data</a>\n     - <a href='#2-1'>2.1 Load libraries</a>\n     - <a href='#2-2'>2.2 Read the Data</a>\n- <a href='#3'>3. Glimpse of Data</a>\n     - <a href='#3-1'>3.1 Overview of tables</a>\n     - <a href='#3-2'>3.2 Statistical overview of the Data</a>\n- <a href='#4'>4. Check for missing data</a>\n- <a href='#5'>5. Data Exploration</a>\n    - <a href='#5-1'>5.1 Distribution of Host(from which website Question & Answers collected)</a>\n    - <a href='#5-2'>5.2 Distribution of categories</a>\n    - <a href='#5-3'>5.3 Distribution of Target variables</a>\n    - <a href='#5-4'>5.4 Venn Diagram(Common Features values in training and test data)</a>\n    - <a href='#5-5'>5.5 Distribution of Question Title</a>\n    - <a href='#5-6'>5.6 Distribution of Question Body</a>\n    - <a href='#5-7'>5.7 Distribution of Answers</a>\n    - <a href='#5-8'>5.8 Duplicate Questions Title & Most popular Questions</a>\n- <a href='#6'>6. Data Preparation & Feature Engineering</a>\n    - <a href='#6-1'>6.1 Data Cleaning</a>\n    - <a href='#6-2'>6.2 Feature Engineering</a>\n        - <a href='#6-2-1'>6.2.1 Text Based Features</a>\n        - <a href='#6-2-2'>6.2.2 TF-IDF Features</a>"},{"metadata":{},"cell_type":"markdown","source":"# <a id='1'>1. Introduction</a>"},{"metadata":{},"cell_type":"markdown","source":"In this competition, you’re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a \"common-sense\" fashion. Our raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this data set. What you see is what you get!"},{"metadata":{},"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-media/competitions/google-research/human_computable_dimensions_1.png)"},{"metadata":{},"cell_type":"markdown","source":" # <a id='2'>2. Retrieving the Data</a>"},{"metadata":{},"cell_type":"markdown","source":" ## <a id='2-1'>2.1 Load libraries</a>"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import pandas as pd # package for high-performance, easy-to-use data structures and data analysis\nimport numpy as np # fundamental package for scientific computing with Python\nimport matplotlib\nimport matplotlib.pyplot as plt # for plotting\nimport seaborn as sns # for making plots with seaborn\ncolor = sns.color_palette()\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.offline as offline\noffline.init_notebook_mode()\n#import cufflinks and offline mode\nimport cufflinks as cf\ncf.go_offline()\n\n# Venn diagram\nfrom matplotlib_venn import venn2\nimport re\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\neng_stopwords = stopwords.words('english')\nimport gc\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input/google-quest-challenge\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='2-2'>2.2 Reading Data</a>"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"print('Reading data...')\ntrain_data = pd.read_csv('../input/google-quest-challenge/train.csv')\ntest_data = pd.read_csv('../input/google-quest-challenge/test.csv')\nsample_submission = pd.read_csv('../input/google-quest-challenge/sample_submission.csv')\nprint('Reading data completed')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"print('Size of train_data', train_data.shape)\nprint('Size of test_data', test_data.shape)\nprint('Size of sample_submission', sample_submission.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='3'>3. Glimpse of Data</a>"},{"metadata":{},"cell_type":"markdown","source":"## <a id='3-1'>3.1 Overview of tables</a>"},{"metadata":{},"cell_type":"markdown","source":"**train_data**"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**train_data columns**"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**test_data**"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**test_data columns**"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"test_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**sample_submission**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Target variables**"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"targets = list(sample_submission.columns[1:])\ntargets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='3-2'> 3.2 Statistical overview of the Data</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[targets].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='4'> 4 Check for missing data</a>"},{"metadata":{},"cell_type":"markdown","source":"**checking missing data in train_data **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# checking missing data\ntotal = train_data.isnull().sum().sort_values(ascending = False)\npercent = (train_data.isnull().sum()/train_data.isnull().count()*100).sort_values(ascending = False)\nmissing_train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**checking missing data in test_data **"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# checking missing data\ntotal = test_data.isnull().sum().sort_values(ascending = False)\npercent = (test_data.isnull().sum()/test_data.isnull().count()*100).sort_values(ascending = False)\nmissing_test_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_test_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5'>5. Data Exploration</a>"},{"metadata":{},"cell_type":"markdown","source":"# <a id='5-1'>5.1 Distribution of Host(from which website Question & Answers collected)</a>"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"temp = train_data[\"host\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\ndf.iplot(kind='pie',labels='labels',values='values', title='Distribution of hosts in Training data')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"temp = test_data[\"host\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\ndf.iplot(kind='pie',labels='labels',values='values', title='Distribution of hosts in test data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5-2'>5.2 Distribution of categories</a>"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"temp = train_data[\"category\"].value_counts()\n#print(\"Total number of states : \",len(temp))\ntrace = go.Bar(\n    x = temp.index,\n    y = (temp / temp.sum())*100,\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Distribution of categories in training data in % \",\n    xaxis=dict(\n        title='category',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Count in %',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='test')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"temp = test_data[\"category\"].value_counts()\n#print(\"Total number of states : \",len(temp))\ntrace = go.Bar(\n    x = temp.index,\n    y = (temp / temp.sum())*100,\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Distribution of categories in test data in % \",\n    xaxis=dict(\n        title='category',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Count in %',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5-3'>5.3 Distribution of Target variables</a>"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(6, 5, figsize=(18, 15))\naxes = axes.ravel()\nbins = np.linspace(0, 1, 20)\n\nfor i, col in enumerate(targets):\n    ax = axes[i]\n    sns.distplot(train_data[col], label=col, kde=False, bins=bins, ax=ax)\n    # ax.set_title(col)\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 6079])\nplt.tight_layout()\nplt.show()\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5-4'>5.4 Venn Diagram(Common Features values in training and test data)</a>"},{"metadata":{},"cell_type":"markdown","source":"* **A Venn diagram uses overlapping circles or other shapes to illustrate the logical relationships between two or more sets of items. Often, they serve to graphically organize things, highlighting how the items are similar and different.**"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(23,13))\n\nplt.subplot(321)\nvenn2([set(train_data.question_user_name.unique()), set(test_data.question_user_name.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common question_user_name in training and test data\", fontsize=15)\n#plt.show()\n\n#plt.figure(figsize=(15,8))\nplt.subplot(322)\nvenn2([set(train_data.answer_user_name.unique()), set(test_data.answer_user_name.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common answer_user_name in training and test data\", fontsize=15)\n#plt.show()\n\n#plt.figure(figsize=(15,8))\nplt.subplot(323)\nvenn2([set(train_data.question_title.unique()), set(test_data.question_title.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common question_title in training and test data\", fontsize=15)\n#plt.show()\n\n#plt.figure(figsize=(15,8))\nplt.subplot(324)\nvenn2([set(train_data.question_user_name.unique()), set(train_data.answer_user_name.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common users in both question & answeer in train data\", fontsize=15)\n\n#plt.figure(figsize=(15,8))\nplt.subplot(325)\nvenn2([set(test_data.question_user_name.unique()), set(test_data.answer_user_name.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common users in both question & answeer in test data\", fontsize=15)\n\nplt.subplots_adjust(wspace = 0.5, hspace = 0.5,\n                    top = 0.9)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5-5'>5.5 Distribution for Question Title</a>"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train_question_title=train_data['question_title'].str.len()\ntest_question_title=test_data['question_title'].str.len()\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,6))\nsns.distplot(train_question_title,ax=ax1,color='blue')\nsns.distplot(test_question_title,ax=ax2,color='green')\nax2.set_title('Distribution for Question Title in test data')\nax1.set_title('Distribution for Question Title in Training data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5-6'>5.6 Distribution for Question body</a>"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train_question_title=train_data['question_body'].str.len()\ntest_question_title=test_data['question_body'].str.len()\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,6))\nsns.distplot(train_question_title,ax=ax1,color='blue')\nsns.distplot(test_question_title,ax=ax2,color='green')\nax2.set_title('Distribution for Question Body in test data')\nax1.set_title('Distribution for Question Body in Training data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5-7'>5.7 Distribution for Answers</a>"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train_question_title=train_data['answer'].str.len()\ntest_question_title=test_data['answer'].str.len()\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,6))\nsns.distplot(train_question_title,ax=ax1,color='blue')\nsns.distplot(test_question_title,ax=ax2,color='green')\nax2.set_title('Distribution for Answers in test data')\nax1.set_title('Distribution for Answers in Training data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='5-8'>5.8 Duplicate Questions Title & Most popular Questions</a>"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Duplicate Questions\nprint(\"Number of duplicate questions in descending order\")\nprint(\"------------------------------------------------------\")\ntrain_data.groupby('question_title').count()['qa_id'].sort_values(ascending=False).head(25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Most popular questions**"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train_data[train_data['question_title'] == 'What is the best introductory Bayesian statistics textbook?']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='6'>6. Data Preparation & Feature Engineering</a>"},{"metadata":{},"cell_type":"markdown","source":"## <a id='6-1'>6.1 Data cleaning</a>"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#https://www.kaggle.com/urvishp80/quest-encoding-ensemble\nprint(\"Data cleaning started........\")\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(text):\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = text.lower().split()\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops]    \n    text = \" \".join(text)\n    return(text)\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\ndef clean_data(df, columns: list):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"columns = ['question_title','question_body','answer']\ntrain_data = clean_data(train_data, columns)\ntest_data = clean_data(test_data, columns)\nprint(\"Data cleaning Done........\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='6-2'>6.2 Word frequency</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# training data\nfreq_dist = FreqDist([word for text in train_data['question_title'].str.replace('[^a-za-z0-9^,!.\\/+-=]',' ') for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question title (Training Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()\n\n# test data\nfreq_dist = FreqDist([word for text in test_data['question_title'] for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question title (Test Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training data\nfreq_dist = FreqDist([word for text in train_data['question_body'].str.replace('[^a-za-z0-9^,!.\\/+-=]',' ') for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question body (Training Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()\n\n# test data\nfreq_dist = FreqDist([word for text in test_data['question_body'] for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question body (Test Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training data\nfreq_dist = FreqDist([word for text in train_data['question_title'] for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question title (Training Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()\n\n# test data\nfreq_dist = FreqDist([word for text in test_data['question_title'] for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question title (Test Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='6-3'>6.3 Feature Engineering</a>"},{"metadata":{},"cell_type":"markdown","source":"### <a id='6-3-1'>6.3.1 Text based features</a>"},{"metadata":{},"cell_type":"markdown","source":"Text based features are :\n * Number of characters in the question_title\n * Number of characters in the question_body\n * Number of characters in the answer\n * Number of words in the question_title\n * Number of words in the question_body\n * Number of words in the answer\n * Number of unique words in the question_title\n * Number of unique words in the question_body\n * Number of unique words in the answer"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Number of characters in the text\ntrain_data[\"question_title_num_chars\"] = train_data[\"question_title\"].apply(lambda x: len(str(x)))\ntrain_data[\"question_body_num_chars\"] = train_data[\"question_body\"].apply(lambda x: len(str(x)))\ntrain_data[\"answer_num_chars\"] = train_data[\"answer\"].apply(lambda x: len(str(x)))\n\ntest_data[\"question_title_num_chars\"] = test_data[\"question_title\"].apply(lambda x: len(str(x)))\ntest_data[\"question_body_num_chars\"] = test_data[\"question_body\"].apply(lambda x: len(str(x)))\ntest_data[\"answer_num_chars\"] = test_data[\"answer\"].apply(lambda x: len(str(x)))\n\n# Number of words in the text\ntrain_data[\"question_title_num_words\"] = train_data[\"question_title\"].apply(lambda x: len(str(x).split()))\ntrain_data[\"question_body_num_words\"] = train_data[\"question_body\"].apply(lambda x: len(str(x).split()))\ntrain_data[\"answer_num_words\"] = train_data[\"answer\"].apply(lambda x: len(str(x).split()))\n\ntest_data[\"question_title_num_words\"] = test_data[\"question_title\"].apply(lambda x: len(str(x).split()))\ntest_data[\"question_body_num_words\"] = test_data[\"question_body\"].apply(lambda x: len(str(x).split()))\ntest_data[\"answer_num_words\"] = test_data[\"answer\"].apply(lambda x: len(str(x).split()))\n\n# Number of unique words in the text\ntrain_data[\"question_title_num_unique_words\"] = train_data[\"question_title\"].apply(lambda x: len(set(str(x).split())))\ntrain_data[\"question_body_num_unique_words\"] = train_data[\"question_body\"].apply(lambda x: len(set(str(x).split())))\ntrain_data[\"answer_num_unique_words\"] = train_data[\"answer\"].apply(lambda x: len(set(str(x).split())))\n\ntest_data[\"question_title_num_unique_words\"] = test_data[\"question_title\"].apply(lambda x: len(set(str(x).split())))\ntest_data[\"question_body_num_unique_words\"] = test_data[\"question_body\"].apply(lambda x: len(set(str(x).split())))\ntest_data[\"answer_num_unique_words\"] = test_data[\"answer\"].apply(lambda x: len(set(str(x).split())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='6-3-2'>6.3.2 TF-IDF Features</a>"},{"metadata":{},"cell_type":"markdown","source":"#### TF-IDF :\n  *  Term Frequency (TF) and Inverse Document Frequency (IDF)\n  *  TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n  *  IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n  \n**TF-IDF based features are :**\n\n* Character Level N-Gram TF-IDF of question_title\n* Character Level N-Gram TF-IDF of question_body\n* Character Level N-Gram TF-IDF of answer\n* Word Level N-Gram TF-IDF of question_title\n* Word Level N-Gram TF-IDF of question_body\n* Word Level N-Gram TF-IDF of answer"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(ngram_range=(1, 3))\ntsvd = TruncatedSVD(n_components = 128, n_iter=5)\ntfquestion_title = tfidf.fit_transform(train_data[\"question_title\"].values)\ntfquestion_title_test = tfidf.transform(test_data[\"question_title\"].values)\ntfquestion_title = tsvd.fit_transform(tfquestion_title)\ntfquestion_title_test = tsvd.transform(tfquestion_title_test)\n\ntfquestion_body = tfidf.fit_transform(train_data[\"question_body\"].values)\ntfquestion_body_test = tfidf.transform(test_data[\"question_body\"].values)\ntfquestion_body = tsvd.fit_transform(tfquestion_body)\ntfquestion_body_test = tsvd.transform(tfquestion_body_test)\n\ntfanswer = tfidf.fit_transform(train_data[\"answer\"].values)\ntfanswer_test = tfidf.transform(test_data[\"answer\"].values)\ntfanswer = tsvd.fit_transform(tfanswer)\ntfanswer_test = tsvd.transform(tfanswer_test)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train_data[\"tfquestion_title\"] = list(tfquestion_title)\ntest_data[\"tfquestion_title_test\"] = list(tfquestion_title_test)\n\ntrain_data[\"tfquestion_body\"] = list(tfquestion_body)\ntest_data[\"tfquestion_body_test\"] = list(tfquestion_body_test)\n\ntrain_data[\"tfanswer\"] = list(tfanswer)\ntest_data[\"tfanswer_test\"] = list(tfanswer_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# More To Come. Stay Tuned. !!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}