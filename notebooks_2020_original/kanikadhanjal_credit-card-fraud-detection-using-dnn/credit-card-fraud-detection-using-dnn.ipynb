{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# First, we'll import helpful packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the data set\ndf = pd.read_csv('../input/creditcardfraud/creditcard.csv')\n# Determine number of fraud and genuine cases in dataset\nprint(len(df[df['Class'] == 1]))\nprint(len(df[df['Class'] == 0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding duplicate rows in dataset\nduplicateRowsDF = df[df.duplicated()]\n\nprint(\"Duplicate Rows except first occurrence based on all columns are :\")\nprint(duplicateRowsDF.head())\n\nprint(duplicateRowsDF.shape)\n\n# Number of duplicate fraud cases\nprint(len(duplicateRowsDF[duplicateRowsDF[\"Class\"] == 1]))\n\n# Number of duplicate genuine cases\nprint(len(duplicateRowsDF[duplicateRowsDF[\"Class\"] == 0]))\n\nprint(duplicateRowsDF.index)\n\n##Removing duplicates from the dataset\ndf = df.drop(duplicateRowsDF.index)\n\n##after removing duplicate rows, printing the shape of the dataset\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine number of fraud and genuine cases in dataset\nFraud = df[df['Class'] == 1]\nValid = df[df['Class'] == 0]\nprint(len(df[df['Class'] == 1]))\nprint(len(df[df['Class'] == 0]))\nprint(\"Length of dataset after removing duplicates is\", len(df))\nprint(df.shape)\nfraud_perc = len(Fraud) / float(len(df))\nprint(fraud_perc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since most of data has already been scaled, scaling two columns that are left to scale (Amount and Time)\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\n# RobustScaler is less prone to outliers.\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1, 1))\ndf.drop(['Time', 'Amount'], axis=1, inplace=True)\nAmount = df['scaled_amount']\nTime = df['scaled_time']\ndf.insert(0, 'Amount', Amount)\ndf.insert(1, 'Time', Time)\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n\n# Amount and Time are Scaled!\nprint(df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dividing the X and the Y from the dataset\nX = df.drop(['Class'], axis = 1)\n\nY = df[\"Class\"]\nprint(X.shape)\nprint(Y.shape)\n\n# getting just the values for the sake of processing\n\nxData = X.values\nyData = Y.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the data into train and testing\nX_train, X_test, Y_train, Y_test = train_test_split(xData, yData, test_size=0.1, random_state=42, stratify=Y)\nprint(X_train.shape, Y_train.shape)\nprint(X_test.shape, Y_test.shape)\n\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42, stratify=Y_train)\n\nprint(X_train.shape, Y_train.shape)\nprint(X_test.shape, Y_test.shape)\nprint(X_val.shape, Y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 512\n\n#Build Deep neural networks\nn_cols_2 = xData.shape[1]\n\n#create model\nmodel = Sequential()\n\n#add layers to model\nmodel.add(Dense(16, activation='relu', input_shape=(n_cols_2,)))\nmodel.add(Dense(1, activation='sigmoid'))\nprint(model.summary())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fiiting the model\nhistory=model.fit(X_train, Y_train, batch_size = batch_size, epochs = 100, validation_data= (X_val, Y_val), verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the Test set results\ny_pred = model.predict(X_test)\ny_pred = (y_pred > 0.5)\n#print(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see how our model performed\nfrom sklearn.metrics import classification_report\nprint(classification_report(Y_test, y_pred))\nprint(\"Confusion Matrix\\n\",confusion_matrix(Y_test,y_pred))\nprint(\"\\n\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}