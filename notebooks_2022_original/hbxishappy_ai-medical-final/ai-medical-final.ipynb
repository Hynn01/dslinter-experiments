{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"å¿«æ¥æŸ¥çœ‹æˆ‘ç”¨é£žä¹¦åˆ†äº«çš„ã€Medical imaging final projectã€‘ðŸ‘‰https://bytedance.feishu.cn/docx/doxcnUZ0zoBiu33CscEGVrDxF8b","metadata":{}},{"cell_type":"markdown","source":"# 0. Dependency And Configuration","metadata":{}},{"cell_type":"code","source":"!pip install torchinfo\n!pip install openpyxl\n!pip install hvplot","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T06:56:17.867709Z","iopub.execute_input":"2022-05-08T06:56:17.867973Z","iopub.status.idle":"2022-05-08T06:56:53.109783Z","shell.execute_reply.started":"2022-05-08T06:56:17.867898Z","shell.execute_reply":"2022-05-08T06:56:53.108945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install d2l","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T06:56:53.11188Z","iopub.execute_input":"2022-05-08T06:56:53.112257Z","iopub.status.idle":"2022-05-08T06:57:26.63793Z","shell.execute_reply.started":"2022-05-08T06:56:53.112216Z","shell.execute_reply":"2022-05-08T06:57:26.637121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os  \nimport glob\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\nimport PIL \nimport numpy as np\nimport matplotlib.pyplot as plt \n\nimport torch\nimport torch.nn as nn\nfrom torchinfo import summary \n\nimport torch.optim as optim\nfrom IPython.display import Image\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import transforms\n\nimport cv2\n\nimport pandas as pd\n\nfrom dask import bag, diagnostics \nimport hvplot.pandas  \n\nfrom d2l import torch as d2l\nimport torchvision\nimport copy","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:26.641224Z","iopub.execute_input":"2022-05-08T06:57:26.641458Z","iopub.status.idle":"2022-05-08T06:57:32.0755Z","shell.execute_reply.started":"2022-05-08T06:57:26.641429Z","shell.execute_reply":"2022-05-08T06:57:32.074489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Configuration","metadata":{}},{"cell_type":"code","source":"class CFG:\n    seed          = 124\n    debug         = False                             # set debug=False for Full Training\n    exp_name      = 'AI-Medical-Final'\n    model_name    = 'Unet'\n    backbone      = 'efficientnet-b2'\n    train_bs      = 24\n    valid_bs      = 48\n    img_size      = [250,250]\n    epochs        = 50\n    lr            = 5e-3\n    scheduler     = 'CosineAnnealingLR'\n    min_lr        = 1e-6\n    T_max         = int(100*6*1.8)\n    T_0           = 25\n    warmup_epochs = 0\n    wd            = 1e-6\n    n_accumulate  = 32//train_bs\n    n_fold        = 5\n    num_classes   = 1\n    device        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _wandb_kernel = 'awsaf49'\n    # æ–‡ä»¶è·¯å¾„\n    root          = '/kaggle/input/covidct/'\n    csv_file      = root + 'COVID-CT-MetaInfo.xlsx'\n    sheet_name    = 'positive_captions'","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:32.078105Z","iopub.execute_input":"2022-05-08T06:57:32.078887Z","iopub.status.idle":"2022-05-08T06:57:32.089025Z","shell.execute_reply.started":"2022-05-08T06:57:32.078834Z","shell.execute_reply":"2022-05-08T06:57:32.088278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(CFG.seed)\ntorch.manual_seed(CFG.seed)\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:32.090362Z","iopub.execute_input":"2022-05-08T06:57:32.091219Z","iopub.status.idle":"2022-05-08T06:57:32.100295Z","shell.execute_reply.started":"2022-05-08T06:57:32.091161Z","shell.execute_reply":"2022-05-08T06:57:32.09939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data Preparation","metadata":{}},{"cell_type":"code","source":"# List files available\nlist(os.listdir(CFG.root))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:32.101724Z","iopub.execute_input":"2022-05-08T06:57:32.102749Z","iopub.status.idle":"2022-05-08T06:57:32.118532Z","shell.execute_reply.started":"2022-05-08T06:57:32.102709Z","shell.execute_reply":"2022-05-08T06:57:32.11776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_files = glob.glob(os.path.join(CFG.root, \"CT_COVID\",'*.*'))\nneg_files = glob.glob(os.path.join(CFG.root, 'CT_NonCOVID','*.*'))\n\nimages = pos_files + neg_files\n\nlabels = np.array([1]*len(pos_files)+[0]*len(neg_files))\n\n\nimages_tv, images_test, y_tv, y_test  = train_test_split(images, labels, shuffle=True, test_size=0.2, random_state=CFG.seed)\nimages_train, images_val, y_train, y_val  = train_test_split(images_tv, y_tv, shuffle=True, test_size=0.25, random_state=CFG.seed)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-08T06:57:32.11971Z","iopub.execute_input":"2022-05-08T06:57:32.120136Z","iopub.status.idle":"2022-05-08T06:57:32.422629Z","shell.execute_reply.started":"2022-05-08T06:57:32.12009Z","shell.execute_reply":"2022-05-08T06:57:32.421905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_df = pd.read_excel(CFG.csv_file, sheet_name=CFG.sheet_name, header=None)\nmeta_df","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:32.423883Z","iopub.execute_input":"2022-05-08T06:57:32.424206Z","iopub.status.idle":"2022-05-08T06:57:32.638999Z","shell.execute_reply.started":"2022-05-08T06:57:32.424167Z","shell.execute_reply":"2022-05-08T06:57:32.638264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. EDA","metadata":{}},{"cell_type":"code","source":"num_pos, num_neg = len(pos_files), len(neg_files)\n\nplt.title('Distribution of labels')\nplt.bar(['Positive', 'Negative'], [num_pos, num_neg])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:32.640189Z","iopub.execute_input":"2022-05-08T06:57:32.640943Z","iopub.status.idle":"2022-05-08T06:57:32.804784Z","shell.execute_reply.started":"2022-05-08T06:57:32.640904Z","shell.execute_reply":"2022-05-08T06:57:32.804039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im = [cv2.imread(images_train[i]) for i in range(6)]\n\nfig,ax = plt.subplots(ncols=6, figsize=(18,6))\nfor i in range(len(im)):\n    ax[i].imshow(im[i],cmap='gray')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:32.807915Z","iopub.execute_input":"2022-05-08T06:57:32.808111Z","iopub.status.idle":"2022-05-08T06:57:33.652977Z","shell.execute_reply.started":"2022-05-08T06:57:32.808085Z","shell.execute_reply":"2022-05-08T06:57:33.652352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of samples in each set (train, val, test): {len(y_train), len(y_val), len(y_test)}')\n\nprint(f'Number of positive samples in each set: {y_train.sum(), y_val.sum(), y_test.sum()}')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:33.654298Z","iopub.execute_input":"2022-05-08T06:57:33.654718Z","iopub.status.idle":"2022-05-08T06:57:33.65989Z","shell.execute_reply.started":"2022-05-08T06:57:33.65468Z","shell.execute_reply":"2022-05-08T06:57:33.659187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The images are clearly of different dimensions. We can observe the distribution of images' dimension.","metadata":{}},{"cell_type":"code","source":"# get image dimensions\ndef get_dims(file):\n    img = cv2.imread(file)\n    h,w = img.shape[:2]\n    return h,w\n\n# parallelize\ndimsbag = bag.from_sequence(images).map(get_dims)\nwith diagnostics.ProgressBar():\n    dims = dimsbag.compute()\n    \ndim_df = pd.DataFrame(dims, columns=['height', 'width'])\nsizes = dim_df.groupby(['height', 'width']).size().reset_index().rename(columns={0:'count'})\nsizes.hvplot.scatter(x='height', y='width', size='count', xlim=(0,1200), ylim=(0,1200), grid=True, xticks=2, \n        yticks=2, height=500, width=600).options(scaling_factor=0.1, line_alpha=1, fill_alpha=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:33.661333Z","iopub.execute_input":"2022-05-08T06:57:33.661777Z","iopub.status.idle":"2022-05-08T06:57:39.302383Z","shell.execute_reply.started":"2022-05-08T06:57:33.661738Z","shell.execute_reply":"2022-05-08T06:57:39.301693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Dataset","metadata":{}},{"cell_type":"code","source":"class CT_Dataset(Dataset):\n    def __init__(self, img_path, img_labels, img_transforms=None, grayscale=True):\n        self.img_path = img_path\n        self.img_labels = torch.Tensor(img_labels)\n        self.img_transfroms = img_transforms\n        if (img_transforms is None) & (grayscale == True):\n            self.transforms = transforms.Compose([transforms.Grayscale(),\n                                                  transforms.Resize((250, 250)),\n                                                  transforms.ToTensor()])\n        elif grayscale == False:\n            self.transforms = transforms.Compose([transforms.Resize((250, 250)),\n                                                  transforms.ToTensor()])\n        else:\n            self.transforms = img_transforms\n    \n    def __getitem__(self, index):\n        # load image\n        cur_path = self.img_path[index]\n        cur_img = PIL.Image.open(cur_path).convert('RGB')\n        cur_img = self.transforms(cur_img)\n\n        return cur_img, self.img_labels[index]\n    \n    def __len__(self):\n        return len(self.img_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:39.303929Z","iopub.execute_input":"2022-05-08T06:57:39.304333Z","iopub.status.idle":"2022-05-08T06:57:39.313077Z","shell.execute_reply.started":"2022-05-08T06:57:39.304296Z","shell.execute_reply":"2022-05-08T06:57:39.312439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data Augmentation","metadata":{}},{"cell_type":"code","source":"class D_aug:\n    def __init__(self,status):\n        self.status = status\n        self.transform = {\n            'train':\n                    transforms.Compose([transforms.Grayscale(),\n                                transforms.RandomRotation(5),\n                                transforms.Resize(CFG.img_size),\n                                transforms.RandomVerticalFlip(),\n                                transforms.RandomHorizontalFlip(),\n                                transforms.ColorJitter(brightness=0.3, contrast=0.5, saturation=0.5, hue=0.5),\n                                transforms.RandomAffine(degrees=0, scale=(1.1, 1.1), shear=0.9),\n                                transforms.ToTensor()\n                                ]),\n            'valid':\n                    transforms.Compose([transforms.Grayscale(),\n                                transforms.Resize(CFG.img_size),\n                                transforms.ToTensor()\n                                ]),\n            'test' :\n                    transforms.Compose([transforms.Grayscale(),\n                                transforms.Resize(CFG.img_size),\n                                transforms.ToTensor()\n                                ])                    \n                         }\n\n    def __call__(self, image):\n        trans = self.transform[self.status](image)\n        return trans\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:39.314467Z","iopub.execute_input":"2022-05-08T06:57:39.314732Z","iopub.status.idle":"2022-05-08T06:57:39.326337Z","shell.execute_reply.started":"2022-05-08T06:57:39.314697Z","shell.execute_reply":"2022-05-08T06:57:39.325635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = PIL.Image.open(images_train[10])\nD_instance = D_aug('train')\ndisplay(transforms.ToPILImage()(D_instance(img)))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:39.32754Z","iopub.execute_input":"2022-05-08T06:57:39.327856Z","iopub.status.idle":"2022-05-08T06:57:39.40702Z","shell.execute_reply.started":"2022-05-08T06:57:39.327814Z","shell.execute_reply":"2022-05-08T06:57:39.406425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_train_aug = CT_Dataset(img_path=images_train, img_labels=y_train, img_transforms=D_instance)\nprint(len(temp_train_aug))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:39.408079Z","iopub.execute_input":"2022-05-08T06:57:39.40969Z","iopub.status.idle":"2022-05-08T06:57:39.415946Z","shell.execute_reply.started":"2022-05-08T06:57:39.40966Z","shell.execute_reply":"2022-05-08T06:57:39.415132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Single instance of data augmentation method\ndef apply_trans_method(img, aug, num_rows=2, num_cols=4, scale=4):\n    Y = [aug(img) for _ in range(num_rows * num_cols)]\n    d2l.show_images(Y, num_rows, num_cols, scale=scale)\n\napply_trans_method(d2l.Image.open(images_train[10]),transforms.ColorJitter(\n    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:39.417556Z","iopub.execute_input":"2022-05-08T06:57:39.418303Z","iopub.status.idle":"2022-05-08T06:57:40.055196Z","shell.execute_reply.started":"2022-05-08T06:57:39.418266Z","shell.execute_reply":"2022-05-08T06:57:40.05429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compose\ndef visualization_aug_method(dataset,index,row,col):\n    fig, axes = plt.subplots(row, col, figsize=(15, 10))\n    for j in range(row):\n        for i in range(col):\n        # Each time the data is accessed, the result is different due to random augmentation!\n            img, label = temp_train_aug[index]\n            ax = axes[j][i]\n            ax.imshow(img.cpu().numpy().transpose((1, 2, 0)) / 255.)\n            ax.set_title(f\"{index}-th image: label {label}\")\n    plt.show()\nvisualization_aug_method(temp_train_aug,10,2,4)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:40.056471Z","iopub.execute_input":"2022-05-08T06:57:40.057054Z","iopub.status.idle":"2022-05-08T06:57:41.05525Z","shell.execute_reply.started":"2022-05-08T06:57:40.057009Z","shell.execute_reply":"2022-05-08T06:57:41.05465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. DataLoader","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Original Dataset","metadata":{}},{"cell_type":"code","source":"train_dataset = CT_Dataset(img_path=images_train, img_labels=y_train)\nval_dataset = CT_Dataset(img_path=images_val, img_labels=y_val)\ntest_dataset = CT_Dataset(img_path=images_test, img_labels=y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:41.056248Z","iopub.execute_input":"2022-05-08T06:57:41.056662Z","iopub.status.idle":"2022-05-08T06:57:41.0632Z","shell.execute_reply.started":"2022-05-08T06:57:41.056625Z","shell.execute_reply":"2022-05-08T06:57:41.062425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 Augmenting the Entire Training Set","metadata":{}},{"cell_type":"code","source":"D_instance = D_aug('train')\ntrain_dataset_full_aug = CT_Dataset(img_path=images_train, img_labels=y_train, img_transforms=D_instance)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:41.064633Z","iopub.execute_input":"2022-05-08T06:57:41.064864Z","iopub.status.idle":"2022-05-08T06:57:41.071988Z","shell.execute_reply.started":"2022-05-08T06:57:41.064835Z","shell.execute_reply":"2022-05-08T06:57:41.071181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3 Concatenate Augmented Data to Original Dataset","metadata":{}},{"cell_type":"code","source":"train_dataset_fin = torch.utils.data.ConcatDataset([train_dataset,train_dataset_full_aug])\n\nprint(len(train_dataset_fin))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:41.073642Z","iopub.execute_input":"2022-05-08T06:57:41.073894Z","iopub.status.idle":"2022-05-08T06:57:41.081192Z","shell.execute_reply.started":"2022-05-08T06:57:41.073861Z","shell.execute_reply":"2022-05-08T06:57:41.080284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Baseline Model","metadata":{}},{"cell_type":"markdown","source":"## 6.1 Encoder & Decoder","metadata":{}},{"cell_type":"code","source":"# Encoder\nclass Encoder(nn.Module):\n    def __init__(self, dropout=0.5):\n        super(Encoder, self).__init__()\n        self.encoder = nn.Sequential(\n          # input (num_batch, 1, 250, 250)\n          nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3),  # (num_batch, 64, 248, 248)\n          nn.BatchNorm2d(64),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2),  # (num_batch, 64, 124, 124)\n\n          nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3), # (num_batch, 128, 122, 122)\n          nn.BatchNorm2d(128),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2),  # (num_batch, 128, 61, 61)\n\n          nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3), # (num_batch, 256, 59, 59)\n          nn.BatchNorm2d(256),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2),  # (num_batch, 256, 29, 29)\n\n          nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3), # (num_batch, 128, 27, 27)\n          nn.BatchNorm2d(512),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2),  # (num_batch, 128, 13, 13)\n\n          nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3), # (num_batch, 64, 11, 11)\n          nn.BatchNorm2d(512),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2),  # (num_batch, 64, 5, 5)\n          nn.Flatten() # (num_batch, 1600)\n        )\n    def forward(self, x):\n        x = self.encoder(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:41.08272Z","iopub.execute_input":"2022-05-08T06:57:41.082968Z","iopub.status.idle":"2022-05-08T06:57:41.096271Z","shell.execute_reply.started":"2022-05-08T06:57:41.082935Z","shell.execute_reply":"2022-05-08T06:57:41.095305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decoder\nclass Decoder(nn.Module):\n    def __init__(self, dropout=0.5):\n        super(Decoder, self).__init__()\n        self.decoder = nn.Sequential(\n            nn.Dropout(dropout),  # Dropout before first linear layer since it has a large number of trainable parameters\n            nn.Linear(in_features= 12800, out_features=512),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(in_features=512, out_features=256),\n            nn.ReLU(),\n            nn.Linear(in_features=256, out_features=128),\n            nn.ReLU(),\n            nn.Linear(in_features=128, out_features=1)\n        )\n    \n    def forward(self, x):\n        x = self.decoder(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:41.097865Z","iopub.execute_input":"2022-05-08T06:57:41.098167Z","iopub.status.idle":"2022-05-08T06:57:41.108994Z","shell.execute_reply.started":"2022-05-08T06:57:41.098074Z","shell.execute_reply":"2022-05-08T06:57:41.108304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2 Convnet","metadata":{}},{"cell_type":"code","source":"class Convnet(nn.Module):\n    \n    def __init__(self, encoder, decoder, **kwargs):\n        super(Convnet, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, enc_X, *args):\n        enc_outputs = self.encoder(enc_X, *args)\n        return (enc_outputs,self.decoder(enc_outputs,*args))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:41.110356Z","iopub.execute_input":"2022-05-08T06:57:41.110667Z","iopub.status.idle":"2022-05-08T06:57:41.120109Z","shell.execute_reply.started":"2022-05-08T06:57:41.110632Z","shell.execute_reply":"2022-05-08T06:57:41.119491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.3 Unit Test","metadata":{}},{"cell_type":"code","source":"encoder_temp = Encoder()\nencoder_temp.eval()\nX_temp = torch.zeros((1,1, 250, 250), dtype=torch.float)\nencoder_temp_output = encoder_temp(X_temp)\nencoder_temp_output.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:41.121581Z","iopub.execute_input":"2022-05-08T06:57:41.121828Z","iopub.status.idle":"2022-05-08T06:57:41.368453Z","shell.execute_reply.started":"2022-05-08T06:57:41.121792Z","shell.execute_reply":"2022-05-08T06:57:41.367762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(encoder_temp,(1,1, 250, 250))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:41.369712Z","iopub.execute_input":"2022-05-08T06:57:41.369954Z","iopub.status.idle":"2022-05-08T06:57:49.114968Z","shell.execute_reply.started":"2022-05-08T06:57:41.369919Z","shell.execute_reply":"2022-05-08T06:57:49.114274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoder_temp = Decoder()\nsummary(decoder_temp,encoder_temp_output.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:49.116141Z","iopub.execute_input":"2022-05-08T06:57:49.116493Z","iopub.status.idle":"2022-05-08T06:57:49.950965Z","shell.execute_reply.started":"2022-05-08T06:57:49.116454Z","shell.execute_reply":"2022-05-08T06:57:49.950279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"convnet_temp = Convnet(encoder_temp,decoder_temp)\nsummary(convnet_temp,(1,1,250, 250))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:49.95499Z","iopub.execute_input":"2022-05-08T06:57:49.95519Z","iopub.status.idle":"2022-05-08T06:57:49.973379Z","shell.execute_reply.started":"2022-05-08T06:57:49.955164Z","shell.execute_reply":"2022-05-08T06:57:49.972772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"convnet_temp.eval()\nX_temp = torch.zeros((1,1, 250, 250), dtype=torch.float).to(CFG.device)\nconvnet_temp(X_temp)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:49.974623Z","iopub.execute_input":"2022-05-08T06:57:49.974856Z","iopub.status.idle":"2022-05-08T06:57:50.021382Z","shell.execute_reply.started":"2022-05-08T06:57:49.974825Z","shell.execute_reply":"2022-05-08T06:57:50.020762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.4 Training Part","metadata":{}},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:50.023047Z","iopub.execute_input":"2022-05-08T06:57:50.023686Z","iopub.status.idle":"2022-05-08T06:57:50.724497Z","shell.execute_reply.started":"2022-05-08T06:57:50.023649Z","shell.execute_reply":"2022-05-08T06:57:50.723363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:50.728653Z","iopub.execute_input":"2022-05-08T06:57:50.731596Z","iopub.status.idle":"2022-05-08T06:57:51.662334Z","shell.execute_reply.started":"2022-05-08T06:57:50.73155Z","shell.execute_reply":"2022-05-08T06:57:51.661438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define training function\ndef train_model(model, train_dataset, val_dataset, test_dataset, device, model_name,\n                lr=0.02, epochs=35, batch_size=32, weight_decay=0.9, gamma=0.9,\n                patience=5, early_stop=3, verbose=False, save=True):\n    model = model.to(device)\n\n    # construct dataloader\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\n    # history\n    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"test_acc\":0}\n    best_valid_acc = float(\"-inf\")\n    early_stop_step = 0\n    best_epoch = 0\n\n    # set up loss function and optimizer\n    criterion = nn.BCEWithLogitsLoss()  \n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=patience, gamma=gamma)\n\n    # Training Loop\n    if verbose:\n        print(\"Training Start:\")\n    for epoch in range(epochs):\n        # shuffle training data in each epoch\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        # train\n        train_loss = 0\n        train_acc = 0\n        model.train() \n        for i, (images, labels) in enumerate(train_loader):\n            # move batch to device\n            images = images.to(device)\n            labels = labels.to(device)\n            # forward\n            outputs_list = model(images)\n            enc_outputs = outputs_list[0].view(-1)\n            outputs = outputs_list[1].view(-1)\n            # pred\n            pred = torch.sigmoid(outputs)\n            pred = torch.round(pred)\n            # loss\n            cur_train_loss = criterion(outputs, labels)\n            # acc\n            cur_train_acc = (pred == labels).sum().item() / batch_size\n            # backward\n            cur_train_loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            # update total loss\n            train_loss += cur_train_loss \n            train_acc += cur_train_acc\n        \n        # valid\n        val_loss = 0\n        val_acc = 0\n        model.eval()\n        with torch.no_grad():\n            for images, labels in val_loader:\n                # calculate validation loss\n                images = images.to(device)\n                labels = labels.to(device)\n                outputs_list = model(images)\n                enc_outputs = outputs_list[0].view(-1)\n                outputs = outputs_list[1].view(-1)\n                # loss\n                cur_valid_loss = criterion(outputs, labels)\n                val_loss += cur_valid_loss\n                # acc\n                pred = torch.sigmoid(outputs)\n                pred = torch.round(pred)\n                val_acc += (pred == labels).sum().item() / batch_size\n\n        # update learning rate after every epoch\n        scheduler.step()\n        # print training feedback\n        train_loss = train_loss / len(train_loader)\n        train_acc = train_acc / len(train_loader)\n        val_loss = val_loss / len(val_loader)\n        val_acc = val_acc / len(val_loader)\n        if verbose:\n            print(f\"Epoch:{epoch + 1} / {epochs}, lr: {optimizer.param_groups[0]['lr']:.5f} train loss:{train_loss:.5f}, train acc: {train_acc:.5f}, valid loss:{val_loss:.5f}, valid acc:{val_acc:.5f}\")\n        # update history\n        history['train_loss'].append(train_loss.cpu().detach().numpy())\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss.cpu().detach().numpy())\n        history['val_acc'].append(val_acc)\n        # early stop\n        if val_acc >= best_valid_acc:\n            best_valid_acc = val_acc\n            best_model = copy.deepcopy(model)\n            early_stop_step = 0\n            best_epoch = epoch\n        else:\n            early_stop_step += 1\n            if early_stop_step >= early_stop:\n                model = best_model\n                break \n                \n    # test\n    test_acc = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs_list = model(images)\n            enc_outputs = outputs_list[0]\n            outputs = outputs_list[1]\n            pred = torch.sigmoid(outputs)\n            pred = torch.round(pred)\n            test_acc += (pred == labels).sum().item()\n\n    history[\"test_acc\"] = test_acc / len(test_loader)\n    print(f'Best Epoch: {best_epoch}, Test Accuracy: {history[\"test_acc\"]}')\n\n    model = model.to(\"cpu\")\n    if save:\n        torch.save(model, model_name)\n    \n    return history","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:51.664608Z","iopub.execute_input":"2022-05-08T06:57:51.664995Z","iopub.status.idle":"2022-05-08T06:57:51.688934Z","shell.execute_reply.started":"2022-05-08T06:57:51.664946Z","shell.execute_reply":"2022-05-08T06:57:51.688245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot loss and accuracy\ndef plot_training_curves(hist):\n    # plot training curves\n    epochs = range(1, len(hist['train_loss']) + 1)\n\n    fig, ax = plt.subplots(1,2, figsize=(18,6))\n    ax[0].plot(epochs, hist['train_loss'], 'r-', label='Train')\n    ax[0].plot(epochs, hist['val_loss'], 'b-', label='Evaluation')\n    ax[0].set_title('Loss')\n    ax[0].set_xlabel('Epochs')\n    ax[0].set_ylabel('Loss')\n    ax[0].legend()\n\n    ax[1].plot(epochs, hist['train_acc'], 'r-', label='Train')\n    ax[1].plot(epochs, hist['val_acc'], 'b-', label='Evaluation')\n    ax[1].set_title('Accuracy')\n    ax[1].set_xlabel('Epochs')\n    ax[1].set_ylabel('Acc')\n    ax[1].legend()\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:57:51.69017Z","iopub.execute_input":"2022-05-08T06:57:51.690584Z","iopub.status.idle":"2022-05-08T06:57:51.70768Z","shell.execute_reply.started":"2022-05-08T06:57:51.690542Z","shell.execute_reply":"2022-05-08T06:57:51.706913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.5 Original DS Training","metadata":{}},{"cell_type":"code","source":"result_set = {}\nencoder = Encoder()\ndecoder = Decoder()\n#baseline_model1 = Convnet(encoder,decoder)\nbaseline_model1 = torch.load(\"../input/baseline-model-data/model_baseline_model1.pth\")\n\n#=========æ³¨é‡Šè®­ç»ƒ=============\nhist = train_model(baseline_model1, train_dataset, val_dataset, test_dataset, CFG.device, \n                   model_name='./model_baseline_model1.pth',\n                   lr=0.002, epochs=50, batch_size=32, weight_decay=0.05, gamma=0.5,\n                   patience=5, early_stop=10, verbose=True)\n\n#=========å¯¼å…¥æ–‡ä»¶=============\n#hist = np.load('../input/baseline-model-data/hist.npy',allow_pickle=True)\n#print(hist)\nplot_training_curves(hist)\n\n#result_set[\"selfModel_origData\"] = hist","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T06:57:51.710861Z","iopub.execute_input":"2022-05-08T06:57:51.711551Z","iopub.status.idle":"2022-05-08T07:00:18.058232Z","shell.execute_reply.started":"2022-05-08T06:57:51.711513Z","shell.execute_reply":"2022-05-08T07:00:18.057539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.6 Aug DS Training","metadata":{}},{"cell_type":"code","source":"encoder = Encoder()\ndecoder = Decoder()\n#baseline_model2_aug = Convnet(encoder,decoder)\n\nbaseline_model2_aug = torch.load('../input/baseline-model-data/model_baseline_model2.pth')\n\nhist2 = train_model(baseline_model2_aug, train_dataset_fin, val_dataset, test_dataset, CFG.device, \n                   model_name='./model_baseline_model2.pth',\n                   lr=0.002, epochs=50, batch_size=32, weight_decay=0.05, gamma=0.5,\n                   patience=5, early_stop=10, verbose=True)\n\n\nplot_training_curves(hist2)\n\n#result_set[\"selfModel_ConcatData\"] = hist2","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T07:00:18.059713Z","iopub.execute_input":"2022-05-08T07:00:18.060122Z","iopub.status.idle":"2022-05-08T07:05:51.910074Z","shell.execute_reply.started":"2022-05-08T07:00:18.060083Z","shell.execute_reply":"2022-05-08T07:05:51.909324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Contrastive Loss","metadata":{}},{"cell_type":"markdown","source":"The next part:\n- https://www.kaggle.com/code/zhengxiaofancn/ai-medical-final-7d5a2d/notebook","metadata":{}}]}