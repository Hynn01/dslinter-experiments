{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n\n<br><center><img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/27923/logos/header.png?t=2021-06-02-20-30-25\" width=100%></center>\n\n<h2 style=\"text-align: center; font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">UWM - GI Tract Image Segmentation Challenge - EDA</h2>\n<h5 style=\"text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: black; background-color: #ffffff;\">CREATED BY: DARIEN SCHETTLER, <br> EDITTED BY SAMUEL CORTINHAS</h5>","metadata":{}},{"cell_type":"markdown","source":"**Acknowledgement:**\n* The [original notebook](https://www.kaggle.com/code/dschettler8845/uwm-gi-tract-image-segmentation-eda/notebook) was created by [Darien Schettler](https://www.kaggle.com/dschettler8845). If you haven't seen his work already then go and give him an upvote please!\n* My contribution is in section 4.9 where I extend the 3D GIFs to work on a case by case basis, showing all the days simultaneously.","metadata":{}},{"cell_type":"markdown","source":"<p id=\"toc\"></p>\n\n<br><br>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\">TABLE OF CONTENTS</h1>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#imports\">0&nbsp;&nbsp;&nbsp;&nbsp;IMPORTS</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#background_information\">1&nbsp;&nbsp;&nbsp;&nbsp;BACKGROUND INFORMATION</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#setup\">2&nbsp;&nbsp;&nbsp;&nbsp;SETUP</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#helper_functions\">3&nbsp;&nbsp;&nbsp;&nbsp;HELPER FUNCTIONS</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#create_dataset\">4&nbsp;&nbsp;&nbsp;&nbsp;DATASET EXPLORATION</a></h3>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"imports\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: teal;\" id=\"imports\">0&nbsp;&nbsp;IMPORTS&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>","metadata":{}},{"cell_type":"code","source":"print(\"\\n... IMPORTS STARTING ...\\n\")\n\nprint(\"\\n\\tVERSION INFORMATION\")\n# Machine Learning and Data Science Imports\nimport tensorflow as tf; print(f\"\\t\\t– TENSORFLOW VERSION: {tf.__version__}\");\nimport tensorflow_hub as tfhub; print(f\"\\t\\t– TENSORFLOW HUB VERSION: {tfhub.__version__}\");  # trained models\nimport tensorflow_addons as tfa; print(f\"\\t\\t– TENSORFLOW ADDONS VERSION: {tfa.__version__}\"); # community contributions\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np; print(f\"\\t\\t– NUMPY VERSION: {np.__version__}\");\nimport sklearn; print(f\"\\t\\t– SKLEARN VERSION: {sklearn.__version__}\");\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures  # Generate polynomial and interaction features.\nfrom pandarallel import pandarallel; pandarallel.initialize();  # parallelerise pandas\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nfrom scipy.spatial import cKDTree  # kd-tree is a space partitioning function\n\n# # RAPIDS\n# import cudf, cupy, cuml\n# from cuml.neighbors import NearestNeighbors\n# from cuml.manifold import TSNE, UMAP\n\n# Built In Imports\nfrom kaggle_datasets import KaggleDatasets\nfrom collections import Counter\nfrom datetime import datetime\nfrom glob import glob # pathnames\nimport warnings\nimport requests\nimport hashlib\nimport imageio # read image data\nimport IPython # interactive shells\nimport sklearn\nimport urllib # working with urls\nimport zipfile # work with zip files\nimport pickle # save trained models\nimport random\nimport shutil # work with files\nimport string\nimport json # work with json data\nimport math\nimport time\nimport gzip # work with gzips\nimport ast # abstract syntax trees\nimport sys # system specific functions\nimport io\nimport os\nimport gc\nimport re\n\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.patches import Rectangle\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm; tqdm.pandas(); # progress bars\nimport plotly.express as px\nimport seaborn as sns\nfrom PIL import Image, ImageEnhance\nimport matplotlib; print(f\"\\t\\t– MATPLOTLIB VERSION: {matplotlib.__version__}\");\nfrom matplotlib import animation, rc; rc('animation', html='jshtml')\nimport plotly\nimport PIL # python image library\nimport cv2 # OpenCV - image processing for computer vision\n\nimport plotly.io as pio\nprint(pio.renderers)\n\ndef seed_it_all(seed=7):\n    \"\"\" Attempt to be Reproducible \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    \nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-26T19:33:05.732652Z","iopub.execute_input":"2022-04-26T19:33:05.733013Z","iopub.status.idle":"2022-04-26T19:33:14.811606Z","shell.execute_reply.started":"2022-04-26T19:33:05.732923Z","shell.execute_reply":"2022-04-26T19:33:14.810681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"background_information\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"background_information\">1&nbsp;&nbsp;BACKGROUND INFORMATION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">1.1 BASIC COMPETITION INFORMATION</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">PRIMARY TASK DESCRIPTION</b>\n\nIn this competition, you’ll create a model to automatically segment the stomach and intestines on MRI scans. The MRI scans are from actual cancer patients who had 1-5 MRI scans on separate days during their radiation treatment. You'll base your algorithm on a dataset of these scans to come up with creative deep learning solutions that will help cancer patients get better care.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">BASIC BACKGROUND INFORMATION</b>\n\nIn 2019, an estimated 5 million people were diagnosed with a cancer of the gastro-intestinal tract worldwide. Of these patients, about half are eligible for radiation therapy, usually delivered over 10-15 minutes a day for 1-6 weeks. Radiation oncologists try to deliver high doses of radiation using X-ray beams pointed to tumors while avoiding the stomach and intestines. With newer technology such as integrated magnetic resonance imaging and linear accelerator systems, also known as MR-Linacs, <b>oncologists are able to visualize the daily position of the tumor and intestines, <mark>which can vary day to day</mark></b>. \n\nIn these scans, radiation oncologists must manually outline the position of the stomach and intestines in order to adjust the direction of the x-ray beams to increase the dose delivery to the tumor and avoid the stomach and intestines. This is a time-consuming and labor intensive process that can prolong treatments from 15 minutes a day to an hour a day, which can be difficult for patients to tolerate—unless deep learning could help automate the segmentation process. <b><mark>A method to segment the stomach and intestines would make treatments much faster and would allow more patients to get more effective treatment.</mark></b>\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">COMPETITION HOST INFORMATION</b>\n\nThe UW-Madison Carbone Cancer Center is a pioneer in MR-Linac based radiotherapy, and has treated patients with MRI guided radiotherapy based on their daily anatomy since 2015. UW-Madison has generously agreed to support this project which provides anonymized MRIs of patients treated at the UW-Madison Carbone Cancer Center. The University of Wisconsin-Madison is a public land-grant research university in Madison, Wisconsin. The Wisconsin Idea is the university's pledge to the state, the nation, and the world that their endeavors will benefit all citizens.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">VISUAL EXPLANATION</b>\n\n<center><img src=\"https://lh5.googleusercontent.com/zbBUgbj1jyZxyu3r1vr5zKKr8yK1hSdwAM3HpD_n6j2W-5-wKP3ZRusi_3yskSgnC-tMRKqOEtLycbLkTWCJAUe4Cylv_VsW81DYI4ray02uZLeSnlzAuZRIU7L2Q0KURYSMqFI\"></center><br>\n\n<i>The tumor above (pink thick line) is close to the stomach (red thick line). High doses of radiation are directed to the tumor while avoiding the stomach. Dose levels are represented by colour. Higher doses are represented by red and lower doses are represented by green.</i><br>\n\n<br><center><img src=\"https://www.humonc.wisc.edu/wp-content/uploads/2017/09/Bayouth_Project4_72ppi.png\"></center><br>\n\n<i>MRI is an excellent imaging modality for visualization of soft tissues. This is particularly useful for tumors of the abdomen, such as pancreatic cancer shown below.  The left image shows the patient’s anatomy during exhale, while the image on the right shows the anatomical change during a maximum inspiration breath hold (MIBH). In the MIBH image we can see motion of nearly all the soft tissue, providing us superior ability to align the tumor during our treatment delivery. We are analyzing the clinical impact of using these treatment planning and delivery techniques and our patient’s ability to comply with self-guided breathing maneuvers.<b><a href=\"https://www.humonc.wisc.edu/research/medical-physics_research/mr-guided-radiation-therapy-research-2/\">[REF]</a></b></i>\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">COMPETITION IMPACT STATEMENT</b>\n\nCancer takes enough of a toll. If successful, you'll enable radiation oncologists to safely deliver higher doses of radiation to tumors while avoiding the stomach and intestines. This will make cancer patients' daily treatments faster and allow them to get more effective treatment with less side effects and better long-term cancer control.","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">1.2 COMPETITION EVALUATION</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">GENERAL EVALUATION INFORMATION</b>\n\nThis competition is evaluated on the mean <a href=\"https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\"><b>Dice coefficient</b></a> and <a href=\"https://github.com/scipy/scipy/blob/master/scipy/spatial/_hausdorff.pyx\"><b>3D Hausdorff distance</b></a>. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n\n$$\n\\frac{2 * |X \\cap Y|}{|X| + |Y|}\n$$\n\nwhere $X$ is the predicted set of pixels and $Y$ is the ground truth. \n* The Dice coefficient is defined to be $1$ when both $X$ and $Y$ are empty. \n* The leaderboard score is the <b>mean of the Dice coefficients for each image in the test set.</b>\n\nHausdorff distance is a method for calculating the distance between segmentation objects A and B, by calculating the furthest point on object A from the nearest point on object B. For 3D Hausdorff, we construct 3D volumes by combining each 2D segmentation with slice depth as the Z coordinate and then find the Hausdorff distance between them. **(In this competition, the slice depth for all scans is set to 1.)** <a href=\"https://github.com/scipy/scipy/blob/master/scipy/spatial/_hausdorff.pyx\"><b>The scipy code for Hausdorff is linked</b></a>. The expected / predicted pixel locations are normalized by image size to create a bounded 0-1 score.\n\n<br>\n    \n---\n\n<b>NOTE: The two metrics are combined during evaluation!</b>\n\n* <b>Weight of 0.4 for the Dice metric</b>\n* <b>Weight of 0.6 for the Hausdorff distance.</b>\n\n---\n\n<br>\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">SUBMISSION FILE INFORMATION</b>\n\nIn order to reduce the submission file size, our metric uses **run-length encoding** on the pixel values.  \n* Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length\n* E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n* Note that, at the time of encoding, the mask should be **binary**\n    * The masks for all objects in an image are joined into a single large mask\n    * The value of **0** should indicate pixels that are not **masked**\n    * The value of **1** will indicate pixels that are **masked**.\n\nThe competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.\n\n<br>\n\nThe file should contain a header and have the following format:\n\n```\nid,class,predicted\n1,large_bowel,1 1 5 1\n1,small_bowel,1 1\n1,stomach,1 1\n2,large_bowel,1 5 2 17\netc.\n```\n\n<br><font color=\"red\"><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">IS THIS A CODE COMPETITION?</b></font>\n\n<font color=\"red\" style=\"font-size: 30px\"><b>YES</b></font>\n\n<br>","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">1.3 DATASET OVERVIEW</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">GENERAL INFORMATION</b>\n\n<b><mark>In this competition we are segmenting organs cells in images</mark></b>. \n\nThe training **<mark>annotations are provided as RLE-encoded masks</mark>**, and the images are in **<mark>16-bit</mark>**, **<mark>grayscale</mark>**, **<mark>PNG format</mark>**.\n\nEach case in this competition is represented by multiple sets of scan slices\n* Each set is identified by the day the scan took place\n* Some cases are split by time\n    * early days are in train\n    * later days are in test\n* Some cases are split by case\n    * the entirety of the case is in train or test\n\n<b><mark>The goal of this competition is to be able to generalize to both partially and wholly unseen cases.</mark></b>\n\nNote that, in this case, the test set is entirely unseen.\n* It is roughly 50 cases\n* It contains a varying number of days and slices, (similar to the training set)\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">FILE INFORMATION</b>\n\n**`train.csv`** \n- IDs and masks for all training objects.\n- **Columns**\n    * **`id`**\n        * unique identifier for object\n    * **`class`**\n        * the predicted class for the object\n    * **`EncodedPixels`**\n        * RLE-encoded pixels for the identified object\n\n<br>\n\n**`sample_submission.csv`**\n- A sample submission file in the correct format\n\n<br>\n\n**`train/`**\n- a folder of case/day folders, each containing slice images for a particular case on a given day.\n\n<br>\n\n<center><div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 18px;\">⚠️ &nbsp; NOTE &nbsp; ⚠️</b><br><br><b style=\"font-size: 22px; color: darkorange\"></b><br><br>The <b>image filenames</b> include 4 numbers <b>(ex. 276_276_1.63_1.63.png)</b>.<br><br>These four numbers are representative of:<ul><li><b>slice height</b> (integer in pixels)</li><li><b>slice width</b> (integer in pixels)</li><li><b>heigh pixel spacing</b> (floating point in mm)</li><li><b>width pixel spacing</b> (floating point in mm)</li></ul><br>The first two defines the resolution of the slide. The last two record the physical size of each pixel.<br><br>\n</div></center>\n\n \n\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"background_information\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"setup\">2&nbsp;&nbsp;SETUP&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">2.1 ACCELERATOR DETECTION</h3>\n\n---","metadata":{}},{"cell_type":"code","source":"TPU=None","metadata":{"execution":{"iopub.status.busy":"2022-04-26T19:33:14.813196Z","iopub.execute_input":"2022-04-26T19:33:14.813432Z","iopub.status.idle":"2022-04-26T19:33:14.818264Z","shell.execute_reply.started":"2022-04-26T19:33:14.813404Z","shell.execute_reply":"2022-04-26T19:33:14.817548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">2.2 COMPETITION DATA ACCESS</h3>\n\n---\n\nTPUs read data must be read directly from **G**oogle **C**loud **S**torage **(GCS)**. Kaggle provides a utility library – **`KaggleDatasets`** – which has a utility function **`.get_gcs_path`** that will allow us to access the location of our input datasets within **GCS**.<br><br>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">📌 &nbsp; TIPS:</b><br><br>- If you have multiple datasets attached to the notebook, you should pass the name of a specific dataset to the <b><code>`get_gcs_path()`</code></b> function. <i>In our case, the name of the dataset is the name of the directory the dataset is mounted within.</i><br><br>\n</div>","metadata":{}},{"cell_type":"code","source":"print(\"\\n... DATA ACCESS SETUP STARTED ...\\n\")\n\nif TPU:\n    # Google Cloud Dataset path to training and validation images\n    DATA_DIR = KaggleDatasets().get_gcs_path('uw-madison-gi-tract-image-segmentation')\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\nelse:\n    # Local path to training and validation images\n    DATA_DIR = \"/kaggle/input/uw-madison-gi-tract-image-segmentation\"\n    save_locally = None\n    load_locally = None\n\nprint(f\"\\n... DATA DIRECTORY PATH IS:\\n\\t--> {DATA_DIR}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF DATA DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(\"\\n\\n... DATA ACCESS SETUP COMPLETED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-26T19:33:14.819265Z","iopub.execute_input":"2022-04-26T19:33:14.820679Z","iopub.status.idle":"2022-04-26T19:33:14.847138Z","shell.execute_reply.started":"2022-04-26T19:33:14.820628Z","shell.execute_reply":"2022-04-26T19:33:14.846406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">2.3 LEVERAGING XLA OPTIMIZATIONS</h3>\n\n---","metadata":{}},{"cell_type":"code","source":"'''\nprint(f\"\\n... XLA OPTIMIZATIONS STARTING ...\\n\")\n\nprint(f\"\\n... CONFIGURE JIT (JUST IN TIME) COMPILATION ...\\n\")\n# enable XLA optmizations (10% speedup when using @tf.function calls)\ntf.config.optimizer.set_jit(True)\n\nprint(f\"\\n... XLA OPTIMIZATIONS COMPLETED ...\\n\")\n'''","metadata":{"execution":{"iopub.status.busy":"2022-04-26T19:33:14.848573Z","iopub.execute_input":"2022-04-26T19:33:14.848798Z","iopub.status.idle":"2022-04-26T19:33:14.857085Z","shell.execute_reply.started":"2022-04-26T19:33:14.848771Z","shell.execute_reply":"2022-04-26T19:33:14.856428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">2.4 BASIC DATA DEFINITIONS & INITIALIZATIONS</h3>\n\n---\n","metadata":{}},{"cell_type":"code","source":"print(\"\\n... BASIC DATA SETUP STARTING ...\\n\\n\")\n\n# Open the training dataframe and display the initial dataframe\nTRAIN_DIR = os.path.join(DATA_DIR, \"train\")\nTRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\ntrain_df = pd.read_csv(TRAIN_CSV)\n\n# Get all training images\nall_train_images = glob(os.path.join(TRAIN_DIR, \"**\", \"*.png\"), recursive=True)\n\nprint(\"\\n... ORIGINAL TRAINING DATAFRAME... \\n\")\ndisplay(train_df)\n\nTEST_DIR = os.path.join(DATA_DIR, \"test\")\nSS_CSV   = os.path.join(DATA_DIR, \"sample_submission.csv\")\nss_df = pd.read_csv(SS_CSV)\n\n# Get all testing images if there are any\nall_test_images = glob(os.path.join(TEST_DIR, \"**\", \"*.png\"), recursive=True)\n\nprint(\"\\n\\n\\n... ORIGINAL SUBMISSION DATAFRAME... \\n\")\ndisplay(ss_df)\n\n# For debugging purposes when the test set hasn't been substituted we will know\nDEBUG=len(ss_df)==0\n\nif DEBUG:\n    TEST_DIR = TRAIN_DIR\n    all_test_images = all_train_images\n    ss_df = train_df.iloc[:10]\n    ss_df = ss_df[[\"id\", \"class\"]]\n    ss_df[\"predicted\"] = \"\"\n    \n    print(\"\\n\\n\\n... DEBUG SUBMISSION DATAFRAME... \\n\")\n    display(ss_df)\n\n    \n\nSF2LF = {\"lb\":\"Large Bowel\",\"sb\":\"Small Bowel\",\"st\":\"Stomach\"}\nLF2SF = {v:k for k,v in SF2LF.items()}\nprint(f\"\\n\\n\\n... ARE WE DEBUGGING: {DEBUG}... \\n\")\n\nprint(\"\\n... BASIC DATA SETUP FINISHED ...\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-26T19:33:14.858069Z","iopub.execute_input":"2022-04-26T19:33:14.858746Z","iopub.status.idle":"2022-04-26T19:33:19.496481Z","shell.execute_reply.started":"2022-04-26T19:33:14.858701Z","shell.execute_reply":"2022-04-26T19:33:19.495549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">2.5 UPDATE DATAFRAMES WITH ACCESSIBLE EXTRA INFORMATION</h3>\n\n---\n\n**NOTE: I have changed the column identifiers as follows for the sake of brevity:**\n* **large_bowel** --> **lb**\n* **small_bowel** --> **sb**\n* **stomach** --> **st**","metadata":{}},{"cell_type":"code","source":"print(\"\\n... UPDATING DATAFRAMES WITH ACCESSIBLE INFORMATION STARTED ...\\n\\n\")\n\n# 1. Get Case-ID as a column (str and int)\ntrain_df[\"case_id_str\"] = train_df[\"id\"].apply(lambda x: x.split(\"_\", 2)[0])\ntrain_df[\"case_id\"] = train_df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[0].replace(\"case\", \"\")))\n\n# 2. Get Day as a column\ntrain_df[\"day_num_str\"] = train_df[\"id\"].apply(lambda x: x.split(\"_\", 2)[1])\ntrain_df[\"day_num\"] = train_df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[1].replace(\"day\", \"\")))\n\n# 3. Get Slice Identifier as a column\ntrain_df[\"slice_id\"] = train_df[\"id\"].apply(lambda x: x.split(\"_\", 2)[2])\n\n# 4. Get full file paths for the representative scans\ntrain_df[\"_partial_ident\"] = (TRAIN_DIR+\"/\"+ # /kaggle/input/uw-madison-gi-tract-image-segmentation/train/\n                             train_df[\"case_id_str\"]+\"/\"+ # .../case###/\n                             train_df[\"case_id_str\"]+\"_\"+train_df[\"day_num_str\"]+ # .../case###_day##/\n                             \"/scans/\"+train_df[\"slice_id\"]) # .../slice_#### \n_tmp_merge_df = pd.DataFrame({\"_partial_ident\":[x.rsplit(\"_\",4)[0] for x in all_train_images], \"f_path\":all_train_images})\ntrain_df = train_df.merge(_tmp_merge_df, on=\"_partial_ident\").drop(columns=[\"_partial_ident\"])\n\n# Minor cleanup of our temporary workaround\ndel _tmp_merge_df; gc.collect(); gc.collect()\n\n# 5. Get slice dimensions from filepath (int in pixels)\ntrain_df[\"slice_h\"] = train_df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\ntrain_df[\"slice_w\"] = train_df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n\n# 6. Pixel spacing from filepath (float in mm)\ntrain_df[\"px_spacing_h\"] = train_df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[3]))\ntrain_df[\"px_spacing_w\"] = train_df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[4]))\n\n# 7. Merge 3 Rows Into A Single Row (As This/Segmentation-RLE Is The Only Unique Information Across Those Rows)\nl_bowel_train_df = train_df[train_df[\"class\"]==\"large_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"lb_seg_rle\"})\ns_bowel_train_df = train_df[train_df[\"class\"]==\"small_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"sb_seg_rle\"})\nstomach_train_df = train_df[train_df[\"class\"]==\"stomach\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"st_seg_rle\"})\ntrain_df = train_df.merge(l_bowel_train_df, on=\"id\", how=\"left\")\ntrain_df = train_df.merge(s_bowel_train_df, on=\"id\", how=\"left\")\ntrain_df = train_df.merge(stomach_train_df, on=\"id\", how=\"left\")\ntrain_df = train_df.drop_duplicates(subset=[\"id\",]).reset_index(drop=True)\ntrain_df[\"lb_seg_flag\"] = train_df[\"lb_seg_rle\"].apply(lambda x: not pd.isna(x))\ntrain_df[\"sb_seg_flag\"] = train_df[\"sb_seg_rle\"].apply(lambda x: not pd.isna(x))\ntrain_df[\"st_seg_flag\"] = train_df[\"st_seg_rle\"].apply(lambda x: not pd.isna(x))\ntrain_df[\"n_segs\"] = train_df[\"lb_seg_flag\"].astype(int)+train_df[\"sb_seg_flag\"].astype(int)+train_df[\"st_seg_flag\"].astype(int)\n\n# 8. Reorder columns to the a new ordering (drops class and segmentation as no longer necessary)\ntrain_df = train_df[[\"id\", \"f_path\", \"n_segs\",\n                     \"lb_seg_rle\", \"lb_seg_flag\",\n                     \"sb_seg_rle\", \"sb_seg_flag\", \n                     \"st_seg_rle\", \"st_seg_flag\",\n                     \"slice_h\", \"slice_w\", \"px_spacing_h\", \n                     \"px_spacing_w\", \"case_id_str\", \"case_id\", \n                     \"day_num_str\", \"day_num\", \"slice_id\",]]\n\n# 9. Display update dataframe\nprint(\"\\n... UPDATED TRAINING DATAFRAME... \\n\")\ndisplay(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T19:33:19.499222Z","iopub.execute_input":"2022-04-26T19:33:19.499448Z","iopub.status.idle":"2022-04-26T19:33:22.063033Z","shell.execute_reply.started":"2022-04-26T19:33:19.499421Z","shell.execute_reply":"2022-04-26T19:33:22.06221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"helper_functions\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"helper_functions\">\n    3&nbsp;&nbsp;HELPER FUNCTION & CLASSES&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---","metadata":{}},{"cell_type":"code","source":"# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n# modified from: https://www.kaggle.com/inversion/run-length-decoding-quick-start\ndef rle_decode(mask_rle, shape, color=1):\n    \"\"\" TBD\n    \n    Args:\n        mask_rle (str): run-length as string formated (start length)\n        shape (tuple of ints): (height,width) of array to return \n    \n    Returns: \n        Mask (np.array)\n            - 1 indicating mask\n            - 0 indicating background\n\n    \"\"\"\n    # Split the string by space, then convert it into a integer array\n    s = np.array(mask_rle.split(), dtype=int)\n\n    # Every even value is the start, every odd value is the \"run\" length\n    starts = s[0::2] - 1\n    lengths = s[1::2]\n    ends = starts + lengths\n\n    # The image is actually flattened since RLE is a 1D \"run\"\n    if len(shape)==3:\n        h, w, d = shape\n        img = np.zeros((h * w, d), dtype=np.float32)\n    else:\n        h, w = shape\n        img = np.zeros((h * w,), dtype=np.float32)\n\n    # The color here is actually just any integer you want!\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n        \n    # Don't forget to change the image back to the original shape\n    return img.reshape(shape)\n\n# https://www.kaggle.com/namgalielei/which-reshape-is-used-in-rle\ndef rle_decode_top_to_bot_first(mask_rle, shape):\n    \"\"\" TBD\n    \n    Args:\n        mask_rle (str): run-length as string formated (start length)\n        shape (tuple of ints): (height,width) of array to return \n    \n    Returns:\n        Mask (np.array)\n            - 1 indicating mask\n            - 0 indicating background\n\n    \"\"\"\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape((shape[1], shape[0]), order='F').T  # Reshape from top -> bottom first\n\n# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_encode(img):\n    \"\"\" TBD\n    \n    Args:\n        img (np.array): \n            - 1 indicating mask\n            - 0 indicating background\n    \n    Returns: \n        run length as string formated\n    \"\"\"\n    \n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef flatten_l_o_l(nested_list):\n    \"\"\" Flatten a list of lists \"\"\"\n    return [item for sublist in nested_list for item in sublist]\n\ndef load_json_to_dict(json_path):\n    \"\"\" tbd \"\"\"\n    with open(json_path) as json_file:\n        data = json.load(json_file)\n    return data\n\ndef tf_load_png(img_path):\n    return tf.image.decode_png(tf.io.read_file(img_path), channels=3)\n\ndef open_gray16(_path, normalize=True, to_rgb=False):\n    \"\"\" Helper to open files \"\"\"\n    if normalize:\n        if to_rgb:\n            return np.tile(np.expand_dims(cv2.imread(_path, cv2.IMREAD_ANYDEPTH)/65535., axis=-1), 3)\n        else:\n            return cv2.imread(_path, cv2.IMREAD_ANYDEPTH)/65535.\n    else:\n        # np.expand_dims turns (266,266) into (266,266,1)\n        # np.tile turns (266,266,1) into (266,266,3) (1st frame copied 3 times)\n        if to_rgb:\n            return np.tile(np.expand_dims(cv2.imread(_path, cv2.IMREAD_ANYDEPTH), axis=-1), 3)\n        else:\n            return cv2.imread(_path, cv2.IMREAD_ANYDEPTH)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T19:33:22.064919Z","iopub.execute_input":"2022-04-26T19:33:22.065224Z","iopub.status.idle":"2022-04-26T19:33:22.087468Z","shell.execute_reply.started":"2022-04-26T19:33:22.065183Z","shell.execute_reply":"2022-04-26T19:33:22.086798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"create_dataset\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"create_dataset\">\n    4&nbsp;&nbsp;DATASET EXPLORATION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.0 LOOK AT A SINGLE EXAMPLE PRIOR TO INVESTIGATION</h3>\n\n---\n\nWe simply do this to make sure everything is where it should be and we understand the basics of how to access all the relevant data.\n\nWe will wrap this basic exploration functionality as single function to allow for easy examination of any passed identifier","metadata":{}},{"cell_type":"code","source":"def get_overlay(img_path, rle_strs, img_shape, _alpha=0.999, _beta=0.35, _gamma=0):\n    _img = open_gray16(img_path, to_rgb=True)\n    _img = ((_img-_img.min())/(_img.max()-_img.min())).astype(np.float32)\n    _seg_rgb = np.stack([rle_decode(rle_str, shape=img_shape, color=1) if rle_str is not None else np.zeros(img_shape, dtype=np.float32) for rle_str in rle_strs], axis=-1).astype(np.float32)\n    seg_overlay = cv2.addWeighted(src1=_img, alpha=_alpha, \n                                  src2=_seg_rgb, beta=_beta, gamma=_gamma)\n    return seg_overlay\n\ndef examine_id(ex_id, df=train_df, plot_overlay=True, print_meta=False, plot_grayscale=False, plot_binary_segmentation=False):\n    \"\"\" Wrapper function to allow for easy visual exploration of an example \"\"\"\n    print(f\"\\n... ID ({ex_id}) EXPLORATION STARTED ...\\n\\n\")\n    demo_ex = df[df.id==ex_id].squeeze()\n\n    if print_meta:\n        print(f\"\\n... WITH DEMO_ID=`{DEMO_ID}` WE HAVE THE FOLLOWING DEMO EXAMPLE TO WORK FROM... \\n\\n\")\n        display(demo_ex.to_frame())\n\n    if plot_grayscale:\n        print(f\"\\n\\n... GRAYSCALE IMAGE PLOT ...\\n\")\n        plt.figure(figsize=(12,12))\n        plt.imshow(open_gray16(demo_ex.f_path), cmap=\"gray\")\n        plt.title(f\"Original Grayscale Image For ID: {demo_ex.id}\", fontweight=\"bold\")\n        plt.axis(False)\n        plt.show()\n\n    if plot_binary_segmentation:\n        print(f\"\\n\\n... BINARY SEGMENTATION MASKS ...\\n\")\n        plt.figure(figsize=(20,10))\n        for i, _seg_type in enumerate([\"lb\", \"sb\", \"st\"]):\n            if pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]): continue\n            plt.subplot(1,3,i+1)\n            plt.imshow(rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1))\n            plt.title(f\"RLE Encoding For {SF2LF[_seg_type]} Segmentation\", fontweight=\"bold\")\n            plt.axis(False)\n        plt.tight_layout()\n        plt.show()\n\n    if plot_overlay:\n        print(f\"\\n\\n... IMAGE WITH RGB SEGMENTATION MASK OVERLAY ...\\n\")\n        # We need to normalize the loaded image values to be between 0 and 1 or else our plot will look weird\n        # _img = open_gray16(demo_ex.f_path, to_rgb=True)\n        #_img = ((_img-_img.min())/(_img.max()-_img.min())).astype(np.float32)\n        #_seg_rgb = np.stack([rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1) if not pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]) else np.zeros((demo_ex.slice_w, demo_ex.slice_h)) for _seg_type in [\"lb\", \"sb\", \"st\"]], axis=-1).astype(np.float32)\n        #seg_overlay = cv2.addWeighted(src1=_img, alpha=0.99, \n                                      #src2=_seg_rgb, beta=0.33, gamma=0)\n        _rle_strs = [demo_ex[f\"{_seg_type}_seg_rle\"] if not pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]) else None for _seg_type in [\"lb\", \"sb\", \"st\"]]\n        seg_overlay = get_overlay(demo_ex.f_path, _rle_strs, img_shape=(demo_ex.slice_w, demo_ex.slice_h))\n\n        plt.figure(figsize=(12,12))\n        plt.imshow(seg_overlay)\n        plt.title(f\"Segmentation Overlay For ID: {demo_ex.id}\", fontweight=\"bold\")\n        handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\n        labels = [\"Large Bowel Segmentation Map\", \"Small Bowel Segmentation Map\", \"Stomach Segmentation Map\"]\n        plt.legend(handles,labels)\n        plt.axis(False)\n        plt.show()\n\n    print(\"\\n\\n... SINGLE ID EXPLORATION FINISHED ...\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-26T19:33:22.089017Z","iopub.execute_input":"2022-04-26T19:33:22.089579Z","iopub.status.idle":"2022-04-26T19:33:22.110353Z","shell.execute_reply.started":"2022-04-26T19:33:22.089534Z","shell.execute_reply":"2022-04-26T19:33:22.109661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n... SINGLE ID EXPLORATION STARTED ...\\n\\n\")\n\nDEMO_ID = \"case123_day20_slice_0082\"\ndemo_ex = train_df[train_df.id==DEMO_ID].squeeze()\n\nprint(f\"\\n... WITH DEMO_ID=`{DEMO_ID}` WE HAVE THE FOLLOWING DEMO EXAMPLE TO WORK FROM... \\n\\n\")\ndisplay(demo_ex.to_frame())\n\nprint(f\"\\n\\n... LET'S PLOT THE IMAGE FIRST ...\\n\")\nplt.figure(figsize=(12,12))\nplt.imshow(open_gray16(demo_ex.f_path), cmap=\"gray\")  # show grayscale image\nplt.title(f\"Original Grayscale Image For ID: {demo_ex.id}\", fontweight=\"bold\")\nplt.axis(False)\nplt.show()\n\nprint(f\"\\n\\n... LET'S PLOT THE 3 SEGMENTATION MASKS ...\\n\")\n\nplt.figure(figsize=(20,10))\nfor i, _seg_type in enumerate([\"lb\", \"sb\", \"st\"]):\n    if pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]): continue # skip iteration if no mask available\n    plt.subplot(1,3,i+1)\n    plt.imshow(rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1)) # decode rle first\n    plt.title(f\"RLE Encoding For {SF2LF[_seg_type]} Segmentation\", fontweight=\"bold\")\n    plt.axis(False)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n\\n... LET'S PLOT THE IMAGE WITH AN RGB SEGMENTATION MASK OVERLAY ...\\n\")\n\n# We need to normalize the loaded image values to be between 0 and 1 or else our plot will look weird\n_img = open_gray16(demo_ex.f_path, to_rgb=True)\n_img = ((_img-_img.min())/(_img.max()-_img.min())).astype(np.float32)\n_seg_rgb = np.stack([rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1) if not pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]) else np.zeros((demo_ex.slice_w, demo_ex.slice_h)) for _seg_type in [\"lb\", \"sb\", \"st\"]], axis=-1).astype(np.float32) # shape (266,266,3)\nseg_overlay = cv2.addWeighted(src1=_img, alpha=0.99, \n                              src2=_seg_rgb, beta=0.33, gamma=0.0) # linear blending: alpha*src1+beta*src2+gamma\n\n# Note: (R = G = B) => gray value. This is why _img looks gray whilst _seg_rgb is rgb.\n\nplt.figure(figsize=(12,12))\nplt.imshow(seg_overlay)\nplt.title(f\"Segmentation Overlay For ID: {demo_ex.id}\", fontweight=\"bold\")\nhandles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\nlabels = [\"Large Bowel Segmentation Map\", \"Small Bowel Segmentation Map\", \"Stomach Segmentation Map\"]\nplt.legend(handles,labels)\nplt.axis(False)\nplt.show()\n\nprint(f\"\\n\\n... LET'S PRINT THE RELEVANT INFORMATION ...\\n\")\nprint(f\"\\t--> IMAGE CASE ID              : {demo_ex.case_id}\")\nprint(f\"\\t--> IMAGE DAY NUMBER           : {demo_ex.day_num}\")\nprint(f\"\\t--> IMAGE SLICE WIDTH          : {demo_ex.slice_w}\")\nprint(f\"\\t--> IMAGE SLICE HEIGHT         : {demo_ex.slice_h}\")\nprint(f\"\\t--> IMAGE PIXEL SPACING WIDTH  : {demo_ex.px_spacing_w}\")\nprint(f\"\\t--> IMAGE PIXEL SPACING HEIGHT : {demo_ex.px_spacing_h}\")\n\nprint(\"\\n\\n... SINGLE ID EXPLORATION FINISHED ...\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-26T19:33:22.111967Z","iopub.execute_input":"2022-04-26T19:33:22.112262Z","iopub.status.idle":"2022-04-26T19:33:23.21492Z","shell.execute_reply.started":"2022-04-26T19:33:22.112224Z","shell.execute_reply":"2022-04-26T19:33:23.214291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.1 INVESTIGATE THE OCCURENCE SEGMENTATION MAP TYPES</h3>\n\n---\n\nIt's quite apparent that not all images have segmentation maps for the various regions (stomach, large-bowel, small-bowel), so we will identify the frequency for which these occur independently... as well as the frequency for which these maps co-occur.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n\n* There are **38,496** total examples.\n* It can be observed that more than half of the given examples have no annotations present!\n    * There are **21,906** (56.9046%) examples with no annotations/masks/segmentation present\n    * Inversely there are **16,590** (43.0954%) examples with one or more annotations present\n* There are **2,468** (6.41%) examples with **one annotation present**. \n* It can be observed that the vast majority of single mask annotations are **Stomach**!\n    * Of these annotations, **2286** (~92.6%) are **Stomach**\n    * Of these annotations, **123** (~4.98%) are **Large Bowel**\n    * Of these annotations, **59** (~2.39%) are **Small Bowel**\n* There are **10,921** (28.37%) examples with **two annotations present**. \n* It can be observed, in contrast to the single annotation examples, that the majority of annotations do NOT include stomach i.e. **'Large Bowel, Small Bowel'**!\n    * Of these annotations, **7781** (~71.3%) are **'Large Bowel, Small Bowel'**\n    * Of these annotations, **2980** (~27.3%) are **'Large Bowel, Stomach'**\n    * Of these annotations, **160** (~1.47%) are **'Small Bowel, Stomach'**\n* Finally, there are **3,201** (8.32%) examples with **all three annotations present**. \n\n<!--  # print(len(train_df))\n# print(len(train_df[train_df[\"seg_combo_str\"]==\"No Mask\"]), len(train_df[train_df[\"seg_combo_str\"]==\"No Mask\"])/len(train_df))\n# print(len(train_df[train_df[\"seg_combo_str\"]==\"Large Bowel\"])/2468)\n# print(len(train_df[train_df[\"seg_combo_str\"]==\"Small Bowel\"])/2468)\n# print(len(train_df[train_df[\"seg_combo_str\"]==\"Stomach\"])/2468)\n# print(len(train_df[train_df[\"seg_combo_str\"].apply(lambda x: x.count(\",\")==1)]))\n# print(len(train_df[train_df[\"seg_combo_str\"]==\"Large Bowel, Stomach\"])/10921)\n# print(len(train_df[train_df[\"seg_combo_str\"]==\"Large Bowel, Small Bowel\"])/10921)\n# print(len(train_df[train_df[\"seg_combo_str\"]==\"Small Bowel, Stomach\"])/10921)\n# print(len(train_df[train_df[\"seg_combo_str\"].apply(lambda x: x.count(\",\")==2)])) -->","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.2 INVESTIGATE THE IMAGE SIZES</h3>\n\n---\n\nIt's observable that not all images have the same size... however, given that, there is not that much variation between image slice sizes.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n* Remember, there are **38,496** total examples.\n* Globally, we can see that 3 of the image shapes are **square** while one is **rectangular** and they all fall within a fairly tight distribution of relatively small sizes\n* Of these there are **4** unique sizes:\n    * $234 \\times 234$\n        * **Least frequent** image size\n        * **Smallest** image size\n        * Only **144** of the 38,496 occurences are this size (0.37%)\n    * $266 \\times 266$\n        * **Most frequent** image size\n        * **Second smallest** image size\n        * **25,920** of the 38,496 occurences are this size (67.33%)\n    * $276 \\times 276$\n        * **Second least frequent** image size\n        * **Second largest** image size\n        * **1,200** of the 38,496 occurences are this size (3.12%)\n    * $310 \\times 360$\n        * **Second most frequent** image size\n        * **Largest** image size\n        * **11,232** of the 38,496 occurences are this size (29.17%)\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.3 INVESTIGATE THE PIXEL SPACING</h3>\n\n---\n\nIt's observable that not all images have the same pixel spacing... however, given that, there is not that much variation between pixel spacing.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n* Remember, there are **38,496** total examples.\n* Globally, we can see that all of the pixel spacings are **square** and that the vast majority are $1.50mm \\times 1.50mm$\n* There are only **2** unique sets of pixel spacings:\n    * $1.50mm \\times 1.50mm$\n        * **Most frequent** pixel spacing\n        * **Smallest** pixel spacing (barely)\n        * **37,296** of the 38,496 occurences are this size (96.88%)\n    * $1.63mm \\times 1.63mm$\n        * **Least frequent** image size\n        * **Largest** pixel spacing (barely)\n        * **1,200** of the 38,496 occurences are this size (3.12%)","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.4 INVESTIGATE CASE IDS</h3>\n\n---\n\nHere's the host description of **`case_id`**\n\n> \"Each case in this competition is represented by multiple sets of scan slices (each set is identified by the day the scan took place). Some cases are split by time (early days are in train, later days are in test) while some cases are split by case - the entirety of the case is in train or test. The goal of this competition is to be able to generalize to both partially and wholly unseen cases.\"\n\nI don't really observe any oddities associated with any particular **`case_id`** values. I would probably attempt to group them when stratifying/creating-folds... however, they don't seem to perpetrate an obvious bias.\n\nWhen we colour by **day**, we can see that all cases are made up (mostly) of groups of **144**, or less frequently, **80**, images from different days.","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.5 MASK SIZES/AREAS</h3>\n\n---\n\nWe know that every other number in an RLE encoding represents a run of mask... so if we add up all those numbers we get the total number of masked pixels in an image. This is much faster than opening and closing each image.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n\n* It's observable that the distributions of mask area is mostly normal although it skews slightly to the smaller side...\n* All the distributions are similar although the Stomach distribution has an odd gap between 400-750 pixels.\n* It's interesting to note that, while not common, we do have some VERY large masks (>7500 pixels)\n    * Also, it's kind of funny that the biggest masks are for **small** bowel\n    ","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.6 MASK DATASET CREATION, CLASS OVERLAP & MASK HEATMAP</h3>\n\n---\n\nIt's important to determine if the the masks overlap one another (**multilabel**) or not (**multiclass**). To do this, we will quickly create a dataset of **`npy`** files. During this creation process we will check for overlap.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n\n* There is overlap, and while it is not that common, some images exhibit a high degree of overlap.\n* This means that we cannot frame the problem as simple categorical semantic segmentation.\n* We must instead frame the problem as multi-label semantic segmentation\n* This means our mask will take the form --> $W \\times H \\times 3$\n    * Where the channel dimensions are binary masks for each respective segmentation type\n    * This will allow for the masks to overlap","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.7 PIXEL VALUES IN OUR DATASET</h3>\n\n---\n\nIt's important to analyse the dataset because we will need to normalize the data to convert it into a format that is more expected for machine learning (uint8 (0-255) or float32 (0-1)). Without knowing the limits of the images, we may diminish the resolution of the data by accident when normalizing.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n\nInterestingly the maximum value in the dataset is equiavlent to less than half of an int16 or a quarter of a uint16.\n* Max Value for UINT16\n    * **65535**\n* Max Value for INT16\n    * **32767**\n* Half of Max Value for INT16\n    * **16384**\n* Actual Max Value in the dataset\n    * **15865**","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.8 IDENTIFY ANY HEURISTICS OR RULES REGARDING SEGMENTATION</h3>\n\n---\n\nFor a given **case-id** and **day number** there are two different amounts of scans present\n* 144 slices --> 259 instances\n* 80 slices ---> 15 instances\n\nSome other observations about our training dataset\n* There are no examples for slices number **1, 138, 139, 140, 141, 142, 143 or 144** that have any segmentation masks\n* If we break it down by organ we get the following no-value slices for each respective organ\n    * Large Bowel – **1, 138, 139, 140, 141, 142, 143, 144**\n    * Small Bowel – **1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 138, 139, 140, 141, 142, 143, 144**\n    * Stomach – **1, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144**","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.9 CREATE 3D GIF FOR CASE GROUPS OF SLICES (WITH MASK!!)</h3>\n\n---\n","metadata":{}},{"cell_type":"code","source":"def get_overlay(img_path, rle_strs, img_shape, _alpha=0.999, _beta=0.35, _gamma=0):\n    _img = open_gray16(img_path, to_rgb=True)\n    _img = ((_img-_img.min())/(_img.max()-_img.min())).astype(np.float32)\n    _seg_rgb = np.stack([rle_decode(rle_str, shape=img_shape, color=1) if (rle_str is not None and not pd.isna(rle_str)) else np.zeros(img_shape, dtype=np.float32) for rle_str in rle_strs], axis=-1).astype(np.float32)\n    seg_overlay = cv2.addWeighted(src1=_img, alpha=_alpha, \n                                  src2=_seg_rgb, beta=_beta, gamma=_gamma)\n    return seg_overlay","metadata":{"execution":{"iopub.status.busy":"2022-04-26T19:33:23.216961Z","iopub.execute_input":"2022-04-26T19:33:23.217472Z","iopub.status.idle":"2022-04-26T19:33:23.225412Z","shell.execute_reply.started":"2022-04-26T19:33:23.217436Z","shell.execute_reply":"2022-04-26T19:33:23.2247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Multiple animations simultaneously\ndef case_animation(case_id, day_nums=True, df=train_df):\n    # Day info\n    if day_nums:\n        days = np.sort(df[df['case_id']==case_id]['day_num'].unique()) # Show all days\n    else:\n        days = np.sort(day_nums.unique()) # Show subset of days\n    n_days = len(days)\n    \n    # Loop over each day\n    for index, day_num in enumerate(days):\n        # Desired subset\n        sub_df = df[(df.case_id==case_id) & (df.day_num==day_num)]\n        \n        # Metadata\n        f_paths  = sub_df.f_path.tolist()\n        lb_rles  = sub_df.lb_seg_rle.tolist()\n        sb_rles  = sub_df.sb_seg_rle.tolist()\n        st_rles  = sub_df.st_seg_rle.tolist()\n        slice_ws = sub_df.slice_w.tolist()\n        slice_hs = sub_df.slice_h.tolist()\n        \n        # Images stacked together\n        animation_arr = np.stack([\n            get_overlay(img_path=_f, rle_strs=(_lb, _sb, _st), img_shape=(_w, _h)) \\\n            for _f, _lb, _sb, _st, _w, _h in \\\n            zip(f_paths, lb_rles, sb_rles, st_rles, slice_ws, slice_hs)\n        ], axis=0)\n        \n        # Images can have different sizes between days\n        if index==0:\n            animation_arr1 = animation_arr\n        elif index==1:\n            animation_arr2 = animation_arr\n        elif index==2:\n            animation_arr3 = animation_arr\n        elif index==3:\n            animation_arr4 = animation_arr\n        elif index==4:\n            animation_arr5 = animation_arr\n        elif index==5:\n            animation_arr6 = animation_arr\n            \n    # Initialise plot sizes\n    if n_days==1:\n        fig, (ax1) = plt.subplots(1,1)\n        fig.set_figheight(4)\n        fig.set_figwidth(4)\n    elif n_days==2:\n        fig, (ax1, ax2) = plt.subplots(1,2)\n        fig.set_figheight(4)\n        fig.set_figwidth(8)\n    elif n_days==3:\n        fig, (ax1, ax2, ax3) = plt.subplots(1,3)\n        fig.set_figheight(4)\n        fig.set_figwidth(12)\n    elif n_days==4:\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n        fig.set_figheight(6) # gifs get truncated if size is too large (on kaggle)\n        fig.set_figwidth(6)\n    elif n_days==5:\n        fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2,3)\n        fig.set_figheight(6)\n        fig.set_figwidth(9)\n    elif n_days==6:\n        fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2,3)\n        fig.set_figheight(6)\n        fig.set_figwidth(9)\n        \n    # Initialise plots\n    frame_min=1e6\n    if n_days>=1:\n        ax1.axis('off')\n        im1 = ax1.imshow(animation_arr1[0])\n        ax1.title.set_text(f\"Day {days[0]}\")\n        frame_min = min(frame_min,animation_arr1.shape[0])\n    if n_days>=2:\n        ax2.axis('off')\n        im2 = ax2.imshow(animation_arr2[0])\n        ax2.title.set_text(f\"Day {days[1]}\")\n        frame_min = min(frame_min,animation_arr2.shape[0])\n    if n_days>=3:\n        ax3.axis('off')\n        im3 = ax3.imshow(animation_arr3[0])\n        ax3.title.set_text(f\"Day {days[2]}\")\n        frame_min = min(frame_min,animation_arr3.shape[0])\n    if n_days>=4:\n        ax4.axis('off')\n        im4 = ax4.imshow(animation_arr4[0])\n        ax4.title.set_text(f\"Day {days[3]}\")\n        frame_min = min(frame_min,animation_arr4.shape[0])\n    if n_days>=5:\n        ax5.axis('off')\n        ax6.axis('off')\n        im5 = ax5.imshow(animation_arr5[0])\n        ax5.title.set_text(f\"Day {days[4]}\")\n        frame_min = min(frame_min,animation_arr5.shape[0])\n    if n_days==6:\n        im6 = ax6.imshow(animation_arr6[0])\n        ax6.title.set_text(f\"Day {days[5]}\")\n        frame_min = min(frame_min,animation_arr6.shape[0])\n    \n    # Set overall title\n    fig.suptitle(f\"3D Animation for Case {case_id}\", fontweight=\"bold\")\n    \n    # Animate function\n    def animate_func(i):\n        out=[]\n        if n_days>=1:\n            im1.set_array(animation_arr1[i])\n            out.append(im1)\n        if n_days>=2:\n            im2.set_array(animation_arr2[i])\n            out.append(im2)\n        if n_days>=3:\n            im3.set_array(animation_arr3[i])\n            out.append(im3)\n        if n_days>=4:\n            im4.set_array(animation_arr4[i])\n            out.append(im4)\n        if n_days>=5:\n            im5.set_array(animation_arr5[i])\n            out.append(im5)\n        if n_days>=6:\n            im6.set_array(animation_arr6[i])\n            out.append(im6)\n        return out\n    plt.close()\n    \n    return animation.FuncAnimation(fig, animate_func, frames = frame_min, interval = 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T19:33:23.22997Z","iopub.execute_input":"2022-04-26T19:33:23.230316Z","iopub.status.idle":"2022-04-26T19:33:23.265749Z","shell.execute_reply.started":"2022-04-26T19:33:23.230274Z","shell.execute_reply":"2022-04-26T19:33:23.264692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Examples**","metadata":{}},{"cell_type":"code","source":"# Example with 1 day side-to-side\n#case_animation(case_id=16)\n\n# Example with 2 days side-by-side\n#case_animation(case_id=33)\n\n# Example with 3 days side-by-side\nanim1 = case_animation(case_id=149)\nanim1","metadata":{"execution":{"iopub.status.busy":"2022-04-26T20:37:00.345034Z","iopub.execute_input":"2022-04-26T20:37:00.345361Z","iopub.status.idle":"2022-04-26T20:37:18.970923Z","shell.execute_reply.started":"2022-04-26T20:37:00.345328Z","shell.execute_reply":"2022-04-26T20:37:18.969726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example with 4 days side-by-side\n#case_animation(case_id=139)\n\n# Example with 5 days side-by-side\n#case_animation(case_id=122)\n\n# Example with 6 days side-by-side\nanim2=case_animation(case_id=36)\nanim2","metadata":{"execution":{"iopub.status.busy":"2022-04-26T19:35:46.967649Z","iopub.execute_input":"2022-04-26T19:35:46.967956Z","iopub.status.idle":"2022-04-26T19:36:15.975927Z","shell.execute_reply.started":"2022-04-26T19:35:46.967924Z","shell.execute_reply":"2022-04-26T19:36:15.973941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save gifs\n!pip install imagemagick\n\nanim1.save('case149.gif', writer='imagemagick')\nanim2.save('case36.gif', writer='imagemagick')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T19:37:29.267803Z","iopub.execute_input":"2022-04-26T19:37:29.268123Z","iopub.status.idle":"2022-04-26T19:41:15.734313Z","shell.execute_reply.started":"2022-04-26T19:37:29.268086Z","shell.execute_reply":"2022-04-26T19:41:15.733189Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.10 SCANS WITH ERRORS</h3>\n\n---\n\n* [**Paul G**](https://www.kaggle.com/pgeiger) identified two cases with errors in the segmentation masks in [**this discussion post**](https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/discussion/319963)\n  * Case 7\n      * Day 0\n\n<img src=\"https://i.ibb.co/M8p8Xfk/case7-day0-slice-0096.png\">\n\n  * Case 81\n      * Day 30\n      \n<img src=\"https://i.ibb.co/jkdcdzR/case81-day30-slice-0096.png\">","metadata":{}}]}