{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sea Turtle Face Detection\n\nThis notebook shows how to do simple face detection using the Kaggle Sea Turtle dataset.  I shold be run connected to the Sea Turtle dataset--check the Data->Input tab on the right side to make sure you are connected to sea-turtle-face-detection\n\n## Face Detection vs Recognition\n\n* Face detections is the process of finding faces in an image and drawing bounding boxes around them. \n  * This requires a dataset with bounding boxes marked on each image like the sea turtle dataset \n* Face recognition is the process of identifing the person represented by a face\n  * This requires a dataset with bounding boxes and IDs for each face. We will not be doing this with the dataset since we don't have unique ids for each turtle.\n\n## Running with a GPU\n\nThis notebooks runs much faster with the GPU enabled by clicking on the ... menu on the right and selecting Accelerator","metadata":{}},{"cell_type":"code","source":"# This simply makes sure Kaggle session is started and that we can run code\nprint (\"Getting started\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:02:37.512278Z","iopub.execute_input":"2022-05-08T01:02:37.512541Z","iopub.status.idle":"2022-05-08T01:02:37.518047Z","shell.execute_reply.started":"2022-05-08T01:02:37.512511Z","shell.execute_reply":"2022-05-08T01:02:37.516344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First, we'll import pandas and numpy, two data processing libraries\nimport pandas as pd\nimport numpy as np\n\n# We'll also import seaborn and matplot, twp Python graphing libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Import the needed sklearn libraries\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\n\n# The Keras library provides support for neural networks and deep learning\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, Dropout, SpatialDropout2D, Activation, Lambda, Flatten, LSTM\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam, RMSprop\n#from tensorflow.keras.utils import np_utils\nfrom tensorflow.keras import utils\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nprint (\"Libraries Imported\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:02:37.565564Z","iopub.execute_input":"2022-05-08T01:02:37.565999Z","iopub.status.idle":"2022-05-08T01:02:37.574306Z","shell.execute_reply.started":"2022-05-08T01:02:37.565969Z","shell.execute_reply":"2022-05-08T01:02:37.57349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# define path to the read-only input folder\ndataPath = \"/kaggle/input/sea-turtle-face-detection/data/\"\n# define path to the writable working folder\nworking_dir = \"/kaggle/working/\"\n\n# read text file into pandas DataFrame\ndf_labels = pd.read_csv(\"/kaggle/input/sea-turtle-face-detection/data/labels.csv\", header=None)\ndf_labels.columns = ['species', 'upper_left_x', 'upper_left_y', 'bbwidth', 'bblength', 'filename', 'image_width', 'image_length' ]\n# display DataFrame\ndf_labels.head","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:02:37.598076Z","iopub.execute_input":"2022-05-08T01:02:37.598397Z","iopub.status.idle":"2022-05-08T01:02:37.632458Z","shell.execute_reply.started":"2022-05-08T01:02:37.598365Z","shell.execute_reply":"2022-05-08T01:02:37.631786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Notes on file names and bounding boxes\n\n## file names\n\nMost of the files follow the format Image_<num>.jpg but some files have extendsion of \".jpeg\" or \".png\" instead of \".jpg\"\n\n## Multiple turtle images\n\nThere are about 100 images with 2+ turtle faces in them. There are 2000 images and 2103 bounding boxes. We will remove the images with multiple turtles and end up with 1934 images\n    \n* The labels.csv file only has multiple rows for multi-turtle images\n* The image.txt file has all the bounding boxes for the multi-turtles listed in the text file\n","metadata":{}},{"cell_type":"markdown","source":"## Remove images with multiple faces","metadata":{}},{"cell_type":"code","source":"print(\"Shape before dropping duplicates \" + str(df_labels.shape))\ndf_labels = df_labels.drop_duplicates(subset='filename', keep=False)\nprint(\"Shape after dropping duplicates \" + str(df_labels.shape))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:02:37.634908Z","iopub.execute_input":"2022-05-08T01:02:37.635093Z","iopub.status.idle":"2022-05-08T01:02:37.642917Z","shell.execute_reply.started":"2022-05-08T01:02:37.635069Z","shell.execute_reply":"2022-05-08T01:02:37.641954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set number of images\n\nThere are 2,000 images. We elinimate 66 of them due to mulitple turtle faces\n\nOf the remaining 1934 images, the following code selects how many to use for training...","metadata":{}},{"cell_type":"code","source":"# This code uses all the images available\ndf_labels = df_labels[0:]\n# This code limits the images to the first 500 and will run faster for training if you need to\n#df_labels = df_labels[0:500]\n\ndf_labels","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:02:37.644429Z","iopub.execute_input":"2022-05-08T01:02:37.64467Z","iopub.status.idle":"2022-05-08T01:02:37.663509Z","shell.execute_reply.started":"2022-05-08T01:02:37.644629Z","shell.execute_reply":"2022-05-08T01:02:37.662846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the image size used by the network input later\nIMAGE_SIZE = (224, 224)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:02:37.665406Z","iopub.execute_input":"2022-05-08T01:02:37.665988Z","iopub.status.idle":"2022-05-08T01:02:37.672276Z","shell.execute_reply.started":"2022-05-08T01:02:37.665953Z","shell.execute_reply":"2022-05-08T01:02:37.6715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bounding Box Formats and Units\n\n# Formats\n\nThere are two basic formats for bounding boxes:\n* Corners (x1, y1, x2, y2): specifies the upper left corner (x1, y1) and the lower right corner (x2, y2) of the bounding box\n* Center-Size (x, y, width, lenght): this specifies the center of the bounding box and the width and lenght of the box\n\n# Units\n\nThe units can be specified in absolute pixes or as a percent of the entire image size.  YOLO uses a popular format that specifies the Center-Size of the bounding box is percent of the image sizes.  \n\nSo, an 200 x 300 pixel image with a bounding box from (150, 150) to (200,200) would have the following values:\n*  Corners in pixels: (x1, y1, x2, y2) = (150, 150, 200, 200)\n*  YOLO Center-Size as percent: (x, y, width, lenght) = (0.75, 0.5, 1.0, 0.67)\n\nThe label.txt files contain YOLO format bounding boxes.  We will read these in below.","metadata":{}},{"cell_type":"markdown","source":"## Append yolo bounding boxes and create a new dataframe named df_all","metadata":{}},{"cell_type":"code","source":"# Get YOLO bounding boxes from text files\n#df_yolo = pd.DataFrame.empty\nlabelName = dataPath + \"labels/\" + df_labels[\"filename\"][0][:-3] + 'txt'\ndf_yolo = pd.read_csv(labelName, delim_whitespace=True, header=None)\ndf_yolo.columns = ['idd', 'yolo_x', 'yolo_y','yolo_width', 'yolo_length'] \n\n# Loop over all the file names, skipping the first one that is done above\nfor imageName in df_labels[\"filename\"][1:]:\n    if (imageName[-4] == '.'):\n        # some images have \".jpeg\" instead of \".jpg\"\n        labelName = dataPath + \"labels/\" + imageName[:-3] + 'txt'\n    else:\n        labelName = dataPath + \"labels/\" + imageName[:-4] + 'txt'\n    #print (labelName)\n    bb_txt = pd.read_csv(labelName, delim_whitespace=True, header=None)\n    bb_txt.columns = ['idd', 'yolo_x', 'yolo_y','yolo_width', 'yolo_length'] \n    df_yolo = pd.concat([df_yolo, bb_txt.iloc[[0]] ], ignore_index=True)\n\n    #print (df_yolo)\n    \ndf_yolo.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:02:37.673423Z","iopub.execute_input":"2022-05-08T01:02:37.674045Z","iopub.status.idle":"2022-05-08T01:02:44.430762Z","shell.execute_reply.started":"2022-05-08T01:02:37.674008Z","shell.execute_reply":"2022-05-08T01:02:44.430058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_labels.reset_index(drop=True, inplace=True)\ndf_yolo.reset_index(drop=True, inplace=True)\n\ndf_all = pd.concat([df_labels, df_yolo], axis=1)\nprint (\"df_labels shape = \"+ str(df_labels.shape))\nprint (\"df_yolo shape = \"+ str(df_yolo.shape))\nprint (\"df_all shape = \"+ str(df_all.shape))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:02:44.432006Z","iopub.execute_input":"2022-05-08T01:02:44.432753Z","iopub.status.idle":"2022-05-08T01:02:44.441404Z","shell.execute_reply.started":"2022-05-08T01:02:44.432691Z","shell.execute_reply":"2022-05-08T01:02:44.44067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Resize all the images\n\nMost models will work with images of size 224x224.  It is faster to resize the images once, rather than each time they are used for training.  The code below resizes all the images and stores them in folder working/images224.\n\nWe also need to recalculate bounding boxes that use the pixel format for images resized to 224x224","metadata":{}},{"cell_type":"code","source":"# Resizing the images into a new folder images224\nimport os\nimport skimage.io\nimport skimage\nimport cv2\nfrom tqdm import tqdm    # needed for progress bar\n\nload_dir = \"/kaggle/input/sea-turtle-face-detection/data/images/\"\nsave_dir = \"/kaggle/working/images224/\"\nprint ('Creating a new folder at '+save_dir)\nos.makedirs(save_dir, exist_ok=True)\n\nprint ('Looping through all the images in the data frame')\n#for imageName in df_labels[\"filename\"][1:]:\nfor imageName in tqdm(df_labels[\"filename\"]):\n\n    load_path = load_dir + imageName\n    save_path = save_dir + imageName\n    img = mpimg.imread(load_path)\n    img_resize = cv2.resize(img, (224, 224))\n    mpimg.imsave(save_path, img_resize)\n\nprint (\"All images resized and saved to /kaggle/working/images224/ folder\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:02:44.443552Z","iopub.execute_input":"2022-05-08T01:02:44.444367Z","iopub.status.idle":"2022-05-08T01:04:14.039582Z","shell.execute_reply.started":"2022-05-08T01:02:44.444329Z","shell.execute_reply":"2022-05-08T01:04:14.0368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# add bounding box fields for the resized images assuming size is 224x224","metadata":{}},{"cell_type":"code","source":"width = 224\nlength = 224\ndf_all['resize_upper_x'] = df_all.apply(lambda row: round(row['yolo_x'] * width - 0.5 * row['yolo_width'] * width), axis = 1)\ndf_all['resize_left_y'] = df_all.apply(lambda row:  round(row['yolo_y'] * length - 0.5 * row['yolo_length'] * length), axis = 1)\ndf_all['resize_lower_x'] = df_all.apply(lambda row: round(row['yolo_x'] * width + 0.5 * row['yolo_width'] * width), axis = 1)\ndf_all['resize_right_y'] = df_all.apply(lambda row: round(row['yolo_y'] * length + 0.5 * row['yolo_length'] * length), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:04:14.041017Z","iopub.execute_input":"2022-05-08T01:04:14.041264Z","iopub.status.idle":"2022-05-08T01:04:14.181474Z","shell.execute_reply.started":"2022-05-08T01:04:14.041228Z","shell.execute_reply":"2022-05-08T01:04:14.180766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:04:14.182863Z","iopub.execute_input":"2022-05-08T01:04:14.183115Z","iopub.status.idle":"2022-05-08T01:04:14.205477Z","shell.execute_reply.started":"2022-05-08T01:04:14.183079Z","shell.execute_reply":"2022-05-08T01:04:14.204599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter_computer-vision/bounding-box.ipynb#scrollTo=wASmLzN_HAlN\n\ndef box_corner_to_center_arr(boxes):\n    \"\"\"Convert from (upper-left, lower-right) to (center, width, height).\"\"\"\n    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n    cx = (x1 + x2) / 2\n    cy = (y1 + y2) / 2\n    w = x2 - x1\n    h = y2 - y1\n    boxes = np.stack((cx, cy, w, h), axis=-1)\n    return boxes\n\ndef box_center_to_corner_arr(boxes):\n    \"\"\"Convert from (center, width, height) to (upper-left, lower-right).\"\"\"\n    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n    x1 = cx - 0.5 * w\n    y1 = cy - 0.5 * h\n    x2 = cx + 0.5 * w\n    y2 = cy + 0.5 * h\n    boxes = np.stack((x1, y1, x2, y2), axis=-1)\n    return boxes\n\ndef bbox_to_rect(bbox, color):\n    \"\"\"Convert bounding box to matplotlib format.\"\"\"\n    # Convert the bounding box (upper-left x, upper-left y, lower-right x,\n    # lower-right y) format to the matplotlib format: ((upper-left x,\n    # upper-left y), width, height)\n    return plt.Rectangle(\n        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],\n        fill=False, edgecolor=color, linewidth=2)\n\n\ndef box_center_to_corner(box, width, length):\n    \"\"\"Convert from (center, width, height) to (upper-left, lower-right).\"\"\"\n    cx, cy, w, h = box[0], box[1], box[2], box[3]\n    x1 = round(cx * width - 0.5 * w * width)\n    y1 = round(cy * length - 0.5 * h * length)\n    x2 = round(cx * width + 0.5 * w * width)\n    y2 = round(cy * length + 0.5 * h * length)\n    bbox = [x1, y1, x2, y2]\n    return bbox","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:04:14.206923Z","iopub.execute_input":"2022-05-08T01:04:14.207249Z","iopub.status.idle":"2022-05-08T01:04:14.219375Z","shell.execute_reply.started":"2022-05-08T01:04:14.20721Z","shell.execute_reply":"2022-05-08T01:04:14.218605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract bounding boxes from the labels.\n#trainBboxes = df_labels.iloc[:,1:6]\n#trainBboxes.columns = ['upper_left_x', 'upper_left_y', 'width', 'length', 'filename' ]\n#trainBboxes.columns = ['yolo_x', 'yolo_y', 'yolo_width', 'yolo_length', 'filename' ]\n#trainBboxes.columns = ['resize_upper_x', 'resize_left_y', 'resize_lower_x', 'resize_right_y', 'filename' ]\n\n\n#print (trainBboxes)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:04:14.220549Z","iopub.execute_input":"2022-05-08T01:04:14.222865Z","iopub.status.idle":"2022-05-08T01:04:14.231777Z","shell.execute_reply.started":"2022-05-08T01:04:14.222827Z","shell.execute_reply":"2022-05-08T01:04:14.231004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Display some sample image and check bounding boxes","metadata":{}},{"cell_type":"code","source":"from glob import glob\nfrom PIL import Image\n\ndef plot_images(imgs, rows=5):\n    # Set figure to 15 inches x 8 inches\n    figure = plt.figure(figsize=(15, 8))\n    cols = len(imgs) // rows + 1\n    for i in range(len(imgs)):\n        subplot = figure.add_subplot(rows, cols, i + 1)\n        subplot.axis('Off')\n        plt.imshow(imgs[i])\n\ndef plot_images_for_filenames(filenames, rows=5):\n    imgs = [plt.imread(f'{filename}') for filename in filenames]\n    return plot_images(imgs, rows)\n\ndef displayTurtle(turtleNum):\n    dataPath = \"/kaggle/input/sea-turtle-face-detection/data/\"\n    image_path = dataPath + \"images/Image_\" + str(turtleNum) + \".jpg\"\n    lable_path = dataPath + \"labels/Image_\" + str(turtleNum) + \".txt\"\n    # display the image\n    img = mpimg.imread(image_path)\n    height, width, depth = img.shape\n    plt.figure(figsize = (15,15))\n    fig = plt.imshow(img)\n    # read the yolo cordinates.  read_csv stores the numbers in the column names\n    label = pd.read_csv(lable_path, sep=\" \")\n    yolobbox = list(label.columns[1:5])\n    yolobbox = [float(i) for i in yolobbox]    # convert numbers from strings to floats\n    print (yolobbox)\n    # convert to corners\n    bbox = box_center_to_corner(yolobbox, width, height)\n    print (bbox)\n    fig.axes.add_patch(bbox_to_rect(bbox, 'blue'))\n\ndef displayTurtle_resized(turtleNum):\n    dataPath = \"/kaggle/input/sea-turtle-face-detection/data/\"\n    image_path = '/kaggle/working/images224/' + \"Image_\" + str(turtleNum) + \".jpg\"\n    lable_path = dataPath + \"labels/Image_\" + str(turtleNum) + \".txt\"\n    # display the image\n    img = mpimg.imread(image_path)\n    height, width, depth = img.shape\n    plt.figure(figsize = (5,5))\n    fig = plt.imshow(img)\n    # read the yolo cordinates.  read_csv stores the numbers in the column names\n    label = pd.read_csv(lable_path, sep=\" \")\n    yolobbox = list(label.columns[1:5])\n    yolobbox = [float(i) for i in yolobbox]    # convert numbers from strings to floats\n    print (yolobbox)\n    # convert to corners\n    bbox = box_center_to_corner(yolobbox, width, height)\n    print (bbox)\n    fig.axes.add_patch(bbox_to_rect(bbox, 'blue'))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:04:14.2349Z","iopub.execute_input":"2022-05-08T01:04:14.235092Z","iopub.status.idle":"2022-05-08T01:04:14.251376Z","shell.execute_reply.started":"2022-05-08T01:04:14.235068Z","shell.execute_reply":"2022-05-08T01:04:14.250642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_images = glob(\"../input/sea-turtle-face-detection/data/images/Image_101*.jpg\")\n\nprint (\"Original Sized Images\")\nprint (sample_images)\nplot_images_for_filenames(sample_images)  ","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:04:14.252761Z","iopub.execute_input":"2022-05-08T01:04:14.253016Z","iopub.status.idle":"2022-05-08T01:04:16.952226Z","shell.execute_reply.started":"2022-05-08T01:04:14.252982Z","shell.execute_reply":"2022-05-08T01:04:16.951567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_images = glob(\"/kaggle/working/images224/Image_101*.jpg\")\n\nprint (\"Resized Images\")\nprint (sample_images)\nplot_images_for_filenames(sample_images)  ","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:04:16.955401Z","iopub.execute_input":"2022-05-08T01:04:16.956212Z","iopub.status.idle":"2022-05-08T01:04:17.604558Z","shell.execute_reply.started":"2022-05-08T01:04:16.956168Z","shell.execute_reply":"2022-05-08T01:04:17.603704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This is an example of a image with two turtle faces\n# from https://www.kaggle.com/code/foucardm/draw-bounding-boxes-for-object-detection\none_image_path = \"/kaggle/input/sea-turtle-face-detection/data/images/Image_18.png\"\nimage = mpimg.imread(one_image_path)\nplt.figure(figsize = (10,10))\nimgplot = plt.imshow(image)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:04:17.605852Z","iopub.execute_input":"2022-05-08T01:04:17.606113Z","iopub.status.idle":"2022-05-08T01:04:17.931329Z","shell.execute_reply.started":"2022-05-08T01:04:17.60608Z","shell.execute_reply":"2022-05-08T01:04:17.930587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"displayTurtle(34)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:04:17.93258Z","iopub.execute_input":"2022-05-08T01:04:17.932998Z","iopub.status.idle":"2022-05-08T01:04:18.632942Z","shell.execute_reply.started":"2022-05-08T01:04:17.932962Z","shell.execute_reply":"2022-05-08T01:04:18.632289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"displayTurtle_resized(34)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:04:18.63392Z","iopub.execute_input":"2022-05-08T01:04:18.634244Z","iopub.status.idle":"2022-05-08T01:04:18.896322Z","shell.execute_reply.started":"2022-05-08T01:04:18.634213Z","shell.execute_reply":"2022-05-08T01:04:18.895598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"displayTurtle(1015)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:04:18.897477Z","iopub.execute_input":"2022-05-08T01:04:18.897809Z","iopub.status.idle":"2022-05-08T01:04:19.909753Z","shell.execute_reply.started":"2022-05-08T01:04:18.897771Z","shell.execute_reply":"2022-05-08T01:04:19.909045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"displayTurtle_resized(1015)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:04:19.911025Z","iopub.execute_input":"2022-05-08T01:04:19.911397Z","iopub.status.idle":"2022-05-08T01:04:20.185187Z","shell.execute_reply.started":"2022-05-08T01:04:19.911362Z","shell.execute_reply":"2022-05-08T01:04:20.184586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set up CNN ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from https://stackoverflow.com/questions/41749398/using-keras-imagedatagenerator-in-a-regression-model\n\nIMAGE_SIZE = (224, 224)\n\ntrain_datagen = ImageDataGenerator(\n    rescale = 1./255, \n    horizontal_flip = True\n) \n\n# This version uses the resized images\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=df_all, \n    directory=save_dir,                                               \n    x_col=\"filename\", \n    #y_col=['resize_upper_x', 'resize_left_y', 'resize_lower_x', 'resize_right_y'], \n    y_col=['yolo_x', 'yolo_y', 'yolo_width', 'yolo_length'], \n    #has_ext=True, \n    class_mode=\"raw\", \n    target_size=IMAGE_SIZE,\n    batch_size=32\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:04:20.186592Z","iopub.execute_input":"2022-05-08T01:04:20.187016Z","iopub.status.idle":"2022-05-08T01:04:20.213608Z","shell.execute_reply.started":"2022-05-08T01:04:20.186982Z","shell.execute_reply":"2022-05-08T01:04:20.212892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='loss', \n                                            patience=5, \n                                            verbose=2, \n                                            factor=0.5,                                            \n                                            min_lr=0.000001)\n\nearly_stops = EarlyStopping(monitor='loss', \n                            min_delta=0, \n                            patience=10, \n                            verbose=2, \n                            mode='auto')\n\ncheckpointer = ModelCheckpoint(filepath = 'cis3115.{epoch:02d}-{accuracy:.6f}.hdf5',\n                               verbose=2,\n                               save_best_only=True, \n                               save_weights_only = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:04:20.214749Z","iopub.execute_input":"2022-05-08T01:04:20.215177Z","iopub.status.idle":"2022-05-08T01:04:20.221423Z","shell.execute_reply.started":"2022-05-08T01:04:20.215135Z","shell.execute_reply":"2022-05-08T01:04:20.220699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up the Neural Network\nIMAGE_SIZE = (224, 224)\n# ==== Select one of the pre-trained models from Keras.  Samples are shown below\n#pretrained_model = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n#pretrained_model = tf.keras.applications.EfficientNetB4(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n#pretrained_model = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n#pretrained_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n#pretrained_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n#pretrained_model = tf.keras.applications.ResNet152V2(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\npretrained_model = tf.keras.applications.VGG19(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n#pretrained_model = tf.keras.applications.Xception(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n\n# Set the following to False so that the pre-trained weights are not changed \npretrained_model.trainable = False \n\nmodel = Sequential()\n#  Start with the pretrained model defined above\nmodel.add(pretrained_model)\n\n# Flatten 2D images into 1D data for final layers like traditional neural network\nmodel.add(Flatten())\n# GlobalAveragePooling2D is an alternative to Flatter and reduces the size of the layer while flattening\n#model.add(GlobalAveragePooling2D())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(32, activation='relu'))\n\n# The final output layer\n# Use Sigmoid when predicting YOLO bounding box since that output is between 0 and 1\n#model.add(Dense(4, activation='sigmoid'))\n# Use relu when predicting corner pixels since the outputs are intergers larger than 1\n#model.add(Dense(4, activation='relu'))\nmodel.add(Dense(4, activation='linear'))\n\n\nprint (\"Pretrained model used:\")\npretrained_model.summary()\n\nprint (\"Final model created:\")\nmodel.summary()\n\n# Compile neural network model\n#model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\nmodel.compile(optimizer='adam', loss='mean_absolute_error', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:04:20.222764Z","iopub.execute_input":"2022-05-08T01:04:20.223211Z","iopub.status.idle":"2022-05-08T01:04:21.120497Z","shell.execute_reply.started":"2022-05-08T01:04:20.223175Z","shell.execute_reply":"2022-05-08T01:04:21.119768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model with the images in the folders\nhistory = model.fit(\n        train_generator,\n        # validation_data=(testImages, testTargets),\n        batch_size=16,                  # Number of image batches to process per epoch \n        epochs=100,                      # Number of epochs\n        callbacks=[learning_rate_reduction, early_stops],\n        )\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:04:21.123355Z","iopub.execute_input":"2022-05-08T01:04:21.123566Z","iopub.status.idle":"2022-05-08T01:12:39.947465Z","shell.execute_reply.started":"2022-05-08T01:04:21.12354Z","shell.execute_reply":"2022-05-08T01:12:39.946783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will display the loss and the accuracy of the model for each epoch\n# NOTE: this is a little fancier display than is shown in the textbook\ndef display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:12:39.949131Z","iopub.execute_input":"2022-05-08T01:12:39.949404Z","iopub.status.idle":"2022-05-08T01:12:39.956816Z","shell.execute_reply.started":"2022-05-08T01:12:39.949367Z","shell.execute_reply":"2022-05-08T01:12:39.955161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\ndisplay_training_curves(history.history['loss'], history.history['loss'], 'loss', 211)\ndisplay_training_curves(history.history['accuracy'], history.history['accuracy'], 'accuracy', 212)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:12:39.958249Z","iopub.execute_input":"2022-05-08T01:12:39.958726Z","iopub.status.idle":"2022-05-08T01:12:40.3972Z","shell.execute_reply.started":"2022-05-08T01:12:39.958689Z","shell.execute_reply":"2022-05-08T01:12:40.39656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict the bounding box\n# Original box is in Blue\n# Predicted box is in Yellow\n\ndef predict_resized_pixels(rowNum):\n    row = df_all.iloc[[rowNum]]\n    filename = row['filename']\n    filename = filename.iloc[0]\n    image_path = '/kaggle/working/images224/' + str(filename)\n    # display the image\n    img = mpimg.imread(image_path)\n    img = img/255     # rescale the image values\n    #height, width, depth = img.shape\n    plt.figure(figsize = (5,5))\n    fig = plt.imshow(img)\n    \n    # Display to original bounding box\n    orig_bbox = row[['resize_upper_x', 'resize_left_y', 'resize_lower_x', 'resize_right_y']]\n    orig_bbox = orig_bbox.values.tolist()\n    orig_bbox = orig_bbox[0]\n    print ('Actual Bounding Box')\n    print (orig_bbox)\n    fig.axes.add_patch(bbox_to_rect(orig_bbox, 'blue'))\n    \n    # Predict and display the predicted bounding box\n    img2 = cv2.imread(image_path)\n    #img = cv2.resize(img,(320,240))\n    img = np.reshape(img,[1,224,224,3])\n\n    pred_bbox = model.predict(img)\n    pred_bbox = pred_bbox[0]\n    print ('Predicted Bounding Box')\n    print (pred_bbox)\n    fig.axes.add_patch(bbox_to_rect(pred_bbox, 'yellow'))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:12:40.398394Z","iopub.execute_input":"2022-05-08T01:12:40.399181Z","iopub.status.idle":"2022-05-08T01:12:40.407924Z","shell.execute_reply.started":"2022-05-08T01:12:40.399143Z","shell.execute_reply":"2022-05-08T01:12:40.40715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict the bounding box\n# Original box is in Blue\n# Predicted box is in Yellow\n\ndef predict_yolo(rowNum):\n    row = df_all.iloc[[rowNum]]\n    filename = row['filename']\n    filename = filename.iloc[0]\n    image_path = '/kaggle/working/images224/' + str(filename)\n    # display the image\n    img = mpimg.imread(image_path)\n    height, width, depth = img.shape\n    print ('Read in image of size '+str(height) + ' by ' + str(width) + ' by '+ str(depth))\n    img = img/255     # rescale the image values\n    plt.figure(figsize = (5,5))\n    fig = plt.imshow(img)\n    \n    # Display to original bounding box\n    yolo_orig_bbox = row[['yolo_x', 'yolo_y', 'yolo_width', 'yolo_length']]\n    yolo_orig_bbox = yolo_orig_bbox.values.tolist()\n    yolo_orig_bbox = yolo_orig_bbox[0]\n    print ('Actual YOLO Bounding Box')\n    print (yolo_orig_bbox)\n    orig_bbox = box_center_to_corner(yolo_orig_bbox, 224, 224)\n    print ('Actual Pixel Corners Bounding Box')\n    print (orig_bbox)\n    fig.axes.add_patch(bbox_to_rect(orig_bbox, 'blue'))\n    \n    # Predict and display the predicted bounding box\n    img = np.reshape(img,[1,224,224,3])\n    yolo_pred_bbox = model.predict(img)\n    yolo_pred_bbox = yolo_pred_bbox[0]\n    print ('Predicted YOLO Bounding Box')\n    print (yolo_pred_bbox)\n    pred_bbox = box_center_to_corner(yolo_pred_bbox, 224, 224)\n    print ('Predicted Pixel Corners Bounding Box')\n    print (pred_bbox)\n    fig.axes.add_patch(bbox_to_rect(pred_bbox, 'yellow'))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:12:40.40911Z","iopub.execute_input":"2022-05-08T01:12:40.409401Z","iopub.status.idle":"2022-05-08T01:12:40.422559Z","shell.execute_reply.started":"2022-05-08T01:12:40.409358Z","shell.execute_reply":"2022-05-08T01:12:40.421931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predict_resized(5)\nfor i in range(9):\n    predict_yolo(i)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:13:27.856216Z","iopub.execute_input":"2022-05-08T01:13:27.856479Z","iopub.status.idle":"2022-05-08T01:13:30.475264Z","shell.execute_reply.started":"2022-05-08T01:13:27.856441Z","shell.execute_reply":"2022-05-08T01:13:30.47442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predict_resized_pixels(10)\npredict_yolo(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:12:40.888623Z","iopub.execute_input":"2022-05-08T01:12:40.889312Z","iopub.status.idle":"2022-05-08T01:12:41.195434Z","shell.execute_reply.started":"2022-05-08T01:12:40.889274Z","shell.execute_reply":"2022-05-08T01:12:41.194794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}