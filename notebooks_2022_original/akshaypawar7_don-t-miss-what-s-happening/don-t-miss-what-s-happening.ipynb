{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Don‚Äôt miss what‚Äôs happening...üê§\n\n### People on Twitter are the first to know.\n\n\n![tweet](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTigQWzoYCNiDyrz1BN4WTf2X2k9OZ_yvW-FsmcIMsdS9fppNmh)\n* In this competition, you‚Äôre challenged to build a machine learning model that predicts which Tweets are about real disasters and which one‚Äôs aren‚Äôt.","metadata":{}},{"cell_type":"markdown","source":"# Summary\n\nIn this notebook I tried to apply what I learned on NLP.\nThis notebook includes:\n* Preprocessing the text\n* visualizing the processed data by several methods like\n * tweet lenghts\n * word counts\n * average word lengths\n * ngrams\n* build baseline models.\n\nI hope this notebook helps some of you as others helped me a lot.","metadata":{}},{"cell_type":"markdown","source":"# Getting the Text Data Ready","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\n\n\nimport re\nimport string\n\nimport nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nfrom nltk.corpus import wordnet, stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-26T11:29:18.02785Z","iopub.execute_input":"2022-04-26T11:29:18.028227Z","iopub.status.idle":"2022-04-26T11:29:18.525586Z","shell.execute_reply.started":"2022-04-26T11:29:18.02819Z","shell.execute_reply":"2022-04-26T11:29:18.524577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\n\n# dispaly data\nprint(f'Train shape: {train.shape}\\nTest shape: {test.shape}\\n', ' '.join(['-']*70))\n\nprint('Train data')\ndisplay(train.head())\nprint('Test Data')\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:29:18.527983Z","iopub.execute_input":"2022-04-26T11:29:18.528368Z","iopub.status.idle":"2022-04-26T11:29:18.623806Z","shell.execute_reply.started":"2022-04-26T11:29:18.528321Z","shell.execute_reply":"2022-04-26T11:29:18.622913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Combine Data For Preprossesing","metadata":{}},{"cell_type":"code","source":"train= pd.concat([train,test]).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:29:18.62549Z","iopub.execute_input":"2022-04-26T11:29:18.626034Z","iopub.status.idle":"2022-04-26T11:29:18.637516Z","shell.execute_reply.started":"2022-04-26T11:29:18.625987Z","shell.execute_reply":"2022-04-26T11:29:18.636672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target Distribution\n\nWhen we check our target variables and look at how they disturbuted we can say it not bad. There is no huge difference between classes we can say it's good sign for modelling","metadata":{"execution":{"iopub.status.busy":"2022-04-25T07:47:48.466766Z","iopub.execute_input":"2022-04-25T07:47:48.46705Z","iopub.status.idle":"2022-04-25T07:47:48.486038Z","shell.execute_reply.started":"2022-04-25T07:47:48.46702Z","shell.execute_reply":"2022-04-25T07:47:48.485401Z"}}},{"cell_type":"code","source":"# Target Distribution\n\nfig, ax = plt.subplots(ncols=2,nrows=1,figsize=(18,6),dpi=100)\n\nsns.countplot(train.target, ax=ax[0])\nax[0].bar_label(ax[0].containers[0])\nax[0].set_xticklabels(['Not Disaster', 'Disaster'])\n\n\nax[1].pie(train.target.value_counts(),\n          labels=['Not Disaster', 'Disaster'],\n          autopct='%.2f%%',\n          explode=(0.05, 0),\n          #textprops={'fontsize': 12},\n          #shadow=True,\n          startangle=90)\n\n\nfig.suptitle('Distribution of the Target', fontsize=24)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:29:18.640331Z","iopub.execute_input":"2022-04-26T11:29:18.640912Z","iopub.status.idle":"2022-04-26T11:29:18.964932Z","shell.execute_reply.started":"2022-04-26T11:29:18.640862Z","shell.execute_reply":"2022-04-26T11:29:18.964154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning Text\n\nSo basically what we did are:\n* Removed urls, emojis, html tags and punctuations\n* Some basic helper reg compiled functions to clean text by removing urls, emojis, html tags and punctuation","metadata":{}},{"cell_type":"code","source":"# In a loop, it would be better to compile the regular expression first\n\nurl_reg = re.compile(r'https?://\\S+|www\\.\\S+')\n\nemoji_reg = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n\nhtml_reg = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n\npunct_table = str.maketrans('', '', string.punctuation)\n\n# Applying helper reg functions\n\ntrain['text_clean'] = train['text'].apply(lambda x: url_reg.sub(r'',x))\ntrain['text_clean'] = train['text_clean'].apply(lambda x: emoji_reg.sub(r'', x))\ntrain['text_clean'] = train['text_clean'].apply(lambda x: html_reg.sub(r'', x))\ntrain['text_clean'] = train['text_clean'].apply(lambda x: x.translate(punct_table))\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:29:18.966103Z","iopub.execute_input":"2022-04-26T11:29:18.967013Z","iopub.status.idle":"2022-04-26T11:29:19.17929Z","shell.execute_reply.started":"2022-04-26T11:29:18.96697Z","shell.execute_reply":"2022-04-26T11:29:19.178077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# word lemmatizer\nIn many languages, words appear in several inflected forms. For example, in English, the verb 'to walk' may appear as 'walk', 'walked', 'walks' or 'walking'. The base form, 'walk', that one might look up in a dictionary, is called the lemma for the word. The association of the base form with a part of speech is often called a lexeme of the word.\n\nLemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster. The reduced \"accuracy\" may not matter for some applications. In fact, when used within information retrieval systems, stemming improves query recall accuracy, or true positive rate, when compared to lemmatisation. Nonetheless, stemming reduces precision, or the proportion of positively-labeled instances that are actually positive, for such systems.","metadata":{}},{"cell_type":"code","source":"# Tokenizing the tweet base texts.\n\ntrain['text_clean']=train['text_clean'].str.lower().apply(word_tokenize).apply(nltk.tag.pos_tag) # Applying part of speech tags.\n\n\n# Removing stopwords.\n\nstop = set(stopwords.words('english'))\n\ntrain['text_clean'] = train['text_clean'].apply(lambda x: [word for word in x if word not in stop])\n\n# Converting part of speeches to wordnet format.\n\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\n\ntrain['text_clean']= train['text_clean'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n\n# Applying word lemmatizer.\n\nwnl = WordNetLemmatizer()\n\ntrain['text_clean']= train['text_clean'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n\n\ntrain['text_clean']= train['text_clean'].apply(lambda x: ' '.join(x))\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:29:19.180834Z","iopub.execute_input":"2022-04-26T11:29:19.181124Z","iopub.status.idle":"2022-04-26T11:29:36.037947Z","shell.execute_reply.started":"2022-04-26T11:29:19.181083Z","shell.execute_reply":"2022-04-26T11:29:36.037262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing the processed data\n## Tweet lengths\n\nLet's start with the number of characters per tweet and compare if it's disaster related or not. It seems disaster tweets are longer than non disaster tweets in general. We can assume longer tweets are more likely for disasters but this is only an assumption and might be not true...¬∂","metadata":{}},{"cell_type":"code","source":"def plot_dist(col, sup_title, xlabel):\n    fig, ax = plt.subplots(ncols=2,nrows=1,figsize=(18,6), sharey=True)\n    \n    for index, x in enumerate(['Non Disaster Tweets', 'Disaster Tweets']):\n        sns.distplot(col.loc[train.target==index],ax=ax[index], color='#e74c3c')\n        \n        ax[index].set_xlabel(xlabel)\n        ax[index].set_ylabel('Frequency')\n        ax[index].set_title(x)\n    \n    fig.suptitle(sup_title, fontsize=24, va='baseline')\n    fig.tight_layout()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-04-26T11:29:36.039082Z","iopub.execute_input":"2022-04-26T11:29:36.040215Z","iopub.status.idle":"2022-04-26T11:29:36.049324Z","shell.execute_reply.started":"2022-04-26T11:29:36.040124Z","shell.execute_reply":"2022-04-26T11:29:36.047816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_dist(col= train.text_clean.str.len(), sup_title= 'Characters Per Tweet', xlabel='Character Count')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:29:36.050704Z","iopub.execute_input":"2022-04-26T11:29:36.050976Z","iopub.status.idle":"2022-04-26T11:29:36.695073Z","shell.execute_reply.started":"2022-04-26T11:29:36.050947Z","shell.execute_reply":"2022-04-26T11:29:36.693893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word Counts\nOk let's check number of words per tweet now, they both look somewhat normally distributed, again disaster tweets seems to have slightly more words than non disaster ones. We might dig this deeper to get some more info in next part...","metadata":{}},{"cell_type":"code","source":"plot_dist(col= train.text_clean.str.split().map(lambda x: len(x)), sup_title= 'words Per Tweet', xlabel='Word count')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:29:36.696841Z","iopub.execute_input":"2022-04-26T11:29:36.697171Z","iopub.status.idle":"2022-04-26T11:29:37.350604Z","shell.execute_reply.started":"2022-04-26T11:29:36.697125Z","shell.execute_reply":"2022-04-26T11:29:37.349802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mean Word Lengths\nThis time we're gonna check if word complexity differs from tweet class. It looks like disaster tweets has longer words than non disaster ones in general. It's pretty visible which is good sign, yet again we can only assume at this stage...","metadata":{}},{"cell_type":"code","source":"plot_dist(train.text_clean.str.split().apply(lambda x: np.mean([len(i) for i in x])),'Mean Word Lengths','Word Lengths')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:29:37.353772Z","iopub.execute_input":"2022-04-26T11:29:37.354193Z","iopub.status.idle":"2022-04-26T11:29:38.317328Z","shell.execute_reply.started":"2022-04-26T11:29:37.354151Z","shell.execute_reply":"2022-04-26T11:29:38.31628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Most Common Words\nIt's time to move to words themselves instead of their quantitative features. We start with most common words in both classes. I'd say it's pretty obvious if it's from disaster tweets or not. Disaster tweets has words like killed, news, bomb indicating disasters. Meanwhile non disaster ones looks like pretty generic.","metadata":{}},{"cell_type":"code","source":"def plot_ngram(ngram, sup_title):\n    vectorizer = CountVectorizer(stop_words='english', ngram_range=(ngram, ngram))\n    df = vectorizer.fit_transform(train.text_clean)\n    df = pd.DataFrame(df.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    fig, ax = plt.subplots(ncols=2,nrows=1,figsize=(18,8))\n    \n    for index, xlabel in enumerate(['Non Disaster Tweets', 'Disaster Tweets']):\n        \n        df_class = df.loc[train.target==index].sum().sort_values(ascending=False).head(15)\n\n        sns.barplot(x= df_class, y = df_class.index, ax=ax[index], palette='plasma')\n        \n        ax[index].set_xlabel('Count')\n        ax[index].set_ylabel('Word')\n        ax[index].set_title(xlabel)\n    \n    fig.suptitle(sup_title, fontsize=24, va='baseline')\n    fig.tight_layout()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-04-26T11:29:38.318832Z","iopub.execute_input":"2022-04-26T11:29:38.319124Z","iopub.status.idle":"2022-04-26T11:29:38.330994Z","shell.execute_reply.started":"2022-04-26T11:29:38.319084Z","shell.execute_reply":"2022-04-26T11:29:38.329674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_ngram(1,'Most Common Unigrams')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:29:38.332965Z","iopub.execute_input":"2022-04-26T11:29:38.333224Z","iopub.status.idle":"2022-04-26T11:29:41.047774Z","shell.execute_reply.started":"2022-04-26T11:29:38.333194Z","shell.execute_reply":"2022-04-26T11:29:41.046499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Most Common Bigrams\nLet's have a look for bigrams this time, which they are sequences of adjacent two words. Again it's pretty obvious to seperate two classes if it's disaster related or not. There are some confusing bigrams in non disaster ones like body bag, emergency services etc. which needs deeper research but we'll leave it here since we got what we looking for in general.","metadata":{}},{"cell_type":"code","source":"plot_ngram(2,'Most Common Bigrams')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:29:41.049547Z","iopub.execute_input":"2022-04-26T11:29:41.049903Z","iopub.status.idle":"2022-04-26T11:29:47.086539Z","shell.execute_reply.started":"2022-04-26T11:29:41.049869Z","shell.execute_reply":"2022-04-26T11:29:47.085814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Most Common Trigrams\nAlright! Things are much clearer with sequences of 3 words. The confusing body bags were cross body bags (Who uses them in these days anyways!) which I found it pretty funny when I found the reason of the confusion. Anyways we can see disasters are highly seperable now from non disaster ones, which is great!","metadata":{}},{"cell_type":"code","source":"plot_ngram(3,'Most Common Trigrams')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:29:47.087767Z","iopub.execute_input":"2022-04-26T11:29:47.088524Z","iopub.status.idle":"2022-04-26T11:29:55.178974Z","shell.execute_reply.started":"2022-04-26T11:29:47.088483Z","shell.execute_reply":"2022-04-26T11:29:55.177942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Baseline Model","metadata":{"execution":{"iopub.status.busy":"2022-04-26T04:07:09.933705Z","iopub.execute_input":"2022-04-26T04:07:09.933979Z","iopub.status.idle":"2022-04-26T04:07:10.693568Z","shell.execute_reply.started":"2022-04-26T04:07:09.933941Z","shell.execute_reply":"2022-04-26T04:07:10.692615Z"}}},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Dropout\nfrom tensorflow.keras.layers import TextVectorization\n\nfrom sklearn.model_selection import train_test_split","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-26T11:29:55.180458Z","iopub.execute_input":"2022-04-26T11:29:55.181052Z","iopub.status.idle":"2022-04-26T11:29:55.186507Z","shell.execute_reply.started":"2022-04-26T11:29:55.181014Z","shell.execute_reply":"2022-04-26T11:29:55.185789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# separate train and test set\n\ntest = train.loc[train.target.isna()]\ntrain = train.loc[train.target.notna()]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:29:55.187608Z","iopub.execute_input":"2022-04-26T11:29:55.188933Z","iopub.status.idle":"2022-04-26T11:29:55.209894Z","shell.execute_reply.started":"2022-04-26T11:29:55.188886Z","shell.execute_reply":"2022-04-26T11:29:55.20879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#A preprocessing layer which maps text features to integer sequences.\n\nvectorizer= TextVectorization()\nvectorizer.adapt(train.text_clean)\nvectorizer(train.text_clean)\n\n# vectorize train & test text data\n\ntrain_vec= vectorizer(train.text_clean).numpy()\ntest_vec= vectorizer(test.text_clean).numpy()\n\n\n# model\n\nmodel = tf.keras.Sequential([\n    Embedding(\n        len(vectorizer.get_vocabulary()) + 1, # Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n        100,                                  # Integer. Dimension of the dense embedding.\n        input_length= train_vec[0].shape[0]), # Length of input sequences\n    Dropout(0.2),\n    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n    Dense(1, activation='sigmoid')])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:29:55.211211Z","iopub.execute_input":"2022-04-26T11:29:55.211809Z","iopub.status.idle":"2022-04-26T11:29:56.023965Z","shell.execute_reply.started":"2022-04-26T11:29:55.211771Z","shell.execute_reply":"2022-04-26T11:29:56.022829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy', 'AUC'])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:29:56.025435Z","iopub.execute_input":"2022-04-26T11:29:56.025665Z","iopub.status.idle":"2022-04-26T11:29:56.038454Z","shell.execute_reply.started":"2022-04-26T11:29:56.025637Z","shell.execute_reply":"2022-04-26T11:29:56.037246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split training set into train test\n\nX_train, X_test, y_train, y_test= train_test_split(train_vec, train.target.astype(int).values)\n\n# classes weights for imbalance data\n# Scaling by total/2 helps keep the loss to a similar magnitude.\n# The sum of the weights of all examples stays the same.\ntotal = 7613\n\nweight_for_0 = (1 / 4342) * (total / 2.0)\nweight_for_1 = (1 / 3271) * (total / 2.0)\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))\n\n# fit the model\nhistory=model.fit(X_train, y_train,\n                  #batch_size=7,\n                  epochs=4,\n                  validation_data=(X_test,y_test),\n                  class_weight=class_weight)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:38:25.952175Z","iopub.execute_input":"2022-04-26T11:38:25.953341Z","iopub.status.idle":"2022-04-26T11:40:43.133372Z","shell.execute_reply.started":"2022-04-26T11:38:25.953285Z","shell.execute_reply":"2022-04-26T11:40:43.132085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for accuracy\n\nfig, ax= plt.subplots(nrows=1,ncols=2,figsize=(18,4),sharex=True)\n\nfor i, m in enumerate(['accuracy', 'loss']):\n    ax[i].plot(history.history[m])\n    ax[i].plot(history.history[f'val_{m}'])\n    ax[i].set_title(f'model {m}')\n    ax[i].set_ylabel(m)\n    ax[i].set_xlabel('epoch')\n    ax[i].legend(['train', 'test'], loc='upper left')\n\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:41:32.314352Z","iopub.execute_input":"2022-04-26T11:41:32.315062Z","iopub.status.idle":"2022-04-26T11:41:32.771645Z","shell.execute_reply.started":"2022-04-26T11:41:32.314985Z","shell.execute_reply":"2022-04-26T11:41:32.770774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"#Making our submission\n\nsample_sub=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\n\n\nsample_sub['target'] = model.predict(test_vec).round().astype(int)\n\nsample_sub.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T11:42:33.24608Z","iopub.execute_input":"2022-04-26T11:42:33.24643Z","iopub.status.idle":"2022-04-26T11:42:35.636436Z","shell.execute_reply.started":"2022-04-26T11:42:33.246396Z","shell.execute_reply":"2022-04-26T11:42:35.63521Z"},"trusted":true},"execution_count":null,"outputs":[]}]}