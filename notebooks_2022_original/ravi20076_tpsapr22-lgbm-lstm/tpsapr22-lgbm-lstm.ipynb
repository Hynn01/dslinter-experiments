{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing general packages:-\nimport numpy as np;\nimport pandas as pd;\nfrom scipy.stats import iqr, skew, kurtosis, mode;\nfrom scipy.fft import rfft;\nfrom warnings import filterwarnings;\nfrom termcolor import colored;\nimport gc;\n\nimport seaborn as sns;\nimport matplotlib.pyplot as plt;\n%matplotlib inline\n\nnp.random.seed(10);\npd.options.display.float_format = '{:.2f}'.format;","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-27T13:59:32.017507Z","iopub.execute_input":"2022-04-27T13:59:32.018439Z","iopub.status.idle":"2022-04-27T13:59:32.028296Z","shell.execute_reply.started":"2022-04-27T13:59:32.018299Z","shell.execute_reply":"2022-04-27T13:59:32.027577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Important sklearn and ensemble specific packages:-\nfrom sklearn.base import BaseEstimator, TransformerMixin;\nfrom sklearn.pipeline import Pipeline;\nfrom sklearn_pandas import DataFrameMapper, gen_features;\nfrom sklearn.preprocessing import FunctionTransformer,RobustScaler, StandardScaler;\nfrom sklearn.model_selection import GridSearchCV, GroupKFold, cross_val_score;\nfrom sklearn.decomposition import PCA;\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score;\n\nfrom sklearn.tree import DecisionTreeClassifier as DTree;\nfrom xgboost import XGBClassifier;\nfrom lightgbm import LGBMClassifier;\nfrom catboost import CatBoostClassifier;","metadata":{"execution":{"iopub.status.busy":"2022-04-27T13:59:32.0296Z","iopub.execute_input":"2022-04-27T13:59:32.030674Z","iopub.status.idle":"2022-04-27T13:59:33.759349Z","shell.execute_reply.started":"2022-04-27T13:59:32.030505Z","shell.execute_reply":"2022-04-27T13:59:33.758373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tabular Playground Series- April 2022:-","metadata":{}},{"cell_type":"markdown","source":"# 1. Data Loading and pre-model checks\n\nWe load the train-test data into the kernel, elicit sample data to assess the structures and engender basic checks for further steps in this section","metadata":{}},{"cell_type":"code","source":"# Loading the relevant data-sets with basic checks:-\nxtrain = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv', encoding = 'utf8');\nytrain = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv', encoding = 'utf8');\nxtest = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv', encoding = 'utf8');\nsub_fl = pd.read_csv('../input/tabular-playground-series-apr-2022/sample_submission.csv', encoding = 'utf8');\n\nprint(colored(f\"Train, train-label,test set lengths = {len(xtrain), len(ytrain), len(xtest)}\\n\", \n              color = 'blue', attrs= ['bold']));\n\nprint(colored(f\"\\nTrain-test data samples\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(xtrain.head(5));\nprint('\\n');\ndisplay(xtest.head(5));\n\nprint(colored(f\"\\nSample submission\\n\", color = 'blue', attrs= ['bold', 'dark']));\ndisplay(sub_fl.head(5));\n\nprint(colored(f\"\\nTrain-set columns\\n{list(xtrain.columns)}\\n\", color = 'blue'));\nprint(colored(f\"\\nTest-set columns\\n{list(xtest.columns)}\\n\", color = 'blue'));\n\nprint(colored(f\"\\nTrain labels-set columns\\n{list(ytrain.columns)}\\n\", color = 'blue'));\nprint(colored(f\"\\nTrain labels sample\\n\", color = 'blue', attrs = ['bold', 'dark']));\ndisplay(ytrain.head(5));","metadata":{"execution":{"iopub.status.busy":"2022-04-27T13:59:33.760654Z","iopub.execute_input":"2022-04-27T13:59:33.760905Z","iopub.status.idle":"2022-04-27T13:59:46.779196Z","shell.execute_reply.started":"2022-04-27T13:59:33.760846Z","shell.execute_reply":"2022-04-27T13:59:46.77843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking if the target class is balanced/ imbalanced:-\nfig, ax= plt.subplots(1,1, figsize= (4,6));\nsns.barplot(y= ytrain.state.value_counts(normalize= False), x= ytrain.state.unique(), palette= 'Blues',\n            saturation= 0.90, ax=ax);\nax.set_title(f\"Target column distribution for (im)balanced classes\\n\", color= 'tab:blue', fontsize= 12);\nax.set_yticks(range(0,14001,1000));\nax.set_xlabel('Target Class');\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-04-27T13:59:46.780324Z","iopub.execute_input":"2022-04-27T13:59:46.780553Z","iopub.status.idle":"2022-04-27T13:59:47.000825Z","shell.execute_reply.started":"2022-04-27T13:59:46.780517Z","shell.execute_reply":"2022-04-27T13:59:46.999844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Development Plan:-\n\nThese tables are structured as below-\n1. xtrain:- This encapsulates the training features only. Each sequence number is associated with 60 time steps with numerical readings for that sequence number from 13 sensors on a unique subject number for the sequence.\n2. ytrain:- This is the target table, with sequence numbers and class labels for the sequence. This data is balanced, as seen in the previous cell target distribution, hence, no over-sampling is needed\n3. xtest:- This continues the sequences from the train-set but we are unaware of the classification state\n\nModel development requires feature engineering with new features encapsulating the sensor readings' descriptive statistics over the sequence number. \nNew columns including the mean, std, skewness, kurtosis, IQR, median, etc. can be created across all 13 sensors and their efficacy in the model development may be assessed for inclusion. \nClassifier models like an LSTM/ ML models could be used after eliciting relevant features and pre-processing","metadata":{}},{"cell_type":"markdown","source":"# 2. Feature processing\n\nIn this section, we analyse the train-set features, plot graphs as deemed necessary and elicit model development specific inferences from the raw data. We prepare an interim table xytrain with all features and the target state to facilitate lower computational burden during the analysis herewith.","metadata":{}},{"cell_type":"code","source":"# Displaying train-set information and description:-\nprint(colored(f\"\\nTrain-set information\\n\", color = 'blue', attrs = ['bold', 'dark']));\ndisplay(xtrain.info());\n\nprint(colored(f\"\\nTrain-set description\\n\", color = 'blue', attrs = ['bold', 'dark']));\ndisplay(xtrain.describe().transpose().style.format('{:,.1f}'));\n\nprint(colored(f\"\\nTrain-test set dtypes\\n\", color = 'blue', attrs = ['bold', 'dark']));\ndisplay(pd.concat((xtrain.dtypes, xtest.dtypes), axis= 1).rename({0: 'Train_Dtypes', 1: 'Test_Dtypes'}, axis=1));","metadata":{"execution":{"iopub.status.busy":"2022-04-27T13:59:47.003808Z","iopub.execute_input":"2022-04-27T13:59:47.004381Z","iopub.status.idle":"2022-04-27T13:59:48.287281Z","shell.execute_reply.started":"2022-04-27T13:59:47.004324Z","shell.execute_reply":"2022-04-27T13:59:48.286336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting correlation heatmap for the train data sensor readings:-\nfig, ax= plt.subplots(1,1, figsize= (18,10));\nsns.heatmap(data= xtrain.iloc[:, -13:].corr(), vmin= 0.0, vmax=1.00, annot= True, fmt= '.1%',\n            cmap= sns.color_palette('Spectral_r'), linecolor='black', linewidth= 1.00,\n            ax=ax);\nax.set_title('Correlation heatmap for the train-set sensor data\\n', fontsize=12, color= 'black');\nplt.yticks(rotation= 0, fontsize= 9);\nplt.xticks(rotation= 90, fontsize= 9);\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-04-27T13:59:48.288681Z","iopub.execute_input":"2022-04-27T13:59:48.288937Z","iopub.status.idle":"2022-04-27T13:59:50.171228Z","shell.execute_reply.started":"2022-04-27T13:59:48.288903Z","shell.execute_reply":"2022-04-27T13:59:50.170371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preparing an interim dataframe with all the training features and their state for analysis:-\nxytrain = xtrain.merge(ytrain, how= 'left', on='sequence');","metadata":{"execution":{"iopub.status.busy":"2022-04-27T13:59:50.172477Z","iopub.execute_input":"2022-04-27T13:59:50.173052Z","iopub.status.idle":"2022-04-27T13:59:50.381345Z","shell.execute_reply.started":"2022-04-27T13:59:50.173005Z","shell.execute_reply":"2022-04-27T13:59:50.380283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting sensor readings with the target to elicit mutual information and importance:-\nfig, ax= plt.subplots(1,1, figsize= (12,6));\nxytrain.drop(['sequence', 'subject', 'step'], axis=1).corr()[['state']].drop('state').plot.bar(ax= ax);\nax.set_title(\"Correlation analysis for all sensor columns\\n\", color= 'tab:blue', fontsize= 12);\nax.grid(visible= True, which= 'both', color= 'grey', linestyle= '--', linewidth= 0.50);\nax.set_xlabel('\\nColumns', color= 'black');\nax.set_ylabel('Correlation', color= 'black');\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-04-27T13:59:50.383027Z","iopub.execute_input":"2022-04-27T13:59:50.383339Z","iopub.status.idle":"2022-04-27T13:59:51.75191Z","shell.execute_reply.started":"2022-04-27T13:59:50.383295Z","shell.execute_reply":"2022-04-27T13:59:51.750954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting boxplots to study the column distributions:-\n\nfig, ax= plt.subplots(1,1, figsize= (18,10));\nsns.boxplot(data= xtrain.iloc[:, -13:], ax=ax);\nax.set_title(f\"Distribution analysis for sensor data in train-set\\n\", color= 'tab:blue', fontsize= 12);\nax.set_yticks(range(-600,700,100));\nax.grid(visible=True, which='both', color= 'lightgrey', linestyle= '--');\nax.set_xlabel('\\nSensor Columns\\n', fontsize= 12, color= 'tab:blue');\nax.set_ylabel(f'Sensor Readings\\n', fontsize= 12, color= 'tab:blue');\n\nplt.xticks(rotation= 90);\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T13:59:51.753337Z","iopub.execute_input":"2022-04-27T13:59:51.753689Z","iopub.status.idle":"2022-04-27T13:59:57.653925Z","shell.execute_reply.started":"2022-04-27T13:59:51.753642Z","shell.execute_reply":"2022-04-27T13:59:57.652959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Subject analysis:- \n\nThis sub-section elicits key insights derived from the train-test set subjects as below-\n1. We plan to study the common subject characteristics for state\n2. We also plan to develop descriptive statistics of sensor readings based on subjects \n3. We will check if data leakage exists between the train-test subjects for any manual adjustments over the model results at the end of the assignment","metadata":{}},{"cell_type":"code","source":"# Analyzing subject characteristics:-\nsub_prf_train= \\\nxytrain[['subject', 'sequence', 'state']].drop_duplicates().set_index('sequence').\\\npivot_table(index= 'subject', values= 'state', aggfunc= [np.size, np.sum]);\nsub_prf_train.columns= ['Nb_Min', 'Nb_S1'];\nsub_prf_train['Nb_S0'] = sub_prf_train['Nb_Min'] - sub_prf_train['Nb_S1'];\nsub_prf_train['S1_Rate'] = sub_prf_train['Nb_S1']/ sub_prf_train['Nb_Min'];\n\nsub_prf_train.sort_values(['S1_Rate'], ascending= False);\n\nprint(colored(f\"\\nTrain set subject inferences:-\", color= 'red', attrs= ['bold', 'dark']));\nprint(colored(f\"Number of train-set subjects = {len(sub_prf_train)}\", color = 'blue'));\nprint(colored(f\"Number of train-set subjects never going to state 1 = {len(sub_prf_train.query('S1_Rate == 0.0'))}\", \n              color = 'blue'));\nprint(colored(f\"Number of train-set subjects never going to state 0 = {len(sub_prf_train.query('S1_Rate == 1.0'))}\", \n              color = 'blue'));\n\nprint(colored(f\"\\nDescriptive summary statistics for the training subjects\\n\", color = 'red', attrs= ['bold']));\ndisplay(sub_prf_train.iloc[:,:-1].describe().transpose().style.format('{:.1f}'));\n\nprint(colored(f\"\\nDescriptive summary statistics for the test-set subjects\\n\", color = 'red', attrs= ['bold']));\ndisplay(xtest[['sequence', 'subject']].drop_duplicates().groupby(['subject']).\\\nagg(Nb_Min = pd.NamedAgg('sequence', np.size)).describe().transpose().style.format('{:.1f}'));\n\nprint(colored(f\"\\nDescriptive summary statistics for the training subjects never in state 1\\n\", \n              color = 'red', attrs= ['bold']));\ndisplay(sub_prf_train.loc[sub_prf_train.S1_Rate== 0.0].describe().transpose().style.format('{:.1f}'));\n\nprint(colored(f\"\\nSensor summary statistics for the training subjects never in state 1\\n\", \n              color = 'red', attrs= ['bold']));\ndisplay(xtrain.loc[xtrain.subject.isin(sub_prf_train.loc[sub_prf_train.S1_Rate== 0.0].index), \n           xtrain.columns.str.startswith('sensor')].describe().transpose().style.format('{:,.1f}'));\n\nprint(colored(f\"\\nSensor summary statistics for the training subjects in state 1 and 0\\n\", \n              color = 'red', attrs= ['bold']));\ndisplay(xtrain.loc[xtrain.subject.isin(sub_prf_train.loc[sub_prf_train.S1_Rate > 0.0].index), \n           xtrain.columns.str.startswith('sensor')].describe().transpose().style.format('{:,.1f}'));\n\nprint(colored(f\"\\nSensor summary statistics for all training subjects\\n\", color = 'red', attrs= ['bold']));\ndisplay(xtrain.loc[:,xtrain.columns.str.startswith('sensor')].describe().transpose().style.format('{:,.1f}'));","metadata":{"execution":{"iopub.status.busy":"2022-04-27T13:59:57.655181Z","iopub.execute_input":"2022-04-27T13:59:57.655419Z","iopub.status.idle":"2022-04-27T14:00:00.191005Z","shell.execute_reply.started":"2022-04-27T13:59:57.655389Z","shell.execute_reply":"2022-04-27T14:00:00.189958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting unique sequences per subject:-\n\n_ = xytrain[['sequence', 'subject', 'state']].drop_duplicates().\\\n             groupby(['subject','state'])['sequence'].nunique().reset_index().\\\n             pivot_table(index= 'subject', columns= 'state', values= 'sequence', aggfunc= [np.sum]);\n_.columns = ['Nb_Unq_Seq0', 'Nb_Unq_Seq1'];\n\nfig, ax= plt.subplots(2,1, figsize= (18,15));\n\nsns.lineplot(data= _ , palette= 'rainbow', ax= ax[0], linestyle= '-');\nax[0].set_title(f\"Number of unique sequences per subject in the training set\\n\", color= 'black', fontsize= 12);\nax[0].legend(loc= 'upper right', fontsize= 8);\nax[0].set_xlabel('Subjects\\n', color= 'black', fontsize= 10);\nax[0].set_ylabel('Sequences', color= 'black', fontsize= 10);\nax[0].grid(visible= True, which= 'both', linestyle= '-', color= 'lightgrey');\nax[0].set_xticks(range(0, 680, 25));\nax[0].set_yticks(range(0, 181, 15));\n\nsns.lineplot(data=_.loc[sub_prf_train.loc[sub_prf_train.S1_Rate== 0.0].index][['Nb_Unq_Seq0']].values, \n             palette= 'Dark2',ax= ax[1]);\nax[1].set_title(f\"\\nNumber of unique sequences per subject in the training set never in state1\\n\",\n                color= 'black', fontsize= 12);\nax[1].grid(visible= True, which= 'both', linestyle= '-', color= 'lightgrey');\nax[1].set_xticks(range(0, 65, 5));\nplt.show();\n\ndel _;","metadata":{"execution":{"iopub.status.busy":"2022-04-27T14:00:00.192547Z","iopub.execute_input":"2022-04-27T14:00:00.192983Z","iopub.status.idle":"2022-04-27T14:00:01.091251Z","shell.execute_reply.started":"2022-04-27T14:00:00.192935Z","shell.execute_reply":"2022-04-27T14:00:01.090228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing the distributions of the sensor readings across state:-\n_ = xytrain.iloc[0:2,:].columns[xytrain.iloc[0:2,:].columns.str.startswith('sensor_')];\nfor col in _:\n    fig, ax= plt.subplots(1,1, figsize = (12,3.5));\n    sns.kdeplot(data=xytrain[[col, 'state']], x= col, hue=\"state\", multiple=\"stack\", palette = 'rainbow', ax= ax);\n    ax.grid(visible= True, which= 'both', color= 'lightgrey');\n    ax.set_xlabel('');\n    ax.set_title(f'\\n{col}\\n');\n    plt.show();\n\ndel _;","metadata":{"execution":{"iopub.status.busy":"2022-04-27T14:00:01.092592Z","iopub.execute_input":"2022-04-27T14:00:01.092842Z","iopub.status.idle":"2022-04-27T14:01:16.016068Z","shell.execute_reply.started":"2022-04-27T14:00:01.092809Z","shell.execute_reply":"2022-04-27T14:01:16.015119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing the univariate characteristics for all sensors across the states:-\n\nprint(colored(f\"\\nState 0 descriptions\\n\", color= 'blue', attrs= ['bold', 'dark'])); \ndisplay(xytrain.loc[xytrain.state == 0, xytrain.columns.str.startswith('sensor')].describe().transpose()\\\n        .style.format('{:,.2f}'));\nprint();\n\nprint(colored(f\"\\nState 1 descriptions\\n\", color= 'blue', attrs= ['bold', 'dark'])); \ndisplay(xytrain.loc[xytrain.state == 1, xytrain.columns.str.startswith('sensor')].describe().transpose().\\\n        style.format('{:,.2f}'));","metadata":{"execution":{"iopub.status.busy":"2022-04-27T14:01:16.017216Z","iopub.execute_input":"2022-04-27T14:01:16.01744Z","iopub.status.idle":"2022-04-27T14:01:17.235675Z","shell.execute_reply.started":"2022-04-27T14:01:16.017411Z","shell.execute_reply":"2022-04-27T14:01:17.234761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Deleting the combined analysis table after usage:-\ndel xytrain;\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2022-04-27T14:01:17.238704Z","iopub.execute_input":"2022-04-27T14:01:17.239082Z","iopub.status.idle":"2022-04-27T14:01:17.408381Z","shell.execute_reply.started":"2022-04-27T14:01:17.23904Z","shell.execute_reply":"2022-04-27T14:01:17.407393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline adjutant functions and classes:-\n\nThe below functions and classes are used to develop the pipeline for data transformation and processing","metadata":{}},{"cell_type":"code","source":"# Reducing memory usage by reassigning new datatypes to the relevant tables:-\ndef ReduceMemory(df:pd.DataFrame):\n    \"\"\"\n    This function assigns new dtypes to the relevant dataset attributes and reduces memory usage.\n    The relevant data-type is determined from the description seen earlier in the kernel.\n    \n    Input:- df (dataframe):- Analysis dataframe\n    Returns:- df (dataframe):- Modified dataframe\n    \"\"\"; \n    \n    df[['subject']] = df[['subject']].astype(np.int16);\n    df[['sequence']] = df[['sequence']].astype(np.int32);\n    df[['step']] = df[['step']].astype(np.int8);\n    \n    #  selecting all sensor float columns and reassigning data-types:-   \n    _ = df.iloc[0, -13:].index;\n    df[_] = df[_].astype(np.float32);\n    del _;\n    \n    return df; ","metadata":{"execution":{"iopub.status.busy":"2022-04-27T14:01:17.409944Z","iopub.execute_input":"2022-04-27T14:01:17.410718Z","iopub.status.idle":"2022-04-27T14:01:17.423202Z","shell.execute_reply.started":"2022-04-27T14:01:17.41067Z","shell.execute_reply":"2022-04-27T14:01:17.422209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureCreator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This class does the below tasks:-\n    1. Generates a model dataframe object as return object for all transforms and all sensors\n    2. Creates descriptive statistics based summaries \n    3. Appends each set of transforms to the master dataframe (mdl_df)\n    4. Separately calculates the MAD and appends it to the model master dataframe\n    5. Creates FFT based features if requested by the user and appends to the model dataframe\n    6. Creates a global feature list to be used elsewhere in the code\n    \"\"\";\n    \n    def __init__(self, FFT_req:str):self.FFT_req = FFT_req;    \n    def fit(self, X, y= None, **fit_params): \n        if self.FFT_req == 'Y': self.nb_FFT = np.int8(len(X[['step']].drop_duplicates())/2+1);\n        else: self.nb_FFT = 0;\n            \n        self.sensor_col = X.iloc[0,-13:].index; \n        return self;   \n    \n    # Creating FFT facilitator function:-\n    def DoFFT(self, X):\n        \"\"\"This function generates real valued Fast Fourier Transforms for the sensor readings in the dataframe\"\"\" ;        \n        FFT_df = \\\n        pd.concat([pd.Series(np.abs(rfft(X[col].values)), \n                             index=[f'freq{i}_{col}' for i in range(self.nb_FFT)]) \n               for col in self.sensor_col]);\n        return FFT_df; \n        \n    def transform(self, X, y= None, **transform_params):\n        \"This function provides the transformed features for the dataframe\";\n        \n        # Creating output master dataframe with sequence and subject:- \n        mdl_df = X[['sequence', 'subject']].drop_duplicates().set_index('sequence'); \n  \n        # Creating summary statistics based features:-\n        for col in self.sensor_col:\n            _xform = X.groupby('sequence').agg({col: [np.mean, np.amin, np.median, np.amax, iqr, skew, kurtosis]})\n            _xform.columns= [j+'_'+i for i, j in _xform.columns.to_flat_index()];\n            mdl_df = mdl_df.join(_xform);\n            del _xform; \n            \n         # Calculating MAD and appending with the master dataframe:-   \n        mdl_df = mdl_df.join(X.loc[:, self.sensor_col].groupby(X.sequence).mad().add_prefix('mad_'));\n        \n          # Creating the occurances of a given subject in the data-set as a feature:-   \n        _nb_subj_rcrd = X[['subject', 'sequence']].groupby('subject')[['sequence']].nunique().rank(method= 'max');\n        _nb_subj_rcrd['pctl_rcrd_subj'] = (_nb_subj_rcrd.sequence - 1)*100/len(_nb_subj_rcrd);\n        mdl_df = mdl_df.merge(_nb_subj_rcrd[['pctl_rcrd_subj']], how= 'left', \n                              left_on= 'subject', right_index= True);\n        del _nb_subj_rcrd;\n        \n        # Creating FFT if requested by user:-\n        if self.FFT_req == 'Y': \n            _fft = X.groupby(['sequence', 'subject']).apply(self.DoFFT).droplevel(level=1, axis='index');\n            mdl_df = pd.concat((mdl_df, _fft), axis=1);\n            del _fft;\n        \n        # Creating the list of features engineered from the give dataset:-        \n        global Ftre_Engr_Lst;\n        Ftre_Engr_Lst = list(mdl_df.columns);    \n        print(colored(f\"\\nTotal features generated ={len(Ftre_Engr_Lst):,.0f}\\n\", color= 'blue')); \n        \n        return mdl_df; ","metadata":{"execution":{"iopub.status.busy":"2022-04-27T14:01:17.42466Z","iopub.execute_input":"2022-04-27T14:01:17.424932Z","iopub.status.idle":"2022-04-27T14:01:17.44484Z","shell.execute_reply.started":"2022-04-27T14:01:17.424881Z","shell.execute_reply":"2022-04-27T14:01:17.44402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating an output table structure for all transforms across all features without FFT:- \ndef MakeFeatures(df:pd.DataFrame):\n    \"\"\"\n    This class does the below tasks:-\n    1. Generates a model dataframe object as return object for all transforms and all sensors\n    2. Creates descriptive statistics based summaries \n    3. Appends each set of transforms to the master dataframe (mdl_df)\n    4. Separately calculates the MAD and appends it to the model master dataframe\n    5. Creates a global feature list to be used elsewhere in the code\n    \"\"\";\n    \n    # Creating output master dataframe with sequence and subject:- \n    mdl_df = df[['sequence', 'subject']].drop_duplicates().set_index('sequence');\n    \n    # Aggregating sensor columns:-    \n    sensor_col_lst = df.iloc[0,-13:].index; \n    \n    for col in sensor_col_lst:\n        _xform = df.groupby('sequence').agg({col: [np.mean, np.amin, np.median, np.amax, iqr, skew, kurtosis]})\n        _xform.columns= [j+'_'+i for i, j in _xform.columns.to_flat_index()];\n        mdl_df = mdl_df.join(_xform);\n        del _xform;\n    \n    # Calculating MAD and appending with the master dataframe:-   \n    mdl_df = mdl_df.join(df.loc[:, sensor_col_lst].groupby(df.sequence).mad().add_prefix('mad_'));\n    \n    # Creating the occurances of a given subject in the data-set as a feature:-   \n    _nb_subj_rcrd = df[['subject', 'sequence']].groupby('subject')[['sequence']].nunique().rank(method= 'max');\n    _nb_subj_rcrd['pctl_rcrd_subj'] = (_nb_subj_rcrd.sequence - 1)*100/len(_nb_subj_rcrd);\n    mdl_df = mdl_df.merge(_nb_subj_rcrd[['pctl_rcrd_subj']], how= 'left', \n                          left_on= 'subject', right_index= True);\n        \n    del sensor_col_lst, _nb_subj_rcrd;       \n    global Ftre_Engr_Lst;\n    Ftre_Engr_Lst = list(mdl_df.columns);    \n    print(colored(f\"\\nTotal features generated are {len(Ftre_Engr_Lst):,.0f}\\n\", color= 'blue')); \n    return mdl_df;","metadata":{"execution":{"iopub.status.busy":"2022-04-27T14:01:17.446391Z","iopub.execute_input":"2022-04-27T14:01:17.446789Z","iopub.status.idle":"2022-04-27T14:01:17.466853Z","shell.execute_reply.started":"2022-04-27T14:01:17.446756Z","shell.execute_reply":"2022-04-27T14:01:17.465955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing outliers from the feature engineered dataset:-\nclass OutlierRemover(BaseEstimator, TransformerMixin):\n    \"This class removes outliers based on IQR multiplier (usually 1.5*IQR)\";\n    \n    def __init__(self, iqr_mult:float = 1.50):\n        \"This function initializes the IQR multiplier for the outlier removal\";\n        self.iqr_mult_ = iqr_mult;\n        \n    def fit(self, X, y=None, **fit_params):\n        \"This function calculates the cutoff for outlier removal on the train-data\";\n        X_iqr = iqr(X, axis=0);\n        self.OtlrLB_ = np.percentile(X, 25, axis=0) - self.iqr_mult_* X_iqr;\n        self.OtlrUB_ = np.percentile(X, 75, axis=0) + self.iqr_mult_* X_iqr;\n        del X_iqr;\n        return self;\n    \n    def transform(self, X, y= None, **transform_params):\n        \"This function clips the outliers off the data-set\";\n        return pd.DataFrame(data= np.clip(X, a_min= self.OtlrLB_, a_max= self.OtlrUB_), \n                            index= X.index, columns= X.columns);","metadata":{"execution":{"iopub.status.busy":"2022-04-27T14:01:17.468449Z","iopub.execute_input":"2022-04-27T14:01:17.468926Z","iopub.status.idle":"2022-04-27T14:01:17.48779Z","shell.execute_reply.started":"2022-04-27T14:01:17.468843Z","shell.execute_reply":"2022-04-27T14:01:17.48693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Performing univariate analysis on the relevant columns and selecting useful model features:-\nclass FeatureSelector(BaseEstimator, TransformerMixin):\n    \"This class calculates the correlation and mutual information scores for the features to help in model selection\";\n    \n    def __init__(self, abs_corr_cutoff:np.float32= 0.0,std_cutoff:np.float32 = 0.0): \n        \"This method initializes the cutoffs for the correlation and mutual information metrics\";\n        self.corr_cutoff_ = abs_corr_cutoff;\n        self.std_cutoff_ = std_cutoff;\n    \n    def fit(self, X, y, **fit_params):\n        \"\"\"\n        This method calculates the correlation on the training data \n        It also shortlists selected columns for the transform step\n        \"\"\";\n        global Ftre_Engr_Lst, Sel_Ftre_Lst_V1, Unv_Prf;\n        \n        _ = pd.concat((X, y), axis=1);       \n        self.Unv_Prf_ = \\\n        pd.concat((_.corr('pearson')[['state']].drop(['state'], axis=0).rename({'state': 'PRSN_COR_VAL'}, axis=1),\n                  pd.DataFrame(np.std(_, axis=0), index= Ftre_Engr_Lst, columns= ['STD_VAL'])                  \n                  ), axis=1).drop(['subject'], axis=0);\n\n        self.Unv_Sel_Ftre_ = \\\n        self.Unv_Prf_.loc[(abs(self.Unv_Prf_['PRSN_COR_VAL']) >= self.corr_cutoff_) &\n                          (self.Unv_Prf_['STD_VAL'] >= self.std_cutoff_)].index;\n        \n        Unv_Prf = self.Unv_Prf_;\n        \n        Sel_Ftre_Lst_V1 = list(self.Unv_Sel_Ftre_);\n        print(colored(f\"\\nSelected features after feature engineering\", color= 'blue', attrs= ['bold']));\n        print(colored(f\"{list(Sel_Ftre_Lst_V1)}\\n\", color = 'blue'));\n        return self;\n    \n    def transform(self, X, y=None, **transform_param):\n        \"\"\"\n        This function returns the correlation and mutual information results.\n        It also returns the data-set with reduced columns as per feature selection strategy\n        \"\"\";\n        X1= X.copy();\n        sel_cols = Sel_Ftre_Lst_V1 + ['subject'];\n        return X1[sel_cols];                      ","metadata":{"execution":{"iopub.status.busy":"2022-04-27T14:01:17.489564Z","iopub.execute_input":"2022-04-27T14:01:17.490063Z","iopub.status.idle":"2022-04-27T14:01:17.508503Z","shell.execute_reply.started":"2022-04-27T14:01:17.490006Z","shell.execute_reply":"2022-04-27T14:01:17.507081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Further shortlisting features based on purging highly correlated features to avoid collinearity:-\nclass CorrFeatureDropper(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This class shortlists features to drop after preliminary feature selection based on correlation among variables\n    \"\"\";\n    \n    def __init__(self, corr_cutoff:np.float32 = 0.80): \n        self.corr_cutoff = corr_cutoff;\n        \n    def fit(self, X, y= None,  **fit_params):\n        \"\"\"This function determines the columns to be dropped based on feature-correlation\"\"\";\n        \n        global Unv_Prf;\n        \n        # Generating the root column for further group-by object:-\n        Unv_Prf['ROOT_FTRE_NM'] = np.where(Unv_Prf.index.str.contains('sensor'), \n                                           Unv_Prf.index.str[-9:], Unv_Prf.index);\n        Unv_Prf['ABS_CORR_VAL'] = abs(Unv_Prf.PRSN_COR_VAL);\n\n        # Creating a lower triangular matrix of correlations from the shortlisted features and dropping 'subject':-\n        _ftre_corr_prf = \\\n        pd.DataFrame(np.tril(X.corr()),index= X.columns, columns= X.columns).\\\n        drop('subject', axis=0).drop('subject', axis=1);\n\n        # Collating columns to be dropped based on high correlations among features:-\n        drop_ftre_ = \\\n        pd.concat((Unv_Prf, \n                   _ftre_corr_prf[(abs(_ftre_corr_prf) > self.corr_cutoff) & \n                                  (abs(_ftre_corr_prf) < 1.0)].any(axis=0)), axis=1).\\\n        rename({0: 'CORR_FTRE_FL'}, axis=1).query(\"CORR_FTRE_FL == True\").\\\n        groupby('ROOT_FTRE_NM')[['ABS_CORR_VAL','STD_VAL']].rank(method= 'dense',ascending= False).\\\n        query(\"ABS_CORR_VAL != 1.0 and STD_VAL !=1.0\").index;\n        \n        self.drop_ftre_lst_ = list(drop_ftre_);\n        \n        print(colored(F\"\\nColumns to be dropped based on collinearity check are\", \n                      color= 'blue', attrs= ['bold', 'dark']));\n        print(colored(f\"{list(drop_ftre_)}\\n\", color = 'blue'));       \n        return self;\n    \n    def transform(self, X, y= None, **transform_params):\n        \"This function drops the shortlisted columns based on the fit method results\";\n        X1 = X.drop(self.drop_ftre_lst_, axis=1);\n        Sel_Ftre_Lst_V2 = list(X1.columns);\n        return X1;","metadata":{"execution":{"iopub.status.busy":"2022-04-27T14:01:17.510233Z","iopub.execute_input":"2022-04-27T14:01:17.51089Z","iopub.status.idle":"2022-04-27T14:01:17.532304Z","shell.execute_reply.started":"2022-04-27T14:01:17.510821Z","shell.execute_reply":"2022-04-27T14:01:17.531043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline development:-\n\nThis pipeline does the below-\n1. Reduce memory usage by reassigning data-types\n2. Develop new features from the sensor readings using descriptive statistics aggregators and if needed, FFT\n3. Remove outliers using the column IQR\n4. Shortlist important features using Pearson Correlation and standard deviation\n5. Drop correlated features to avoid collinearity issues\n6. Standardize the data using appropriate scaling and centering strategy\n\n**The FFT based step is adapted with thanks from the notebook titled 'LGBM with Fourier transform' originally written by PAVEL SALIKOV**","metadata":{}},{"cell_type":"code","source":"# Data processing specific globals:-\n# 1. Creating empty lists for the data-processor pipeline for feature shortlisting:-\nSel_Ftre_Lst_V1 = [];\nSel_Ftre_Lst_V2 = [];\n\n# 2. Other cutoffs for the feature-selection:-\nabs_corr_cutoff = 0.05;\nhighcorr_cutoff = 0.68;\nstd_cutoff = 0.30;\n\n# 2. Standardization class label (StandardScaler/ RobustScaler):\nStd_Class_Lbl = RobustScaler;\n\n# 3. FFT requirement flag:-\nFFT_req= 'Y';","metadata":{"execution":{"iopub.status.busy":"2022-04-27T14:01:17.534199Z","iopub.execute_input":"2022-04-27T14:01:17.535191Z","iopub.status.idle":"2022-04-27T14:01:17.550686Z","shell.execute_reply.started":"2022-04-27T14:01:17.535141Z","shell.execute_reply":"2022-04-27T14:01:17.549247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Designing the data processor pipeline:-\nData_Processor = \\\nPipeline(verbose= True,  \n         steps= \\\n         [('ReduceMemory', FunctionTransformer(ReduceMemory)),\n          ('MakeFeatures', FeatureCreator(FFT_req='Y')),\n          ('RemoveOutliers',OutlierRemover(iqr_mult=1.5)),\n          ('SelectFeatures', FeatureSelector(abs_corr_cutoff= abs_corr_cutoff, std_cutoff = std_cutoff)),\n          ('DropCorrFeatures', CorrFeatureDropper(highcorr_cutoff)),\n          ('StdFeatures', DataFrameMapper(drop_cols= None, input_df= True, df_out= True, default= None,\n                                          features= gen_features(\n                                              columns=[col.split(' ') for col in Sel_Ftre_Lst_V2 if col != 'subject'],\n                                              classes= [Std_Class_Lbl])\n                                         ))\n         ]\n        );\n\n# Implementing the pipeline on the train-test sets:-\nprint(colored(f\"\\nImplementing the data processor pipeline on the training data\\n\",\n              color= 'blue', attrs= ['dark', 'bold']));\nFtre_Prf_Train = Data_Processor.fit_transform(xtrain, ytrain.state);\nprint(colored(f\"\\nTraining data shape = {Ftre_Prf_Train.shape}\\n\", color= 'blue'));\n\nprint(colored(f\"\\nImplementing the data processor pipeline on the test data\\n\",\n              color= 'blue', attrs= ['dark', 'bold']));\nFtre_Prf_Test = Data_Processor.transform(xtest);\nprint(colored(f\"\\nTest data shape = {Ftre_Prf_Test.shape}\\n\", color= 'blue'));","metadata":{"execution":{"iopub.status.busy":"2022-04-27T14:01:17.552482Z","iopub.execute_input":"2022-04-27T14:01:17.553Z","iopub.status.idle":"2022-04-27T14:07:01.061198Z","shell.execute_reply.started":"2022-04-27T14:01:17.552954Z","shell.execute_reply":"2022-04-27T14:07:01.059207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Model Training\n\nWe shall now train appropriate ML models on the features to elicit appropriate test set predictions\n\nWe shall use the Group KFold cross validation strategy with training and evaluation metrics displayed through the model training cycle.We will also use 'early stopping' to prevent overfitting\n\nWe will then store the predictions from each model component in a dataframe to be used later for the submission file","metadata":{}},{"cell_type":"code","source":"# Model training specific globals:-\n# 1. Verbose indicator:-\nverbose_nb = 150;\n# 2. Early cut-off indicator:-\nearly_cutoff_itr_nb = 300;\n# 3. CV splits with Group Kfold:-\nn_splits_cv = 5;\n# 4. Estimators for ensemble:-\nnb_trees = 2500;","metadata":{"execution":{"iopub.status.busy":"2022-04-27T14:07:01.062699Z","iopub.status.idle":"2022-04-27T14:07:01.063834Z","shell.execute_reply.started":"2022-04-27T14:07:01.063625Z","shell.execute_reply":"2022-04-27T14:07:01.063648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating output dataframe to store the diagnostics:-\nmdl_diag_prf = pd.DataFrame(data=None, index= None, \n                            columns= ['Mdl_Lbl', 'Fold_Nb', 'Precision', 'Recall', 'F1', 'Accuracy', 'GINI']);\n\n# Creating output dataframe to store the test-set predictions:-\nmdl_pred_prf = pd.DataFrame(data= None, index= sub_fl.sequence, columns= None);\n\n# Developing model scoring function:-\ndef Score_Model(y, ypred, mdl_lbl, fold_nb):\n    \"This functions creates the precision, recall, F1, accuracy and GINI metrics for the given model instance\";\n    global mdl_diag_prf;\n    return pd.concat((mdl_diag_prf, \n                      pd.DataFrame(data= ([mdl_lbl, fold_nb, \n                                           precision_score(y, ypred), recall_score(y, ypred), \n                                           f1_score(y, ypred),accuracy_score(y, ypred), \n                                           roc_auc_score(y, ypred)]), \n                                   index = mdl_diag_prf.columns).T), axis=0,ignore_index= True);\n\n# Defining the cross-validation strategy using Group-KFold:-\nfold_ = GroupKFold(n_splits=n_splits_cv);\n\n# Designing the model training function:-\ndef Train_Model(mdl, mdl_nm, fold_, verbose_nb=verbose_nb, early_cutoff_itr_nb = early_cutoff_itr_nb):\n    \"\"\"\n    This function trains the model based on the provided k-fold structure and provides the test-set diagnostics and predictions\n    Inputs- \n    mdl (model instance)\n    mdl_nm (string)- name of the model\n    fold_ (CV structure)\n    globals (early_cutoff_itr_nb, verbose_nb)- global variables for verbose and early-cutoff            \n    \"\"\";\n    \n    global mdl_diag_prf;    \n    print(colored(f\"\\nCurrent model is {mdl_nm}\\n\", color= 'red', attrs= ['dark', 'bold']));\n    \n    for fold_nb, (train_idx, dev_idx) in enumerate(list(\n        fold_.split(X= Ftre_Prf_Train,y= None,groups= Ftre_Prf_Train.subject))):\n        \n        xtr, xdev = Ftre_Prf_Train.loc[train_idx], Ftre_Prf_Train.loc[dev_idx];\n        ytr, ydev = ytrain[['state']].loc[train_idx], ytrain[['state']].loc[dev_idx];\n\n        mdl = mdl;\n        \n        if mdl_nm.lower() != 'catboost':\n            mdl.fit(xtr, ytr, \n                    early_stopping_rounds= early_cutoff_itr_nb,\n                    eval_set=[(xtr, ytr), (xdev, ydev)],verbose= verbose_nb, eval_metric=['auc']\n                   );\n        elif mdl_nm.lower() == 'catboost':\n            mdl.fit(xtr, ytr, early_stopping_rounds= early_cutoff_itr_nb,\n                    eval_set=[(xtr, ytr), (xdev, ydev)],verbose= verbose_nb);\n            \n        ytr_pred= mdl.predict(xtr);\n        ydev_pred= mdl.predict(xdev);\n        \n        mdl_diag_prf = Score_Model(ydev, ydev_pred, mdl_lbl= mdl_nm, fold_nb=fold_nb);\n        mdl_pred_prf[mdl_nm + str(fold_nb)] = mdl.predict_proba(Ftre_Prf_Test)[:,1];","metadata":{"execution":{"iopub.status.busy":"2022-04-26T06:40:41.063125Z","iopub.execute_input":"2022-04-26T06:40:41.063831Z","iopub.status.idle":"2022-04-26T06:40:41.082445Z","shell.execute_reply.started":"2022-04-26T06:40:41.063789Z","shell.execute_reply":"2022-04-26T06:40:41.081624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implementing the model training functions:-\nfilterwarnings('ignore')\n\n# 1.LGBM classifier:-\nTrain_Model(mdl= LGBMClassifier(random_state= 10, metric= 'auc', n_estimators= nb_trees, \n                                boosting_type= 'gbdt',max_depth = 7,learning_rate= 0.10,\n                                subsample= 0.80, colsample_bytree= 0.75, objective= 'binary'), \n            mdl_nm= 'LGBM', fold_= fold_);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implementing the model training functions:-\nfilterwarnings('ignore')\n\n# 1.LGBM classifier:-\nTrain_Model(mdl= LGBMClassifier(random_state= 10, metric= 'auc', n_estimators= nb_trees, \n                                boosting_type= 'gbdt',max_depth = 7,learning_rate= 0.10,\n                                subsample= 0.80, colsample_bytree= 0.75, objective= 'binary'), \n            mdl_nm= 'LGBM', fold_= fold_);\n\n# 2.XgBoost classifier:-\nTrain_Model(mdl= XGBClassifier(random_state= 10, n_estimators= nb_trees, learning_rate= 0.10, eval_metric= 'logloss',\n                              use_label_encoder=False),\n           mdl_nm= 'XGBoost', fold_ = fold_);\n\n# 3.CatBoost classifier:-\nTrain_Model(mdl = CatBoostClassifier(verbose= False, eval_metric='AUC'), mdl_nm= 'CatBoost', fold_= fold_);\n\n# Printing all model diagnostics:-\nprint(colored(\"\\nModel dev-set diagnostics\\n\", color= 'blue', attrs= ['bold', 'dark']));\ndisplay(mdl_diag_prf.style.highlight_max(subset= ['Precision', 'Recall', 'F1', 'Accuracy', 'GINI'], \n                                     color= 'lightblue').format(precision= 4));","metadata":{"execution":{"iopub.status.busy":"2022-04-26T06:40:44.950918Z","iopub.execute_input":"2022-04-26T06:40:44.951223Z","iopub.status.idle":"2022-04-26T06:41:04.529359Z","shell.execute_reply.started":"2022-04-26T06:40:44.951193Z","shell.execute_reply":"2022-04-26T06:41:04.528021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Submission File\n\nWe shall prepare the submission file based on the mean probability of the classifiers built to enrapture all candidate models' inputs","metadata":{}},{"cell_type":"code","source":"mdl_pred_prf['AllModels'] = mdl_pred_prf.mean(axis=1);\nmdl_pred_prf[['AllModels']].reset_index().rename({'AllModels': 'state'}, axis=1).\\\nto_csv(\"Submission.csv\", index= False);","metadata":{"execution":{"iopub.status.busy":"2022-04-26T06:06:35.448006Z","iopub.execute_input":"2022-04-26T06:06:35.448344Z","iopub.status.idle":"2022-04-26T06:06:35.50422Z","shell.execute_reply.started":"2022-04-26T06:06:35.448306Z","shell.execute_reply":"2022-04-26T06:06:35.503238Z"},"trusted":true},"execution_count":null,"outputs":[]}]}