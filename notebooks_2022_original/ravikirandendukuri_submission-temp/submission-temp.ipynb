{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam, Adamax\n\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\n\nfrom sklearn.model_selection import train_test_split\n\nimport gc\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport warnings,json\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-07T19:31:32.08242Z","iopub.execute_input":"2022-05-07T19:31:32.08349Z","iopub.status.idle":"2022-05-07T19:31:40.894371Z","shell.execute_reply.started":"2022-05-07T19:31:32.083361Z","shell.execute_reply":"2022-05-07T19:31:40.893375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\"","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:31:40.897296Z","iopub.execute_input":"2022-05-07T19:31:40.897654Z","iopub.status.idle":"2022-05-07T19:31:40.902536Z","shell.execute_reply.started":"2022-05-07T19:31:40.897593Z","shell.execute_reply":"2022-05-07T19:31:40.901584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Init_TPU():  \n\n    try:\n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(resolver)\n        tf.tpu.experimental.initialize_tpu_system(resolver)\n        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n        REPLICAS = strategy.num_replicas_in_sync\n        print(\"Connected to TPU Successfully:\\n TPUs Initialised with Replicas:\",REPLICAS)\n        \n        return strategy\n    \n    except ValueError:\n        \n        print(\"Connection to TPU Falied\")\n        print(\"Using default strategy for CPU and single GPU\")\n        strategy = tf.distribute.get_strategy()\n        \n        return strategy\n    \nstrategy=Init_TPU()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:31:40.903985Z","iopub.execute_input":"2022-05-07T19:31:40.904297Z","iopub.status.idle":"2022-05-07T19:31:46.970329Z","shell.execute_reply.started":"2022-05-07T19:31:40.904257Z","shell.execute_reply":"2022-05-07T19:31:46.969504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/contradictory-my-dear-watson/'","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:31:46.972738Z","iopub.execute_input":"2022-05-07T19:31:46.973237Z","iopub.status.idle":"2022-05-07T19:31:46.978022Z","shell.execute_reply.started":"2022-05-07T19:31:46.973206Z","shell.execute_reply":"2022-05-07T19:31:46.977055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_url = os.path.join(path,'train.csv')\ntrain_data = pd.read_csv(train_url, header='infer')\n\nsample_sub_url = os.path.join(path,'sample_submission.csv')\nsample_sub = pd.read_csv(sample_sub_url, header='infer')\n\ntest_url = os.path.join(path,'test.csv')\ntest_data = pd.read_csv(test_url, header='infer')","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:31:46.980292Z","iopub.execute_input":"2022-05-07T19:31:46.980591Z","iopub.status.idle":"2022-05-07T19:31:47.241091Z","shell.execute_reply.started":"2022-05-07T19:31:46.980544Z","shell.execute_reply":"2022-05-07T19:31:47.240407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:31:47.242508Z","iopub.execute_input":"2022-05-07T19:31:47.243068Z","iopub.status.idle":"2022-05-07T19:31:47.266896Z","shell.execute_reply.started":"2022-05-07T19:31:47.243011Z","shell.execute_reply":"2022-05-07T19:31:47.265984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:31:47.268776Z","iopub.execute_input":"2022-05-07T19:31:47.269667Z","iopub.status.idle":"2022-05-07T19:31:47.465116Z","shell.execute_reply.started":"2022-05-07T19:31:47.26962Z","shell.execute_reply":"2022-05-07T19:31:47.464193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transformer Model Name\ntransformer_model = 'distilbert-base-multilingual-cased'\n\n# Define Tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained(transformer_model)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:31:47.466984Z","iopub.execute_input":"2022-05-07T19:31:47.467627Z","iopub.status.idle":"2022-05-07T19:31:49.663402Z","shell.execute_reply.started":"2022-05-07T19:31:47.467579Z","shell.execute_reply":"2022-05-07T19:31:49.662557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the output of tokenizer\ntokenizer.convert_tokens_to_ids(list(tokenizer.tokenize(\"Elementary, My Dear Watson!\")))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:31:49.6647Z","iopub.execute_input":"2022-05-07T19:31:49.665011Z","iopub.status.idle":"2022-05-07T19:31:49.673967Z","shell.execute_reply.started":"2022-05-07T19:31:49.664978Z","shell.execute_reply":"2022-05-07T19:31:49.673251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create seperate list from Train & Test Dataframes with only Premise & Hypothesis\ntrain = train_data[['premise','hypothesis']].values.tolist()\ntest = test_data[['premise','hypothesis']].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:31:49.675522Z","iopub.execute_input":"2022-05-07T19:31:49.676069Z","iopub.status.idle":"2022-05-07T19:31:49.710312Z","shell.execute_reply.started":"2022-05-07T19:31:49.676031Z","shell.execute_reply":"2022-05-07T19:31:49.709495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define Max Length\nmax_len = 80   # << change if you wish\n\n# Encode the training & test data \ntrain_encode = tokenizer.batch_encode_plus(train, pad_to_max_length=True, max_length=max_len)\ntest_encode = tokenizer.batch_encode_plus(test, pad_to_max_length=True, max_length=max_len)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:31:49.711871Z","iopub.execute_input":"2022-05-07T19:31:49.712371Z","iopub.status.idle":"2022-05-07T19:32:08.024154Z","shell.execute_reply.started":"2022-05-07T19:31:49.712336Z","shell.execute_reply":"2022-05-07T19:32:08.022968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the Training Data into Training (90%) & Validation (10%)\n\ntest_size = 0.1  # << change if you wish\nx_train, x_val, y_train, y_val = train_test_split(train_encode['input_ids'], train_data.label.values, test_size=test_size)\n\n\n# Split Test Data\nx_test = test_encode['input_ids']","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:32:08.025564Z","iopub.execute_input":"2022-05-07T19:32:08.025861Z","iopub.status.idle":"2022-05-07T19:32:08.040084Z","shell.execute_reply.started":"2022-05-07T19:32:08.025829Z","shell.execute_reply":"2022-05-07T19:32:08.03911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#garbage collect\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:32:08.041471Z","iopub.execute_input":"2022-05-07T19:32:08.041749Z","iopub.status.idle":"2022-05-07T19:32:08.287207Z","shell.execute_reply.started":"2022-05-07T19:32:08.041691Z","shell.execute_reply":"2022-05-07T19:32:08.286304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading Data Into TensorFlow Dataset\nAUTO = tf.data.experimental.AUTOTUNE\nbatch_size = 16 * strategy.num_replicas_in_sync\n\ntrain_ds = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(3072).batch(batch_size).prefetch(AUTO))\nval_ds = (tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size).prefetch(AUTO))\n\ntest_ds = (tf.data.Dataset.from_tensor_slices(x_test).batch(batch_size))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:32:08.290791Z","iopub.execute_input":"2022-05-07T19:32:08.291243Z","iopub.status.idle":"2022-05-07T19:32:13.170924Z","shell.execute_reply.started":"2022-05-07T19:32:08.291197Z","shell.execute_reply":"2022-05-07T19:32:13.170201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Garbage Collection\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:32:13.172504Z","iopub.execute_input":"2022-05-07T19:32:13.173111Z","iopub.status.idle":"2022-05-07T19:32:13.415664Z","shell.execute_reply.started":"2022-05-07T19:32:13.173063Z","shell.execute_reply":"2022-05-07T19:32:13.414742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(strategy,transformer):\n    with strategy.scope():\n        transformer_encoder = TFDistilBertModel.from_pretrained(transformer)  #Pretrained BERT Transformer Model\n        \n        input_layer = Input(shape=(max_len,), dtype=tf.int32, name=\"input_layer\")\n        \n        sequence_output = transformer_encoder(input_layer)[0]\n        \n        cls_token = sequence_output[:, 0, :]\n        \n        output_layer = Dense(3, activation='softmax')(cls_token)\n        \n        model = Model(inputs=input_layer, outputs=output_layer)\n        \n        model.compile(\n            Adamax(lr=1e-5), \n            loss='sparse_categorical_crossentropy', \n            metrics=['accuracy']\n        )\n        \n        return model\n    \n\n# Applying the build model function\nmodel = build_model(strategy,transformer_model)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:32:13.417449Z","iopub.execute_input":"2022-05-07T19:32:13.417802Z","iopub.status.idle":"2022-05-07T19:33:06.12119Z","shell.execute_reply.started":"2022-05-07T19:32:13.417759Z","shell.execute_reply":"2022-05-07T19:33:06.120425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Summary\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:33:06.122448Z","iopub.execute_input":"2022-05-07T19:33:06.122932Z","iopub.status.idle":"2022-05-07T19:33:06.139367Z","shell.execute_reply.started":"2022-05-07T19:33:06.122895Z","shell.execute_reply":"2022-05-07T19:33:06.138116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the Model\n\nepochs = 30  # < change if you wish\nn_steps = len(train_data) // batch_size \n\nmodel.fit(train_ds, \n          steps_per_epoch = n_steps, \n          validation_data = val_ds,\n          epochs = epochs)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:33:06.141162Z","iopub.execute_input":"2022-05-07T19:33:06.141419Z","iopub.status.idle":"2022-05-07T19:36:59.493718Z","shell.execute_reply.started":"2022-05-07T19:33:06.141389Z","shell.execute_reply":"2022-05-07T19:36:59.492817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Garbage Collection\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:36:59.495178Z","iopub.execute_input":"2022-05-07T19:36:59.495472Z","iopub.status.idle":"2022-05-07T19:37:00.150978Z","shell.execute_reply.started":"2022-05-07T19:36:59.495439Z","shell.execute_reply":"2022-05-07T19:37:00.14992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model.predict(test_ds, verbose=0)\nsample_sub['prediction'] = prediction.argmax(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:37:00.152359Z","iopub.execute_input":"2022-05-07T19:37:00.152632Z","iopub.status.idle":"2022-05-07T19:37:09.477576Z","shell.execute_reply.started":"2022-05-07T19:37:00.152604Z","shell.execute_reply":"2022-05-07T19:37:09.476607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:37:09.479707Z","iopub.execute_input":"2022-05-07T19:37:09.480037Z","iopub.status.idle":"2022-05-07T19:37:09.500949Z","shell.execute_reply.started":"2022-05-07T19:37:09.48Z","shell.execute_reply":"2022-05-07T19:37:09.499911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:37:09.502072Z","iopub.execute_input":"2022-05-07T19:37:09.50231Z","iopub.status.idle":"2022-05-07T19:37:09.512295Z","shell.execute_reply.started":"2022-05-07T19:37:09.502282Z","shell.execute_reply":"2022-05-07T19:37:09.511338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}