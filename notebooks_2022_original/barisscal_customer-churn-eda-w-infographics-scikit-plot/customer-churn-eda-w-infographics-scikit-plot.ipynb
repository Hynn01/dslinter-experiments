{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Credit Card Customers EDA & Churn Prediction\n\nIn this notebook I made Data Analysis on Credit Card Customers dataset and trained a Machine Learning algorithm. You will find informative infographics about churn and classification metrics in the notebook. You will find graphs created with the scikit-plot library that measures the performance of the model I trained.\n \nI wish you pleasant reading.\n\n**Note:** Please open the infographics in a new tab or hide the 'Table of Content' section on the right for easy reading.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-04T09:09:49.879618Z","iopub.execute_input":"2022-05-04T09:09:49.880364Z","iopub.status.idle":"2022-05-04T09:09:49.922723Z","shell.execute_reply.started":"2022-05-04T09:09:49.880252Z","shell.execute_reply":"2022-05-04T09:09:49.921861Z"}}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport math\npd.set_option('display.max_columns', None)\nimport seaborn as sns; sns.set()\nimport scikitplot as skplt\nimport matplotlib.style as style\nstyle.use(\"fivethirtyeight\")\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport matplotlib.gridspec as grid_spec\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ncolors = [\"#8ecae6\",\"#219ebc\",\"#023047\",\"#ffb703\",\"#fb8500\"]\nsns.palplot(sns.color_palette(colors));\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_read = pd.read_csv(\"/kaggle/input/credit-card-customers/BankChurners.csv\")\ndf = first_read.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Information About Dataset","metadata":{}},{"cell_type":"code","source":"df.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1', 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], inplace = True, axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Info #####################\")\n    print(dataframe.info())\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.1, 0.25, 0.50, 0.75, 0.95, 0.99, 1]).T)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def column_information(dataframe, cat_th=10, car_th=20):\n    \n    #object\n    categoric_columns = [col for col in dataframe.columns if dataframe[col].dtypes == \"object\"]\n    \n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    \n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    \n    categoric_columns = categoric_columns + num_but_cat\n    categoric_columns = [col for col in categoric_columns if col not in cat_but_car]\n\n    #num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n    \n    #float64\n    float64_columns = [col for col in dataframe.columns if dataframe[col].dtypes == \"float64\"]\n    \n    #int64\n    int64_columns = [col for col in dataframe.columns if dataframe[col].dtypes == \"int64\"]\n    \n\n    print(f\"# of Samples: {dataframe.shape[0]}\")\n    print(f\"# of Columns: {dataframe.shape[1]}\")\n    print(f'# of Categoric Columns: {len(categoric_columns)}')\n    print(f'Name of Categoric Columns: {(categoric_columns)}')\n    print(f'# of float64 Numeric Columns: {len(float64_columns)}')\n    print(f'Name of float64 Numeric Columns: {(float64_columns)}')\n    print(f'# of int64 Numeric Columns: {len(num_cols)}')\n    print(f'Name of int64 Numeric Columns: {(num_cols)}')\n    print(f'# of Total Numeric Columns: {len(int64_columns)}')\n    print(f'# of Categorical But Cardinal Columns: {len(cat_but_car)}')\n    print(f'Name of Categorical But Cardinal Columns: {(cat_but_car)}')\n    print(f'# of Numerical But Categoric Columns: {len(num_but_cat)}')\n    print(f'Name of Numerical But Categoric Columns: {(num_but_cat)}')\n\n    return categoric_columns, float64_columns, int64_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_df(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categoric_columns, float64_columns, int64_columns = column_information(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Variable Descriptions","metadata":{}},{"cell_type":"markdown","source":"<html>\n<head>\n<style>\ntable {\n  font-family: arial, sans-serif;\n  border-collapse: collapse;\n  width: 100%;\n}\n\ntd, th {\n  border: 1px solid #dddddd;\n  text-align: left;\n  padding: 8px;\n}\n\ntr:nth-child(even) {\n  background-color: #dddddd;\n}\n</style>\n</head>\n<body>\n    \n<table>\n    <tr>\n        <th>COLUMN NAME</th>\n        <th>DESCRIPTION </th>\n    </tr>\n    <tr>\n        <td>CLIENTNUM</td>\n        <td>Client number. Unique identifier for the customer holding the account </td>\n    </tr>\n    <tr>\n        <td>Attrition_Flag</td>\n        <td>Internal event (customer activity) variable - if the account is closed then 1 else 0 </td>\n    </tr>\n    <tr>\n        <td>Customer_Age</td>\n        <td>Demographic variable - Customer's Age in Years </td>\n    </tr>\n    <tr>\n        <td>Gender</td>\n        <td>Demographic variable - M=Male, F=Female </td>\n    </tr>\n    <tr>\n        <td>Dependent_count</td>\n        <td>Demographic variable - Number of dependents </td>\n    </tr>\n    <tr>\n        <td>Education_Level</td>\n        <td>Demographic variable - Educational Qualification of the account holder (example: high school, college graduate, etc.) </td>\n    </tr>\n    <tr>\n        <td>Marital_Status</td>\n        <td>Demographic variable - Married, Single, Divorced, Unknown </td>\n    </tr>\n    <tr>\n        <td>Income_Category</td>\n        <td>Demographic variable - Annual Income Category of the account holder (&lt; $40K, $40K - 60K, $60K - $80K, $80K-$120K, &gt; $120K, Unknown) </td>\n    </tr>\n    <tr>\n        <td>Card_Category</td>\n        <td>Product Variable - Type of Card (Blue, Silver, Gold, Platinum) </td>\n    </tr>\n    <tr>\n        <td>Months_on_book</td>\n        <td>Period of relationship with bank </td>\n    </tr>\n    <tr>\n        <td>Total_Relationship_Count</td>\n        <td>Total no. of products held by the customer </td>\n    </tr>\n    <tr>\n        <td>Months_Inactive_12_mon</td>\n        <td>No. of Contacts in the last 12 months </td>\n    </tr>\n    <tr>\n        <td>Contacts_Count_12_mon</td>\n        <td>No. of Contacts in the last 12 months </td>\n    </tr>\n    <tr>\n        <td>Credit_Limit</td>\n        <td>Credit Limit on the Credit Card </td>\n    </tr>\n    <tr>\n        <td>Total_Revolving_Bal</td>\n        <td>Total Revolving Balance on the Credit Card </td>\n    </tr>\n    <tr>\n        <td>Avg_Open_To_Buy</td>\n        <td>Open to Buy Credit Line (Average of last 12 months) </td>\n    </tr>\n    <tr>\n        <td>Total_Amt_Chng_Q4_Q1</td>\n        <td>Change in Transaction Amount (Q4 over Q1) </td>\n    </tr>\n    <tr>\n        <td>Total_Trans_Amt</td>\n        <td>Total Transaction Amount (Last 12 months) </td>\n    </tr>\n    <tr>\n        <td>Total_Trans_Ct</td>\n        <td>Total Transaction Count (Last 12 months) </td>\n    </tr>\n    <tr>\n        <td>Total_Ct_Chng_Q4_Q1</td>\n        <td>Change in Transaction Count (Q4 over Q1) </td>\n    </tr>\n    <tr>\n        <td>Avg_Utilization_Ratio</td>\n        <td>Average Card Utilization Ratio </td>\n    </tr>\n</table>\n\n</body>\n</html>","metadata":{}},{"cell_type":"markdown","source":"### Univariate Variable Analysis\n\n#### Categoric Variables","metadata":{}},{"cell_type":"code","source":"def bar_plot(variable):\n    # get feature\n    var = df[variable]\n    # count number of categorical variable(value/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}:\\n{}\".format(variable,varValue))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in categoric_columns:\n    bar_plot(c)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Numeric Variables (int64)","metadata":{}},{"cell_type":"code","source":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(df[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for n in int64_columns:\n    plot_hist(n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Numeric Variables (float64)","metadata":{}},{"cell_type":"code","source":"for n in float64_columns:\n    plot_hist(n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"color_palette=[\"gray\",\"#0e4f66\"]\nbackground_color = '#fafafa'\nstyle.use(\"fivethirtyeight\")\nfig = plt.figure(figsize=(60,20), dpi=250)\nfig.patch.set_facecolor(background_color) # figure background color\ngs = fig.add_gridspec(1,2)\ngs.update(wspace=.0, hspace=0)\nax0 = fig.add_subplot(gs[0, 0])\nax0.tick_params(axis='y', which='both', labelleft='off', labelright='on')\n\n\nlabels = df['Attrition_Flag'].value_counts().index\nsizes = df['Attrition_Flag'].value_counts().values\nexplode = [0,.1]\nax0 = plt.pie(sizes, labels=labels, shadow = True, explode = explode, startangle=360, colors = colors, autopct='%1.1f%%',textprops={'fontsize': 18, 'fontfamily': 'serif', 'fontweight':'bold', 'fontsize': 20});\nplt.title('Distribution of Customers by Churn',color = 'black',fontsize = 40, style = 'normal',fontweight='bold', fontfamily='serif')\n\n\n\nfig.text(0.5, 0.87, 'Customer Churn', fontsize=55, fontweight='bold', fontfamily='serif',color='#323232')\n\nfig.text(0.5, 0.80, 'What is Churn?', fontsize=40, fontweight='bold', fontfamily='serif',color='#323232')\nfig.text(0.5, 0.65, '''\nCustomer churn analysis is the process of reviewing the purchasing behavior of your customers, identifying the profiles of customers who are likely to quit working with you, \nand predicting those who are likely to leave (Churn). Customer churn refers to the percentage of customers who stop using your company's product or service over a period of time. \nThe churn rate is calculated by dividing the number of customers lost during that period by the number of customers acquired at the beginning of that period.\n\nConsidering that finding a new customer is much more costly than retaining your existing customers, it can be seen how important a customer churn analysis is. \nThis analysis has now become a tool frequently used by strategic decision-making and planning officials.'''\n, fontsize=25, fontweight='light', fontfamily='serif',color='#323232')\n\n\nfig.text(0.5, 0.55, 'Types of Customer Churn', fontsize=40, fontweight='bold', fontfamily='serif',color='#323232')\nfig.text(0.5, 0.35, '''\nIt consists of two types depending on whether it is voluntary or involuntary in terms of the circumstances and conditions that cause the loss of customers. \nVoluntary losses; is the situation in which a customer leaves the current business and buys or prefers the same good or service from another business. \nInvoluntary losses; It is the situation of abandonment caused by force majeure or undesirable reasons that are not counted in line with the customers' own wishes and demands. \nThe important part here is about the reasons why the person who voluntarily parted ways with us and chose the competitor changed his choice. \nHowever, if various risk measures are taken in involuntary situations, a solution process can be created that can prevent this situation. \nTypes of involuntary loss are often ignored in statistical and data mining studies. The main reason for this is the inability to prevent the loss of customers. \nIn case of canceling the subscription, it may occur that we are in less contact with the customers and cannot determine the desired situation, \nor it may be closed by the legal representatives in case of the customer's death. Closing the account may be voluntarily, or it may be due to compelling reasons.'''\n, fontsize=25, fontweight='light', fontfamily='serif',color='#323232')\n\nfig.text(0.5, 0.25, 'How is Customer Churn Calculated?', fontsize=40, fontweight='bold', fontfamily='serif',color='#323232')\nfig.text(0.5, 0.07, '''\nThe ratio of the difference between the number of customers at the beginning of the period and the number of customers at the end of the period to \nthe number of customers at the beginning of the period allows us to find the “Periodic Loss Rate”. We can also calculate these periods in different \nperiods according to the nature and frequency of the work. We also need to take into account the loss of income per capita. With these calculations, \nnecessary interpretations should be made, goods and services should be diversified accordingly, analyzes should be made and determined for needs,\nand communication channels and methods should be reviewed.'''\n, fontsize=25, fontweight='light', fontfamily='serif',color='#323232')\n\n\n\nimport matplotlib.lines as lines\nl1 = lines.Line2D([0.48, 0.48], [0.1, 0.9], transform=fig.transFigure, figure=fig,color='black',lw=5)\nfig.lines.extend([l1])\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_palette=[\"gray\",\"#0e4f66\"]\nbackground_color = '#fafafa'\n#style.use(\"fivethirtyeight\")\nfig = plt.figure(figsize=(60,50), dpi=250)\nfig.patch.set_facecolor(background_color) # figure background color\ngs = fig.add_gridspec(6,6)\ngs.update(wspace=.6, hspace=.3)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\nax2 = fig.add_subplot(gs[1, 0])\nax3 = fig.add_subplot(gs[1, 1])\nax4 = fig.add_subplot(gs[2, 0])\nax5 = fig.add_subplot(gs[2, 1])\nax6 = fig.add_subplot(gs[3, 0])\nax7 = fig.add_subplot(gs[3, 1])\nax8 = fig.add_subplot(gs[4, 0])\n\nax0 = sns.countplot(ax = ax0, x=\"Attrition_Flag\", hue = 'Gender', data=df, palette = colors)\nax0.set_xlabel('Churned', fontsize = 16, fontfamily = 'serif')\nax0.set_ylabel('Count', fontsize = 16, fontfamily = 'serif')\nax0.set_title('Distribution of Churned Customers',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\nax0.grid(color='black', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n\nax1 = sns.countplot(ax = ax1, x=\"Attrition_Flag\", hue = 'Education_Level', data=df, palette = colors)\nax1.set_xlabel('Churned', fontsize = 16, fontfamily = 'serif')\nax1.set_ylabel('Count', fontsize = 16, fontfamily = 'serif')\nax1.set_title('Distribution of Customers',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\nax1.grid(color='black', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n\n\nax2 = sns.countplot(ax = ax2, x=\"Attrition_Flag\", hue = 'Marital_Status', data=df, palette = colors)\nax2.set_xlabel('Churned', fontsize = 16, fontfamily = 'serif')\nax2.set_ylabel('Count', fontsize = 16, fontfamily = 'serif')\nax2.set_title('Distribution of Churned Customers',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\nax2.grid(color='black', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n\n\nax3 = sns.countplot(ax = ax3, x=\"Attrition_Flag\", hue = 'Income_Category', data=df, palette = colors)\nax3.set_xlabel('Churned', fontsize = 16, fontfamily = 'serif')\nax3.set_ylabel('Count', fontsize = 16, fontfamily = 'serif')\nax3.set_title('Distribution of Churned Customers',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\nax3.grid(color='black', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n\n\n\nax4 = sns.countplot(ax = ax4, x=\"Attrition_Flag\", hue = 'Card_Category', data=df, palette = colors)\nax4.set_xlabel('Churned', fontsize = 16, fontfamily = 'serif')\nax4.set_ylabel('Count', fontsize = 16, fontfamily = 'serif')\nax4.set_title('Distribution of Churned Customers',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\nax4.grid(color='black', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n\n\nax5 = sns.countplot(ax = ax5, x=\"Attrition_Flag\", hue = 'Dependent_count', data=df, palette = colors)\nax5.set_xlabel('Churned', fontsize = 16, fontfamily = 'serif')\nax5.set_ylabel('Count', fontsize = 16, fontfamily = 'serif')\nax5.set_title('Distribution of Churned Customers',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\nax5.grid(color='black', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n\n\n\nax6 = sns.countplot(ax = ax6, x=\"Attrition_Flag\", hue = 'Total_Relationship_Count', data=df, palette = colors)\nax6.set_xlabel('Churned', fontsize = 16, fontfamily = 'serif')\nax6.set_ylabel('Count', fontsize = 16, fontfamily = 'serif')\nax6.set_title('Distribution of Churned Customers',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\nax6.grid(color='black', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n\n\nax7 = sns.countplot(ax = ax7, x=\"Attrition_Flag\", hue = 'Months_Inactive_12_mon', data=df, palette = colors)\nax7.set_xlabel('Churned', fontsize = 16, fontfamily = 'serif')\nax7.set_ylabel('Count', fontsize = 16, fontfamily = 'serif')\nax7.set_title('Distribution of Churned Customers',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\nax7.grid(color='black', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n\n\nax8 = sns.countplot(ax = ax8, x=\"Attrition_Flag\", hue = 'Contacts_Count_12_mon', data=df, palette = colors)\nax8.set_xlabel('Churned', fontsize = 16, fontfamily = 'serif')\nax8.set_ylabel('Count', fontsize = 16, fontfamily = 'serif')\nax8.set_title('Distribution of Churned Customers',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\nax8.grid(color='black', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_palette=[\"gray\",\"#0e4f66\"]\nbackground_color = '#fafafa'\n#style.use(\"fivethirtyeight\")\nfig = plt.figure(figsize=(20,15), dpi=250)\nfig.patch.set_facecolor(background_color) # figure background color\ngs = fig.add_gridspec(2,2)\ngs.update(wspace=.6, hspace=.3)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\nax2 = fig.add_subplot(gs[1, 0])\nax3 = fig.add_subplot(gs[1, 1])\nax0.tick_params(axis='y', which='both', labelleft='off', labelright='on')\n\n\nax0 = sns.countplot(ax = ax0, x=\"Gender\", data=df, palette = colors)\nax0.set_xlabel('Gender', fontsize = 16, fontfamily = 'serif')\nax0.set_ylabel('Count', fontsize = 16, fontfamily = 'serif')\nax0.set_title('Distribution of Genders',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\n\n\nax1 = sns.countplot(ax = ax1, x=\"Gender\", hue = 'Attrition_Flag', data=df, palette = colors)\nax1.set_xlabel('Gender', fontsize = 16, fontfamily = 'serif')\nax1.set_ylabel('Count', fontsize = 16, fontfamily = 'serif')\nax1.set_title('Distribution of Genders',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\n\n\nax2 = sns.countplot(ax = ax2, x=\"Gender\", hue = 'Education_Level', data=df, palette = colors)\nax2.set_xlabel('Gender', fontsize = 16, fontfamily = 'serif')\nax2.set_ylabel('Count', fontsize = 16, fontfamily = 'serif')\nax2.set_title('Distribution of Genders',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\n\n\nax3 = sns.countplot(ax = ax3, x=\"Gender\", hue = 'Marital_Status', data=df, palette = colors)\nax3.set_xlabel('Gender', fontsize = 16, fontfamily = 'serif')\nax3.set_ylabel('Count', fontsize = 16, fontfamily = 'serif')\nax3.set_title('Distribution of Genders',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\n\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categoric_columns, float64_columns, int64_columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"float64_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_palette=[\"gray\",\"#0e4f66\"]\nbackground_color = '#fafafa'\n#style.use(\"fivethirtyeight\")\nfig = plt.figure(figsize=(20,15), dpi=250)\nfig.patch.set_facecolor(background_color) # figure background color\ngs = fig.add_gridspec(2,2)\ngs.update(wspace=.6, hspace=.3)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\nax2 = fig.add_subplot(gs[1, 0])\nax3 = fig.add_subplot(gs[1, 1])\nax0.tick_params(axis='y', which='both', labelleft='off', labelright='on')\n\n\nax0 = sns.boxplot(ax = ax0, x=\"Attrition_Flag\", y = 'Total_Trans_Amt', data=df, palette = colors)\nax0.set_xlabel('Attrition_Flag', fontsize = 16, fontfamily = 'serif')\nax0.set_ylabel('')\nax0.set_title('Total_Trans_Amt',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\n\n\nax1 = sns.boxplot(ax = ax1, x=\"Attrition_Flag\", y = 'Total_Trans_Ct', data=df, palette = colors)\nax1.set_xlabel('Attrition_Flag', fontsize = 16, fontfamily = 'serif')\nax1.set_ylabel('')\nax1.set_title('Total_Trans_Ct',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\n\n\nax2 = sns.boxplot(ax = ax2, x=\"Attrition_Flag\", y = 'Total_Ct_Chng_Q4_Q1', data=df, palette = colors)\nax2.set_xlabel('Attrition_Flag', fontsize = 16, fontfamily = 'serif')\nax2.set_ylabel('')\nax2.set_title('Total_Ct_Chng_Q4_Q1',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\n\n\nax3 = sns.boxplot(ax = ax3, x=\"Attrition_Flag\", y = 'Avg_Utilization_Ratio', data=df, palette = colors)\nax3.set_xlabel('Attrition_Flag', fontsize = 16, fontfamily = 'serif')\nax3.set_ylabel('')\nax3.set_title('Avg_Utilization_Ratio',fontfamily = 'serif', fontsize=20, fontweight = 'bold', loc='center')\n\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nstyle.use(\"fivethirtyeight\")\nmask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\nheatmap = sns.heatmap(df.corr(), mask=mask,annot=True, cmap=colors, linewidths = 2)\nheatmap.set_title('Correlation of Features', fontdict={'fontsize':30, 'fontfamily' : 'serif', 'fontweight' : 'bold'}, pad=16);","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cardinality","metadata":{}},{"cell_type":"code","source":"for column in categoric_columns:\n    print('Column: {} - Unique Values: {}'.format(column, df[column].unique()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Values","metadata":{}},{"cell_type":"code","source":"import missingno as mno\nmno.matrix(df, figsize = (20, 6))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding\n\n## Label Encoding","metadata":{}},{"cell_type":"code","source":"label_encoding = []\none_hot = []\n\nfor x in categoric_columns:\n    a = df[x].unique()\n    print(f'Unique Values for {x}: ', df[x].unique())\n    if(len(a) == 2):\n        label_encoding.append(x)\n    else:\n        one_hot.append(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for y in label_encoding:\n    var = df[y].unique()\n    y_mapping = {var[0]: 0, var[1]: 1}\n    df[y] = df[y].map(y_mapping)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One-Hot Encoding","metadata":{}},{"cell_type":"code","source":"for i in range(0, len(one_hot)):\n    df[f'{one_hot[i]}'] = pd.Categorical(df[f'{one_hot[i]}'])\n    dummies = pd.get_dummies(df[f'{one_hot[i]}'], prefix = f'{one_hot[i]}_encoded', drop_first=True)\n    df.drop([f'{one_hot[i]}'], axis=1, inplace=True)\n    df = pd.concat([df, dummies], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train - Test Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df.drop(['CLIENTNUM','Attrition_Flag'], axis = 1)\ny = df['Attrition_Flag']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 101)\n\nprint(\"##################### Length #####################\")\nprint(f'Total # of sample in whole dataset: {len(X_train)+len(X_test)}')\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Total # of sample in test dataset: {len(X_test)}')\n\nprint(\"##################### Shape #####################\")\nprint(f'Shape of train dataset: {X_train.shape}')\nprint(f'Shape of test dataset: {X_test.shape}')\n\nprint(\"##################### Percantage #####################\")\nprint(f'Percentage of train dataset: {round((len(X_train)/(len(X_train)+len(X_test)))*100,2)}%')\nprint(f'Percentage of validation dataset: {round((len(X_test)/(len(X_train)+len(X_test)))*100,2)}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Anomaly Detection","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\ndef detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df =  pd.concat([X_train, y_train], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categoric_columns, float64_columns, int64_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df.loc[detect_outliers(temp_df,float64_columns)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop outliers\ntemp_df = temp_df.drop(detect_outliers(temp_df,float64_columns),axis = 0).reset_index(drop = True)\ntemp_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = temp_df.drop(['Attrition_Flag'], axis = 1)\ny_train = temp_df.Attrition_Flag\n\nprint(\"##################### Length #####################\")\nprint(f'Total # of sample in whole dataset: {len(X_train)+len(X_test)}')\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Total # of sample in test dataset: {len(X_test)}')\n\nprint(\"##################### Shape #####################\")\nprint(f'Shape of train dataset: {X_train.shape}')\nprint(f'Shape of test dataset: {X_test.shape}')\n\nprint(\"##################### Percantage #####################\")\nprint(f'Percentage of train dataset: {round((len(X_train)/(len(X_train)+len(X_test)))*100,2)}%')\nprint(f'Percentage of validation dataset: {round((len(X_test)/(len(X_train)+len(X_test)))*100,2)}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GradientBoostingClassifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\ngbc_model = GradientBoostingClassifier()\ngbc_model.fit(X_train, y_train)\n\ntrain_score = gbc_model.score(X_train, y_train)\nprint(f'Train score of trained model: {train_score*100}')\n\ntest_score = gbc_model.score(X_test, y_test)\nprint(f'Test score of trained model: {test_score*100}')\n\ny_predictions = gbc_model.predict(X_test)\ny_proba = gbc_model.predict_proba(X_test)\nconf_matrix = confusion_matrix(y_predictions, y_test)\n\nprint(f'Confussion matrix: \\n{conf_matrix}\\n')\n\nsns.heatmap(conf_matrix, annot=True, color = colors)\n\ntn = conf_matrix[0,0]\nfp = conf_matrix[0,1]\ntp = conf_matrix[1,1]\nfn = conf_matrix[1,0]\n\ntotal = tn + fp + tp + fn\nreal_positive = tp + fn\nreal_negative = tn + fp\n\naccuracy  = (tp + tn) / total # Accuracy Rate\nprecision = tp / (tp + fp) # Positive Predictive Value\nrecall    = tp / (tp + fn) # True Positive Rate\nf1score  = 2 * precision * recall / (precision + recall)\nspecificity = tn / (tn + fp) # True Negative Rate\nerror_rate = (fp + fn) / total # Missclassification Rate\nprevalence = real_positive / total\nmiss_rate = fn / real_positive # False Negative Rate\nfall_out = fp / real_negative # False Positive Rate\n\n\nprint('Evaluation Metrics:')\nprint(f'Accuracy    : {accuracy}')\nprint(f'Precision   : {precision}')\nprint(f'Recall      : {recall}')\nprint(f'F1 score    : {f1score}')\nprint(f'Specificity : {specificity}')\nprint(f'Error Rate  : {error_rate}')\nprint(f'Prevalence  : {prevalence}')\nprint(f'Miss Rate   : {miss_rate}')\nprint(f'Fall Out    : {fall_out}')\n\nprint(\"\") \nprint(f'Classification Report: \\n{classification_report(y_predictions, y_test)}\\n')\nprint(\"\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_palette=[\"gray\",\"#0e4f66\"]\nbackground_color = '#fafafa'\n#style.use(\"fivethirtyeight\")\nfig = plt.figure(figsize=(40,20), dpi=500)\nfig.patch.set_facecolor(background_color) # figure background color\ngs = fig.add_gridspec(3,3)\ngs.update(wspace=.6, hspace=.3)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\nax2 = fig.add_subplot(gs[1, 0])\nax3 = fig.add_subplot(gs[1, 1])\nax4 = fig.add_subplot(gs[2, 0])\nax5 = fig.add_subplot(gs[2, 1])\n\n\nax0 = skplt.metrics.plot_roc( y_test, y_proba, ax = ax0, title = 'ROC Vurve for GBC',figsize = (10,6), title_fontsize = 20, text_fontsize = 15)\n\nax1 = skplt.metrics.plot_precision_recall(y_test, y_proba, ax = ax1, title = 'PR Curve for GBC',figsize = (10,6), title_fontsize = 20, text_fontsize = 15)\n\nax2 = skplt.metrics.plot_cumulative_gain(y_test, y_proba, ax = ax2, title = 'Cumulative Gains Chart for GBC',figsize = (10,6), title_fontsize = 20, text_fontsize = 15)\n\nax3 = skplt.metrics.plot_lift_curve(y_test, y_proba, ax = ax3, title = 'Lift Curve for GBC',figsize = (10,6), title_fontsize = 20, text_fontsize = 15)\n\nax4 = skplt.estimators.plot_learning_curve(gbc_model, X_train, y_train, ax = ax4, title = 'Learning Curve for GBC',figsize = (10,6), title_fontsize = 20, text_fontsize = 15)\n\nax5 = skplt.estimators.plot_feature_importances(gbc_model, feature_names=X.columns, ax = ax5, x_tick_rotation = 90, title = 'Feature Importance',figsize = (10,6), title_fontsize = 20, text_fontsize = 15 )\n\n\nfig.text(0.7, 0.85, 'ROC Curve', fontsize=45, fontweight='bold', fontfamily='serif',color='#323232')\nfig.text(0.7, 0.75, '''On the ROC curve, the true positive rate (Sensitivity) is plotted as a function of the false positive rate (Specificity) of a parameter for different cut-off points. \nEach point on the ROC curve represents a sensitivity/specificity pair corresponding to a certain decision threshold. \nThe area under the ROC curve (AUC) is a measure of how well a parameter can distinguish between two groups.'''\n, fontsize=30, fontweight='light', fontfamily='serif',color='#323232')\n\n\nfig.text(0.7, 0.68, 'PR Curve', fontsize=45, fontweight='bold', fontfamily='serif',color='#323232')\n# https://www.geeksforgeeks.org/precision-recall-curve-ml/\nfig.text(0.7, 0.55, '''A PR curve is simply a graph with Precision values on the y-axis and Recall values on the x-axis. \nIn other words, the PR curve contains TP/(TP+FN) on the y-axis and TP/(TP+FP) on the x-axis.\nIt is important to note that Precision is also called the Positive Predictive Value (PPV).\nRecall is also called Sensitivity, Hit Rate or True Positive Rate (TPR).'''\n, fontsize=30, fontweight='light', fontfamily='serif',color='#323232')\n\nfig.text(0.7, 0.48, 'Cumulative Gains Chart & Lift Curve', fontsize=45, fontweight='bold', fontfamily='serif',color='#323232')\n# https://www.geeksforgeeks.org/understanding-gain-chart-and-lift-chart/\nfig.text(0.7, 0.20, '''The gain chart and lift chart are two measures that are used for Measuring the benefits of using the model and are used \nin business contexts such as target marketing. It’s not just restricted to marketing analysis. It can also be used in other domains such as risk modeling, \nsupply chain analytics, etc. In other words, \nGain and Lift charts are two approaches used while solving classification problems with imbalanced data sets.\nThe gain and lift chart is obtained using the following steps:\n1 - Predict the probability Y = 1 (positive) using the LR model and arrange the observation in the decreasing order of predicted probability [i.e., P(Y = 1)].\n2 - Divide the data sets into deciles. Calculate the number of positives (Y = 1) in each decile and the cumulative number of positives up to a decile.\n3 - Gain is the ratio between the cumulative number of positive observations up to a decile to the total number of positive observations in the data. \nThe gain chart is a chart drawn between the gain on the vertical axis and the decile on the horizontal axis.\n''', fontsize=30, fontweight='light', fontfamily='serif',color='#323232')\n\n\nfig.text(0.7, 0.15, 'Learning Curve', fontsize=45, fontweight='bold', fontfamily='serif',color='#323232')\n# https://en.wikipedia.org/wiki/Learning_curve_(machine_learning)\nfig.text(0.7, 0.00, '''In machine learning, a learning curve (or training curve) plots the optimal value of a model's loss function for a training set against this loss function evaluated on a \nvalidation data set with same parameters as produced the optimal function. It is a tool to find out how much a machine model benefits from adding \nmore training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value \nthat is too low with increasing size of the training set, it will not benefit much from more training data.\n'''\n, fontsize=30, fontweight='light', fontfamily='serif',color='#323232')\n\nfig.text(0.7, -0.05, 'Feature Importance', fontsize=45, fontweight='bold', fontfamily='serif',color='#323232')\n# https://machinelearningmastery.com/calculate-feature-importance-with-python/\nfig.text(0.7, -0.25, '''\nFeature importance refers to a class of techniques for assigning scores to input features to a predictive model that indicates the relative \nimportance of each feature when making a prediction.Feature importance scores can be calculated for problems that involve predicting \na numerical value, called regression, and those problems that involve predicting a class label, called classification.\nThe scores are useful and can be used in a range of situations in a predictive modeling problem, such as:\n- Better understanding the data.\n- Better understanding a model.\n- Reducing the number of input features.\n'''\n, fontsize=30, fontweight='light', fontfamily='serif',color='#323232')\n\n\nimport matplotlib.lines as lines\nl1 = lines.Line2D([0.65, 0.65], [-0.2, 0.91], transform=fig.transFigure, figure=fig,color='black',lw=5)\nfig.lines.extend([l1])\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nIn this notebook, I examined Credit Card Customers Dataset. Firstly, I made Exploratory Data Analysis, Visualization, then I applied Gradient Boosting algorithm to this dataset.\n\nIf you liked this notebook, please let me know :)\n\nThank you for your time.","metadata":{}}]}