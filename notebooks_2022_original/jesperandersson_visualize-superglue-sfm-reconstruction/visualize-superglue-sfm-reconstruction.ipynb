{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Run SfM and visualize reconstruction + ground truth.\n\nBased on notebook https://colab.research.google.com/drive/1MrVs9b8aQYODtOGkoaGNF9Nji3sbCNMQ from this repo https://github.com/cvg/Hierarchical-Localization","metadata":{}},{"cell_type":"code","source":"!git clone --recursive https://github.com/cvg/Hierarchical-Localization/\nimport os\nos.chdir(\"./Hierarchical-Localization\")\n!python -m pip install -e .\n!curl https://cvg-data.inf.ethz.ch/hloc/netvlad/Pitts30K_struct.mat --create-dirs -o /kaggle/working/Hierarchical-Localization/third_party/netvlad/VGG16-NetVLAD-Pitts30K.mat\n!pip install --upgrade --quiet plotly\n\nimport pandas as pd\nimport numpy as np\nimport tqdm, tqdm.notebook\ntqdm.tqdm = tqdm.notebook.tqdm  # notebook-friendly progress bars\nfrom pathlib import Path\n\nfrom hloc import extract_features, match_features, reconstruction, pairs_from_retrieval\nfrom hloc.utils import viz_3d\nimport plotly.graph_objects as go\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-29T07:34:35.834857Z","iopub.execute_input":"2022-04-29T07:34:35.835564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run SfM\nTakes about 20 minutes for british museum","metadata":{}},{"cell_type":"code","source":"scene = \"british_museum\"\n\nimages = Path(f'/kaggle/input/image-matching-challenge-2022/train/{scene}/images')\n\noutputs = Path('outputs/sfm/')\nsfm_pairs = outputs / 'pairs-netvlad.txt'\nsfm_dir = outputs / 'sfm_superpoint+superglue'\n\nretrieval_conf = extract_features.confs['netvlad']\nfeature_conf = extract_features.confs['superpoint_aachen']\nmatcher_conf = match_features.confs['superglue']\n\n\nretrieval_path = extract_features.main(retrieval_conf, images, outputs)\npairs_from_retrieval.main(retrieval_path, sfm_pairs, num_matched=5)\n\nfeature_path = extract_features.main(feature_conf, images, outputs)\nmatch_path = match_features.main(matcher_conf, sfm_pairs, feature_conf['output'], outputs)\n\nmodel = reconstruction.main(sfm_dir, images, sfm_pairs, feature_path, match_path, verbose=False)\n\nfig = viz_3d.init_figure()\n\nviz_3d.plot_reconstruction(fig, model, color='rgba(255,0,0,0.5)', points=True, cameras=False)\nviz_3d.plot_reconstruction(fig, model, color='rgba(0,255,0,0.5)', points=False, cameras=True)\n\nfig.show()","metadata":{"execution":{"iopub.status.idle":"2022-04-29T07:47:45.256096Z","shell.execute_reply.started":"2022-04-29T07:37:17.497848Z","shell.execute_reply":"2022-04-29T07:47:45.255318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Align to ground truth","metadata":{}},{"cell_type":"code","source":"cal_df = pd.read_csv(f\"/kaggle/input/image-matching-challenge-2022/train/{scene}/calibration.csv\")\n\nname_index_dict = {}\n\nfor k, v in model.images.items():\n    name_index_dict[v.name] = k\n    \nname_location_dict = {}\ncameras = []\nfor index, row in cal_df.iterrows():\n    R = np.array([float(x) for x in row.rotation_matrix.split(\" \")]).reshape(3, 3)\n    t = np.array([float(x) for x in row.translation_vector.split(\" \")])\n    K = np.array([float(x) for x in row.camera_intrinsics.split(\" \")]).reshape(3, 3)\n    \n    image_name = row.image_id + \".jpg\"\n    name_location_dict[image_name] = t\n\n    camera = {\"R\": R, \"t\": t, \"K\": K, \"name\": name_index_dict[image_name]}\n    cameras.append(camera)\n\nmodel.align_robust(list(name_location_dict.keys()), list(name_location_dict.values()), len(name_location_dict))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-29T07:47:45.25766Z","iopub.execute_input":"2022-04-29T07:47:45.257875Z","iopub.status.idle":"2022-04-29T07:47:45.301333Z","shell.execute_reply.started":"2022-04-29T07:47:45.257846Z","shell.execute_reply":"2022-04-29T07:47:45.300632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize\n\nGreen cameras: reconstruction cameras\n\nBlue cameras: ground truth cameras\n\nYellow lines: matching cameras","metadata":{}},{"cell_type":"code","source":"fig = viz_3d.init_figure()\n\nviz_3d.plot_reconstruction(fig, model, color='rgba(0,255,0,0.5)', points=False, cameras=True)\n\n# Plot ground truth cameras\nfor camera in cameras:\n    cam = camera.copy()\n    \n    # Not so sure about this transform\n    cam[\"R\"] = np.matmul(camera[\"R\"], np.array([[-1, 0, 0], [0, 1, 0], [0, 0, -1]])).T\n    \n    viz_3d.plot_camera(fig, color='rgba(0,0,255,0.5)', **cam)\n\n# Plot lines connecting reconstruction cameras and ground truth cameras\nlines = []\nfor image in model.images.values():\n    t1 = image.projection_center()\n    t2 = name_location_dict[image.name]\n    x = np.array([t1[0], t2[0]])\n    y = np.array([t1[1], t2[1]])\n    z = np.array([t1[2], t2[2]])\n    line = go.Scatter3d(x=x, y=y, z=z, mode='lines', line=dict(color='rgb(255,255,0)', width=1), name=image.name)\n    lines.append(line)\nfig.add_traces(lines)\n\nfig.update_layout(showlegend=False)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T07:47:45.303163Z","iopub.execute_input":"2022-04-29T07:47:45.303355Z","iopub.status.idle":"2022-04-29T07:47:47.799743Z","shell.execute_reply.started":"2022-04-29T07:47:45.30333Z","shell.execute_reply":"2022-04-29T07:47:47.798955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}