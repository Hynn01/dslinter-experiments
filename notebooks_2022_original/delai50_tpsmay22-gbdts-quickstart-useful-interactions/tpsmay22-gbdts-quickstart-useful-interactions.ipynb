{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Description and shotout\nThis notebook is almost the same as [this one](https://www.kaggle.com/code/ambrosm/tpsmay22-gradient-boosting-quickstart) from @ambrosm but with some feature interactions added. Therefore, if you are going to upvote this notebook please upvote the other as well.\n\n#### What's new?\nI added some feature interactions that I found useful that improves the CV and Public LB:\n\nOld CV (Fold 0): **0.991** vs new CV (Fold 0): **0.992**\n\nOld Public LB: **0.99275** , new Public LB: **0.99369**\n\nThere is a post in the discussion forums in which I explain how I came across with those features.","metadata":{}},{"cell_type":"markdown","source":"# Gradient-Boosting Quickstart for TPSMAY22\n\nThis notebook shows how to train a gradient booster with minimal feature engineering. For the corresponding EDA, see the [separate EDA notebook](https://www.kaggle.com/code/ambrosm/tpsmay22-eda-which-makes-sense).\n\nRelease notes:\n- V1: XGB\n- V2: LightGBM, one more feature","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nfrom cycler import cycler\nfrom IPython.display import display\nimport datetime\nimport scipy.stats\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.calibration import CalibrationDisplay\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nplt.rcParams['axes.facecolor'] = '#0057b8' # blue\nplt.rcParams['axes.prop_cycle'] = cycler(color=['#ffd700'] +\n                                         plt.rcParams['axes.prop_cycle'].by_key()['color'][1:])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T10:21:45.936999Z","iopub.execute_input":"2022-05-05T10:21:45.937309Z","iopub.status.idle":"2022-05-05T10:21:48.555126Z","shell.execute_reply.started":"2022-05-05T10:21:45.937277Z","shell.execute_reply":"2022-05-05T10:21:48.55398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering\n\nWe read the data and apply minimal feature engineering: We only split the `f_27` string into ten separate features as described in the [EDA](https://www.kaggle.com/code/ambrosm/tpsmay22-eda-which-makes-sense), and we count the unique characters in the string.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv')\nfloat_feats_names = [c for c in train.columns if train[c].dtype == \"float\"]\n\nfor df in [train, test]:\n    \n    for i in range(10):\n        df[f'ch{i}'] = df.f_27.str.get(i).apply(ord) - ord('A')\n        \n    # unique_characters feature is from https://www.kaggle.com/code/cabaxiom/tps-may-22-eda-lgbm-model\n    df[\"unique_characters\"] = df.f_27.apply(lambda s: len(set(s)))\n    \n    \n    # THESE ARE THE NEW FEATURES\n    float_feats = df[float_feats_names]\n    inv_float_feats = 1 / float_feats\n    inv_float_feats.rename(columns=lambda x: f\"inv_{x}\", inplace=True)\n    for c in [\"inv_f_26\", \"inv_f_21\", \"inv_f_22\"]:\n        df[c] = inv_float_feats[c]\n    \n    poly = PolynomialFeatures(degree=2, interaction_only=True)\n    float_inters = poly.fit_transform(float_feats)\n    float_inters = pd.DataFrame(float_inters, columns=poly.get_feature_names(float_feats.columns))\n    float_inters = float_inters.loc[:, (float_inters != float_inters.iloc[0]).any()]\n    for c in [\"f_00 f_26\", \"f_01 f_26\", \"f_05 f_22\", \"f_02 f_21\", \"f_21 f_26\", \"f_21 f_22\", \"f_22 f_26\"]:\n        df[c] = float_inters[c]\n    \nfeatures = [f for f in test.columns if f != 'id' and f != 'f_27']\ntest[features].head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T10:21:48.994007Z","iopub.execute_input":"2022-05-05T10:21:48.994346Z","iopub.status.idle":"2022-05-05T10:22:26.714251Z","shell.execute_reply.started":"2022-05-05T10:21:48.994308Z","shell.execute_reply":"2022-05-05T10:22:26.713346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-validation\n\nFor cross-validation, we use a simple KFold with five splits. It turned out that the scores of the five splits are very similar so that I usually run only the first split. This one split is good enough to evaluate the model.","metadata":{}},{"cell_type":"code","source":"%%time\n# Cross-validation of the classifier\n\ndef my_booster(random_state=1):\n#     return HistGradientBoostingClassifier(learning_rate=0.4, max_leaf_nodes=150,\n#                                           max_iter=1000, min_samples_leaf=4000,\n#                                           l2_regularization=1,\n#                                           validation_fraction=0.05,\n#                                           max_bins=255,\n#                                           random_state=random_state, verbose=1)\n#     return XGBClassifier(n_estimators=400, n_jobs=-1,\n#                          eval_metric=['logloss'],\n#                          #max_depth=10,\n#                          colsample_bytree=0.8,\n#                          #gamma=1.4,\n#                          reg_alpha=6, reg_lambda=1.5,\n#                          tree_method='hist',\n#                          #max_bin=511,\n#                          learning_rate=0.4,\n#                          verbosity=1,\n#                          use_label_encoder=False, random_state=random_state)\n    return LGBMClassifier(n_estimators=5000, min_child_samples=80,\n                          max_bins=511, random_state=random_state)\n      \nprint(f\"{len(features)} features\")\nscore_list = []\nkf = KFold(n_splits=5)\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(train)):\n    X_tr = train.iloc[idx_tr][features]\n    X_va = train.iloc[idx_va][features]\n    y_tr = train.iloc[idx_tr].target\n    y_va = train.iloc[idx_va].target\n    \n    model = my_booster()\n\n    if True or type(model) != XGBClassifier:\n        model.fit(X_tr.values, y_tr)\n    else:\n        model.fit(X_tr.values, y_tr, eval_set = [(X_va.values, y_va)], \n                  early_stopping_rounds=30, verbose=10)\n    y_va_pred = model.predict_proba(X_va.values)[:,1]\n    score = roc_auc_score(y_va, y_va_pred)\n    try:\n        print(f\"Fold {fold}: n_iter ={model.n_iter_:5d}    AUC = {score:.3f}\")\n    except AttributeError:\n        print(f\"Fold {fold}:                  AUC = {score:.3f}\")\n    score_list.append(score)\n    break # we only need the first fold\n    \nprint(f\"OOF AUC:                       {np.mean(score_list):.3f}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-05T10:22:26.716412Z","iopub.execute_input":"2022-05-05T10:22:26.716683Z","iopub.status.idle":"2022-05-05T10:30:25.301524Z","shell.execute_reply.started":"2022-05-05T10:22:26.716649Z","shell.execute_reply":"2022-05-05T10:30:25.300214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Three diagrams for model evaluation\n\nWe plot the ROC curve just because it looks nice. The area under the red curve is the score of our model.\n","metadata":{}},{"cell_type":"code","source":"# Plot the roc curve for the last fold\ndef plot_roc_curve(y_va, y_va_pred):\n    plt.figure(figsize=(8, 8))\n    fpr, tpr, _ = roc_curve(y_va, y_va_pred)\n    plt.plot(fpr, tpr, color='r', lw=2)\n    plt.plot([0, 1], [0, 1], color=\"navy\", lw=1, linestyle=\"--\")\n    plt.gca().set_aspect('equal')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"Receiver operating characteristic\")\n    plt.show()\n\nplot_roc_curve(y_va, y_va_pred)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T10:30:25.304014Z","iopub.execute_input":"2022-05-05T10:30:25.30438Z","iopub.status.idle":"2022-05-05T10:30:25.917112Z","shell.execute_reply.started":"2022-05-05T10:30:25.304335Z","shell.execute_reply":"2022-05-05T10:30:25.916098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Second, we plot a histogram of the out-of-fold predictions. Many predictions are near 0.0 or near 1.0; this means that in many cases the classifier's predictions have high confidence:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nplt.hist(y_va_pred, bins=25, density=True)\nplt.title('Histogram of the oof predictions')\nplt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T10:30:25.919632Z","iopub.execute_input":"2022-05-05T10:30:25.920412Z","iopub.status.idle":"2022-05-05T10:30:26.161067Z","shell.execute_reply.started":"2022-05-05T10:30:25.920353Z","shell.execute_reply":"2022-05-05T10:30:26.159925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we plot the calibration curve. The curve here is almost a straight line, which means that the predicted probabilities are almost exact: ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nCalibrationDisplay.from_predictions(y_va, y_va_pred, n_bins=20, strategy='quantile', ax=plt.gca())\nplt.title('Probability calibration')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T10:30:26.162817Z","iopub.execute_input":"2022-05-05T10:30:26.163151Z","iopub.status.idle":"2022-05-05T10:30:26.434323Z","shell.execute_reply.started":"2022-05-05T10:30:26.163104Z","shell.execute_reply":"2022-05-05T10:30:26.433318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n\nFor the submission, we re-train the model on several different seeds and then submit the mean of the ranks.","metadata":{}},{"cell_type":"code","source":"# Create submission\nprint(f\"{len(features)} features\")\n\npred_list = []\nfor seed in range(10):\n    X_tr = train[features]\n    y_tr = train.target\n\n    model = my_booster(random_state=seed)\n    model.fit(X_tr.values, y_tr)\n    pred_list.append(scipy.stats.rankdata(model.predict_proba(test[features].values)[:,1]))\n    print(f\"{seed:2}\", pred_list[-1])\nprint()\nsubmission = test[['id']].copy()\nsubmission['target'] = np.array(pred_list).mean(axis=0)\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-05-05T10:30:26.436173Z","iopub.execute_input":"2022-05-05T10:30:26.436808Z","iopub.status.idle":"2022-05-05T10:41:36.495345Z","shell.execute_reply.started":"2022-05-05T10:30:26.436769Z","shell.execute_reply":"2022-05-05T10:41:36.493491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What next?\n\nNow it's your turn! Try to improve this model by\n- Engineering more features\n- Tuning hyperparameters\n- Replacing LightGBM by XGBoost, HistGradientBoostingClassifier or CatBoost ","metadata":{}}]}