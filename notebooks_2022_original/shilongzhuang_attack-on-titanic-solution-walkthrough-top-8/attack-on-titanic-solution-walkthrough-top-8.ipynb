{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic Solution Beginner Walkthrough (top 8%)\n\nThe purpose of this notebook is to show you how I have navigated through the Kaggle Titanic project by following an effective Data Science Workflow. To be specific, the project will tackle some of the core elements of a Data Science pipeline as follows: \n- Exploratory Data Analysis\n- Data Visualization\n- Feature Selection\n- Feature Engineering\n- Machine Learning","metadata":{}},{"cell_type":"markdown","source":"## Data Science Workflow\n> I want to give credit to the author of this [notebook](https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy/notebook) for a very comprehensive Data Science guide and for introducing the **4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting**, which was the foundation of this workflow. I just thought, wouldn't it be better if there were more C's.\n- **Comprehend.** Understand the nature and relationships among each features in the datasets through analyzing and visualizing.\n- **Correlate.** Known as *'Feature Selection',* this approach aims to validate the strength of association across features with the appopriate statistical tools and metrics, and to select the features that are deemed relevant.\n- **Correct.** Identify and remedy the missing/null values. May consider imputing them for features that are deemed significant.  \n- **Create.** Known as *'Feature Engineering',* this approach attempts to create new features out of the existing ones which can make better predictions while also reducing noise in the number of features.\n- **Convert.** Perform the necessary adjustments and transformations to make the datasets normally distributed and fit for modelling.\n- **Combine.** Known as *'Ensemble Models',* this approach aims to combine multiple algorithms into one which leverages the strengths and compensates the weaknesses of the tested models.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"## Background of the Problem\nThe complete overview and description of the Kaggle competition be found [here](https://www.kaggle.com/c/titanic). Here are some of information we were provided with the link.\n- On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg.\n- Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n- While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.","metadata":{}},{"cell_type":"code","source":"# data analysis\nimport pandas as pd\nimport numpy as np\n\n# data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:08.075748Z","iopub.execute_input":"2022-04-30T09:53:08.076056Z","iopub.status.idle":"2022-04-30T09:53:08.081988Z","shell.execute_reply.started":"2022-04-30T09:53:08.076026Z","shell.execute_reply":"2022-04-30T09:53:08.081442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Acquire Training and Testing Data\n\nThe information regarding the features are explicitly presented [here](https://www.kaggle.com/competitions/titanic/data?select=train.csv) in detail.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')\ntest_df_copy = test_df.copy()\ndf = [train_df, test_df]\n\ntest_df_copy.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:08.144617Z","iopub.execute_input":"2022-04-30T09:53:08.145411Z","iopub.status.idle":"2022-04-30T09:53:08.173349Z","shell.execute_reply.started":"2022-04-30T09:53:08.145364Z","shell.execute_reply":"2022-04-30T09:53:08.172637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:08.21068Z","iopub.execute_input":"2022-04-30T09:53:08.210918Z","iopub.status.idle":"2022-04-30T09:53:08.225149Z","shell.execute_reply.started":"2022-04-30T09:53:08.210892Z","shell.execute_reply":"2022-04-30T09:53:08.224271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:08.274347Z","iopub.execute_input":"2022-04-30T09:53:08.275063Z","iopub.status.idle":"2022-04-30T09:53:08.288079Z","shell.execute_reply.started":"2022-04-30T09:53:08.275028Z","shell.execute_reply":"2022-04-30T09:53:08.28727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\nThis stage will focus on comprehending the nature and relationships of the features.","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:08.326807Z","iopub.execute_input":"2022-04-30T09:53:08.327331Z","iopub.status.idle":"2022-04-30T09:53:08.339667Z","shell.execute_reply.started":"2022-04-30T09:53:08.327297Z","shell.execute_reply":"2022-04-30T09:53:08.338758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:08.378057Z","iopub.execute_input":"2022-04-30T09:53:08.37882Z","iopub.status.idle":"2022-04-30T09:53:08.389027Z","shell.execute_reply.started":"2022-04-30T09:53:08.378783Z","shell.execute_reply":"2022-04-30T09:53:08.388224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_df)/(len(train_df)+len(test_df))","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:08.434573Z","iopub.execute_input":"2022-04-30T09:53:08.434944Z","iopub.status.idle":"2022-04-30T09:53:08.439218Z","shell.execute_reply.started":"2022-04-30T09:53:08.434915Z","shell.execute_reply":"2022-04-30T09:53:08.43869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-30T09:53:08.491875Z","iopub.execute_input":"2022-04-30T09:53:08.492286Z","iopub.status.idle":"2022-04-30T09:53:08.520438Z","shell.execute_reply.started":"2022-04-30T09:53:08.492243Z","shell.execute_reply":"2022-04-30T09:53:08.519586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:08.540086Z","iopub.execute_input":"2022-04-30T09:53:08.540456Z","iopub.status.idle":"2022-04-30T09:53:08.543486Z","shell.execute_reply.started":"2022-04-30T09:53:08.54041Z","shell.execute_reply":"2022-04-30T09:53:08.542663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations\n\n**Distribution**\n- The train-test is split around 70/30, with testing set representing 31.93% of the combined sets.\n- The survival rate in context of the training set is at 38.38%, which is representative of the original survival rate of 32.46%, 1502 out of 2224.\n- Majority (around 75%) of the people didn't aboard with siblings/spouses\n- The distributions of SibSP and Parch are right skewed, since the means are greater than the medians.\n\n**Data Types**\n- Numerical: *Age, Sibsp, Parch, Fare*\n- Categorical: *Survival, Sex, Pclass, Embarked*\n- Mixed/Alphanumeric: *Name, Ticket, Cabin*\n\n**Assumptions**\n- **Continuous vs Nominal.** Children (people belonging to lower age brackets) are more likely to survive.\n- **Nominal vs Nominal.** Women are more likely to survive than men. \n- **Ordinal vs Nominal.** The higher-echelon and the wealthier classes are more likely to survive.\n- **Continuous vs Nominal.** Individuals who travel with larger families have a lower likelihood of surviving.\n- **Nominal vs Nominal.** Those who are travel alone have a higher chance of surviving.\n- **Nominal vs Nominal.** Are the individuals' port of embarkation associated with their survival?","metadata":{}},{"cell_type":"markdown","source":"# Analyzing the Numerical Features\nFor the numerical variables, the seaborn **pairplot** will be helpful in presenting the pariwise relationships across each numerical variables. The diagonal plots are the main focus here as they are treated as distribution plots of the features. The rest is just being extra and for eye candy.","metadata":{}},{"cell_type":"code","source":"# Separate the training set into groups of numerical and categorical variables.\n# Don't worry, the 'Survived' was only included in the numerical category so we can use it to classify the features when we create the pairplot\ndf_num = train_df[['Age', 'Survived', 'SibSp', 'Parch', 'Fare']]\ndf_cat = train_df[['Survived', 'Pclass', 'Sex', 'Embarked']]\n\n# Classify by 'Survived'\nsns.set_style('darkgrid')\nsns.pairplot(df_num, hue='Survived', palette='Blues')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:08.590698Z","iopub.execute_input":"2022-04-30T09:53:08.591063Z","iopub.status.idle":"2022-04-30T09:53:13.352043Z","shell.execute_reply.started":"2022-04-30T09:53:08.591036Z","shell.execute_reply":"2022-04-30T09:53:13.3512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(24,4))\nplt.subplot(1,4,1)\nsns.histplot(data=train_df, x=\"Age\", hue=\"Survived\")\n\nplt.subplot(1,4,2)\nsns.histplot(data=train_df, x=\"SibSp\", hue=\"Survived\")\n\nplt.subplot(1,4,3)\nsns.histplot(data=train_df, x=\"Parch\", hue=\"Survived\")\n\nplt.subplot(1,4,4)\nsns.histplot(data=train_df, x=\"Fare\", hue=\"Survived\")","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:13.353643Z","iopub.execute_input":"2022-04-30T09:53:13.353864Z","iopub.status.idle":"2022-04-30T09:53:15.025104Z","shell.execute_reply.started":"2022-04-30T09:53:13.353837Z","shell.execute_reply":"2022-04-30T09:53:15.02421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Age**\n- More normally distributed compared to the rest.\n- Majority of passengers fall in the 20-35 age bracket.\n- A large number of 20-30 yr olds didn't survive.\n- Infants (age<4) had one of the highest survival rates.\n- The oldest individual (age=80) survived.\n\n**SibSp.**\n- Skewed to the right.\n- Those with 1-2 siblings/spouses were likely to survive.\n- Large number of passengers didn't have siblings/spouses with them.\n- An outlier, with over 8 siblings/spouses, didn't survive.\n\n**Parch.**\n- Skewed to the right.\n- Large number of passengers didn't have parents/children with them.\n- Passengers without parents and children with them were more likely to die.\n\n**Fare.**\n- Skewed to the right.\n- Majority of the passengers aboarded with cheaper fares.\n- Most passengers with cheaper fares (<50) didn't survive, while those who paid higher fares (>300) tend to survive.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"# Correlating the Numerical Variables","metadata":{}},{"cell_type":"code","source":"## Correlation Matrix\nplt.subplots(figsize=(10,7))\nsns.heatmap(df_num.corr(), cmap='Blues', annot=True, linewidths=2, annot_kws={\"fontsize\":15})","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:15.026535Z","iopub.execute_input":"2022-04-30T09:53:15.026904Z","iopub.status.idle":"2022-04-30T09:53:15.352273Z","shell.execute_reply.started":"2022-04-30T09:53:15.026861Z","shell.execute_reply":"2022-04-30T09:53:15.351515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations\n\n- Parch and SibSp features a positively moderate correlation.\n- Parch and Fare has a positively weak correlation.\n- We noticed that 'Age' has a very weak correlation with our solution goal despite following a normal distribution.\n\n#### Decisions\n- We can try normalizing the skewed distributions of 'SibSp', 'Parch', and 'Fare' to see if it improves the correlations.\n- Create categorical features 'AgeGroup' from existing 'Age'.\n- To address potential multicollinearity among our highly dependent input features 'SibSp' and 'Parch', let's combine them by multiplying both 'SibSp*Parch'.","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"markdown","source":"## Creating 'SibSp*Parch' Feature","metadata":{}},{"cell_type":"code","source":"# Parch*SibSp\nfor dataset in df:\n    dataset['SibSp*Parch'] = dataset['SibSp']*dataset['Parch']\n\nplt.subplots(figsize=(10,7))\ndf_num = train_df[['Age', 'Survived', 'SibSp', 'Parch', 'SibSp*Parch', 'Fare']]\nsns.heatmap(df_num.corr(), cmap='Blues', annot=True, linewidths=2, annot_kws={\"fontsize\":15})","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:15.354397Z","iopub.execute_input":"2022-04-30T09:53:15.354871Z","iopub.status.idle":"2022-04-30T09:53:15.760764Z","shell.execute_reply.started":"2022-04-30T09:53:15.354825Z","shell.execute_reply":"2022-04-30T09:53:15.759878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalizing Numerical Features\nThe **probability plot** or **quantile-quntile plot (QQplot)** allows us to plot our sample data against the quantiles of a normal distribution. This will serve as reference to see how our subsequent data transformations react to the curve, and enable us to select the best form of transformation which resulted to the best fit.\n\nWe can try use the following data transformation techniques:\n- Square Root\n- Cube Root\n- Logarithmic. **Log(x+1)** wil ensure the log transformation won't result in undefined values because our data contains 'zero' values, and log(0) returns undefined.","metadata":{}},{"cell_type":"code","source":"import scipy.stats as stats\n\n# Defining the function to generate the distribution plot alongside QQplot\ndef QQplot(df, col):\n    plt.figure(figsize = (10, 4))\n    plt.subplot(1,2,1)\n    sns.histplot(x=df[col].dropna(), kde=True)\n    \n    plt.subplot(1,2,2)\n    stats.probplot(df[col].dropna(), dist=\"norm\", plot=plt)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:15.761893Z","iopub.execute_input":"2022-04-30T09:53:15.762089Z","iopub.status.idle":"2022-04-30T09:53:15.766755Z","shell.execute_reply.started":"2022-04-30T09:53:15.762065Z","shell.execute_reply":"2022-04-30T09:53:15.766208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Normalizing Age","metadata":{}},{"cell_type":"code","source":"QQplot(train_df, 'Age')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:15.767687Z","iopub.execute_input":"2022-04-30T09:53:15.767869Z","iopub.status.idle":"2022-04-30T09:53:16.223444Z","shell.execute_reply.started":"2022-04-30T09:53:15.767845Z","shell.execute_reply":"2022-04-30T09:53:16.222654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Normalizing SibSp","metadata":{}},{"cell_type":"code","source":"QQplot(train_df, 'SibSp')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:16.224556Z","iopub.execute_input":"2022-04-30T09:53:16.224776Z","iopub.status.idle":"2022-04-30T09:53:16.70722Z","shell.execute_reply.started":"2022-04-30T09:53:16.224749Z","shell.execute_reply":"2022-04-30T09:53:16.706694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform data transformations and generate QQplots for 'SibSp'\n\ndf_num = train_df[['Age', 'Survived', 'SibSp', 'Parch', 'Fare']]\n\ndf_num[\"SibSp_sqrt\"] = df_num['SibSp']**(1./2)\nQQplot(df_num, 'SibSp_sqrt')\n\ndf_num[\"SibSp_cbrt\"] = df_num['SibSp']**(1./3)\nQQplot(df_num, 'SibSp_cbrt')\n\ndf_num[\"SibSp_log(x+1)\"] = np.log(df_num['SibSp'] + 1)\nQQplot(df_num, 'SibSp_log(x+1)')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:16.708063Z","iopub.execute_input":"2022-04-30T09:53:16.708613Z","iopub.status.idle":"2022-04-30T09:53:18.148454Z","shell.execute_reply.started":"2022-04-30T09:53:16.70858Z","shell.execute_reply":"2022-04-30T09:53:18.147798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Normalizing Parch","metadata":{}},{"cell_type":"code","source":"QQplot(train_df, 'Parch')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:18.149307Z","iopub.execute_input":"2022-04-30T09:53:18.149525Z","iopub.status.idle":"2022-04-30T09:53:18.624204Z","shell.execute_reply.started":"2022-04-30T09:53:18.1495Z","shell.execute_reply":"2022-04-30T09:53:18.623462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform data transformations and generate QQplots\ndf_num[\"Parch_sqrt\"] = df_num['Parch']**(1./2)\nQQplot(df_num, 'Parch_sqrt')\n\ndf_num[\"Parch_cbrt\"] = df_num['Parch']**(1./3)\nQQplot(df_num, 'Parch_cbrt')\n\ndf_num[\"Parch_log(x+1)\"] = np.log(df_num['Parch'] + 1)\nQQplot(df_num, 'Parch_log(x+1)')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:18.627897Z","iopub.execute_input":"2022-04-30T09:53:18.628114Z","iopub.status.idle":"2022-04-30T09:53:20.315156Z","shell.execute_reply.started":"2022-04-30T09:53:18.628089Z","shell.execute_reply":"2022-04-30T09:53:20.31449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Normalizing Fare","metadata":{}},{"cell_type":"code","source":"QQplot(train_df, 'Fare')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:20.316591Z","iopub.execute_input":"2022-04-30T09:53:20.316959Z","iopub.status.idle":"2022-04-30T09:53:20.922543Z","shell.execute_reply.started":"2022-04-30T09:53:20.316915Z","shell.execute_reply":"2022-04-30T09:53:20.921663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform data transformations and generate QQplots for 'Fare'\ntr_Fare = train_df[['Survived', 'Fare']]\n\ntr_Fare[\"Fare_sqrt\"] = tr_Fare['Fare']**(1./2)\nQQplot(tr_Fare, 'Fare_sqrt')\n\ntr_Fare[\"Fare_cbrt\"] = tr_Fare['Fare']**(1./3)\nQQplot(tr_Fare, 'Fare_cbrt')\n\ntr_Fare[\"Fare_log(x+1)\"] = np.log(tr_Fare['Fare'] + 1)\nQQplot(tr_Fare, 'Fare_log(x+1)')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:20.923588Z","iopub.execute_input":"2022-04-30T09:53:20.923799Z","iopub.status.idle":"2022-04-30T09:53:22.381996Z","shell.execute_reply.started":"2022-04-30T09:53:20.923773Z","shell.execute_reply":"2022-04-30T09:53:22.381184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations\n\n- **Age.** Normally distributed. No need to transform.\n- **SibSp.** Still highly skewed. \n- **Parch.** Still highly skewed.\n- **Fare.** The log(x+1) transformation yielded the best fit.\n\n#### Decisions\n- Peform log(x+1) transformation on 'Fare' feature.\n- Create a new 'Family' feature which combines the numbers in 'SibSp' and 'Parch'.\n- Assess if 'Family' is normally distributed and correlated with the solution goal.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"## Creating 'FamilySize' Feature","metadata":{}},{"cell_type":"code","source":"for dataset in df:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\nQQplot(train_df, 'FamilySize')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:22.383515Z","iopub.execute_input":"2022-04-30T09:53:22.383805Z","iopub.status.idle":"2022-04-30T09:53:22.891007Z","shell.execute_reply.started":"2022-04-30T09:53:22.383767Z","shell.execute_reply":"2022-04-30T09:53:22.890169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform data transformations and generate QQplots for 'Family'\ndf_num = train_df[['FamilySize']]\n\ndf_num[\"Family_sqrt\"] = df_num['FamilySize']**(1./2)\nQQplot(df_num, 'Family_sqrt')\n\ndf_num[\"Family_cbrt\"] = df_num['FamilySize']**(1./3)\nQQplot(df_num, 'Family_cbrt')\n\ndf_num[\"Family_log(x+1)\"] = np.log(df_num['FamilySize'])\nQQplot(df_num, 'Family_log(x+1)')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:22.892268Z","iopub.execute_input":"2022-04-30T09:53:22.892508Z","iopub.status.idle":"2022-04-30T09:53:24.363244Z","shell.execute_reply.started":"2022-04-30T09:53:22.892481Z","shell.execute_reply":"2022-04-30T09:53:24.362488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Correlation Matrix for 'Fare' Transformations\nplt.subplots(figsize=(10,7))\nsns.heatmap(tr_Fare.corr(), cmap='Blues', annot=True, linewidths=2, annot_kws={\"fontsize\":15})","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:24.364559Z","iopub.execute_input":"2022-04-30T09:53:24.364759Z","iopub.status.idle":"2022-04-30T09:53:24.698245Z","shell.execute_reply.started":"2022-04-30T09:53:24.364733Z","shell.execute_reply":"2022-04-30T09:53:24.697547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Correlation Matrix for 'Family'\ndf_Family = train_df[['SibSp', 'Parch', 'FamilySize', 'Survived']]\nplt.subplots(figsize=(10,7))\nsns.heatmap(df_Family.corr(), cmap='Blues', annot=True, linewidths=2, annot_kws={\"fontsize\":15})","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:24.69951Z","iopub.execute_input":"2022-04-30T09:53:24.699705Z","iopub.status.idle":"2022-04-30T09:53:24.988212Z","shell.execute_reply.started":"2022-04-30T09:53:24.699681Z","shell.execute_reply":"2022-04-30T09:53:24.98741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations\n\n- After transforming the 'Fare' feature, its correlation with solution goal improved from 0.26 to 0.33.\n- It appears that the 'Family' feature didn't improve the correlation, compared to the likes of 'SibSp' and 'Parch'. \n- 'Age' has a very underwhelming correlation with the solution goal, only at -0..07.\n\n#### Decisions\n- Let's create a new categorical feature called 'withFamily' given 'Family' where we set values to (0 = without family) and (1 = with family). Validate the correlation\n- We can drop the 'SibSp' and 'Parch' features as they are no longer relevant.\n- Create a categorical feature 'AgeGroup' out of 'Age' and see if the correlation improves.","metadata":{}},{"cell_type":"markdown","source":"## Creating 'AgeGroup' Feature from 'Age'","metadata":{}},{"cell_type":"code","source":"for dataset in df:\n    dataset['Age'] = dataset['Age'].fillna(dataset['Age'].mean())\n    dataset['AgeGroup'] = pd.cut(dataset['Age'], 8)\n\ntrain_df[['AgeGroup', 'Survived']].groupby(['AgeGroup'], as_index=False).mean()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:24.989558Z","iopub.execute_input":"2022-04-30T09:53:24.989927Z","iopub.status.idle":"2022-04-30T09:53:25.012397Z","shell.execute_reply.started":"2022-04-30T09:53:24.989886Z","shell.execute_reply":"2022-04-30T09:53:25.011544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting 'AgeGroup' into a Categorical Variable\nfor dataset in df:\n    dataset.loc[dataset['Age'] <= 10, 'AgeGroupNum'] = 0\n    dataset.loc[(dataset['Age'] > 10) & (dataset['Age'] <= 20), 'AgeGroupNum'] = 1\n    dataset.loc[(dataset['Age'] > 20) & (dataset['Age'] <= 30), 'AgeGroupNum'] = 2\n    dataset.loc[(dataset['Age'] > 30) & (dataset['Age'] <= 40), 'AgeGroupNum'] = 3\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 50), 'AgeGroupNum'] = 4\n    dataset.loc[(dataset['Age'] > 50) & (dataset['Age'] <= 60), 'AgeGroupNum'] = 5\n    dataset.loc[(dataset['Age'] > 60) & (dataset['Age'] <= 70), 'AgeGroupNum'] = 6\n    dataset.loc[(dataset['Age'] > 70), 'AgeGroupNum'] = 7\n\ntrain_df['AgeGroupNum'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:25.013606Z","iopub.execute_input":"2022-04-30T09:53:25.013804Z","iopub.status.idle":"2022-04-30T09:53:25.037075Z","shell.execute_reply.started":"2022-04-30T09:53:25.01378Z","shell.execute_reply":"2022-04-30T09:53:25.03619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating 'FareGroup' Feature from 'Fare'","metadata":{}},{"cell_type":"code","source":"# for dataset in df:\n#     dataset['Fare'] = dataset['Fare'].fillna(dataset['Age'].median())\n#     dataset['FareGroup'] = pd.cut(dataset['Fare'], 5)\n\n# train_df[['FareGroup', 'Survived']].groupby(['FareGroup'], as_index=False).mean()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:25.038536Z","iopub.execute_input":"2022-04-30T09:53:25.039554Z","iopub.status.idle":"2022-04-30T09:53:25.043692Z","shell.execute_reply.started":"2022-04-30T09:53:25.039513Z","shell.execute_reply":"2022-04-30T09:53:25.042492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating 'withFamily' Feature from 'Family'","metadata":{}},{"cell_type":"code","source":"for dataset in df:\n    \n    # We represent 1 if Person is with Family, 0 otherwise\n    dataset['withFamily'] = 0\n    dataset.loc[dataset['FamilySize'] > 1, 'withFamily'] = 1\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:25.045172Z","iopub.execute_input":"2022-04-30T09:53:25.045604Z","iopub.status.idle":"2022-04-30T09:53:25.071696Z","shell.execute_reply.started":"2022-04-30T09:53:25.045562Z","shell.execute_reply":"2022-04-30T09:53:25.070869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyzing the Categorical Features\n\nThe categorical variables, along with the features we created previously, that we want to analyze are as follows:\n- Pclass\n- Sex\n- Embarked\n- AgeGroupNum\n- withFamily\n\nLet's use **countplots** to visualize the distribution of each classification with respect to survival, and **lineplots** to determine the corresponding survival rates (in % form). ","metadata":{}},{"cell_type":"code","source":"def Catplot(df, x, y):\n    with sns.axes_style('darkgrid'):\n        plt.figure(figsize = (12, 4))\n        plt.subplot(1,2,1)\n        sns.countplot( x=df[x].dropna(), hue=df[y], palette='Blues')\n        \n        plt.subplot(1,2,2)\n        plt.ylim(0,1)\n        sns.lineplot( x=df[x], y=df[y], data=df, ci=None, linewidth=2, marker=\"o\")\n        \nCatplot(train_df, 'Sex', 'Survived')\nCatplot(train_df, 'Pclass', 'Survived')\nCatplot(train_df, 'Embarked', 'Survived')\nCatplot(train_df, 'AgeGroupNum', 'Survived')\nCatplot(train_df, 'withFamily', 'Survived')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:25.072645Z","iopub.execute_input":"2022-04-30T09:53:25.072918Z","iopub.status.idle":"2022-04-30T09:53:26.977322Z","shell.execute_reply.started":"2022-04-30T09:53:25.07289Z","shell.execute_reply":"2022-04-30T09:53:26.976378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations\n\n**Sex**\n- More males were on board.\n- Females had a higher survival rate than males did. No wonder Jack died over Rose.\n\n**Pclass**\n- Most passengers were in Pclass 3. \n- The significance of higher classes being correlated with higher survival rate is justified.\n- Pclass 1 is the only class with more survived passengers than dead passengers.\n\n**Embarked**\n- A major chunk of the passengers embarked from Southampton, and the least from Queensland.\n- Highest survival rate were found for passengers that embarked from C.\n\n**AgeGroupNum**\n- Most passengers belong to the 16-32 yr old group. \n- Highest survival rate came from toddlers to teens group (<16).\n- Least survival rate came from senior age group (>64), followed by the adolescents to mid-age group (16-32).\n\n**WithFamily**\n- Those who aboarded alone are likely to die than those with their families.\n> *\"There is nothing stronger than family.\" - Dom* \n\n#### Decisions\n- Validate the orrelations of 'Sex', 'Pclass', 'Embarked', and 'withFamily' with solution goal.\n- Create a new categorical feature 'Title' by extracting the titles from the given names of the passengers. Validate correlation.\n- Create a new categorical feature 'Unit' by extracting the units (i.e. letters, numbers) from the cabin numbers. Validate correlation.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"# Analyzing the Alphanumeric Variables\n- Does a passenger's title relate to survival rates?\n- Does the cabin unit *(extract first letter from alphanumeric string)*  affect survival rates?","metadata":{}},{"cell_type":"code","source":"# Analyzing 'Name'\n\n# Split the full name into a list by comma, then return the title by indexing the 2nd position [1]\n# Split the name into a list by period, then return the title by indexing the 1st position [0]\n\ntrain_df['Title'] = train_df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:26.978588Z","iopub.execute_input":"2022-04-30T09:53:26.978815Z","iopub.status.idle":"2022-04-30T09:53:26.985488Z","shell.execute_reply.started":"2022-04-30T09:53:26.978787Z","shell.execute_reply":"2022-04-30T09:53:26.984521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Title'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:26.986685Z","iopub.execute_input":"2022-04-30T09:53:26.986935Z","iopub.status.idle":"2022-04-30T09:53:27.001209Z","shell.execute_reply.started":"2022-04-30T09:53:26.986908Z","shell.execute_reply":"2022-04-30T09:53:27.000481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Catplot_Title(df, x, y):\n    with sns.axes_style('darkgrid'):\n        plt.figure(figsize = (15, 5))\n        plt.subplot(1,2,1)\n        sns.countplot( x=df[x].dropna(), hue=df[y], palette='Blues')\n        plt.xticks(rotation=45)\n        \n        plt.subplot(1,2,2)\n        plt.xticks(rotation=45)\n        sns.lineplot( x=df[x], y=df[y], data=df, ci=None, linewidth=2, marker=\"o\")\n    \nCatplot_Title(train_df, 'Title', 'Survived')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:27.004588Z","iopub.execute_input":"2022-04-30T09:53:27.005126Z","iopub.status.idle":"2022-04-30T09:53:27.61261Z","shell.execute_reply.started":"2022-04-30T09:53:27.005093Z","shell.execute_reply":"2022-04-30T09:53:27.611816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Prefixes Meaning**\n\n- **Rev.** Ministers of most Christian denominations; ordained clergymen since 17th century.\n- **Mlle and Miss.** Unmarried female; young lady.\n- **Mme.** Woman\n- **Master, Major, and Don.** Lord, master, or owner (of a household).\n- **Col.** Colonel; army officer of high rank.\n- **the Countess.** Wife or widow of a count.\n- **Capt.** could refer to the captain of the ship.\n- **Ms.** Any women regardless of marital status.\n- **Lady.** Princesses or daughters of royal blood.\n- **Jonkheer.** Female equivalent denoting the lowest rank within the nobility.\n\n**Classify Titles**\n- Mlle and Ms > Miss\n- Mme > Mrs\n- Uncommon titles will be treated as a new category > Others","metadata":{}},{"cell_type":"code","source":"for dataset in df:\n    dataset['Title'] = dataset['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n# Returns the unique 'titles' from the dataset\nUnique_titles = np.union1d(train_df['Title'].unique(), test_df['Title'].unique())\nUnique_titles","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:27.613747Z","iopub.execute_input":"2022-04-30T09:53:27.613976Z","iopub.status.idle":"2022-04-30T09:53:27.624413Z","shell.execute_reply.started":"2022-04-30T09:53:27.613947Z","shell.execute_reply":"2022-04-30T09:53:27.623475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dataset in df:\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    dataset['Title'] = dataset['Title'].replace(['Capt', 'Col', 'Don', 'Dona', 'Dr', 'Jonkheer', 'Lady', 'Major', 'Rev', 'Sir','the Countess'], 'Others')\n    \nprint(train_df['Title'].unique())\nprint(test_df['Title'].unique())","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:27.625595Z","iopub.execute_input":"2022-04-30T09:53:27.62581Z","iopub.status.idle":"2022-04-30T09:53:27.642948Z","shell.execute_reply.started":"2022-04-30T09:53:27.625785Z","shell.execute_reply":"2022-04-30T09:53:27.642133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Catplot(train_df, 'Title', 'Survived')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:27.644231Z","iopub.execute_input":"2022-04-30T09:53:27.644616Z","iopub.status.idle":"2022-04-30T09:53:28.014502Z","shell.execute_reply.started":"2022-04-30T09:53:27.644574Z","shell.execute_reply":"2022-04-30T09:53:28.013682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Titles**\n\n- It justifies the correlation between 'Sex' and 'Survived' as titles with 'Mr' and 'Master' tend to have lower survival rates than 'Mrs' and 'Miss'.\n- Also justifies the correlation between 'withFamily' and 'Survived' as titles with 'Mrs' tend to have higher survival rates than 'Miss'.\n- The 'Others' category compiles a very small sample of the given dataset. Despite these titles having characterized of nobile status, it appears that they were trivial at the time of the crisis.","metadata":{}},{"cell_type":"code","source":"# Analyzing 'Cabin'\nfor dataset in df:\n    dataset['CabinUnit'] = train_df['Cabin'].apply(lambda x: str(x)[0])","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:28.017618Z","iopub.execute_input":"2022-04-30T09:53:28.017912Z","iopub.status.idle":"2022-04-30T09:53:28.025134Z","shell.execute_reply.started":"2022-04-30T09:53:28.01788Z","shell.execute_reply":"2022-04-30T09:53:28.024366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Note that null values will be treated as a category, denoted as CabinUnit 'n'. ","metadata":{}},{"cell_type":"code","source":"pd.pivot_table(train_df, index='CabinUnit', columns='Survived', values='Name', aggfunc='count')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:28.026224Z","iopub.execute_input":"2022-04-30T09:53:28.026528Z","iopub.status.idle":"2022-04-30T09:53:28.046822Z","shell.execute_reply.started":"2022-04-30T09:53:28.026488Z","shell.execute_reply":"2022-04-30T09:53:28.045859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Catplot(train_df, 'CabinUnit', 'Survived')","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:28.047982Z","iopub.execute_input":"2022-04-30T09:53:28.04825Z","iopub.status.idle":"2022-04-30T09:53:28.484071Z","shell.execute_reply.started":"2022-04-30T09:53:28.048214Z","shell.execute_reply":"2022-04-30T09:53:28.4835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cabin Unit.**\n\n- The abundance of missing values makes it difficult for 'CabinUnit' to be representative as a sample.\n- Deck/Cabin A was known as the Pomegrande, which is located at the superstructure of Titanic. The assumption that Cabin A would have a higher survival rate, however, isn't justified by our data. Hence, Cabin Unit column shall be removed.","metadata":{}},{"cell_type":"markdown","source":"# Correlating the Categorical Features\nAssessing the strength of association (correlation) across variables is one way to conduct feature selection. This time, it wouldn't be ideal to use **Pearson's correlation matrix** we did for our numerical variables earlier because we are now dealing with categorical variables (both the predictors and response variables). Using the **[Chi-square test](https://towardsdatascience.com/chi-square-test-for-feature-selection-in-machine-learning-206b1f0b8223#:~:text=In%20feature%20selection%2C%20we%20aim,hypothesis%20of%20independence%20is%20incorrect.)** is the appropriate statistical method here.\n\nBefore we can conduct Chi-square tests, we must ensure that our categorical data are numerically encoded first using `LabelEncoder()`.","metadata":{"tags":[]}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encode = LabelEncoder()\n\ntrain_df_copy = train_df.copy()\n\ntrain_df_copy['Sex'] = label_encode.fit_transform(train_df_copy['Sex'])\ntrain_df_copy['Embarked'] = label_encode.fit_transform(train_df_copy['Embarked'])\ntrain_df_copy['Pclass'] = label_encode.fit_transform(train_df_copy['Pclass'])\ntrain_df_copy['Title'] = label_encode.fit_transform(train_df_copy['Title'])\n\ntrain_df_copy.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:28.485072Z","iopub.execute_input":"2022-04-30T09:53:28.485858Z","iopub.status.idle":"2022-04-30T09:53:28.509974Z","shell.execute_reply.started":"2022-04-30T09:53:28.48581Z","shell.execute_reply":"2022-04-30T09:53:28.509181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import chi2\nfrom sklearn.model_selection import train_test_split\n\n# Split our dataset into x and y variables\nx = train_df_copy[['Sex', 'Pclass', 'Embarked', 'withFamily', 'AgeGroupNum', 'Title']]\ny = train_df_copy['Survived']\nchi2_scores = chi2(x,y)\n\nchi2_scores = pd.DataFrame(np.transpose(chi2_scores), index=['Sex', 'Pclass', 'Embarked', 'withFamily', 'AgeGroupNum', 'Title'], columns=['Chi2', 'p_value']).sort_values('p_value', ascending=True)\nchi2_scores","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:28.511351Z","iopub.execute_input":"2022-04-30T09:53:28.511964Z","iopub.status.idle":"2022-04-30T09:53:28.526812Z","shell.execute_reply.started":"2022-04-30T09:53:28.511931Z","shell.execute_reply":"2022-04-30T09:53:28.52569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\n- Drop null values from 'Embark' in datasets.\n- Dropping irrelevant column features.\n- Impute missing values in testing 'Fare' with median, because it is skewed.\n- Normalize 'Fare' through log transformation.\n- Perform one-hot encoding on our categorical data.\n- Scaling our numerical data.","metadata":{"tags":[]}},{"cell_type":"code","source":"# Remove 'Embark' null values\nfor dataset in df:\n    dataset.dropna(subset=['Embarked'], inplace=True)\n    \ndf = [train_df, test_df]\n\nprint(train_df.shape, test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:28.527867Z","iopub.execute_input":"2022-04-30T09:53:28.528136Z","iopub.status.idle":"2022-04-30T09:53:28.539396Z","shell.execute_reply.started":"2022-04-30T09:53:28.528106Z","shell.execute_reply":"2022-04-30T09:53:28.538494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute missing values in 'Age' and 'Fare' with mean and median respectively\nfor dataset in df:\n    dataset['Fare'] = dataset['Fare'].fillna(dataset['Fare'].median())\n    \ntrain_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:28.540662Z","iopub.execute_input":"2022-04-30T09:53:28.541763Z","iopub.status.idle":"2022-04-30T09:53:28.552941Z","shell.execute_reply.started":"2022-04-30T09:53:28.541722Z","shell.execute_reply":"2022-04-30T09:53:28.552142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Log transform 'Fare'\nfor dataset in df:\n    dataset['Fare'] = np.log(dataset['Fare'] + 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:28.554103Z","iopub.execute_input":"2022-04-30T09:53:28.554755Z","iopub.status.idle":"2022-04-30T09:53:28.559412Z","shell.execute_reply.started":"2022-04-30T09:53:28.554713Z","shell.execute_reply":"2022-04-30T09:53:28.55893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping irrelevant column features\ntrain_df.drop(['PassengerId','Age', 'Name', 'SibSp', 'Parch', 'Ticket', 'FamilySize', 'Cabin', 'CabinUnit', 'AgeGroup'], inplace=True, axis=1)\ntest_df.drop(['PassengerId', 'Age', 'Name', 'SibSp', 'Parch', 'Ticket', 'FamilySize', 'Cabin', 'CabinUnit', 'AgeGroup'], inplace=True, axis=1)\ndf = [train_df, test_df]","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:28.560212Z","iopub.execute_input":"2022-04-30T09:53:28.560884Z","iopub.status.idle":"2022-04-30T09:53:28.570218Z","shell.execute_reply.started":"2022-04-30T09:53:28.560837Z","shell.execute_reply":"2022-04-30T09:53:28.56965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating dummy indicator columns for categorical variables\ntrain_df = pd.get_dummies(train_df, columns=['Sex', 'Pclass', 'Embarked', 'withFamily', 'AgeGroupNum', 'Title'])\ntest_df = pd.get_dummies(test_df, columns=['Sex', 'Pclass', 'Embarked', 'withFamily','AgeGroupNum', 'Title'])\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:28.572241Z","iopub.execute_input":"2022-04-30T09:53:28.572576Z","iopub.status.idle":"2022-04-30T09:53:28.942048Z","shell.execute_reply.started":"2022-04-30T09:53:28.572538Z","shell.execute_reply":"2022-04-30T09:53:28.941254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting Training Data\nThere is a need to split our training data into 2 subsets of training and testing data once more. Why is that? Note that the test.csv file provided is merely just a validation data for our competition submission, so it can't be treated as testing data which our current training data can learn from lest it would lead to overfitting our data.","metadata":{}},{"cell_type":"code","source":"from sklearn import model_selection\nx1 = train_df.drop(['Survived'], axis=1)\ny1 = train_df['Survived']\nx1_train, x1_test, y1_train, y1_test = model_selection.train_test_split(x1, y1, random_state=42)\n\nprint(x1_train.shape, x1_test.shape, y1_train.shape, y1_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:28.943129Z","iopub.execute_input":"2022-04-30T09:53:28.943342Z","iopub.status.idle":"2022-04-30T09:53:28.952391Z","shell.execute_reply.started":"2022-04-30T09:53:28.943317Z","shell.execute_reply":"2022-04-30T09:53:28.951565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preliminary Model Generation\nNow that we have completed the data preparation, we can now begin training our model and predict our solution goal. Since we are working with a given dataset with a predetermined solution goal, we are running a form of machine learning algorithm known as supervised learning. With that, here are some of the few beginner models we can run and see how each of them perform based on accuracy.\n\nIf you want to understand the fundamentals of each ML algorithm, you may click on the hyperlinks below which will redirect you to very helpful and easy-to-folllow youtube video tutorials from *StatQuest with Josh Starmer.*\n\n- [Logistic Regression](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjdkvWQ4qT3AhVEQd4KHUvNDPIQwqsBegQIFhAB&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DyIYKR4sgzI8&usg=AOvVaw3maZPWy-T2rEc4PFDM40af)\n- [Support Vector Machines](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjM0pq45KT3AhVcQfUHHVYJBysQwqsBegQIAhAB&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DefR1C6CvhmE&usg=AOvVaw1alnpuy6aMk4ogaK4NtmXy)\n- [K-Means Nearest Neighbors](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwj60qDK5KT3AhUbAYgKHcyoDbAQwqsBegQIAhAB&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DHVXime0nQeI&usg=AOvVaw1h03i8dfC0gXYPU9lFRzJ_)\n- [Decision Tree](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwi-t-rS5KT3AhWGAogKHbBfBGQQwqsBegQIAhAB&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D7VeUPuFGJHk&usg=AOvVaw2KBODG3Oh7AiSz-4h5wnMd)\n- [Random Forest](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiyptHZ5KT3AhWNEYgKHelnCqgQwqsBegQIAhAB&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DJ4Wdy0Wc_xQ&usg=AOvVaw0moI0sPTwd34hRAKxbDRVN)\n\nAnother important approach which is crucial here is to conduct cross validation. It is a useful technique to address overfitting as it evaluates models through a fixed number of folds k. In my case, I decided to do 10-fold cross-validation. In other words, I do 10 different subsets of sample from training set to arrive at my solutions, then get the mean of all the accuracy scores from these tests.","metadata":{}},{"cell_type":"code","source":"#Common Model Algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Defining a list of Machine Learning Algorithms I will be running\nMLA = [\n    LogisticRegression(max_iter = 2000),\n    SVC(),\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier()   \n]\n\nrow_index = 0\n\n# Setting up the table to compare the performances of each model\nMLA_cols = ['Model', 'Train Accuracy Mean', 'Test Accuracy Mean', 'Fit Time']\nMLA_compare = pd.DataFrame(columns = MLA_cols)\n\nfor model in MLA:\n    MLA_compare.loc[row_index, 'Model'] = model.__class__.__name__\n    cv_results = model_selection.cross_validate(model, x1_train, y1_train, cv=10, return_train_score=True)\n    MLA_compare.loc[row_index, 'Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'Test Accuracy Mean'] = cv_results['test_score'].mean()\n    MLA_compare.loc[row_index, 'Fit Time'] = cv_results['fit_time'].mean()\n    \n    row_index+=1\n\nMLA_compare.sort_values(by=['Test Accuracy Mean'], ascending=False, inplace=True)\nMLA_compare","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:28.953607Z","iopub.execute_input":"2022-04-30T09:53:28.953868Z","iopub.status.idle":"2022-04-30T09:53:32.596119Z","shell.execute_reply.started":"2022-04-30T09:53:28.953841Z","shell.execute_reply":"2022-04-30T09:53:32.595305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a list of Machine Learning Algorithms I will be running\nMLA = [\n    LogisticRegression(max_iter = 2000),\n    SVC(),\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier()   \n]\n\nrow_index = 0\n\n# Setting up the table to compare the performances of each model\nMLA_cols = ['Model', 'Train Accuracy Mean', 'Test Accuracy Mean', 'Fit Time']\nMLA_compare = pd.DataFrame(columns = MLA_cols)\n\nfor model in MLA:\n    MLA_compare.loc[row_index, 'Model'] = model.__class__.__name__\n    cv_results = model_selection.cross_validate(model, x1_train, y1_train, cv=10, return_train_score=True)\n    MLA_compare.loc[row_index, 'Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'Test Accuracy Mean'] = cv_results['test_score'].mean()\n    MLA_compare.loc[row_index, 'Fit Time'] = cv_results['fit_time'].mean()\n    \n    row_index+=1\n\nMLA_compare.sort_values(by=['Test Accuracy Mean'], ascending=False, inplace=True)\nMLA_compare","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Optimization\nAnother effective technique to improve the accuracy that I want to try out is optimizing the model by identifying the best parameters for our model. In my case, I will try optimizing my SVM as it performed the best out of the existing ones.\n\nThere are two parameters that I can optimize in SVM, **gamma** and C, and using GridSearchCV() allows us to test the possible combinations of these parameters.","metadata":{"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nparam_grid = [{'C': [0.5, 1, 10, 100],\n             'gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001]}]\n\noptimal = GridSearchCV(SVC(), param_grid, cv = 10, scoring = 'accuracy')\noptimal.fit(x1_train, y1_train)\nprint(optimal.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:32.597784Z","iopub.execute_input":"2022-04-30T09:53:32.598341Z","iopub.status.idle":"2022-04-30T09:53:38.243159Z","shell.execute_reply.started":"2022-04-30T09:53:32.598295Z","shell.execute_reply":"2022-04-30T09:53:38.242162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run the SVM model again with the optimal parameters\nsvm_optimal = SVC(random_state=42, C=1, gamma='scale')\ncvs = cross_val_score(svm_optimal, x1_train, y1_train, cv=10)\ncvs.mean()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:38.244746Z","iopub.execute_input":"2022-04-30T09:53:38.245034Z","iopub.status.idle":"2022-04-30T09:53:38.446922Z","shell.execute_reply.started":"2022-04-30T09:53:38.244995Z","shell.execute_reply":"2022-04-30T09:53:38.446112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble Learning\nThe accuracy scores we determined earlier only reflect our training set, so these have the likelihood to overfit, which means our model may not necessarily perform well when we test it on our testing data. One approach in addressing this issue is to conduct ensmeble learning. It aims to improve the model by combining multiple algorithms and classifications in order to reduce the biases and address the weaknesses of using standalone models. There are actually several methods of ensemble learning, listed below are some:\n- Majority Voting\n- Bagging\n- Boosting\n- Gradient Boosting\n- Random Forests\n- Stacking\n\nFor now, I will only delve into using the Majority Voting Classifier first as a working example. From the name itself, a voting ensemble involves favoring the class label (i.e. 1=Survived, 0=Died) with the majority or the most votes as the prediction. They are two types of voting:\n- **Hard voting.** summing the votes of class labels from other models and selecting the class label with the most votes as the prediction.\n- **Soft voting.** summing the predicted probabilities of classes from other models and selecting the class label with largest sum probability as the prediction.","metadata":{}},{"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.ensemble import VotingClassifier\n\n# Defining the model algorithms for easier access\nlr = LogisticRegression(max_iter = 2000)\nsvm = SVC()\nknn = KNeighborsClassifier()\ndt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\n\n\n# Creating an ensemble for Hard Voting Classifer for Top 3 models\nEnsemble_HV = VotingClassifier(estimators =[('Support Vector Machines', svm),\n                                      ('K-Means Nearest Neighbors', knn),\n                                      ('Logistic Regression', lr)],\n                         voting = 'hard')\n\n# Compare previous models with Ensemble_HV\nMLA = [\n    LogisticRegression(max_iter = 2000),\n    SVC(),\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    Ensemble_HV\n]\n\nrow_index = 0\n\n# Setting up the table to compare the performances of each model\nMLA_cols = ['Model', 'Train Accuracy Mean', 'Test Accuracy Mean', 'Fit Time']\nMLA_compare = pd.DataFrame(columns = MLA_cols)\n\nfor model in MLA:\n    MLA_compare.loc[row_index, 'Model'] = model.__class__.__name__\n    cv_results = model_selection.cross_validate(model, x1_train, y1_train, cv=10, return_train_score=True)\n    MLA_compare.loc[row_index, 'Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'Test Accuracy Mean'] = cv_results['test_score'].mean()\n    MLA_compare.loc[row_index, 'Fit Time'] = cv_results['fit_time'].mean()\n    \n    row_index+=1\n\nMLA_compare.sort_values(by=['Test Accuracy Mean'], ascending=False, inplace=True)\nMLA_compare","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:38.448289Z","iopub.execute_input":"2022-04-30T09:53:38.448697Z","iopub.status.idle":"2022-04-30T09:53:43.592065Z","shell.execute_reply.started":"2022-04-30T09:53:38.448656Z","shell.execute_reply":"2022-04-30T09:53:43.591256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction Submissions\nWe can now submit our predictions based on the models/ensemble models we tuned and see how our scores perform with other fellow Kaggle competitors. Here are the public scores I have gotten.","metadata":{}},{"cell_type":"code","source":"# Submitting predictions with standalone SVM\n\nX_test = test_df\n\nsvm.fit(x1_train, y1_train)\nY_pred = svm.predict(X_test)\npred = pd.DataFrame({\n    \"PassengerId\": test_df_copy[\"PassengerId\"],\n    \"Survived\": Y_pred\n})\npred.to_csv('submission_svm.csv', index=False)\n\n\nsvm_optimal.fit(x1_train, y1_train)\nY_pred = svm_optimal.predict(X_test)\npred = pd.DataFrame({\n    \"PassengerId\": test_df_copy[\"PassengerId\"],\n    \"Survived\": Y_pred\n})\npred.to_csv('submission_svm_optimal.csv', index=False)\n\n\nrf.fit(x1_train, y1_train)\nY_pred = rf.predict(X_test)\npred = pd.DataFrame({\n    \"PassengerId\": test_df_copy[\"PassengerId\"],\n    \"Survived\": Y_pred\n})\npred.to_csv('submission_rf.csv', index=False)\n\n\nknn.fit(x1_train, y1_train)\nY_pred = knn.predict(X_test)\npred = pd.DataFrame({\n    \"PassengerId\": test_df_copy[\"PassengerId\"],\n    \"Survived\": Y_pred\n})\npred.to_csv('submission_knn.csv', index=False)\n\n\nlr.fit(x1_train, y1_train)\nY_pred = lr.predict(X_test)\npred = pd.DataFrame({\n    \"PassengerId\": test_df_copy[\"PassengerId\"],\n    \"Survived\": Y_pred\n})\npred.to_csv('submission_lr.csv', index=False)\n\n\nEnsemble_HV.fit(x1_train, y1_train)\nY_pred = Ensemble_HV.predict(X_test)\npred = pd.DataFrame({\n    \"PassengerId\": test_df_copy[\"PassengerId\"],\n    \"Survived\": Y_pred\n})\npred.to_csv('predictions_ensemble_hv.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:53:43.593735Z","iopub.execute_input":"2022-04-30T09:53:43.594232Z","iopub.status.idle":"2022-04-30T09:53:44.104285Z","shell.execute_reply.started":"2022-04-30T09:53:43.59419Z","shell.execute_reply":"2022-04-30T09:53:44.103357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stay Safe and Happy Kaggling!\nAny form of feedback and advise are welcome. If you have any questions and clarifications regarding my code and work, feel free to ask them in the comments section and I will be happy to answer them.\n\n## My Other Works\nIf you are interested, you can go to my Kaggle profile [HERE](https://www.kaggle.com/shilongzhuang) and browse through my other works and contributions. Just don't read my bio, I wouldn't if I were you.\n\n---\n# References\nSpecial thanks and credits to these awesome and comprehensively informative resources (notebooks) and guides created by talented professionals in the field. I highly recommend you go check them out especially for beginners like me.\n- [A Data Science Framework: To Achieve 99% Accuracy | Kaggle](https://www.kaggle.com/code/shilongzhuang/a-data-science-framework-to-achieve-99-accuracy/edit)\n- [Titanic Data Science Solutions | Kaggle](https://www.kaggle.com/code/startupsci/titanic-data-science-solutions)","metadata":{}}]}