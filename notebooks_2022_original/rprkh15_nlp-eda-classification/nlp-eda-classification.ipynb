{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div align = 'center'><b>NLP with Disaster Tweets</b></div>\n<img align = middle src=\"https://akm-img-a-in.tosshub.com/aajtak/images/photo_gallery/202105/twitter_final_5.jpg\">\n\n# Installing the Necessary Libraries","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install text-hammer","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-06T04:00:51.852072Z","iopub.execute_input":"2022-05-06T04:00:51.852794Z","iopub.status.idle":"2022-05-06T04:01:19.157802Z","shell.execute_reply.started":"2022-05-06T04:00:51.852689Z","shell.execute_reply":"2022-05-06T04:01:19.156735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom wordcloud import WordCloud\n\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\nimport folium \nfrom folium import plugins \n\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom collections import Counter\nfrom collections import defaultdict\nimport text_hammer as th\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.callbacks import ReduceLROnPlateau\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:06:25.810177Z","iopub.execute_input":"2022-05-06T04:06:25.811103Z","iopub.status.idle":"2022-05-06T04:06:25.819929Z","shell.execute_reply.started":"2022-05-06T04:06:25.811062Z","shell.execute_reply":"2022-05-06T04:06:25.818874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Color Scheme\nSetting the color scheme for the notebook","metadata":{}},{"cell_type":"code","source":"custom_colors = ['#000000', '#E31E33', '#4A53E1', '#F5AD02', '#94D5EA', '#F6F8F7']\ncustom_palette = sns.set_palette(sns.color_palette(custom_colors))\nsns.palplot(sns.color_palette(custom_colors), size = 1)\nplt.tick_params(axis = 'both', labelsize = 0, length = 0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-06T04:01:33.462958Z","iopub.execute_input":"2022-05-06T04:01:33.46354Z","iopub.status.idle":"2022-05-06T04:01:33.96036Z","shell.execute_reply.started":"2022-05-06T04:01:33.463503Z","shell.execute_reply":"2022-05-06T04:01:33.95969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the input files","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:01:33.962228Z","iopub.execute_input":"2022-05-06T04:01:33.962581Z","iopub.status.idle":"2022-05-06T04:01:33.970947Z","shell.execute_reply.started":"2022-05-06T04:01:33.96255Z","shell.execute_reply":"2022-05-06T04:01:33.969949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reading the dataframe","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:01:33.972469Z","iopub.execute_input":"2022-05-06T04:01:33.972722Z","iopub.status.idle":"2022-05-06T04:01:34.052921Z","shell.execute_reply.started":"2022-05-06T04:01:33.972694Z","shell.execute_reply":"2022-05-06T04:01:34.052225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.isna().sum())\nprint('----------------------------')\nprint('Total Missing Values: ', df.isna().sum().sum())\nprint('----------------------------')","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:01:34.054276Z","iopub.execute_input":"2022-05-06T04:01:34.054637Z","iopub.status.idle":"2022-05-06T04:01:34.072663Z","shell.execute_reply.started":"2022-05-06T04:01:34.054607Z","shell.execute_reply":"2022-05-06T04:01:34.071873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA (Exploratory Data Analysis)","metadata":{}},{"cell_type":"markdown","source":"Visualizing the missing data in the form of a chart","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 10))\nsns.heatmap(df.isna(), yticklabels = False, cbar = False, cmap = 'afmhot')\nplt.title(\"Visualizing the Missing Data\", fontsize = 20)\nplt.xticks(rotation = 35, fontsize = 15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:01:34.074313Z","iopub.execute_input":"2022-05-06T04:01:34.074883Z","iopub.status.idle":"2022-05-06T04:01:34.299039Z","shell.execute_reply.started":"2022-05-06T04:01:34.074838Z","shell.execute_reply":"2022-05-06T04:01:34.297903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bar representation of the missing values","metadata":{}},{"cell_type":"code","source":"msno.bar(df, color = (0, 0, 0), sort = \"ascending\", figsize = (15, 10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:01:34.300484Z","iopub.execute_input":"2022-05-06T04:01:34.300709Z","iopub.status.idle":"2022-05-06T04:01:34.936534Z","shell.execute_reply.started":"2022-05-06T04:01:34.300682Z","shell.execute_reply":"2022-05-06T04:01:34.935247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets take a look at the class distribution of our dataset","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 12))\nax = plt.axes()\nax.set_facecolor('black')\nax = sns.countplot(x = 'target', data = df, palette = [custom_colors[2], custom_colors[1]], edgecolor = 'white', linewidth = 1.2)\nplt.title('Disaster Count', fontsize = 25)\nplt.xlabel('Disaster', fontsize = 20)\nplt.ylabel('Count', fontsize = 20)\nax.xaxis.set_tick_params(labelsize = 15)\nax.yaxis.set_tick_params(labelsize = 15)\nbbox_args = dict(boxstyle = 'round', fc = '0.9')\nfor p in ax.patches:\n        ax.annotate('{:.0f} = {:.2f}%'.format(p.get_height(), (p.get_height() / len(df['target'])) * 100), (p.get_x() + 0.25, p.get_height() + 60), \n                   color = 'black',\n                   bbox = bbox_args,\n                   fontsize = 15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:01:34.938724Z","iopub.execute_input":"2022-05-06T04:01:34.939396Z","iopub.status.idle":"2022-05-06T04:01:35.173999Z","shell.execute_reply.started":"2022-05-06T04:01:34.939341Z","shell.execute_reply":"2022-05-06T04:01:35.173285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a class imbalance in the dataset, with 4342 non-disaster tweets and 3271 disaster tweets.\n\nLet's take a look at where most of the tweets in our dataset come from:","metadata":{}},{"cell_type":"code","source":"df['location'].value_counts()[:10]","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:02:44.562693Z","iopub.execute_input":"2022-05-06T04:02:44.563704Z","iopub.status.idle":"2022-05-06T04:02:44.576914Z","shell.execute_reply.started":"2022-05-06T04:02:44.563652Z","shell.execute_reply":"2022-05-06T04:02:44.576052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15, 13))\nax = plt.axes()\nax.set_facecolor('black')\nax = ((df.location.value_counts())[:10]).plot(kind = 'bar', color = custom_colors[2], linewidth = 2, edgecolor = 'white')\nplt.title('Location Count', fontsize = 30)\nplt.xlabel('Location', fontsize = 25)\nplt.ylabel('Count', fontsize = 25)\nax.xaxis.set_tick_params(labelsize = 15, rotation = 30)\nax.yaxis.set_tick_params(labelsize = 15)\nbbox_args = dict(boxstyle = 'round', fc = '0.9')\nfor p in ax.patches:\n        ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x() + 0.15, p.get_height() + 2),\n                   bbox = bbox_args,\n                   color = custom_colors[2],\n                   fontsize = 15)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:02:44.922697Z","iopub.execute_input":"2022-05-06T04:02:44.923196Z","iopub.status.idle":"2022-05-06T04:02:45.285166Z","shell.execute_reply.started":"2022-05-06T04:02:44.923128Z","shell.execute_reply":"2022-05-06T04:02:45.284247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualizing the top 10 locations from where most of the tweets originate from","metadata":{}},{"cell_type":"code","source":"new_df = pd.DataFrame()\nnew_df['location'] = ((df['location'].value_counts())[:10]).index\nnew_df['count'] = ((df['location'].value_counts())[:10]).values\ngeolocator = Nominatim(user_agent = 'Rahil')\ngeocode = RateLimiter(geolocator.geocode, min_delay_seconds = 0.5)\nlat = {}\nlong = {}\nfor i in new_df['location']:\n    location = geocode(i)\n    lat[i] = location.latitude\n    long[i] = location.longitude\nnew_df['latitude'] = new_df['location'].map(lat)\nnew_df['longitude'] = new_df['location'].map(long)\nmap = folium.Map(location = [10.0, 10.0], tiles = 'CartoDB dark_matter', zoom_start = 1.5)\nmarkers = []\ntitle = '''<h1 align = \"center\" style = \"font-size: 35px\"><b>Top 10 Tweet Locations</b></h1>'''\nfor i, r in new_df.iterrows():\n    loss = r['count']\n    if r['count'] > 0:\n        counts = r['count'] * 0.4\n        folium.CircleMarker([float(r['latitude']), float(r['longitude'])], radius = float(counts), color = custom_colors[1], fill = True).add_to(map)\nmap.get_root().html.add_child(folium.Element(title))\nmap","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:02:45.745993Z","iopub.execute_input":"2022-05-06T04:02:45.747252Z","iopub.status.idle":"2022-05-06T04:02:50.672344Z","shell.execute_reply.started":"2022-05-06T04:02:45.747185Z","shell.execute_reply":"2022-05-06T04:02:50.671241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_list_stopwords = stopwords.words('english')\nstopwords = list(stopwords.words('english'))\nstopwords[:10]","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:02:50.674686Z","iopub.execute_input":"2022-05-06T04:02:50.675579Z","iopub.status.idle":"2022-05-06T04:02:50.689755Z","shell.execute_reply.started":"2022-05-06T04:02:50.675528Z","shell.execute_reply":"2022-05-06T04:02:50.688485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_disaster_tweets_length = (df[df['target'] == 0])['text'].str.len()\ndisaster_tweets_length = (df[df['target'] == 1])['text'].str.len()\nprint(non_disaster_tweets_length)\nprint(disaster_tweets_length)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:02:50.691661Z","iopub.execute_input":"2022-05-06T04:02:50.691988Z","iopub.status.idle":"2022-05-06T04:02:50.710417Z","shell.execute_reply.started":"2022-05-06T04:02:50.691947Z","shell.execute_reply":"2022-05-06T04:02:50.709423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize = (30, 15))\nfig.suptitle('Tweet Character Length', fontsize = 45)\n\naxes[0].set_facecolor('black')\naxes[0].hist(non_disaster_tweets_length, color = custom_colors[1], edgecolor = 'white', linewidth = 4)\naxes[0].set_title('Non-Disaster Tweets', fontsize = 40)\naxes[0].set_xlabel('Character Length', fontsize = 35)\naxes[0].set_ylabel('Frequency', fontsize = 35)\naxes[0].xaxis.set_tick_params(labelsize = 30)\naxes[0].yaxis.set_tick_params(labelsize = 30)\n\naxes[1].set_facecolor('black')\naxes[1].hist(disaster_tweets_length, color = custom_colors[2], edgecolor = 'white', linewidth = 4)\naxes[1].set_title('Disaster Tweets', fontsize = 40)\naxes[1].set_xlabel('Character Length', fontsize = 35)\naxes[1].set_ylabel('Frequency', fontsize = 35)\naxes[1].xaxis.set_tick_params(labelsize = 30)\naxes[1].yaxis.set_tick_params(labelsize = 30)\n\nplt.subplots_adjust(wspace = 0.25, hspace = 0.1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:02:52.403403Z","iopub.execute_input":"2022-05-06T04:02:52.404001Z","iopub.status.idle":"2022-05-06T04:02:52.984259Z","shell.execute_reply.started":"2022-05-06T04:02:52.403953Z","shell.execute_reply":"2022-05-06T04:02:52.983576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class tweet_functions:\n    \n    '''Getting the count of different attributes of our tweets'''\n    \n    def __init__(self, column):\n        self.column = column\n        \n    def count_characters(self):\n        return((self.column).apply(lambda word: len(str(word))))\n    \n    def count_words(self):\n        return((self.column).apply(lambda word: len(str(word).split())))\n    \n    def count_urls(self):\n        return((self.column).apply(lambda word: len([url for url in str(word).lower().split() if 'http' in word or 'https' in word])))\n    \n    def count_hashtags(self):\n        return((self.column).apply(lambda word: len([hashtag for hashtag in str(word) if '#' in hashtag])))\n    \n    def count_tags(self):\n        return((self.column).apply(lambda word: len([tag for tag in str(word) if '@' in tag])))\n    \n    def count_stopwords(self):\n        return((self.column).apply(lambda word: len([word for word in str(word).lower().split() if word in stopwords])))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:02:52.985444Z","iopub.execute_input":"2022-05-06T04:02:52.987524Z","iopub.status.idle":"2022-05-06T04:02:52.998654Z","shell.execute_reply.started":"2022-05-06T04:02:52.987486Z","shell.execute_reply":"2022-05-06T04:02:52.997521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows = 3, ncols = 2, figsize = (30, 30))\n\naxes[0][0].set_facecolor('black')\nsns.distplot(tweet_functions((df[df['target'] == 0])['text']).count_characters(), ax = axes[0][0], color = custom_colors[3], label = 'Non-Disaster Tweets', kde_kws = dict(linewidth = 3.5))\nsns.distplot(tweet_functions((df[df['target'] == 1])['text']).count_characters(), ax = axes[0][0], color = custom_colors[4], label = 'Disaster Tweets', kde_kws = dict(linewidth = 3.5))\naxes[0][0].set_title('Character Count', fontsize = 45)\naxes[0][0].set_xlabel('Characters', fontsize = 40)\naxes[0][0].set_ylabel('Density', fontsize = 40)\naxes[0][0].xaxis.set_tick_params(labelsize = 30)\naxes[0][0].yaxis.set_tick_params(labelsize = 30)\naxes[0][0].legend(facecolor = 'black', labelcolor = 'white', prop = {'size': 25}).get_frame().set_linewidth(2.5)\n\naxes[0][1].set_facecolor('black')\nsns.distplot(tweet_functions((df[df['target'] == 0])['text']).count_words(), ax = axes[0][1], color = custom_colors[3], label = 'Non-Disaster Tweets', kde_kws = dict(linewidth = 3.5))\nsns.distplot(tweet_functions((df[df['target'] == 1])['text']).count_words(), ax = axes[0][1], color = custom_colors[4], label = 'Disaster Tweets', kde_kws = dict(linewidth = 3.5))\naxes[0][1].set_title('Word Count', fontsize = 45)\naxes[0][1].set_xlabel('Words', fontsize = 40)\naxes[0][1].set_ylabel('Density', fontsize = 40)\naxes[0][1].xaxis.set_tick_params(labelsize = 30)\naxes[0][1].yaxis.set_tick_params(labelsize = 30)\naxes[0][1].legend(facecolor = 'black', labelcolor = 'white', prop = {'size': 25}).get_frame().set_linewidth(2.5)\n\naxes[1][0].set_facecolor('black')\nsns.distplot(tweet_functions((df[df['target'] == 0])['text']).count_urls(), ax = axes[1][0], color = custom_colors[3], label = 'Non-Disaster Tweets', kde_kws = dict(linewidth = 3.5))\nsns.distplot(tweet_functions((df[df['target'] == 1])['text']).count_urls(), ax = axes[1][0], color = custom_colors[4], label = 'Disaster Tweets', kde_kws = dict(linewidth = 3.5))\naxes[1][0].set_title('URL Count', fontsize = 45)\naxes[1][0].set_xlabel('URLs', fontsize = 40)\naxes[1][0].set_ylabel('Density', fontsize = 40)\naxes[1][0].xaxis.set_tick_params(labelsize = 30)\naxes[1][0].yaxis.set_tick_params(labelsize = 30)\naxes[1][0].legend(facecolor = 'black', labelcolor = 'white', prop = {'size': 25}).get_frame().set_linewidth(2.5)\n\naxes[1][1].set_facecolor('black')\nsns.distplot(tweet_functions((df[df['target'] == 0])['text']).count_hashtags(), ax = axes[1][1], color = custom_colors[3], label = 'Non-Disaster Tweets', kde_kws = dict(linewidth = 3.5))\nsns.distplot(tweet_functions((df[df['target'] == 1])['text']).count_hashtags(), ax = axes[1][1], color = custom_colors[4], label = 'Disaster Tweets', kde_kws = dict(linewidth = 3.5))\naxes[1][1].set_title('Hashtag Count', fontsize = 45)\naxes[1][1].set_xlabel('Hashtags', fontsize = 40)\naxes[1][1].set_ylabel('Density', fontsize = 40)\naxes[1][1].xaxis.set_tick_params(labelsize = 30)\naxes[1][1].yaxis.set_tick_params(labelsize = 30)\naxes[1][1].legend(facecolor = 'black', labelcolor = 'white', prop = {'size': 25}).get_frame().set_linewidth(2.5)\n\naxes[2][0].set_facecolor('black')\nsns.distplot(tweet_functions((df[df['target'] == 0])['text']).count_tags(), ax = axes[2][0], color = custom_colors[3], label = 'Non-Disaster Tweets', kde_kws = dict(linewidth = 3.5))\nsns.distplot(tweet_functions((df[df['target'] == 1])['text']).count_tags(), ax = axes[2][0], color = custom_colors[4], label = 'Disaster Tweets', kde_kws = dict(linewidth = 3.5))\naxes[2][0].set_title('Mention Count', fontsize = 45)\naxes[2][0].set_xlabel('Mentions', fontsize = 40)\naxes[2][0].set_ylabel('Density', fontsize = 40)\naxes[2][0].xaxis.set_tick_params(labelsize = 30)\naxes[2][0].yaxis.set_tick_params(labelsize = 30)\naxes[2][0].legend(facecolor = 'black', labelcolor = 'white', prop = {'size': 25}).get_frame().set_linewidth(2.5)\n\naxes[2][1].set_facecolor('black')\nsns.distplot(tweet_functions((df[df['target'] == 0])['text']).count_stopwords(), ax = axes[2][1], color = custom_colors[3], label = 'Non-Disaster Tweets', kde_kws = dict(linewidth = 3.5))\nsns.distplot(tweet_functions((df[df['target'] == 1])['text']).count_stopwords(), ax = axes[2][1], color = custom_colors[4], label = 'Disaster Tweets', kde_kws = dict(linewidth = 3.5))\naxes[2][1].set_title('Stopword Count', fontsize = 45)\naxes[2][1].set_xlabel('Stopwords', fontsize = 40)\naxes[2][1].set_ylabel('Density', fontsize = 40)\naxes[2][1].xaxis.set_tick_params(labelsize = 30)\naxes[2][1].yaxis.set_tick_params(labelsize = 30)\naxes[2][1].legend(facecolor = 'black', labelcolor = 'white', prop = {'size': 25}).get_frame().set_linewidth(2.5)\n\nplt.subplots_adjust(hspace = 0.5)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-06T04:02:53.419328Z","iopub.execute_input":"2022-05-06T04:02:53.419595Z","iopub.status.idle":"2022-05-06T04:02:56.778237Z","shell.execute_reply.started":"2022-05-06T04:02:53.419568Z","shell.execute_reply":"2022-05-06T04:02:56.777226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing the Tweets","metadata":{}},{"cell_type":"code","source":"def remove_urls(text):\n    urls = re.compile(r'https?://\\S+|www\\.\\S+')\n    return urls.sub(r'', text)\n\ndef remove_HTML(text):\n    html = re.compile('<.*?>')\n    return html.sub(r'', text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile('['\n                           u'\\U0001F600-\\U0001F64F'\n                           u'\\U0001F300-\\U0001F5FF'\n                           u'\\U0001F680-\\U0001F6FF'\n                           u'\\U0001F1E0-\\U0001F1FF'\n                           u'\\U00002702-\\U000027B0'\n                           u'\\U000024C2-\\U0001F251'\n                           ']+', flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_emoticons(text):\n    emoticons = {\n    u\":‑\\)\":\"Happy face or smiley\",\n    u\":\\)\":\"Happy face or smiley\",\n    u\":-\\]\":\"Happy face or smiley\",\n    u\":\\]\":\"Happy face or smiley\",\n    u\":-3\":\"Happy face smiley\",\n    u\":3\":\"Happy face smiley\",\n    u\":->\":\"Happy face smiley\",\n    u\":>\":\"Happy face smiley\",\n    u\"8-\\)\":\"Happy face smiley\",\n    u\":o\\)\":\"Happy face smiley\",\n    u\":-\\}\":\"Happy face smiley\",\n    u\":\\}\":\"Happy face smiley\",\n    u\":-\\)\":\"Happy face smiley\",\n    u\":c\\)\":\"Happy face smiley\",\n    u\":\\^\\)\":\"Happy face smiley\",\n    u\"=\\]\":\"Happy face smiley\",\n    u\"=\\)\":\"Happy face smiley\",\n    u\":‑D\":\"Laughing, big grin or laugh with glasses\",\n    u\":D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8‑D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n    u\"X‑D\":\"Laughing, big grin or laugh with glasses\",\n    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n    u\":-\\)\\)\":\"Very happy\",\n    u\":‑\\(\":\"Frown, sad, andry or pouting\",\n    u\":-\\(\":\"Frown, sad, andry or pouting\",\n    u\":\\(\":\"Frown, sad, andry or pouting\",\n    u\":‑c\":\"Frown, sad, andry or pouting\",\n    u\":c\":\"Frown, sad, andry or pouting\",\n    u\":‑<\":\"Frown, sad, andry or pouting\",\n    u\":<\":\"Frown, sad, andry or pouting\",\n    u\":‑\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\[\":\"Frown, sad, andry or pouting\",\n    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n    u\">:\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\{\":\"Frown, sad, andry or pouting\",\n    u\":@\":\"Frown, sad, andry or pouting\",\n    u\">:\\(\":\"Frown, sad, andry or pouting\",\n    u\":'‑\\(\":\"Crying\",\n    u\":'\\(\":\"Crying\",\n    u\":'‑\\)\":\"Tears of happiness\",\n    u\":'\\)\":\"Tears of happiness\",\n    u\"D‑':\":\"Horror\",\n    u\"D:<\":\"Disgust\",\n    u\"D:\":\"Sadness\",\n    u\"D8\":\"Great dismay\",\n    u\"D;\":\"Great dismay\",\n    u\"D=\":\"Great dismay\",\n    u\"DX\":\"Great dismay\",\n    u\":‑O\":\"Surprise\",\n    u\":O\":\"Surprise\",\n    u\":‑o\":\"Surprise\",\n    u\":o\":\"Surprise\",\n    u\":-0\":\"Shock\",\n    u\"8‑0\":\"Yawn\",\n    u\">:O\":\"Yawn\",\n    u\":-\\*\":\"Kiss\",\n    u\":\\*\":\"Kiss\",\n    u\":X\":\"Kiss\",\n    u\";‑\\)\":\"Wink or smirk\",\n    u\";\\)\":\"Wink or smirk\",\n    u\"\\*-\\)\":\"Wink or smirk\",\n    u\"\\*\\)\":\"Wink or smirk\",\n    u\";‑\\]\":\"Wink or smirk\",\n    u\";\\]\":\"Wink or smirk\",\n    u\";\\^\\)\":\"Wink or smirk\",\n    u\":‑,\":\"Wink or smirk\",\n    u\";D\":\"Wink or smirk\",\n    u\":‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"X‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":‑Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":‑/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":‑\\|\":\"Straight face\",\n    u\":\\|\":\"Straight face\",\n    u\":$\":\"Embarrassed or blushing\",\n    u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\"O:‑\\)\":\"Angel, saint or innocent\",\n    u\"O:\\)\":\"Angel, saint or innocent\",\n    u\"0:‑3\":\"Angel, saint or innocent\",\n    u\"0:3\":\"Angel, saint or innocent\",\n    u\"0:‑\\)\":\"Angel, saint or innocent\",\n    u\"0:\\)\":\"Angel, saint or innocent\",\n    u\":‑b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n    u\">:‑\\)\":\"Evil or devilish\",\n    u\">:\\)\":\"Evil or devilish\",\n    u\"\\}:‑\\)\":\"Evil or devilish\",\n    u\"\\}:\\)\":\"Evil or devilish\",\n    u\"3:‑\\)\":\"Evil or devilish\",\n    u\"3:\\)\":\"Evil or devilish\",\n    u\">;\\)\":\"Evil or devilish\",\n    u\"\\|;‑\\)\":\"Cool\",\n    u\"\\|‑O\":\"Bored\",\n    u\":‑J\":\"Tongue-in-cheek\",\n    u\"#‑\\)\":\"Party all night\",\n    u\"%‑\\)\":\"Drunk or confused\",\n    u\"%\\)\":\"Drunk or confused\",\n    u\":-###..\":\"Being sick\",\n    u\":###..\":\"Being sick\",\n    u\"<:‑\\|\":\"Dump\",\n    u\"\\(>_<\\)\":\"Troubled\",\n    u\"\\(>_<\\)>\":\"Troubled\",\n    u\"\\(';'\\)\":\"Baby\",\n    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-\\)zzz\":\"Sleeping\",\n    u\"\\(\\^_-\\)\":\"Wink\",\n    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n    u\"\\(\\+o\\+\\)\":\"Confused\",\n    u\"\\(o\\|o\\)\":\"Ultraman\",\n    u\"\\^_\\^\":\"Joyful\",\n    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n    u\"\\(\\^O\\^\\)／\":\"Joyful\",\n    u\"\\(\\^o\\^\\)／\":\"Joyful\",\n    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"\\('_'\\)\":\"Sad or Crying\",\n    u\"\\(/_;\\)\":\"Sad or Crying\",\n    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n    u\"\\(;_;\":\"Sad of Crying\",\n    u\"\\(;_:\\)\":\"Sad or Crying\",\n    u\"\\(;O;\\)\":\"Sad or Crying\",\n    u\"\\(:_;\\)\":\"Sad or Crying\",\n    u\"\\(ToT\\)\":\"Sad or Crying\",\n    u\";_;\":\"Sad or Crying\",\n    u\";-;\":\"Sad or Crying\",\n    u\";n;\":\"Sad or Crying\",\n    u\";;\":\"Sad or Crying\",\n    u\"Q\\.Q\":\"Sad or Crying\",\n    u\"T\\.T\":\"Sad or Crying\",\n    u\"QQ\":\"Sad or Crying\",\n    u\"Q_Q\":\"Sad or Crying\",\n    u\"\\(-\\.-\\)\":\"Shame\",\n    u\"\\(-_-\\)\":\"Shame\",\n    u\"\\(一一\\)\":\"Shame\",\n    u\"\\(；一_一\\)\":\"Shame\",\n    u\"\\(=_=\\)\":\"Tired\",\n    u\"\\(=\\^\\·\\^=\\)\":\"cat\",\n    u\"\\(=\\^\\·\\·\\^=\\)\":\"cat\",\n    u\"=_\\^=\t\":\"cat\",\n    u\"\\(\\.\\.\\)\":\"Looking down\",\n    u\"\\(\\._\\.\\)\":\"Looking down\",\n    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n    u\"\\(\\・\\・?\":\"Confusion\",\n    u\"\\(?_?\\)\":\"Confusion\",\n    u\">\\^_\\^<\":\"Normal Laugh\",\n    u\"<\\^!\\^>\":\"Normal Laugh\",\n    u\"\\^/\\^\":\"Normal Laugh\",\n    u\"\\（\\*\\^_\\^\\*）\" :\"Normal Laugh\",\n    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n    u\"\\(\\^—\\^\\）\":\"Normal Laugh\",\n    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n    u\"\\（\\^—\\^\\）\":\"Waving\",\n    u\"\\(;_;\\)/~~~\":\"Waving\",\n    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\":\"Waving\",\n    u\"\\(T_T\\)/~~~\":\"Waving\",\n    u\"\\(ToT\\)/~~~\":\"Waving\",\n    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n    u\"\\(\\*_\\*\\)\":\"Amazed\",\n    u\"\\(\\*_\\*;\":\"Amazed\",\n    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n    u'\\(-\"-\\)':\"Worried\",\n    u\"\\(ーー;\\)\":\"Worried\",\n    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n    u\"\\(\\＾ｖ\\＾\\)\":\"Happy\",\n    u\"\\(\\＾ｕ\\＾\\)\":\"Happy\",\n    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n    u\"\\(\\^O\\^\\)\":\"Happy\",\n    u\"\\(\\^o\\^\\)\":\"Happy\",\n    u\"\\)\\^o\\^\\(\":\"Happy\",\n    u\":O o_O\":\"Surprised\",\n    u\"o_0\":\"Surprised\",\n    u\"o\\.O\":\"Surpised\",\n    u\"\\(o\\.o\\)\":\"Surprised\",\n    u\"oO\":\"Surprised\",\n    u\"\\(\\*￣m￣\\)\":\"Dissatisfied\",\n    u\"\\(‘A`\\)\":\"Snubbed or Deflated\"\n    }\n    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in emoticons) + u')')\n    return emoticon_pattern.sub(r'', text)\n\ndef remove_mentions(text):\n    mentions = re.compile('@[A-Za-z0-9_]+')\n    return mentions.sub(r'', text)\n\ndef word_lemmatizer(text):\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-06T04:02:58.31188Z","iopub.execute_input":"2022-05-06T04:02:58.312177Z","iopub.status.idle":"2022-05-06T04:02:58.349774Z","shell.execute_reply.started":"2022-05-06T04:02:58.312128Z","shell.execute_reply":"2022-05-06T04:02:58.348757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].str.lower() # convert to lowercase\ndf['text'] = df['text'].apply(lambda text: remove_urls(text)) # remove URLs\ndf['text'] = df['text'].apply(lambda text: remove_HTML(text)) # remove HTML tags\ndf['text'] = df['text'].str.translate(str.maketrans('', '', string.punctuation)) # remove punctuations\ndf['text'] = df['text'].apply(lambda text: ' '.join([word for word in str(text).split() if word not in stopwords])) # remove stopwords\ndf['text'] = df['text'].apply(lambda text: remove_emoji(text)) # remove emojis\ndf['text'] = df['text'].apply(lambda text: remove_emoticons(text)) # remove emoticons\ndf['text'] = df['text'].apply(lambda text: remove_mentions(text)) # remove mentions\ndf['text'] = df['text'].apply(lambda text: word_lemmatizer(text)) # lemmatize words\ndf['text'] = df['text'].apply(lambda text: th.cont_exp(text)) # convert i'm to i am, you're to you are, etc\ndf['text']","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:02:58.983192Z","iopub.execute_input":"2022-05-06T04:02:58.984142Z","iopub.status.idle":"2022-05-06T04:03:13.515802Z","shell.execute_reply.started":"2022-05-06T04:02:58.98409Z","shell.execute_reply":"2022-05-06T04:03:13.514822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = Counter()\nfor text in df['text'].values:\n    for word in text.split():\n        counter[word] += 1\ncounter.most_common(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:03:13.517881Z","iopub.execute_input":"2022-05-06T04:03:13.518419Z","iopub.status.idle":"2022-05-06T04:03:13.570898Z","shell.execute_reply.started":"2022-05-06T04:03:13.518367Z","shell.execute_reply":"2022-05-06T04:03:13.570068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = dict(sorted(counter.items(), key = lambda x: x[1] ,reverse = True)[:10])\nwords = list(data.keys())\nfrequency = list(data.values())\n\nfig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (15, 15))\nax.set_facecolor('black')\nax = sns.barplot(x = frequency, y = words, color = '#8699A7', edgecolor = 'white', linewidth = 2)\nplt.title('Word Frequency', fontsize = 35)\nplt.xlabel('Frequency', fontsize = 30)\nplt.ylabel('Words', fontsize = 30)\nplt.xticks(size = 20)\nplt.yticks(size = 20)\nbbox_args = dict(boxstyle = 'round', fc = '0.9')\nfor p in ax.patches:\n    width = p.get_width()\n    plt.text(9.5 + p.get_width(), p.get_y() + 0.5 * p.get_height(), '{:1.0f}'.format(width), \n             ha = 'center', \n             va = 'center', \n             color = 'black', \n             bbox = bbox_args, \n             fontsize = 15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:03:14.805628Z","iopub.execute_input":"2022-05-06T04:03:14.806575Z","iopub.status.idle":"2022-05-06T04:03:15.211293Z","shell.execute_reply.started":"2022-05-06T04:03:14.80653Z","shell.execute_reply":"2022-05-06T04:03:15.21003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Ngrams","metadata":{}},{"cell_type":"code","source":"def generate_ngrams(text, n_gram = 0):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in non_list_stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\ndef generate_ngram_plots(n_gram):\n    non_disaster_ngrams = defaultdict(int)\n    disaster_ngrams = defaultdict(int)\n\n    for tweet in df[df['target'] == 0]['text']:\n        for word in generate_ngrams(tweet, n_gram = n_gram):\n            non_disaster_ngrams[word] += 1\n\n    for tweet in df[df['target'] == 1]['text']:\n        for word in generate_ngrams(tweet, n_gram = n_gram):\n            disaster_ngrams[word] += 1\n\n    non_disaster_ngram_data = dict(sorted(non_disaster_ngrams.items(), key = lambda x: x[1], reverse = True)[:10])\n    non_disaster_ngram_words = list(non_disaster_ngram_data.keys())\n    non_disaster_ngram_frequency = list(non_disaster_ngram_data.values())\n\n    disaster_ngram_data = dict(sorted(disaster_ngrams.items(), key = lambda x: x[1], reverse = True)[:10])\n    disaster_ngram_words = list(disaster_ngram_data.keys())\n    disaster_ngram_frequency = list(disaster_ngram_data.values())\n\n    fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30, 20))\n\n    axes[0].set_facecolor('black')\n    sns.barplot(x = non_disaster_ngram_frequency, y = non_disaster_ngram_words, ax = axes[0], color = '#0057B1', edgecolor = 'white', linewidth = 2)\n    if(n_gram == 1):\n        axes[0].set_title('Non-Disaster Unigrams', fontsize = 45)\n    if(n_gram == 2):\n        axes[0].set_title('Non-Disaster Bigrams', fontsize = 45)\n    if(n_gram == 3):\n        axes[0].set_title('Non-Disaster Trigrams', fontsize = 45)\n    axes[0].set_xlabel('Count', fontsize = 40)\n    axes[0].set_ylabel('Words', fontsize = 40)\n    if(n_gram == 1):\n        axes[0].xaxis.set_tick_params(labelsize = 30)\n        axes[0].yaxis.set_tick_params(labelsize = 30)\n    elif(n_gram == 2):\n        axes[0].xaxis.set_tick_params(labelsize = 20)\n        axes[0].yaxis.set_tick_params(labelsize = 20)\n    else:\n        axes[0].xaxis.set_tick_params(labelsize = 18)\n        axes[0].yaxis.set_tick_params(labelsize = 18)\n    for p in axes[0].patches:\n        width = p.get_width()\n        if(n_gram == 1 or n_gram == 2):\n            axes[0].text(0.75 + p.get_width(), p.get_y() + 0.5 * p.get_height(), '{:1.0f}'.format(width), \n                     ha = 'center', \n                     va = 'center', \n                     color = 'blue', \n                     bbox = bbox_args, \n                     fontsize = 25)\n        if(n_gram == 3):\n            axes[0].text(0.6 + p.get_width(), p.get_y() + 0.5 * p.get_height(), '{:1.0f}'.format(width), \n                     ha = 'center', \n                     va = 'center', \n                     color = 'blue', \n                     bbox = bbox_args, \n                     fontsize = 22)\n\n    axes[1].set_facecolor('black')\n    sns.barplot(x = disaster_ngram_frequency, y = disaster_ngram_words, ax = axes[1], palette = [custom_colors[1]], edgecolor = 'white', linewidth = 2)\n    if(n_gram == 1):\n        axes[1].set_title('Disaster Unigrams', fontsize = 45)\n    if(n_gram == 2):\n        axes[1].set_title('Disaster Bigrams', fontsize = 45)\n    if(n_gram == 3):\n        axes[1].set_title('Disaster Trigrams', fontsize = 45)\n    axes[1].set_xlabel('Count', fontsize = 40)\n    axes[1].set_ylabel('Words', fontsize = 40)\n    if(n_gram == 1):\n        axes[1].xaxis.set_tick_params(labelsize = 30)\n        axes[1].yaxis.set_tick_params(labelsize = 30)\n    elif(n_gram == 2):\n        axes[1].xaxis.set_tick_params(labelsize = 20)\n        axes[1].yaxis.set_tick_params(labelsize = 20)\n    else:\n        axes[1].xaxis.set_tick_params(labelsize = 18)\n        axes[1].yaxis.set_tick_params(labelsize = 18)\n    for p in axes[1].patches:\n        width = p.get_width()\n        if(n_gram == 1 or n_gram == 2):\n            axes[1].text(0.8 + p.get_width(), p.get_y() + 0.5 * p.get_height(), '{:1.0f}'.format(width), \n                     ha = 'center', \n                     va = 'center', \n                     color = 'red', \n                     bbox = bbox_args, \n                     fontsize = 25)\n        if(n_gram == 3):\n            axes[1].text(0.6 + p.get_width(), p.get_y() + 0.5 * p.get_height(), '{:1.0f}'.format(width), \n                     ha = 'center', \n                     va = 'center', \n                     color = 'red', \n                     bbox = bbox_args, \n                     fontsize = 22)\n    if(n_gram == 1 or n_gram == 2):\n        plt.subplots_adjust(wspace = 0.4)\n    if(n_gram == 3):\n        plt.subplots_adjust(wspace = 0.6)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-06T04:03:16.424846Z","iopub.execute_input":"2022-05-06T04:03:16.425124Z","iopub.status.idle":"2022-05-06T04:03:16.458929Z","shell.execute_reply.started":"2022-05-06T04:03:16.425095Z","shell.execute_reply":"2022-05-06T04:03:16.457875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unigrams","metadata":{}},{"cell_type":"code","source":"generate_ngram_plots(1)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:03:17.458203Z","iopub.execute_input":"2022-05-06T04:03:17.459139Z","iopub.status.idle":"2022-05-06T04:03:18.538859Z","shell.execute_reply.started":"2022-05-06T04:03:17.459095Z","shell.execute_reply":"2022-05-06T04:03:18.538227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bigrams","metadata":{}},{"cell_type":"code","source":"generate_ngram_plots(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:03:20.448121Z","iopub.execute_input":"2022-05-06T04:03:20.449106Z","iopub.status.idle":"2022-05-06T04:03:21.606035Z","shell.execute_reply.started":"2022-05-06T04:03:20.449053Z","shell.execute_reply":"2022-05-06T04:03:21.605207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trigrams","metadata":{}},{"cell_type":"code","source":"generate_ngram_plots(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:03:23.602615Z","iopub.execute_input":"2022-05-06T04:03:23.603103Z","iopub.status.idle":"2022-05-06T04:03:24.792876Z","shell.execute_reply.started":"2022-05-06T04:03:23.603068Z","shell.execute_reply":"2022-05-06T04:03:24.791744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wordcloud of Tweets","metadata":{}},{"cell_type":"code","source":"wordcloud = WordCloud(width = 1400, height = 600, background_color = 'black').generate(''.join(text for text in df['text']))\nplt.figure(figsize = (20, 10))\nplt.title('Wordcloud Visualization of Tweets', fontsize = 30)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:03:26.687854Z","iopub.execute_input":"2022-05-06T04:03:26.688126Z","iopub.status.idle":"2022-05-06T04:03:29.832842Z","shell.execute_reply.started":"2022-05-06T04:03:26.688098Z","shell.execute_reply":"2022-05-06T04:03:29.831922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:03:31.485781Z","iopub.execute_input":"2022-05-06T04:03:31.486123Z","iopub.status.idle":"2022-05-06T04:03:31.503721Z","shell.execute_reply.started":"2022-05-06T04:03:31.486088Z","shell.execute_reply":"2022-05-06T04:03:31.502729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df['text']\ny = df['target']\nprint(X)\nprint(y)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:03:32.875741Z","iopub.execute_input":"2022-05-06T04:03:32.876084Z","iopub.status.idle":"2022-05-06T04:03:32.885449Z","shell.execute_reply.started":"2022-05-06T04:03:32.876052Z","shell.execute_reply":"2022-05-06T04:03:32.884702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the Data","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:03:34.251785Z","iopub.execute_input":"2022-05-06T04:03:34.252826Z","iopub.status.idle":"2022-05-06T04:03:34.267096Z","shell.execute_reply.started":"2022-05-06T04:03:34.252772Z","shell.execute_reply":"2022-05-06T04:03:34.26626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train)\nprint(X_test)\nprint(y_train)\nprint(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:03:35.014895Z","iopub.execute_input":"2022-05-06T04:03:35.015704Z","iopub.status.idle":"2022-05-06T04:03:35.02719Z","shell.execute_reply.started":"2022-05-06T04:03:35.015661Z","shell.execute_reply":"2022-05-06T04:03:35.026227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_url = 'https://tfhub.dev/google/universal-sentence-encoder/4'\nsentence_encoder_layer = hub.KerasLayer(model_url, input_shape = [], dtype = tf.string, trainable = False)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-06T04:04:11.908106Z","iopub.execute_input":"2022-05-06T04:04:11.908437Z","iopub.status.idle":"2022-05-06T04:04:35.276932Z","shell.execute_reply.started":"2022-05-06T04:04:11.908395Z","shell.execute_reply":"2022-05-06T04:04:35.275413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the Model","metadata":{}},{"cell_type":"code","source":"model = keras.Sequential([\n    sentence_encoder_layer,\n    layers.Dense(128, activation = 'relu'),\n    layers.Dense(64, activation = 'relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\nmodel.compile(\n    loss = 'binary_crossentropy',\n    optimizer = keras.optimizers.Adam(lr = 1e-4),\n    metrics = ['accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:12:46.511391Z","iopub.execute_input":"2022-05-06T04:12:46.511762Z","iopub.status.idle":"2022-05-06T04:12:46.655327Z","shell.execute_reply.started":"2022-05-06T04:12:46.511727Z","shell.execute_reply":"2022-05-06T04:12:46.654263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:12:46.936262Z","iopub.execute_input":"2022-05-06T04:12:46.936571Z","iopub.status.idle":"2022-05-06T04:12:46.953435Z","shell.execute_reply.started":"2022-05-06T04:12:46.936543Z","shell.execute_reply":"2022-05-06T04:12:46.952418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Learning Rate Reduction","metadata":{}},{"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor = 'val_accuracy', patience = 2, verbose = 1, factor = 0.5, min_lr = 0.00001)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:12:48.273236Z","iopub.execute_input":"2022-05-06T04:12:48.27371Z","iopub.status.idle":"2022-05-06T04:12:48.279577Z","shell.execute_reply.started":"2022-05-06T04:12:48.273657Z","shell.execute_reply":"2022-05-06T04:12:48.278679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model","metadata":{}},{"cell_type":"code","source":"classifier = model.fit(\n    X_train,\n    y_train,\n    epochs = 50,\n    validation_data = (X_test, y_test),\n    callbacks = [reduce_lr]\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-06T04:12:49.6452Z","iopub.execute_input":"2022-05-06T04:12:49.645548Z","iopub.status.idle":"2022-05-06T04:15:09.239192Z","shell.execute_reply.started":"2022-05-06T04:12:49.645507Z","shell.execute_reply":"2022-05-06T04:15:09.238207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Performance","metadata":{}},{"cell_type":"code","source":"def model_performance_graphs():\n    \n    fig, axes = plt.subplots(1, 2, figsize = (15, 8))\n\n    axes[0].plot(classifier.epoch, classifier.history['accuracy'], label = 'acc')\n    axes[0].plot(classifier.epoch, classifier.history['val_accuracy'], label = 'val_acc')\n    axes[0].set_title('Accuracy vs Epochs', fontsize = 20)\n    axes[0].set_xlabel('Epochs', fontsize = 15)\n    axes[0].set_ylabel('Accuracy', fontsize = 15)\n    axes[0].legend()\n\n    axes[1].plot(classifier.epoch, classifier.history['loss'], label = 'loss')\n    axes[1].plot(classifier.epoch, classifier.history['val_loss'], label=\"val_loss\")\n    axes[1].set_title(\"Loss Curve\",fontsize=18)\n    axes[1].set_xlabel(\"Epochs\",fontsize=15)\n    axes[1].set_ylabel(\"Loss\",fontsize=15)\n    axes[1].legend()\n\n    plt.show()\n    \nmodel_performance_graphs()","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-05-06T04:15:12.053601Z","iopub.execute_input":"2022-05-06T04:15:12.054035Z","iopub.status.idle":"2022-05-06T04:15:12.465771Z","shell.execute_reply.started":"2022-05-06T04:15:12.053999Z","shell.execute_reply":"2022-05-06T04:15:12.464518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:15:22.391933Z","iopub.execute_input":"2022-05-06T04:15:22.39226Z","iopub.status.idle":"2022-05-06T04:15:24.385048Z","shell.execute_reply.started":"2022-05-06T04:15:22.392229Z","shell.execute_reply":"2022-05-06T04:15:24.384106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:15:24.387009Z","iopub.execute_input":"2022-05-06T04:15:24.387358Z","iopub.status.idle":"2022-05-06T04:15:24.942611Z","shell.execute_reply.started":"2022-05-06T04:15:24.387316Z","shell.execute_reply":"2022-05-06T04:15:24.941535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making the Predictions","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntest_df = test_df[['id', 'text']]\npred = model.predict(test_df['text'])\nprint(pred)\npred = tf.squeeze(tf.round((pred)))\nprint(np.array(pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:15:29.134907Z","iopub.execute_input":"2022-05-06T04:15:29.135542Z","iopub.status.idle":"2022-05-06T04:15:31.184699Z","shell.execute_reply.started":"2022-05-06T04:15:29.135493Z","shell.execute_reply":"2022-05-06T04:15:31.183355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the Submission File","metadata":{}},{"cell_type":"code","source":"test_df['target'] = pred\ntest_df['target'] = test_df['target'].astype(int)\ntest_df = test_df[['id', 'target']]\ntest_df.to_csv('submission.csv', index = False)\ntest_df","metadata":{"execution":{"iopub.status.busy":"2022-05-06T04:15:33.63326Z","iopub.execute_input":"2022-05-06T04:15:33.633589Z","iopub.status.idle":"2022-05-06T04:15:33.664361Z","shell.execute_reply.started":"2022-05-06T04:15:33.633556Z","shell.execute_reply":"2022-05-06T04:15:33.663437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n> - https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert#2.-Meta-Features\n> - https://www.kaggle.com/code/shahules/basic-eda-cleaning-and-glove\n> - https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing/notebook\n> - https://www.kaggle.com/code/ratan123/start-from-here-disaster-tweets-eda-basic-model#4.-Exploring-location-column","metadata":{"execution":{"iopub.status.busy":"2022-04-25T02:12:28.561622Z","iopub.execute_input":"2022-04-25T02:12:28.562335Z","iopub.status.idle":"2022-04-25T02:12:28.569901Z","shell.execute_reply.started":"2022-04-25T02:12:28.562279Z","shell.execute_reply":"2022-04-25T02:12:28.568876Z"}}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\" role=\"alert\">🚧 Work in Progress 🚧</div>","metadata":{}}]}