{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Problems with NetworkX\n\n<img\n  src=\"https://networkx.org/_static/networkx_logo.svg\"\n  alt=\"nx\"\n  width=\"300\"\n  height=\"300\"\n/>\n\n[NetworkX](https://networkx.org/) is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\n\nNetworkX has an easy and intuitive API but it comes with a major flaw. All the data in NetworkX is stored in-memory. This will fill up the RAM pretty fast and thus lead to out-of-memory (OOM) errors when working with large graphs. Another major problem with NetworkX is that it does not allow for ad-hoc graph querying on edges and vertices. For example, if one wanted to find all the edges connected to a particular set of nodes, we would have to resort to manual looping which is very slow when working with large data.\n\nWe can avoid this two problems entirely by using some solution which accounts for both the above mentioned problems. We can use [Apache Spark](https://spark.apache.org/) for this. Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Scala, Java, Python, and R, and an optimized engine that supports general computation graphs for data analysis. It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing.\n\n<img\n  src=\"https://spark.apache.org/images/spark-logo.png\"\n  alt=\"spark\"\n  width=\"180\"\n  height=\"180\"\n  style=\"fill: black\"\n/>\n\nThis notebook is a tutorial on how to export graph data from NetworkX and import it into Apache Spark.","metadata":{}},{"cell_type":"code","source":"import networkx as nx","metadata":{"execution":{"iopub.status.busy":"2022-04-23T07:02:38.769107Z","iopub.execute_input":"2022-04-23T07:02:38.769524Z","iopub.status.idle":"2022-04-23T07:02:38.775505Z","shell.execute_reply.started":"2022-04-23T07:02:38.76948Z","shell.execute_reply":"2022-04-23T07:02:38.7743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use an already existing NetworkX graph, create a new graph, or generate a random graph. This method works independently to any method by which graph was created.\n\nFor the purpose of this tutorial, we will generate a random graph. The full list of [graph generators](https://networkx.org/documentation/stable/reference/generators.html) can be found in the documentation.","metadata":{}},{"cell_type":"code","source":"g = nx.caveman_graph(120, 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T07:22:17.2385Z","iopub.execute_input":"2022-04-23T07:22:17.238809Z","iopub.status.idle":"2022-04-23T07:22:18.360031Z","shell.execute_reply.started":"2022-04-23T07:22:17.238774Z","shell.execute_reply":"2022-04-23T07:22:18.35903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Spark requires a DataFrame to work with. We can create a pandas DataFrame from NetworkX directly and then read it into Spark.\nOnce we have our graph/network, we use the [`nx.to_pandas_edgelist`](https://networkx.org/documentation/stable/reference/generated/networkx.convert_matrix.from_pandas_edgelist.html) function. The function returns a `pd.DataFrame` object.","metadata":{}},{"cell_type":"code","source":"df = nx.to_pandas_edgelist(g)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T07:22:19.40637Z","iopub.execute_input":"2022-04-23T07:22:19.406684Z","iopub.status.idle":"2022-04-23T07:22:25.2387Z","shell.execute_reply.started":"2022-04-23T07:22:19.406651Z","shell.execute_reply":"2022-04-23T07:22:25.23794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-04-23T07:22:25.242788Z","iopub.execute_input":"2022-04-23T07:22:25.243085Z","iopub.status.idle":"2022-04-23T07:22:25.256054Z","shell.execute_reply.started":"2022-04-23T07:22:25.243044Z","shell.execute_reply":"2022-04-23T07:22:25.254996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Depending on the type of data, we can now save it in multiple formats like CSV, JSON, and Apache Parquet.","metadata":{}},{"cell_type":"code","source":"df.to_csv(\"graph.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T07:23:56.804509Z","iopub.execute_input":"2022-04-23T07:23:56.804834Z","iopub.status.idle":"2022-04-23T07:23:57.859085Z","shell.execute_reply.started":"2022-04-23T07:23:56.804802Z","shell.execute_reply":"2022-04-23T07:23:57.858089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Apache Spark\n\nTo install Apache Spark, use the following command","metadata":{}},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2022-04-23T07:29:44.898534Z","iopub.execute_input":"2022-04-23T07:29:44.898881Z","iopub.status.idle":"2022-04-23T07:29:55.324009Z","shell.execute_reply.started":"2022-04-23T07:29:44.898832Z","shell.execute_reply":"2022-04-23T07:29:55.322802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will now need to initialize a Spark session before we can work with the data.","metadata":{}},{"cell_type":"code","source":"import pyspark.sql as sql","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spark = (\n    sql.SparkSession.builder.config(\"spark.driver.memory\", \"8g\")\n    .config(\"spark.sql.execution.arrow.pyspark.enable\", \"true\")\n    .config(\"spark.driver.maxResultSize\", \"2g\")\n    .getOrCreate()\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T07:30:18.591513Z","iopub.execute_input":"2022-04-23T07:30:18.592012Z","iopub.status.idle":"2022-04-23T07:30:18.602297Z","shell.execute_reply.started":"2022-04-23T07:30:18.591962Z","shell.execute_reply":"2022-04-23T07:30:18.601441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = spark.read.csv(\"graph.csv\", inferSchema=True, header=True)\ndf.createOrReplaceTempView(\"graph\")","metadata":{"execution":{"iopub.status.busy":"2022-04-23T07:52:35.798072Z","iopub.execute_input":"2022-04-23T07:52:35.798389Z","iopub.status.idle":"2022-04-23T07:52:37.806665Z","shell.execute_reply.started":"2022-04-23T07:52:35.798357Z","shell.execute_reply":"2022-04-23T07:52:37.805688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have access to the entire data without ever having to worry about running into OOM errors.\nThe reason this works is Spark is designed to work with the data directly on the disc without having\nto read it into memory. It is able to query the data in distributed way and can scale for even petabytes of data.\n\nWe can now query the data as if it is a table in SQL.","metadata":{}},{"cell_type":"code","source":"# Count of edges from node 15\nspark.sql(\n    \"\"\"\nselect\n  count(target)\nfrom\n  graph\nwhere\n  source = 15\n    \"\"\"\n).collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T07:56:32.741793Z","iopub.execute_input":"2022-04-23T07:56:32.742224Z","iopub.status.idle":"2022-04-23T07:56:33.896349Z","shell.execute_reply.started":"2022-04-23T07:56:32.742181Z","shell.execute_reply":"2022-04-23T07:56:33.895089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of nodes connect to node 35\nspark.sql(\n    \"\"\"\nselect\n  source\nfrom\n  graph\nwhere\n  target = 35\n    \"\"\"\n).collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T07:57:18.866369Z","iopub.execute_input":"2022-04-23T07:57:18.866719Z","iopub.status.idle":"2022-04-23T07:57:19.531178Z","shell.execute_reply.started":"2022-04-23T07:57:18.866686Z","shell.execute_reply":"2022-04-23T07:57:19.530226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of nodes connected with more than 20 nodes and the count of connections\nspark.sql(\n    \"\"\"\nselect\n  source,\n  count(target) cnt\nfrom\n  graph\ngroup by\n  source\nhaving\n  cnt > 20\nlimit\n  20\n    \"\"\"\n).collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T07:59:58.494511Z","iopub.execute_input":"2022-04-23T07:59:58.494918Z","iopub.status.idle":"2022-04-23T07:59:59.687777Z","shell.execute_reply.started":"2022-04-23T07:59:58.49488Z","shell.execute_reply":"2022-04-23T07:59:59.686754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The documentation of Apache Spark contains the full list of functions available for use. Refer to this notebook for a reference on how to use Apache Spark for large scale data analysis - [Exploratory Data Analysis - Taylor Swift](https://www.kaggle.com/code/aneridalwadi/exploratory-data-analysis-taylor-swift)","metadata":{}},{"cell_type":"markdown","source":"## cuGraph\n\n\nSpark resorts to using SQL for data manipulation which can often turn out to be unintuitive and hard to use and NetworkX runs only on CPU.\nThis problem is solved by using `cuGraph` library created by NVIDIA which aims to do data processing and analysis on GPUs directly. It has an API similar to NetworkX and supposed to work as a drop-in replacement. It won't save us from OOM errors because now it will use GPU memory instead but it can speed up complicated graph calculations by orders of magnitude.\n\n<img\n  src=\"https://github.com/rapidsai/cugraph/blob/main/img/rapids_logo.png?raw=true\"\n  alt=\"rapids\"\n  width=\"200\"\n  height=\"200\"\n/>","metadata":{}},{"cell_type":"code","source":"%conda install -c nvidia -c rapidsai -c numba -c conda-forge cugraph","metadata":{"execution":{"iopub.status.busy":"2022-04-23T10:19:47.507863Z","iopub.execute_input":"2022-04-23T10:19:47.508117Z","iopub.status.idle":"2022-04-23T10:20:29.599784Z","shell.execute_reply.started":"2022-04-23T10:19:47.508083Z","shell.execute_reply":"2022-04-23T10:20:29.599004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cudf\nimport cugraph\n\n# read data into a cuDF DataFrame using read_csv\ngdf = cudf.read_csv(\"graph.csv\", dtype=[\"int32\", \"int32\"])\n\n# We now have data as edge pairs\n# create a Graph using the source (src) and destination (dst) vertex pairs\nG = cugraph.Graph()\nG.from_cudf_edgelist(gdf, source=\"source\", destination=\"target\")\n\n# Let's now get the PageRank score of each vertex by calling cugraph.pagerank\ncugraph.pagerank(G)","metadata":{},"execution_count":null,"outputs":[]}]}