{"cells":[{"metadata":{"_uuid":"bf25a9557b7ccd30927631a5d65bc2ac559e7e5f"},"cell_type":"markdown","source":"**This notebook compares the performance of common regression algorithms for the same dataset.\n1. Decision Trees\n2. Random Forest\n3. XGBoost**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"32176f6a9a29eb0a88f9b086f5f15693fb6b796e"},"cell_type":"code","source":"### importing the required libraries\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"448c010b93d1a0394720bdd12139c1c79a462803"},"cell_type":"code","source":"## importing the data and selecting the independent and independent variables \n\ntrain_data = pd.read_csv('../input/train.csv')\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7b98cb64958b0d5dd482c12359b0e9cfdceb0e5","collapsed":true},"cell_type":"code","source":"## splitting the data for training and testing and cleaning it using imputation \n\ntrain_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\nmy_imputer = Imputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.fit_transform(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e60a800475647f4ab177ab2afa83c2d507b9a865","collapsed":true},"cell_type":"code","source":"## making predictions using the Decision Tree algorithm \n\ndecision_model = DecisionTreeRegressor()  \ndecision_model.fit(train_X, train_y) \npredicted_decision_trees = decision_model.predict(test_X)\nprint (\"Mean Absolute Error using Decision Tress :\", mean_absolute_error(test_y, predicted_decision_trees))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8481f1cf093d205fb06d5e536fce1818bb060fd2","collapsed":true},"cell_type":"code","source":"## making predictions using the Random Forest algorithm \n\nforest_model = RandomForestRegressor(n_estimators=100, max_depth=10)\nforest_model.fit(train_X, train_y )\npredicted_random_forest = forest_model.predict(test_X)\nprint(\"Mean Absolute Error using Random Forest:\", mean_absolute_error(test_y, predicted_random_forest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddc3948d79911fc4b5c6a6ec3c23538b5b33033a","collapsed":true},"cell_type":"code","source":"## making predictions using the XGBoost algorithm \n\nxg_model = XGBRegressor(n_estimators=100)\nxg_model.fit(train_X, train_y)\npredicted_XGBoost = xg_model.predict(test_X)\nprint(\"Mean Absolute Error using XGBoost: \", mean_absolute_error(test_y, predicted_XGBoost))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"815981f14adf12aa26af632a94c9067ea98dbb66"},"cell_type":"markdown","source":"<strong>As evident from the above metrics, we can conclude that using a combination of Decision Trees (Random Forest) can prove to be very useful in bringing down the errors (increaing accuracy). Also, further improvement in the results can be made using some kind of boosting algorithm.</strong>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}