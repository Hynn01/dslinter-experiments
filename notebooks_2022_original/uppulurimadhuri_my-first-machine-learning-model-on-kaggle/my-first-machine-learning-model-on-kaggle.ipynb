{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-05T18:04:38.535262Z","iopub.execute_input":"2022-05-05T18:04:38.535981Z","iopub.status.idle":"2022-05-05T18:04:38.573564Z","shell.execute_reply.started":"2022-05-05T18:04:38.535847Z","shell.execute_reply":"2022-05-05T18:04:38.572639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#In this notebook I only used Logistic Regression the basic algorithm for classification.\n# Let us make the best out of it and gain maximum accuracy only usng LR\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv') # loading train data\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:04:38.575047Z","iopub.execute_input":"2022-05-05T18:04:38.575294Z","iopub.status.idle":"2022-05-05T18:04:38.631261Z","shell.execute_reply.started":"2022-05-05T18:04:38.575258Z","shell.execute_reply":"2022-05-05T18:04:38.630239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/titanic/test.csv')\ntest","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:04:38.633231Z","iopub.execute_input":"2022-05-05T18:04:38.633675Z","iopub.status.idle":"2022-05-05T18:04:38.664317Z","shell.execute_reply.started":"2022-05-05T18:04:38.633605Z","shell.execute_reply":"2022-05-05T18:04:38.663296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.isna().sum()) # Finding sum of nan values in each Series","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:04:38.665771Z","iopub.execute_input":"2022-05-05T18:04:38.666191Z","iopub.status.idle":"2022-05-05T18:04:38.673544Z","shell.execute_reply.started":"2022-05-05T18:04:38.66615Z","shell.execute_reply":"2022-05-05T18:04:38.67286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In general if there are missing values which are usually as Nan values then find the percaentage of missing values in each column","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:04:38.674861Z","iopub.execute_input":"2022-05-05T18:04:38.675137Z","iopub.status.idle":"2022-05-05T18:04:38.684298Z","shell.execute_reply.started":"2022-05-05T18:04:38.675092Z","shell.execute_reply":"2022-05-05T18:04:38.68307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(train.columns[[0,3,8,10]],axis=1,inplace=True) #Dropping redundant columns like Name, PassengerId,Cabin,Ticket\n# In this notebook, I did not perform much of EDA  so I just dropped those columns but I would like to do in my upcoming notebooks.\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:04:38.685763Z","iopub.execute_input":"2022-05-05T18:04:38.686039Z","iopub.status.idle":"2022-05-05T18:04:38.704131Z","shell.execute_reply.started":"2022-05-05T18:04:38.686004Z","shell.execute_reply":"2022-05-05T18:04:38.7032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ye = pd.get_dummies(train.Sex) # Converting categorical variables into integer variables\nyew = pd.get_dummies(train.Embarked)\ntrain = train.drop(['Sex','Embarked'], axis =1)\ntrain = pd.concat([train, ye], axis=1)\ntrain = pd.concat([train, yew], axis=1)\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:04:38.705642Z","iopub.execute_input":"2022-05-05T18:04:38.706036Z","iopub.status.idle":"2022-05-05T18:04:38.740795Z","shell.execute_reply.started":"2022-05-05T18:04:38.706003Z","shell.execute_reply":"2022-05-05T18:04:38.739635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tester = test.drop(test.columns[[0,2,7,9]],axis=1) #Dropping redundant columns\n#First method remove all rows with nan values #Returns a dataset without nan values\nprint(tester)\nyr = pd.get_dummies(tester.Sex)\nyew = pd.get_dummies(tester.Embarked)\ntester = tester.drop(['Sex','Embarked'], axis =1)\ntester = pd.concat([tester, yr], axis=1)\ntester = pd.concat([tester, yew], axis=1)\ntester","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:04:38.744602Z","iopub.execute_input":"2022-05-05T18:04:38.745403Z","iopub.status.idle":"2022-05-05T18:04:38.778769Z","shell.execute_reply.started":"2022-05-05T18:04:38.745344Z","shell.execute_reply":"2022-05-05T18:04:38.777901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe() # Describes about our train data like mean,count etc","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:04:38.780432Z","iopub.execute_input":"2022-05-05T18:04:38.781017Z","iopub.status.idle":"2022-05-05T18:04:38.833348Z","shell.execute_reply.started":"2022-05-05T18:04:38.780969Z","shell.execute_reply":"2022-05-05T18:04:38.83227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info() # Gives info about train data its datatype and objects being null/not","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:04:38.834896Z","iopub.execute_input":"2022-05-05T18:04:38.835232Z","iopub.status.idle":"2022-05-05T18:04:38.855058Z","shell.execute_reply.started":"2022-05-05T18:04:38.835184Z","shell.execute_reply":"2022-05-05T18:04:38.853945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def impute_missing_data(test, col, median, mode,mean):\n    \n    \"\"\" This function replaces all nan values in a column by mean or median or mode\n    INPUT : column name,dataframe,which value we want to be replaced\n    OUTPUT: returns the modified dataframe\"\"\"\n\n    nanindex = []  #To store indices of null values in a \n    test.loc[:,col] = test[col].fillna('nan')\n    for i in range(test.shape[0]):\n        if(test.loc[i,col] == 'nan'):\n            nanindex.append(i)\n    #print(nanindex)\n    mod = test[col].value_counts().index[1] \n    a = np.squeeze(np.nanmean(np.array(test[col],dtype = float))) # Finds mean of data excluding nan values\n    #print(a)\n    #print(mod)\n    for j in nanindex:\n        if median == True:\n            test.loc[j,col] = test[col].median()\n            #print(test[col].median())\n        elif mode == True:\n            test.loc[j,col] = mod\n            #print(mod)\n        elif mean == True:\n            test.loc[j,col] = a\n            #print(test.loc[j,col])\n    #print(test)\n    return test   ","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:04:38.856813Z","iopub.execute_input":"2022-05-05T18:04:38.857269Z","iopub.status.idle":"2022-05-05T18:04:38.866936Z","shell.execute_reply.started":"2022-05-05T18:04:38.857235Z","shell.execute_reply":"2022-05-05T18:04:38.866049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Imputing mean values for missing data\ntrain = impute_missing_data(train,'Age' , median =True, mode=False,mean =False)\n#As Embarked is a categorical variable mean and median does not make sense. Hence impute missing values\n# with mode of column\n#print(train)\ntrain.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:04:38.86796Z","iopub.execute_input":"2022-05-05T18:04:38.86877Z","iopub.status.idle":"2022-05-05T18:04:39.028611Z","shell.execute_reply.started":"2022-05-05T18:04:38.868729Z","shell.execute_reply":"2022-05-05T18:04:39.027717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Imputing median values for missing data\ntester = impute_missing_data(tester,'Fare' , median =True, mode=False,mean =False)\ntester = impute_missing_data(tester,'Age' , median =True, mode=False,mean =False)\n#As Embarked is a categorical variable mean and median does not make sense. Hence impute missing values\n# with mode of column\ntester","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:04:39.029879Z","iopub.execute_input":"2022-05-05T18:04:39.030108Z","iopub.status.idle":"2022-05-05T18:04:39.114815Z","shell.execute_reply.started":"2022-05-05T18:04:39.030079Z","shell.execute_reply":"2022-05-05T18:04:39.113873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.array(train.iloc[:,1:])\ny = np.array(train.iloc[:,0])\nX_test = np.array(tester.iloc[:,:])\n#print(X_test)\n#print(y)\n#print(X_train)\ni = np.mean(X_train,axis=0,dtype = float)\nr = np.std(X_train, axis=0,dtype = float)\nX = (X_train - i)/r # Performing mean normalization so that data has 0 mean and unit std\nprint(X)\ni = np.mean(X_test,axis=0,dtype = float)\nr = np.std(X_test, axis=0,dtype = float)\nXtest = (X_test - i)/r\nprint(Xtest)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:04:39.116166Z","iopub.execute_input":"2022-05-05T18:04:39.116405Z","iopub.status.idle":"2022-05-05T18:04:39.141559Z","shell.execute_reply.started":"2022-05-05T18:04:39.116373Z","shell.execute_reply":"2022-05-05T18:04:39.140436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression # Importing required libraries\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:04:39.143383Z","iopub.execute_input":"2022-05-05T18:04:39.143665Z","iopub.status.idle":"2022-05-05T18:04:40.452919Z","shell.execute_reply.started":"2022-05-05T18:04:39.143606Z","shell.execute_reply":"2022-05-05T18:04:40.451905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train the model\nmodel = LogisticRegression()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\nX_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.30, random_state=42)\nmodel.fit(X_train, y_train) #Training the model\nmodel.score(X_cv,y_cv)\n#print(np.shape(X_train))\n#print(np.shape(X_cv))","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:04:40.4541Z","iopub.execute_input":"2022-05-05T18:04:40.454341Z","iopub.status.idle":"2022-05-05T18:04:40.485635Z","shell.execute_reply.started":"2022-05-05T18:04:40.454311Z","shell.execute_reply":"2022-05-05T18:04:40.484415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Using cv set to find best degree\ndef degree(X_train,y_train,X_cv,y_cv,X,y,X_test,y_test):\n    l = []\n    lr = LogisticRegression(max_iter = 250, C =0.0012)\n    for i in range(1,6):\n        poly = PolynomialFeatures(degree = i, include_bias=False)\n        X_poly = poly.fit_transform(X_train)\n        lr.fit(X_poly,y_train)\n        print(i)\n        print(f' Accuracy over train is {lr.score(poly.transform(X_train), y_train)}') \n        print(f' Accuracy over cv is {lr.score(poly.transform(X_cv), y_cv)}') \n        #print(f' Accuracy over test is {lr.score(poly.transform(X_test), y_test)}') \n        l.append({i : lr.score(poly.transform(X_cv), y_cv)})\n    return l \n        \nbestdegree = degree(X_train,y_train,X_cv,y_cv,X,y,X_test,y_test)\nprint(bestdegree)\n#As we have observed that degree=3 performs better than linear logistic logistic regression because its accuracy is 78.6%\n# Whereas degree = 3 accuracy over cv set is 82.88% Hence we use degree = 3 in our model","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:04:40.487419Z","iopub.execute_input":"2022-05-05T18:04:40.488586Z","iopub.status.idle":"2022-05-05T18:04:41.298428Z","shell.execute_reply.started":"2022-05-05T18:04:40.488528Z","shell.execute_reply":"2022-05-05T18:04:41.297414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression(max_iter = 400, C =0.0012)\npoly = PolynomialFeatures(degree = 3, include_bias=False)\nX_poly = poly.fit_transform(X)\nlr.fit(X_poly,y)\nprint(f' Accuracy over entire training set  is {lr.score(poly.transform(X), y)}') \n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:09:11.460826Z","iopub.execute_input":"2022-05-05T18:09:11.461198Z","iopub.status.idle":"2022-05-05T18:09:11.530254Z","shell.execute_reply.started":"2022-05-05T18:09:11.461147Z","shell.execute_reply":"2022-05-05T18:09:11.529356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Test the model\nX_testpoly = poly.fit_transform(Xtest)\npredictions = lr.predict(X_testpoly)\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2022-05-05T18:09:18.385693Z","iopub.execute_input":"2022-05-05T18:09:18.386052Z","iopub.status.idle":"2022-05-05T18:09:18.405121Z","shell.execute_reply.started":"2022-05-05T18:09:18.386013Z","shell.execute_reply":"2022-05-05T18:09:18.404129Z"},"trusted":true},"execution_count":null,"outputs":[]}]}