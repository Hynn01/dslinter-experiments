{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Trash Detection\n\n### Equipe composta por:\n    - Eduardo Sales\n    - Isaque Vilson\n    - Tiago Sá\n    \n#### Objetivo Geral\n\nAssim, implementar um modelo basedo em Aprendizado Profundo para detectar diferentes lixeiras cheias e vazias nas ruas de Manaus.\n\n#### Objetivos Específicos\n\n1. Implementação de um modelo a partir da abordagem \"Transfer Learning\" com YOLOv5;\n2. Aplicar métodos de data augmentation para aumentar a quantidade de imagens no conjunto de treino;\n3. Comparação entre os modelos em termos de métricas de validação (acurácia, precisão, revocação e f1-score).\n\n### Topics:\n\n#### Visão Geral\n\n1. Sobre o Dataset;\n\n2. Análise Exploratória\n\n\n#### Divisão do dataset\n    - Treino e validação\n\n#### Pipeline 1 - Transfer Learning\n1. Preprocessamento;\n\n    1.1 No data augmentation\n    \n2. Treinamento;\n\n    \n3. Avaliação;\n\n    3.1 Precisão, Revocação, Matriz de Confusão, etc...\n    \n\n#### Pipeline 2 - Data augmentation\n\n1. Preprocessamento;\n\n    1.1 Técnicas comuns de data augmentation\n    \n2. Treinamento;\n\n    \n3. Avaliação\n\n    3.1 Precisão, Revocação, Matriz de Confusão...\n    \n   \n#### Comparação entre os resultados obtidos a partir da pipeline 1 e 2\n\n\n####  Considerações Finais\n\n\n\n\n\n#### Data: 05/05/2022","metadata":{}},{"cell_type":"markdown","source":"# Download YOLOv5","metadata":{}},{"cell_type":"code","source":"!python --version","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:10:52.748951Z","iopub.execute_input":"2022-05-06T21:10:52.749687Z","iopub.status.idle":"2022-05-06T21:10:53.423594Z","shell.execute_reply.started":"2022-05-06T21:10:52.749646Z","shell.execute_reply":"2022-05-06T21:10:53.422493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def list_dir(path):\n    \n    \"\"\" List everything inside the directory \"\"\"\n    filenames = os.listdir(path)\n    \n    # return a list of filenames\n    \n    return [filename for filename in filenames]","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:10:53.425465Z","iopub.execute_input":"2022-05-06T21:10:53.425961Z","iopub.status.idle":"2022-05-06T21:10:53.432166Z","shell.execute_reply.started":"2022-05-06T21:10:53.425918Z","shell.execute_reply":"2022-05-06T21:10:53.431011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5  # clone","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:10:54.747523Z","iopub.execute_input":"2022-05-06T21:10:54.747853Z","iopub.status.idle":"2022-05-06T21:10:57.268115Z","shell.execute_reply.started":"2022-05-06T21:10:54.747818Z","shell.execute_reply":"2022-05-06T21:10:57.267372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd yolov5\n%pip install -qr requirements.txt # install\n\nimport torch\n\nfrom yolov5 import utils\ndisplay = utils.notebook_init() # check","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:10:58.576687Z","iopub.execute_input":"2022-05-06T21:10:58.57709Z","iopub.status.idle":"2022-05-06T21:11:07.692053Z","shell.execute_reply.started":"2022-05-06T21:10:58.577042Z","shell.execute_reply":"2022-05-06T21:11:07.691274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2 # Computacional Vision manipulations on Images from Taco Dataset\nimport albumentations as A # To pre processing images and also bounding boxes parameters\n\nimport numpy as np # To perform numeric operations on Digital Image Processing\nimport pandas as pd # To manipulate dataframes\nimport matplotlib.pyplot as plt # To visualize images\nimport seaborn as sns\nimport random # To visualize and image\nfrom IPython.display import Image, display\n\nimport glob\nimport os\nimport shutil\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:11:07.694263Z","iopub.execute_input":"2022-05-06T21:11:07.694724Z","iopub.status.idle":"2022-05-06T21:11:07.700644Z","shell.execute_reply.started":"2022-05-06T21:11:07.694684Z","shell.execute_reply":"2022-05-06T21:11:07.699951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Bounding Boxes Colors\n\nBOX_COLOR = (255, 0, 0) # Red\nTEXT_COLOR = (255, 255, 255) # White\n\ndef visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n    \n    \"\"\" Visualizes a single bounding boxes on the image \"\"\"\n    \n    \n    x_min, y_min, w, h = bbox\n    x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n    \n    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color = color, thickness = thickness)\n    \n    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n    cv2.putText(\n        img,\n        text=class_name,\n        org=(x_min, y_min - int(0.3 * text_height)),\n        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n        fontScale=0.35, \n        color=TEXT_COLOR, \n        lineType=cv2.LINE_AA,\n    )\n    \n    return img\n\ndef visualize(image, bboxes, category_ids, category_id_to_name):\n    img = image.copy()\n    for bbox, category_id in zip(bboxes, category_ids):\n        class_name = category_id_to_name[category_id]\n        img = visualize_bbox(img, bbox, class_name)\n    plt.figure(figsize=(12, 12))\n    plt.axis('off')\n    plt.imshow(img)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:11:07.702257Z","iopub.execute_input":"2022-05-06T21:11:07.702826Z","iopub.status.idle":"2022-05-06T21:11:07.716845Z","shell.execute_reply.started":"2022-05-06T21:11:07.702788Z","shell.execute_reply":"2022-05-06T21:11:07.716162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_yolo_to_coco(image, bboxes, category_ids, category_id_name):\n    \n    height, width, _ = image.shape\n    \n    coco_bboxes = []\n    \n    for bbox in bboxes:\n        alb_bbox = A.bbox_utils.convert_bbox_to_albumentations(bbox,\n                                                                'yolo',\n                                                                height,\n                                                                width,\n                                                                check_validity=False)\n    \n        coco_bbox = A.bbox_utils.convert_bbox_from_albumentations (alb_bbox,\n                                                                   'coco',\n                                                                   height,\n                                                                   width,\n                                                                   check_validity=False)\n        coco_bboxes.append(coco_bbox)\n    return coco_bboxes","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:11:07.719275Z","iopub.execute_input":"2022-05-06T21:11:07.719995Z","iopub.status.idle":"2022-05-06T21:11:07.730311Z","shell.execute_reply.started":"2022-05-06T21:11:07.719954Z","shell.execute_reply":"2022-05-06T21:11:07.729528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_path = '/kaggle/input/trash-detection/train/images/eduardo14.png'\n\n\nimage               = cv2.imread(img_path)\nbboxes              = [[0.427332, 0.569239, 0.062907, 0.153277],\n                       [0.486714, 0.446089, 0.045011, 0.107822],\n                       [0.767625, 0.315539, 0.026573, 0.028541]]\ncategory_ids        = [0, 0, 0]\ncategory_id_to_name = {0:\"empty\"}\n\nbboxes = convert_yolo_to_coco(image, bboxes, category_ids, category_id_to_name)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:11:07.731831Z","iopub.execute_input":"2022-05-06T21:11:07.732463Z","iopub.status.idle":"2022-05-06T21:11:07.865105Z","shell.execute_reply.started":"2022-05-06T21:11:07.732423Z","shell.execute_reply":"2022-05-06T21:11:07.864293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the random image with associated bounding boxes\nvisualize(image, bboxes, category_ids, category_id_to_name)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:11:07.868626Z","iopub.execute_input":"2022-05-06T21:11:07.868817Z","iopub.status.idle":"2022-05-06T21:11:08.413194Z","shell.execute_reply.started":"2022-05-06T21:11:07.868793Z","shell.execute_reply":"2022-05-06T21:11:08.412561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split","metadata":{}},{"cell_type":"code","source":"def create_dir(path):\n    \"\"\" Criar diretórios \"\"\"\n    if (os.path.exists(path) and (os.listdir(path) != [])):\n        shutil.rmtree(path) \n        \n    else:\n        os.makedirs(path)\n        \n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:11:08.414379Z","iopub.execute_input":"2022-05-06T21:11:08.414743Z","iopub.status.idle":"2022-05-06T21:11:08.419931Z","shell.execute_reply.started":"2022-05-06T21:11:08.414706Z","shell.execute_reply":"2022-05-06T21:11:08.41924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_imgs_paths(path):\n    \"\"\" Lista todos os arquivos .png do diretório especificado \"\"\"\n    return glob.glob(os.path.join(path, '*.png'))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:11:08.421256Z","iopub.execute_input":"2022-05-06T21:11:08.421725Z","iopub.status.idle":"2022-05-06T21:11:08.430106Z","shell.execute_reply.started":"2022-05-06T21:11:08.421684Z","shell.execute_reply":"2022-05-06T21:11:08.429099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"src_dir = '/kaggle/input/trash-detection/'\ndest_dir = '/kaggle/working/trash-detection/'\n\ndef split_train_test(src_dir, dest_dir):\n    \n    # Setting train and test paths\n    train_dir = os.path.join(dest_dir, 'train')\n    test_dir  = os.path.join(dest_dir, 'val')\n    \n    # Creating directories\n    create_dir(dest_dir)\n    create_dir(os.path.join(train_dir, 'images'))\n    create_dir(os.path.join(train_dir, 'labels'))      \n    \n    create_dir(os.path.join(test_dir, 'images'))\n    create_dir(os.path.join(test_dir, 'labels'))              \n               \n    # Getting images paths\n    imgs_paths = get_imgs_paths(os.path.join(src_dir, 'train', 'images'))\n    \n    # Creating dataframe with imgs paths\n    df = pd.DataFrame({'imgs' : imgs_paths})\n    \n    # Train test split\n    train, test = train_test_split(df, test_size = 0.2, random_state = 101)\n    \n    print(f\" Train shape: {train.shape}\\n\")\n    print(f\" Test shape: {test.shape}\\n\")\n    \n    splits = [train, test]\n    \n    # Moving images.\n    for split in splits:\n        for img in split.imgs:\n            filename_no_ext = os.path.splitext(os.path.basename(img))[0]\n            if split is train:\n                shutil.copy(img, os.path.join(train_dir, 'images', filename_no_ext + '.png'))\n                shutil.copy(os.path.join(src_dir,'train', 'labels', filename_no_ext + '.txt'),\n                           os.path.join(train_dir, 'labels', filename_no_ext + '.txt'))\n    \n            else:\n                shutil.copy(img, os.path.join(test_dir, 'images', filename_no_ext + '.png'))\n                shutil.copy(os.path.join(src_dir,'train', 'labels', filename_no_ext + '.txt'),\n                           os.path.join(test_dir, 'labels', filename_no_ext + '.txt'))    ","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:11:08.431613Z","iopub.execute_input":"2022-05-06T21:11:08.431944Z","iopub.status.idle":"2022-05-06T21:11:08.448341Z","shell.execute_reply.started":"2022-05-06T21:11:08.431906Z","shell.execute_reply":"2022-05-06T21:11:08.447388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_train_test(src_dir, dest_dir)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:11:08.451457Z","iopub.execute_input":"2022-05-06T21:11:08.451995Z","iopub.status.idle":"2022-05-06T21:11:17.072047Z","shell.execute_reply.started":"2022-05-06T21:11:08.451961Z","shell.execute_reply":"2022-05-06T21:11:17.0713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quantidade de imagens e labels.","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:11:17.075771Z","iopub.execute_input":"2022-05-06T21:11:17.075972Z","iopub.status.idle":"2022-05-06T21:11:17.081358Z","shell.execute_reply.started":"2022-05-06T21:11:17.075947Z","shell.execute_reply":"2022-05-06T21:11:17.079587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Treinamento com YOLOv5","metadata":{}},{"cell_type":"code","source":"!python train.py --img 712 --batch 64 --epochs 250 --data '/kaggle/input/trash-detection/data.yaml' --weights yolov5s.pt --cache","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:11:17.082906Z","iopub.execute_input":"2022-05-06T21:11:17.085084Z","iopub.status.idle":"2022-05-06T21:34:21.706992Z","shell.execute_reply.started":"2022-05-06T21:11:17.085048Z","shell.execute_reply":"2022-05-06T21:34:21.706016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/yolov5/runs/train/exp","metadata":{"execution":{"iopub.status.busy":"2022-05-06T17:54:32.615307Z","iopub.execute_input":"2022-05-06T17:54:32.615603Z","iopub.status.idle":"2022-05-06T17:54:33.279867Z","shell.execute_reply.started":"2022-05-06T17:54:32.615563Z","shell.execute_reply":"2022-05-06T17:54:33.27899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Image(filename = '/kaggle/working/yolov5/runs/train/exp/results.png'))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:38:15.172241Z","iopub.execute_input":"2022-05-06T21:38:15.172919Z","iopub.status.idle":"2022-05-06T21:38:15.1871Z","shell.execute_reply.started":"2022-05-06T21:38:15.172879Z","shell.execute_reply":"2022-05-06T21:38:15.186462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Image(filename = '/kaggle/working/yolov5/runs/train/exp/F1_curve.png'))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:38:22.331046Z","iopub.execute_input":"2022-05-06T21:38:22.331654Z","iopub.status.idle":"2022-05-06T21:38:22.341562Z","shell.execute_reply.started":"2022-05-06T21:38:22.331612Z","shell.execute_reply":"2022-05-06T21:38:22.340815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Image(filename = '/kaggle/working/yolov5/runs/train/exp/PR_curve.png'))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:38:22.767132Z","iopub.execute_input":"2022-05-06T21:38:22.767677Z","iopub.status.idle":"2022-05-06T21:38:22.776557Z","shell.execute_reply.started":"2022-05-06T21:38:22.767638Z","shell.execute_reply":"2022-05-06T21:38:22.775743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Image(filename = '/kaggle/working/yolov5/runs/train/exp/confusion_matrix.png'))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:38:27.172213Z","iopub.execute_input":"2022-05-06T21:38:27.172571Z","iopub.status.idle":"2022-05-06T21:38:27.182838Z","shell.execute_reply.started":"2022-05-06T21:38:27.172533Z","shell.execute_reply":"2022-05-06T21:38:27.182077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Image(filename = '/kaggle/working/yolov5/runs/train/exp/train_batch1.jpg'))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:38:30.461219Z","iopub.execute_input":"2022-05-06T21:38:30.461977Z","iopub.status.idle":"2022-05-06T21:38:30.482402Z","shell.execute_reply.started":"2022-05-06T21:38:30.461942Z","shell.execute_reply":"2022-05-06T21:38:30.481784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Image(filename = '/kaggle/working/yolov5/runs/train/exp/labels.jpg'))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:38:30.854639Z","iopub.execute_input":"2022-05-06T21:38:30.85543Z","iopub.status.idle":"2022-05-06T21:38:30.868351Z","shell.execute_reply.started":"2022-05-06T21:38:30.855384Z","shell.execute_reply":"2022-05-06T21:38:30.867673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation Techniques\n\n## Foi utilizado o roboflow como ferramenta para data augmentation","metadata":{}},{"cell_type":"code","source":"!pip install roboflow\n\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"oVr9WVxspeXzy9q3oEya\")\nproject = rf.workspace(\"trash-detection-lqvbn\").project(\"trash-detection-qcyf3\")\ndataset = project.version(2).download(\"yolov5\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:27:59.335728Z","iopub.execute_input":"2022-05-06T19:27:59.335983Z","iopub.status.idle":"2022-05-06T19:28:49.125704Z","shell.execute_reply.started":"2022-05-06T19:27:59.335948Z","shell.execute_reply":"2022-05-06T19:28:49.124859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls Tras","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:50:07.944665Z","iopub.execute_input":"2022-05-06T19:50:07.94553Z","iopub.status.idle":"2022-05-06T19:50:08.608586Z","shell.execute_reply.started":"2022-05-06T19:50:07.945485Z","shell.execute_reply":"2022-05-06T19:50:08.607773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python train.py --img 712 --batch 64 --epochs 250 --data '/kaggle/working/yolov5/Trash-Detection-2/data.yaml' --weights yolov5s.pt --cache","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:50:12.455706Z","iopub.execute_input":"2022-05-06T19:50:12.456531Z","iopub.status.idle":"2022-05-06T20:31:32.834313Z","shell.execute_reply.started":"2022-05-06T19:50:12.456488Z","shell.execute_reply":"2022-05-06T20:31:32.833378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Image(filename = '/kaggle/working/yolov5/runs/train/exp/results.png'))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T20:46:24.587875Z","iopub.execute_input":"2022-05-06T20:46:24.588184Z","iopub.status.idle":"2022-05-06T20:46:24.606463Z","shell.execute_reply.started":"2022-05-06T20:46:24.588147Z","shell.execute_reply":"2022-05-06T20:46:24.605503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Image(filename = '/kaggle/working/yolov5/runs/train/exp/F1_curve.png'))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T20:46:51.254976Z","iopub.execute_input":"2022-05-06T20:46:51.25553Z","iopub.status.idle":"2022-05-06T20:46:51.266022Z","shell.execute_reply.started":"2022-05-06T20:46:51.255491Z","shell.execute_reply":"2022-05-06T20:46:51.265126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Image(filename = '/kaggle/working/yolov5/runs/train/exp/confusion_matrix.png'))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-06T20:47:13.151398Z","iopub.execute_input":"2022-05-06T20:47:13.151814Z","iopub.status.idle":"2022-05-06T20:47:13.160821Z","shell.execute_reply.started":"2022-05-06T20:47:13.151778Z","shell.execute_reply":"2022-05-06T20:47:13.160142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Image(filename = '/kaggle/working/yolov5/runs/train/exp/train_batch1.jpg'))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-06T20:47:31.392376Z","iopub.execute_input":"2022-05-06T20:47:31.392636Z","iopub.status.idle":"2022-05-06T20:47:31.413752Z","shell.execute_reply.started":"2022-05-06T20:47:31.392608Z","shell.execute_reply":"2022-05-06T20:47:31.413178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusões","metadata":{}},{"cell_type":"markdown","source":"## Os resultados foram muito ruins. Os modelos treinados apresentaram um alto overfitting. Isso é possível verificar pois, durante o treinamento, o erro é minímo ao longo das últimas épocas, porém, ao verificar a performance no conjunto de validação, o erro aumenta drasticamente ao longo das épocas.\n\n## Nem mesmo aplicar técnicas de data augmentation provocaram melhorias significativas no modelo.\n\n## Dentre as possíveis ações para melhorar os resultados, destacam-se:\n\n* Para contornar essa situação, mais imagens deveriam ser rotuladas. Ao todo, foram coletadas aproximadamente 220 imagens. Essa quantidade é muito pouco para a complexidade de objetos ao redor da lixeira. No geral, as imagens possuem misturaas de asfalto, casas, pessoas, animais, carros, e inúmeros outros objetos que dificultam o modelo a aprender quais são os traços da lixeira em um contexto tão complexo. Além disso, foram coletadas diferentes tipos de lixeira, porém poucos exemplos. Acrescenta-se que as imagens foram capturadas sob diferentes condições de iluminação, ângulo e posição das lixeiras, porém poucos exemplos. Em virtude das imagens serem coletadas pelo google maps, muitas vezes as lixeiras possuem um formato distorcido em função da lente esférica que é utilizada para capturar essas imagens.\n\n* As imagens poderiam ser mais centralizadas nas lixeiras. Isso potencialmente tornaria mais fácil o modelo adaptar seus pesos para aprender os traços de lixeira. No dataset gerado, as imagens possuem exemplos de lixeiras que estão muito distantes, com uma área de cobertura de pixel muito inferior a dimensão original da imagem.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}