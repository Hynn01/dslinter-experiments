{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.utils \nimport tensorflow as tf\nimport numpy as np \nimport pandas as pd\nimport numpy as np","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-04-29T04:17:52.557711Z","iopub.execute_input":"2022-04-29T04:17:52.558048Z","iopub.status.idle":"2022-04-29T04:17:57.24827Z","shell.execute_reply.started":"2022-04-29T04:17:52.558014Z","shell.execute_reply":"2022-04-29T04:17:57.24746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/shakespeare-plays/Shakespeare_data.csv')\ndf.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T04:17:57.250245Z","iopub.execute_input":"2022-04-29T04:17:57.250609Z","iopub.status.idle":"2022-04-29T04:17:57.553308Z","shell.execute_reply.started":"2022-04-29T04:17:57.250573Z","shell.execute_reply":"2022-04-29T04:17:57.552434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\ncorpus = []\n\nwith open('/kaggle/input/shakespeare-plays/Shakespeare_data.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    next(reader)        # to pass first row,header\n    for row in reader:\n        corpus.append(row[5])\n        \nprint(len(corpus))\nprint(corpus[:3])","metadata":{"execution":{"iopub.status.busy":"2022-04-29T04:17:57.554759Z","iopub.execute_input":"2022-04-29T04:17:57.555133Z","iopub.status.idle":"2022-04-29T04:17:57.880076Z","shell.execute_reply.started":"2022-04-29T04:17:57.555095Z","shell.execute_reply":"2022-04-29T04:17:57.879304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"import string\n\ndef text_cleaner(text):\n    text = \"\".join(car for car in text if car not in string.punctuation).lower()\n    text = text.encode(\"utf8\").decode(\"ascii\",'ignore')\n    return text\n\ncorpus = [text_cleaner(line) for line in corpus]\n","metadata":{"execution":{"iopub.status.busy":"2022-04-29T04:17:57.881821Z","iopub.execute_input":"2022-04-29T04:17:57.882372Z","iopub.status.idle":"2022-04-29T04:17:58.721594Z","shell.execute_reply.started":"2022-04-29T04:17:57.882312Z","shell.execute_reply":"2022-04-29T04:17:58.720845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenization is the process of splitting up a text into a list of individual words, or tokens.\n# corpus is too big if you try with all data, you can see this message\ncorpus = corpus[:5000]\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\nword_index = tokenizer.word_index\ntotal_words = len(word_index) + 1\ntotal_words","metadata":{"execution":{"iopub.status.busy":"2022-04-29T04:17:58.724068Z","iopub.execute_input":"2022-04-29T04:17:58.724321Z","iopub.status.idle":"2022-04-29T04:17:58.839743Z","shell.execute_reply.started":"2022-04-29T04:17:58.724295Z","shell.execute_reply":"2022-04-29T04:17:58.838883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create input sequences using list of tokens\ninput_sequences =[]\n\nfor sentence in corpus:\n    token_list = tokenizer.texts_to_sequences([sentence])[0]\n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T04:17:58.841299Z","iopub.execute_input":"2022-04-29T04:17:58.841624Z","iopub.status.idle":"2022-04-29T04:17:59.044329Z","shell.execute_reply.started":"2022-04-29T04:17:58.841592Z","shell.execute_reply":"2022-04-29T04:17:59.043487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pad sequences \nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, \n                                         maxlen=max_sequence_len, \n                                         padding='pre'))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T04:17:59.045948Z","iopub.execute_input":"2022-04-29T04:17:59.046349Z","iopub.status.idle":"2022-04-29T04:17:59.297019Z","shell.execute_reply.started":"2022-04-29T04:17:59.046308Z","shell.execute_reply":"2022-04-29T04:17:59.296218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create predictors and label\npredictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n# create one-hot encoding of the labels\nlabel = tensorflow.keras.utils.to_categorical(label, num_classes=total_words)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T04:17:59.298359Z","iopub.execute_input":"2022-04-29T04:17:59.298805Z","iopub.status.idle":"2022-04-29T04:17:59.374186Z","shell.execute_reply.started":"2022-04-29T04:17:59.298742Z","shell.execute_reply":"2022-04-29T04:17:59.373399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(label[0])\nprint(label[0].shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T04:17:59.375795Z","iopub.execute_input":"2022-04-29T04:17:59.376197Z","iopub.status.idle":"2022-04-29T04:17:59.382893Z","shell.execute_reply.started":"2022-04-29T04:17:59.376158Z","shell.execute_reply":"2022-04-29T04:17:59.38187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\nmodel.add(Bidirectional(LSTM(512)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(total_words, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-29T04:17:59.384453Z","iopub.execute_input":"2022-04-29T04:17:59.385123Z","iopub.status.idle":"2022-04-29T04:18:02.385167Z","shell.execute_reply.started":"2022-04-29T04:17:59.385083Z","shell.execute_reply":"2022-04-29T04:18:02.383635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" history = model.fit(predictors, label, epochs=50,  verbose=1)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-29T04:18:02.386534Z","iopub.execute_input":"2022-04-29T04:18:02.386858Z","iopub.status.idle":"2022-04-29T04:32:00.652552Z","shell.execute_reply.started":"2022-04-29T04:18:02.386822Z","shell.execute_reply":"2022-04-29T04:32:00.651337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nacc = history.history['accuracy']\nloss = history.history['loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'b', label='Training accuracy')\nplt.title('Training accuracy')\n\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training Loss')\nplt.title('Training loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T04:32:00.6549Z","iopub.execute_input":"2022-04-29T04:32:00.656169Z","iopub.status.idle":"2022-04-29T04:32:00.970101Z","shell.execute_reply.started":"2022-04-29T04:32:00.656126Z","shell.execute_reply":"2022-04-29T04:32:00.969243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_text = \"help me in this\"\nnext_words = 2\n\n  \nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    predicted = model.predict_classes(token_list, verbose=0)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\n    if len(seed_text) % 10 == 0 :\n        seed_text+= '\\n'\nprint(seed_text)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T04:32:00.971387Z","iopub.execute_input":"2022-04-29T04:32:00.97188Z","iopub.status.idle":"2022-04-29T04:32:01.617393Z","shell.execute_reply.started":"2022-04-29T04:32:00.971839Z","shell.execute_reply":"2022-04-29T04:32:01.616445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_text = \"Love all, trust a few\"\nnext_words = 2\n\n  \nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    predicted = model.predict_classes(token_list, verbose=0)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\n    if len(seed_text) % 10 == 0 :\n        seed_text+= '\\n'\nprint(seed_text)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T04:34:51.506148Z","iopub.execute_input":"2022-04-29T04:34:51.506482Z","iopub.status.idle":"2022-04-29T04:34:51.595131Z","shell.execute_reply.started":"2022-04-29T04:34:51.50645Z","shell.execute_reply":"2022-04-29T04:34:51.594134Z"},"trusted":true},"execution_count":null,"outputs":[]}]}