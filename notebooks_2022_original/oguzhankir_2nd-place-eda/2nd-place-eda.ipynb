{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EnerjiSA Üretim Hackathon - Wind Force\n## Doç. Dr. Oetker\n\n- **Gökay Aydoğan**  \n- **Onur Hakkı Eyüboğlu**  \n- **Oğuzhan Kır**\n\n![Rüzgar Türbini Balıkesir](https://www.enerjisauretim.com.tr/_assets/images/gallery/balikesir_res_5.jpg)","metadata":{}},{"cell_type":"markdown","source":"Note1: We did very, very detailed EDA and feature engineering in 2 weeks. I wanted to share some of the visualizations we made on this notebook. Since the second stage of the competition will be in a similar format, we can share a small part of what we do. After the second stage, we will share our detailed EDA and feature engineering processes.\n\nNote2: I did not plot all the columns in the data to avoid too many graphics.\n\nNote3: Thanks to everyone who contributed to the competition.","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import probplot\nimport missingno as msno\nfrom statsmodels.tsa.stattools import adfuller\nfrom matplotlib import cycler\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-04T16:24:23.745844Z","iopub.execute_input":"2022-05-04T16:24:23.746184Z","iopub.status.idle":"2022-05-04T16:24:23.752678Z","shell.execute_reply.started":"2022-05-04T16:24:23.746147Z","shell.execute_reply":"2022-05-04T16:24:23.751565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read data","metadata":{}},{"cell_type":"code","source":"featuresDF = pd.read_csv(\"../input/enerjisa-uretim-hackathon/features.csv\")\npowerDF = pd.read_csv(\"../input/enerjisa-uretim-hackathon/power.csv\")\nsubmissionCSV = pd.read_csv(\"../input/enerjisa-uretim-hackathon/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-04T16:24:23.782673Z","iopub.execute_input":"2022-05-04T16:24:23.783172Z","iopub.status.idle":"2022-05-04T16:24:26.427586Z","shell.execute_reply.started":"2022-05-04T16:24:23.78313Z","shell.execute_reply":"2022-05-04T16:24:26.426661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess - EDA","metadata":{}},{"cell_type":"code","source":"#This function converts the dates in the Timestamp column to datetime and turns them into an index.\ndef set_time_index(df):\n    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"])\n    df = df.set_index(\"Timestamp\")\n    \n    #change dtype to float32\n    df = df.astype(np.float32)\n    return df\n\nfeaturesDF = set_time_index(featuresDF)\npowerDF = set_time_index(powerDF)\nallDF = pd.concat([featuresDF, powerDF], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T16:24:26.42904Z","iopub.execute_input":"2022-05-04T16:24:26.429281Z","iopub.status.idle":"2022-05-04T16:24:26.642695Z","shell.execute_reply.started":"2022-05-04T16:24:26.429251Z","shell.execute_reply":"2022-05-04T16:24:26.64204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This function divides the data according to the dates given in the submission file.\ndef train_submission_split(df, power=True):\n    \n    split_date = \"2021-08-15\"\n    trainDF = df.loc[df.index < split_date]\n    submissionxDF = df.loc[df.index >= split_date]\n    if power==False:\n        submissionxDF = submissionxDF.drop(\"Power(kW)\", axis=1)\n\n    return trainDF, submissionxDF\n\ntrainDF, submissionxDF = train_submission_split(allDF, power=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T16:24:26.643806Z","iopub.execute_input":"2022-05-04T16:24:26.644126Z","iopub.status.idle":"2022-05-04T16:24:26.712032Z","shell.execute_reply.started":"2022-05-04T16:24:26.644097Z","shell.execute_reply":"2022-05-04T16:24:26.711186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This function prints a summary of the data.\ndef print_summary(df):\n    print(\"Df.shape: \", df.shape, \"\\n\")\n    print(\"Missing values:\\n\", df.isnull().sum(), \"\\n\")\n    print(\"Summary:\\n\", df.describe(), \"\\n\")\n    print(\"İnfo:\\n\", df.info())\n    print(\"Columns:\\n\", df.columns, \"\\n\")\n\nprint_summary(allDF)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-04T16:24:26.713999Z","iopub.execute_input":"2022-05-04T16:24:26.714238Z","iopub.status.idle":"2022-05-04T16:24:27.375374Z","shell.execute_reply.started":"2022-05-04T16:24:26.714205Z","shell.execute_reply":"2022-05-04T16:24:27.373586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We wrote a function where we can examine the general information in the data at once. The first thing that caught our eye here was that there were values like 99999 in the columns. We also thought that too much missing data would cause problems.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nThis function visualizes how many missing values are in the columns in the data, where the missing values are located in the data, \nand how strongly the presence or absence of one variable affects the presence of the other with a heatmap plot.\n\"\"\"\ndef plot_missing_values(df):\n    msno.bar(df)\n    plt.show()\n    msno.matrix(df, freq='M')\n    plt.show()\n    msno.heatmap(df)\n    \nplot_missing_values(allDF[allDF.columns[-20:]])","metadata":{"execution":{"iopub.status.busy":"2022-05-04T16:24:27.376606Z","iopub.execute_input":"2022-05-04T16:24:27.376906Z","iopub.status.idle":"2022-05-04T16:24:32.082005Z","shell.execute_reply.started":"2022-05-04T16:24:27.376864Z","shell.execute_reply":"2022-05-04T16:24:32.081119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we drew the columns in the data with the train and test parts side by side, we clearly understood that the high values that caught our eye greatly distorted the distribution. We thought these were manually added noises rather than a sensor error.","metadata":{}},{"cell_type":"code","source":"trainDF,submissionxDF = train_submission_split(allDF,power=True)\n\nfig, axes = plt.subplots(nrows=19, ncols=4, dpi=60, figsize=(30,50))\nfor i, ax in enumerate(axes.flatten()):\n    if trainDF.columns[i] != \"Power(kW)\":\n        data = trainDF[trainDF.columns[i]]\n        data2 = submissionxDF[submissionxDF.columns[i]]\n        ax.plot(data, color='red', linewidth=0.2)\n        ax.plot(data2, color='blue', linewidth=0.2)\n        ax.set_title(trainDF.columns[i])\n        ax.xaxis.set_ticks_position('none')\n        ax.yaxis.set_ticks_position('none')\n        ax.spines[\"top\"].set_alpha(0)\n        ax.tick_params(labelsize=6)\n\nplt.tight_layout();","metadata":{"execution":{"iopub.status.busy":"2022-05-04T16:24:32.083678Z","iopub.execute_input":"2022-05-04T16:24:32.084109Z","iopub.status.idle":"2022-05-04T16:25:10.642004Z","shell.execute_reply.started":"2022-05-04T16:24:32.084077Z","shell.execute_reply":"2022-05-04T16:25:10.641146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We converted very high values such as 99999 to missing values. Later, when we plotted the distributions again, we were able to see the real outliers. However, we didn't want to play around too much in case these were sensor errors. For this reason, we made manual corrections by investigating the wind turbines and estimating how much the values could take.","metadata":{}},{"cell_type":"code","source":"#This function converts the outlier values we observe in the data to NaN values.\ndef outlier_to_NaN(df):\n    df = df.replace(99999,np.NaN)\n    for i in allDF.columns:\n        if \"Temp\" in i:\n            df[i] = df[i].apply(lambda x: x if (x>0) else np.nan)\n    df[\"Torque\"] = df[\"Torque\"].apply(lambda x: 105 if (x>105) else x)\n    for col in df:\n        if \"Tower Acceleration Normal\" in col:\n            df[col][df[col]>450] = np.NaN\n        if \"Moment D Direction\" in col:\n            df[col][df[col]<-1000] = np.NaN\n        if \"Moment D Filtered\" in col:\n            df[col][df[col]<-1000] = np.NaN\n        if \"Pitch Offset-2 Asymmetric Load Controller\" in col:\n            df[col][df[col]>0.15] = np.NaN\n        if \"Pitch Offset Tower Feedback\" in col:\n            df[col][df[col]<-0.010] = np.NaN\n        if \"External Power Limit\" in col:\n            df[col][df[col]<3050] = np.NaN\n        if \"Tower Accelaration Normal Raw\" in col:\n            df[col][df[col]<-2000] = np.NaN\n        if \"Blade-2 Actual Value_Angle-B\" in col:\n            df[col][df[col]<-250] = np.NaN\n            df[col][df[col]>250] = np.NaN\n        if \"Blade-1 Actual Value_Angle-B\" in col:\n            df[col][df[col]<-250] = np.NaN\n            df[col][df[col]>250] = np.NaN\n        if \"Blade-3 Actual Value_Angle-B\" in col:\n            df[col][df[col]<-50] = np.NaN\n            df[col][df[col]>50] = np.NaN\n        if \"Pitch Offset-1 Asymmetric Load Controller\" in col:\n            df[col][df[col]<-0.15] = np.NaN\n        if \"Tower Accelaration Lateral Raw\" in col:\n            df[col][df[col]>1000] = np.NaN\n        if \"Pitch Offset-3 Asymmetric Load Controller\" in col:\n            df[col][df[col]<-0.05] = np.NaN\n    return df\nallDF = outlier_to_NaN(allDF)\ntrainDF,submissionxDF = train_submission_split(allDF,power=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T16:25:10.643413Z","iopub.execute_input":"2022-05-04T16:25:10.643638Z","iopub.status.idle":"2022-05-04T16:25:12.436468Z","shell.execute_reply.started":"2022-05-04T16:25:10.643611Z","shell.execute_reply":"2022-05-04T16:25:12.435646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=19, ncols=4, dpi=60, figsize=(30,50))\nfor i, ax in enumerate(axes.flatten()):\n    if trainDF.columns[i] != \"Power(kW)\":\n        data = trainDF[trainDF.columns[i]]\n        data2 = submissionxDF[submissionxDF.columns[i]]\n        ax.plot(data, color='red', linewidth=0.2)\n        ax.plot(data2, color='blue', linewidth=0.2)\n        ax.set_title(trainDF.columns[i])\n        ax.xaxis.set_ticks_position('none')\n        ax.yaxis.set_ticks_position('none')\n        ax.spines[\"top\"].set_alpha(0)\n        ax.tick_params(labelsize=6)\n\nplt.tight_layout();\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T16:25:12.43769Z","iopub.execute_input":"2022-05-04T16:25:12.43793Z","iopub.status.idle":"2022-05-04T16:25:52.593748Z","shell.execute_reply.started":"2022-05-04T16:25:12.4379Z","shell.execute_reply":"2022-05-04T16:25:52.592465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the distribution of the data looked as we wanted, we started to examine the values in the power column.","metadata":{}},{"cell_type":"code","source":"trainDF[\"Power(kW)\"].plot(figsize=(30,5),color=\"b\",alpha=0.8,title=\"Power(kW)\",fontsize=15,lw=0.3);","metadata":{"execution":{"iopub.status.busy":"2022-05-04T16:25:52.595314Z","iopub.execute_input":"2022-05-04T16:25:52.595581Z","iopub.status.idle":"2022-05-04T16:25:54.199037Z","shell.execute_reply.started":"2022-05-04T16:25:52.595548Z","shell.execute_reply":"2022-05-04T16:25:54.198418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We examined the distributions of the features in the data, and according to our review, the distributions in most features had a skewed normal distribution. Since tree-based gradient boosting algorithms such as XGBoost work better on normally distributed data, we then transform the distribution of these features to approximate the normal distribution.","metadata":{}},{"cell_type":"code","source":"#This function plots the distribution and probability plot of the given column.\ndef plot_dist(df, target):\n\n    fig, axes = plt.subplots(ncols=2, figsize=(24, 6), dpi=100)\n\n    sns.kdeplot(df[target], label=target, fill=True, ax=axes[0])\n    axes[0].axvline(df[target].mean(), \n                    label='Mean', color='r', \n                    linewidth=2, linestyle='-')\n    \n    axes[0].axvline(df[target].median(), \n                    label='Median', color='b', \n                    linewidth=2, linestyle='-')\n    \n    axes[0].axvline(df[target].min(), \n                    label='Min', \n                    linewidth=2, linestyle='--')\n    \n    axes[0].axvline(df[target].max(), \n                    label='Max', linewidth=2, \n                    linestyle='--')\n    \n    axes[0].legend(prop={'size': 15})\n    probplot(df[target], plot=axes[1])\n    \n    for i in range(2):\n        axes[i].tick_params(axis='x', labelsize=12.5)\n        axes[i].tick_params(axis='y', labelsize=12.5)\n        axes[i].set_xlabel('')\n        axes[i].set_ylabel('')\n        \n    axes[0].set_title(f'{target} Distribution', \n                      fontsize=15, pad=12)\n    \n    axes[1].set_title(f'{target} Probability Plot', \n                      fontsize=15, pad=12)\n    return\n\nfor feature in allDF.columns[-5:]:\n    plot_dist(allDF, feature)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T16:25:54.20111Z","iopub.execute_input":"2022-05-04T16:25:54.201483Z","iopub.status.idle":"2022-05-04T16:26:03.617806Z","shell.execute_reply.started":"2022-05-04T16:25:54.201445Z","shell.execute_reply":"2022-05-04T16:26:03.617137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We investigated whether the power generation is stationary with the dickey fuller test. Since the time series is stationary, we did not need seasonal analysis.","metadata":{}},{"cell_type":"code","source":"#This function checks whether the time series is stationary with the Dickey Fuller test.\ndef check_adfuller(ts):\n    result = adfuller(ts, autolag='AIC')\n    print('Test statistic: ' , result[0])\n    print('p-value: '  ,result[1])\n    print('Critical Values:' ,result[4])\n    if result[4]['1%'] > result[0]:\n        print('Time series is stationary')\n    else:\n        print('Time series is Non-stationary')\n\n#This function softens the time series with rolling mean,std properties and allows it to be displayed together.\ndef check_mean_std(ts, col):\n    rolmean = ts.rolling(window=24).mean()\n    rolstd = ts.rolling(window=24).std()\n    plt.figure(figsize=(22,10))\n    orig = plt.plot(ts, color='red', label='Original')\n    mean = plt.plot(rolmean, color='black', label='Rolling Mean')\n    std = plt.plot(rolstd, color='green', label = 'Rolling Std')\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Mean Temperature\")\n    plt.title(f'Rolling Mean & Standard Deviation {col}')\n    plt.legend()\n    \nselected_columns = [\"Power(kW)\"]\n\nfor i in selected_columns:\n    check_adfuller(allDF[i].dropna(axis = 0))\n    check_mean_std(allDF[i], i)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T16:26:03.618768Z","iopub.execute_input":"2022-05-04T16:26:03.619452Z","iopub.status.idle":"2022-05-04T16:26:50.598373Z","shell.execute_reply.started":"2022-05-04T16:26:03.619407Z","shell.execute_reply":"2022-05-04T16:26:50.597405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This function plots correlation heatmap of data\ndef heatmap_and_corr(df, selected_columns):\n    cmap = sns.diverging_palette(250, 15, s=75, \n                                 l=40, n=9, \n                                 center=\"light\", \n                                 as_cmap=True)\n\n    matrix = df[selected_columns].corr(method=\"pearson\")\n    mask = np.triu(np.ones_like(matrix, dtype=bool))\n    fig, ax = plt.subplots(figsize=(10, 10))\n    \n    sns.heatmap(matrix, mask=mask, cmap=cmap, \n                square=True, annot=True, fmt=\".2f\", ax=ax)\n\n\nselected_columns = allDF.columns[:10]\nheatmap_and_corr(allDF, selected_columns)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T16:26:50.599538Z","iopub.execute_input":"2022-05-04T16:26:50.60019Z","iopub.status.idle":"2022-05-04T16:26:51.306369Z","shell.execute_reply.started":"2022-05-04T16:26:50.600156Z","shell.execute_reply":"2022-05-04T16:26:51.305484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This function visualizes how many unique data are in features in train and test data.\ndef show_unique_values(df,percent, ascending):\n    frame = dict()\n    for i in df.columns:\n        frame[i] = df[i].nunique()\n    result = pd.DataFrame(data = frame.values(), index = frame.keys(),columns = [\"Val\"])\n    result = result.sort_values(\"Val\", ascending= ascending)\n    if percent == True:\n        result = result/result.max()\n    return result\n\n\ndef plot_together(n_th, percent = True, ascending=True):\n    ax = show_unique_values(trainDF, percent, ascending).head(n_th).plot.barh(alpha = 0.7)\n    show_unique_values(submissionxDF, percent, ascending).head(n_th).plot.barh(ax = ax, color=\"red\", alpha = 0.7)\n    plt.legend([\"Train\", \"Test\"])\n    return\n\nplot_together(20, percent = False, ascending=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T16:26:51.307562Z","iopub.execute_input":"2022-05-04T16:26:51.307862Z","iopub.status.idle":"2022-05-04T16:26:51.91047Z","shell.execute_reply.started":"2022-05-04T16:26:51.307829Z","shell.execute_reply":"2022-05-04T16:26:51.90963Z"},"trusted":true},"execution_count":null,"outputs":[]}]}