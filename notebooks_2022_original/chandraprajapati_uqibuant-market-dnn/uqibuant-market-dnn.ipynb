{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-19T08:57:26.768123Z","iopub.execute_input":"2022-04-19T08:57:26.7686Z","iopub.status.idle":"2022-04-19T08:57:26.79346Z","shell.execute_reply.started":"2022-04-19T08:57:26.768512Z","shell.execute_reply":"2022-04-19T08:57:26.792801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc","metadata":{"execution":{"iopub.status.busy":"2022-04-19T08:57:26.79466Z","iopub.execute_input":"2022-04-19T08:57:26.794998Z","iopub.status.idle":"2022-04-19T08:57:27.870756Z","shell.execute_reply.started":"2022-04-19T08:57:26.794971Z","shell.execute_reply":"2022-04-19T08:57:27.870059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nn_features = 300\nfeatures = [f\"f_{i}\" for i in range(300)]\nfeature_column = ['investment_id', 'time_id' ] + features\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl' )\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T08:57:27.872157Z","iopub.execute_input":"2022-04-19T08:57:27.872386Z","iopub.status.idle":"2022-04-19T08:57:43.685444Z","shell.execute_reply.started":"2022-04-19T08:57:27.872358Z","shell.execute_reply":"2022-04-19T08:57:43.684666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\n#print(train.info())","metadata":{"execution":{"iopub.status.busy":"2022-04-19T08:57:43.686902Z","iopub.execute_input":"2022-04-19T08:57:43.687269Z","iopub.status.idle":"2022-04-19T08:57:43.69342Z","shell.execute_reply.started":"2022-04-19T08:57:43.687226Z","shell.execute_reply":"2022-04-19T08:57:43.692417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us try to understand the data\n1. There are 1211 time_id's recorded with min value 0 to some value and max value 1219.\n1. There are 3579 investment_id's recorded with min 0 and max 3773.","metadata":{}},{"cell_type":"code","source":"print(train['time_id'].nunique())\nprint(train['time_id'].min())\nprint(train['time_id'].max())\nprint(train['investment_id'].nunique())\nprint(train['investment_id'].min())\nprint(train['investment_id'].max())","metadata":{"execution":{"iopub.status.busy":"2022-04-19T08:57:43.695044Z","iopub.execute_input":"2022-04-19T08:57:43.695253Z","iopub.status.idle":"2022-04-19T08:57:43.765632Z","shell.execute_reply.started":"2022-04-19T08:57:43.695227Z","shell.execute_reply":"2022-04-19T08:57:43.764681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras import Input, layers, Model\nfrom keras.metrics import RootMeanSquaredError","metadata":{"execution":{"iopub.status.busy":"2022-04-19T08:57:43.766979Z","iopub.execute_input":"2022-04-19T08:57:43.767859Z","iopub.status.idle":"2022-04-19T08:57:49.422605Z","shell.execute_reply.started":"2022-04-19T08:57:43.767815Z","shell.execute_reply":"2022-04-19T08:57:49.421768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_id = train['investment_id']\nlist(investment_id.unique())\ninvestment_ids = list(investment_id.unique())\n#print(len(investment_ids))\n#investment_ids","metadata":{"execution":{"iopub.status.busy":"2022-04-19T08:57:49.423813Z","iopub.execute_input":"2022-04-19T08:57:49.424035Z","iopub.status.idle":"2022-04-19T08:57:49.465271Z","shell.execute_reply.started":"2022-04-19T08:57:49.424009Z","shell.execute_reply":"2022-04-19T08:57:49.464162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_id_size = len(investment_ids) + 1\nprint(investment_id_size)\n# an IntegerLookup layer maps integer features to contiguous ranges. This maps a set of arbitrary integer input token \n# into indexed integer output via a table based vocabulary lookup. The output indices will be contigously arranged upto\n# maximum vocab size. The layer supports multiple options for encoding the output via output_mode \ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens = investment_id_size) # lookup layer with adapted vocab\ninvestment_id_lookup_layer.adapt(pd.DataFrame({'investment_ids':investment_ids})) # \n#investment_id_lookup_layer.get_vocabulary()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T08:57:49.466577Z","iopub.execute_input":"2022-04-19T08:57:49.46732Z","iopub.status.idle":"2022-04-19T08:57:49.828537Z","shell.execute_reply.started":"2022-04-19T08:57:49.467279Z","shell.execute_reply":"2022-04-19T08:57:49.827785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets split the features and target\ninvestment_id = train['investment_id'] \ntarget = train['target']\ntrain = train.drop(['time_id','investment_id','target'], axis=1)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T08:57:49.829749Z","iopub.execute_input":"2022-04-19T08:57:49.829965Z","iopub.status.idle":"2022-04-19T08:57:54.940951Z","shell.execute_reply.started":"2022-04-19T08:57:49.829941Z","shell.execute_reply":"2022-04-19T08:57:54.940029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare the dataset into X and Y, with X being features and investment_id and Y being the target\ndef preprocess(X,Y):\n    return X,Y\n\ndef make_dataset(features, investment_id, target, batch_size = 1024, mode = 'train'):#investment_id, \n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, features), target)) #, investment_id\n    #ds = tf.constant(((features, investment_id), target))\n    ds = ds.map(preprocess)\n    if mode == 'train':\n        ds = ds.shuffle(4096)\n    \n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    # cache transform keeps the dataset either into menory or on local storage after they are loaded after firts epoch\n    # prefetch makes sure to overlap the preprocessing the dataset and model execution while training\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-04-19T08:57:54.942477Z","iopub.execute_input":"2022-04-19T08:57:54.942715Z","iopub.status.idle":"2022-04-19T08:57:54.948604Z","shell.execute_reply.started":"2022-04-19T08:57:54.942686Z","shell.execute_reply":"2022-04-19T08:57:54.947694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build the deep neural network model with the help of helper funtion build_model\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay( initial_learning_rate = 0.001,\n                                                              decay_steps = 4000, decay_rate = 0.98)\ndef build_model():\n    # the inputs are investment_id and featuresso we need to create two separate inputs \n    input_investment_id = Input((1,), dtype=tf.uint16)\n    input_features = Input((300,), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(input_investment_id)\n    # create embedding layer of shape (1 X 32)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length = 1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1,))(investment_id_x)  # get layer of shape (32)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x) # size = 64\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x) # size = 64\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    #feature_x = layers.Reshape((-1,))(input_features)\n    feature_x = layers.Dense(256, activation = 'swish')(input_features)\n    feature_x = layers.Dense(256, activation = 'swish')(feature_x)\n    feature_x = layers.Dense(256, activation = 'swish')(feature_x)\n    feature_x = layers.Dense(256, activation = 'swish')(feature_x)\n    \n    # concatenate the two input layers\n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x]) #investment_id_x, \n    x = layers.Dense(512, activation='swish',kernel_regularizer='l2')(x)\n    x = layers.Dense(128, activation='swish',kernel_regularizer='l2')(x)\n    x = layers.Dense(32, activation='swish',kernel_regularizer='l2')(x) \n    \n    output = layers.Dense(1)(x)\n    rmse = RootMeanSquaredError(name = 'rmse')\n    model = Model(inputs = [input_investment_id, input_features], outputs = [output]) #input_investment_id, \n    model.compile(optimizer=tf.optimizers.Adam(lr_schedule), loss = 'mse', metrics=['mse', 'mae', 'mape', rmse])\n    return model\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-19T08:57:54.952278Z","iopub.execute_input":"2022-04-19T08:57:54.952604Z","iopub.status.idle":"2022-04-19T08:57:55.394082Z","shell.execute_reply.started":"2022-04-19T08:57:54.952561Z","shell.execute_reply":"2022-04-19T08:57:55.393443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\nimport scipy.stats as stats\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-04-19T08:57:55.395175Z","iopub.execute_input":"2022-04-19T08:57:55.395785Z","iopub.status.idle":"2022-04-19T08:57:55.58937Z","shell.execute_reply.started":"2022-04-19T08:57:55.395747Z","shell.execute_reply":"2022-04-19T08:57:55.588512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# lets use stratified K-Fold to do cross validation\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=112)\n#kfold.get_n_splits([investment_id, features], target)\nmodels = []\nfor index, (train_index, val_index) in enumerate(kfold.split(train, investment_id)): #\n    #print('index: ', index, 'train_index: ', train_index, 'test_index: ', val_index)\n    X_train, X_val = train.iloc[train_index], train.iloc[val_index]\n    Y_train, Y_val = target[train_index], target[val_index]#print(X_train)\n    investment_id_train = investment_id[train_index]\n    investment_id_val = investment_id[val_index]\n    \n    train_ds = make_dataset(X_train, investment_id_train, Y_train) #investment_id_train,\n    val_ds = make_dataset(X_val, investment_id_val, Y_val, mode = 'validation') #investment_id_val, \n    checkpoint = ModelCheckpoint(f\"model_{index}\", save_best_only=True)\n    earlystoping = EarlyStopping(patience=10)\n    \n    model = build_model()\n    print(model.summary())\n    history = model.fit(train_ds, epochs=20, validation_data=val_ds, callbacks=[checkpoint, earlystoping])\n    model = tf.keras.models.load_model(f\"model_{index}\")\n    models.append(model)\n    pearson_coef = stats.pearsonr(model.predict(val_ds).ravel(), Y_val.values)[0]\n    print('pearson coefficients is: ', pearson_coef)\n    \n    del investment_id_train\n    del investment_id_val\n    del X_train\n    del X_val\n    del Y_train\n    del Y_val\n    del train_ds\n    del val_ds\n    gc.collect()\n    break","metadata":{"execution":{"iopub.status.busy":"2022-04-19T08:57:55.590551Z","iopub.execute_input":"2022-04-19T08:57:55.590817Z","iopub.status.idle":"2022-04-19T09:33:07.884816Z","shell.execute_reply.started":"2022-04-19T08:57:55.590789Z","shell.execute_reply":"2022-04-19T09:33:07.883802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tf.keras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T09:33:07.887596Z","iopub.execute_input":"2022-04-19T09:33:07.887835Z","iopub.status.idle":"2022-04-19T09:33:07.893868Z","shell.execute_reply.started":"2022-04-19T09:33:07.887809Z","shell.execute_reply":"2022-04-19T09:33:07.892793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/ubiquant-market-test/example_test.csv')\nprint(test.shape)\ntest.head()\nX_test = test.drop(['row_id','time_id','investment_id'],axis=1)\nX_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T09:33:07.895456Z","iopub.execute_input":"2022-04-19T09:33:07.895877Z","iopub.status.idle":"2022-04-19T09:33:07.956283Z","shell.execute_reply.started":"2022-04-19T09:33:07.895843Z","shell.execute_reply":"2022-04-19T09:33:07.955654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocess test dataset\ndef preprocess_test(investment_id, test):\n    return (investment_id, test), 0\ndef make_test_dataset(features, investment_id, batch_size = 1024):\n    test_ds = tf.data.Dataset.from_tensor_slices((investment_id, features))\n    test_ds = test_ds.map(preprocess_test)\n    test_ds = test_ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return test_ds\ndef predict_test(model, ds):\n    test_preds = []\n    for model in models:\n        test_pred = model.predict(ds)\n        test_preds.append(test_pred)\n    return np.mean(test_preds, axis=0)\n\ntest_ds = make_test_dataset(X_test, test['investment_id']) \ntest_pred = predict_test(model, test_ds)\ntest_pred","metadata":{"execution":{"iopub.status.busy":"2022-04-19T09:33:07.957225Z","iopub.execute_input":"2022-04-19T09:33:07.957815Z","iopub.status.idle":"2022-04-19T09:33:08.269601Z","shell.execute_reply.started":"2022-04-19T09:33:07.957786Z","shell.execute_reply":"2022-04-19T09:33:08.268764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/ubiquant-market-test/example_sample_submission.csv')\nsample_submission['target'] = test_pred\nsample_submission\nsample_submission.to_csv('sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-19T09:33:08.270756Z","iopub.execute_input":"2022-04-19T09:33:08.270996Z","iopub.status.idle":"2022-04-19T09:33:08.28433Z","shell.execute_reply.started":"2022-04-19T09:33:08.270969Z","shell.execute_reply":"2022-04-19T09:33:08.283696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    test_ds = make_test_dataset(test_df[features], test_df['investment_id'])\n    sample_prediction_df['target'] = predict_test(model, test_ds) # make your predictions here\n    env.predict(sample_prediction_df)   # register your predictionsa","metadata":{"execution":{"iopub.status.busy":"2022-04-19T09:33:08.285499Z","iopub.execute_input":"2022-04-19T09:33:08.286366Z","iopub.status.idle":"2022-04-19T09:33:08.785587Z","shell.execute_reply.started":"2022-04-19T09:33:08.286327Z","shell.execute_reply":"2022-04-19T09:33:08.784767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}