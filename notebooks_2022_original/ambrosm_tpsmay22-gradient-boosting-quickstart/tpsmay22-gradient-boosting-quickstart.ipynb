{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gradient-Boosting Quickstart for TPSMAY22\n\nThis notebook shows how to train a gradient booster with minimal feature engineering. For the corresponding EDA, see the [separate EDA notebook](https://www.kaggle.com/code/ambrosm/tpsmay22-eda-which-makes-sense).\n\nRelease notes:\n- V1: XGB\n- V2: LightGBM, one more feature","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nfrom cycler import cycler\nfrom IPython.display import display\nimport datetime\nimport scipy.stats\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.calibration import CalibrationDisplay\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nplt.rcParams['axes.facecolor'] = '#0057b8' # blue\nplt.rcParams['axes.prop_cycle'] = cycler(color=['#ffd700'] +\n                                         plt.rcParams['axes.prop_cycle'].by_key()['color'][1:])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-01T08:40:27.66491Z","iopub.execute_input":"2022-05-01T08:40:27.665581Z","iopub.status.idle":"2022-05-01T08:40:28.960464Z","shell.execute_reply.started":"2022-05-01T08:40:27.665455Z","shell.execute_reply":"2022-05-01T08:40:28.959163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering\n\nWe read the data and apply minimal feature engineering: We only split the `f_27` string into ten separate features as described in the [EDA](https://www.kaggle.com/code/ambrosm/tpsmay22-eda-which-makes-sense), and we count the unique characters in the string.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv')\nfor df in [train, test]:\n    for i in range(10):\n        df[f'ch{i}'] = df.f_27.str.get(i).apply(ord) - ord('A')\n    # unique_characters feature is from https://www.kaggle.com/code/cabaxiom/tps-may-22-eda-lgbm-model\n    df[\"unique_characters\"] = df.f_27.apply(lambda s: len(set(s)))\nfeatures = [f for f in test.columns if f != 'id' and f != 'f_27']\ntest[features].head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T08:40:28.96254Z","iopub.execute_input":"2022-05-01T08:40:28.96283Z","iopub.status.idle":"2022-05-01T08:41:00.77025Z","shell.execute_reply.started":"2022-05-01T08:40:28.962795Z","shell.execute_reply":"2022-05-01T08:41:00.769291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-validation\n\nFor cross-validation, we use a simple KFold with five splits. It turned out that the scores of the five splits are very similar so that I usually run only the first split. This one split is good enough to evaluate the model.","metadata":{}},{"cell_type":"code","source":"%%time\n# Cross-validation of the classifier\n\ndef my_booster(random_state=1):\n#     return HistGradientBoostingClassifier(learning_rate=0.4, max_leaf_nodes=150,\n#                                           max_iter=1000, min_samples_leaf=4000,\n#                                           l2_regularization=1,\n#                                           validation_fraction=0.05,\n#                                           max_bins=255,\n#                                           random_state=random_state, verbose=1)\n#     return XGBClassifier(n_estimators=400, n_jobs=-1,\n#                          eval_metric=['logloss'],\n#                          #max_depth=10,\n#                          colsample_bytree=0.8,\n#                          #gamma=1.4,\n#                          reg_alpha=6, reg_lambda=1.5,\n#                          tree_method='hist',\n#                          #max_bin=511,\n#                          learning_rate=0.4,\n#                          verbosity=1,\n#                          use_label_encoder=False, random_state=random_state)\n    return LGBMClassifier(n_estimators=5000, min_child_samples=80,\n                          max_bins=511, random_state=random_state)\n      \nprint(f\"{len(features)} features\")\nscore_list = []\nkf = KFold(n_splits=5)\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(train)):\n    X_tr = train.iloc[idx_tr][features]\n    X_va = train.iloc[idx_va][features]\n    y_tr = train.iloc[idx_tr].target\n    y_va = train.iloc[idx_va].target\n    \n    model = my_booster()\n\n    if True or type(model) != XGBClassifier:\n        model.fit(X_tr.values, y_tr)\n    else:\n        model.fit(X_tr.values, y_tr, eval_set = [(X_va.values, y_va)], \n                  early_stopping_rounds=30, verbose=10)\n    y_va_pred = model.predict_proba(X_va.values)[:,1]\n    score = roc_auc_score(y_va, y_va_pred)\n    try:\n        print(f\"Fold {fold}: n_iter ={model.n_iter_:5d}    AUC = {score:.3f}\")\n    except AttributeError:\n        print(f\"Fold {fold}:                  AUC = {score:.3f}\")\n    score_list.append(score)\n    break # we only need the first fold\n    \nprint(f\"OOF AUC:                       {np.mean(score_list):.3f}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-01T08:41:00.771952Z","iopub.execute_input":"2022-05-01T08:41:00.772467Z","iopub.status.idle":"2022-05-01T08:45:41.250476Z","shell.execute_reply.started":"2022-05-01T08:41:00.772419Z","shell.execute_reply":"2022-05-01T08:45:41.249461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Three diagrams for model evaluation\n\nWe plot the ROC curve just because it looks nice. The area under the red curve is the score of our model.\n","metadata":{}},{"cell_type":"code","source":"# Plot the roc curve for the last fold\ndef plot_roc_curve(y_va, y_va_pred):\n    plt.figure(figsize=(8, 8))\n    fpr, tpr, _ = roc_curve(y_va, y_va_pred)\n    plt.plot(fpr, tpr, color='r', lw=2)\n    plt.plot([0, 1], [0, 1], color=\"navy\", lw=1, linestyle=\"--\")\n    plt.gca().set_aspect('equal')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"Receiver operating characteristic\")\n    plt.show()\n\nplot_roc_curve(y_va, y_va_pred)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-01T08:45:41.25248Z","iopub.execute_input":"2022-05-01T08:45:41.252736Z","iopub.status.idle":"2022-05-01T08:45:41.666101Z","shell.execute_reply.started":"2022-05-01T08:45:41.252705Z","shell.execute_reply":"2022-05-01T08:45:41.664944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Second, we plot a histogram of the out-of-fold predictions. Many predictions are near 0.0 or near 1.0; this means that in many cases the classifier's predictions have high confidence:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nplt.hist(y_va_pred, bins=25, density=True)\nplt.title('Histogram of the oof predictions')\nplt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-01T08:45:41.667881Z","iopub.execute_input":"2022-05-01T08:45:41.668347Z","iopub.status.idle":"2022-05-01T08:45:41.902101Z","shell.execute_reply.started":"2022-05-01T08:45:41.668293Z","shell.execute_reply":"2022-05-01T08:45:41.901186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we plot the calibration curve. The curve here is almost a straight line, which means that the predicted probabilities are almost exact: ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nCalibrationDisplay.from_predictions(y_va, y_va_pred, n_bins=20, strategy='quantile', ax=plt.gca())\nplt.title('Probability calibration')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-01T08:45:41.903301Z","iopub.execute_input":"2022-05-01T08:45:41.903541Z","iopub.status.idle":"2022-05-01T08:45:42.158098Z","shell.execute_reply.started":"2022-05-01T08:45:41.90351Z","shell.execute_reply":"2022-05-01T08:45:42.157472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n\nFor the submission, we re-train the model on several different seeds and then submit the mean of the ranks.","metadata":{}},{"cell_type":"code","source":"# Create submission\nprint(f\"{len(features)} features\")\n\npred_list = []\nfor seed in range(10):\n    X_tr = train[features]\n    y_tr = train.target\n\n    model = my_booster(random_state=seed)\n    model.fit(X_tr.values, y_tr)\n    pred_list.append(scipy.stats.rankdata(model.predict_proba(test[features].values)[:,1]))\n    print(f\"{seed:2}\", pred_list[-1])\nprint()\nsubmission = test[['id']].copy()\nsubmission['target'] = np.array(pred_list).mean(axis=0)\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-05-01T08:45:42.15917Z","iopub.execute_input":"2022-05-01T08:45:42.159916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What next?\n\nNow it's your turn! Try to improve this model by\n- Engineering more features\n- Tuning hyperparameters\n- Replacing LightGBM by XGBoost, HistGradientBoostingClassifier or CatBoost ","metadata":{}}]}