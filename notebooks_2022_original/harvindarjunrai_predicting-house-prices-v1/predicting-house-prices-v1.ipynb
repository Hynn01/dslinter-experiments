{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-06T10:48:30.128435Z","iopub.execute_input":"2022-05-06T10:48:30.129098Z","iopub.status.idle":"2022-05-06T10:48:30.172438Z","shell.execute_reply.started":"2022-05-06T10:48:30.128993Z","shell.execute_reply":"2022-05-06T10:48:30.171569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting House Prices (Ames Housing Dataset)","metadata":{}},{"cell_type":"markdown","source":"For this project, the house prices from the Ames Housing test dataset will be predicted. There are two datasets that will be imported from this competition:\n\n- Training dataset \"train.csv\" for whom the Supervised Learning Models will be trained upon;\n- Test dataset \"test.csv\" where the house prices will be predicted.","metadata":{}},{"cell_type":"markdown","source":"## Acknowledgements\n\nThe work presented was inspired from the Machine Learning Tutorials I have gone through here on Kaggle and from other sources. I would also like to thank [A. Qua](https://www.kaggle.com/adibouayjan) and his [notebook](https://www.kaggle.com/code/adibouayjan/house-price-step-by-step-modeling) which provided the inspiration for this notebook and I highly recommended looking through if you want to pick up some important techniques with regards to exploratory data analysis, feature engineering and training and analysis of supervised learning models.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Model functions\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n# Statistics functions\nfrom scipy import stats\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import chi2\n\n# Functions to calculate Mean Absolute Error (MAE) and Mean Squared Error (MSE)\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\n# Function to split data into different groups\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import VarianceThreshold\n\n# Function to deal with missing values via imputation\nfrom sklearn.impute import SimpleImputer\n\n# Function that converts categorical values into numerical values via ordinal encoding or one-hot encoding\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:30.209836Z","iopub.execute_input":"2022-05-06T10:48:30.210952Z","iopub.status.idle":"2022-05-06T10:48:31.559352Z","shell.execute_reply.started":"2022-05-06T10:48:30.210843Z","shell.execute_reply":"2022-05-06T10:48:31.558325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, we import the training and test datasets from the House Prices Advanced Regression competition into data_train and data_test respectively. As seen by the shape of the two datasets, data_test has one less column than data_train as it does not contain the target variable \"SalePrice\".","metadata":{}},{"cell_type":"code","source":"data_train = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ndata_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n\nprint(f\"Training set shape: {data_train.shape}\\n\")\nprint(f\"Test set shape: {data_test.shape}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:31.561008Z","iopub.execute_input":"2022-05-06T10:48:31.561632Z","iopub.status.idle":"2022-05-06T10:48:31.628646Z","shell.execute_reply.started":"2022-05-06T10:48:31.561587Z","shell.execute_reply":"2022-05-06T10:48:31.627821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:31.629835Z","iopub.execute_input":"2022-05-06T10:48:31.630081Z","iopub.status.idle":"2022-05-06T10:48:31.660003Z","shell.execute_reply.started":"2022-05-06T10:48:31.630054Z","shell.execute_reply":"2022-05-06T10:48:31.659427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the information from the data_train DataFrame, we see that some of these columns have missing values. These missing values will be dealt with later. But before we do some exploratory data analysis and feature engineering, we check to see that both datasets have the same feature variables (i.e. they both have the same columns except for \"SalePrice\" in data_train).","metadata":{}},{"cell_type":"code","source":"# Checking if column headings are the same in both data set\ndif_1 = [x for x in data_train.columns if x not in data_test.columns]\nprint(f\"Columns present in df_train and absent in df_test: {dif_1}\\n\")\n\ndif_2 = [x for x in data_test.columns if x not in data_train.columns]\nprint(f\"Columns present in df_test set and absent in df_train: {dif_2}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:31.661456Z","iopub.execute_input":"2022-05-06T10:48:31.661803Z","iopub.status.idle":"2022-05-06T10:48:31.666819Z","shell.execute_reply.started":"2022-05-06T10:48:31.661764Z","shell.execute_reply":"2022-05-06T10:48:31.666069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we proceed investigating the numerical and categorical features from both datasets, the \"Id\" column will be dropped from both datasets as well. The list of IDs from the test set will be saved so that it can be used later on when we predict the house prices from that dataset.","metadata":{}},{"cell_type":"code","source":"# Drop the 'Id' column from the training set\ndata_train.drop([\"Id\"], axis=1, inplace=True)\n\n# Save the 'Id' list before dropping it from the test set\nId_test_list = data_test[\"Id\"].tolist()\ndata_test.drop([\"Id\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:31.667921Z","iopub.execute_input":"2022-05-06T10:48:31.668545Z","iopub.status.idle":"2022-05-06T10:48:31.688926Z","shell.execute_reply.started":"2022-05-06T10:48:31.668504Z","shell.execute_reply":"2022-05-06T10:48:31.688289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At certain stages of this project, the shape of both datasets will be printed out to clarify that both datasets have had the same features either added on or removed. The shape of both datasets now before we investigate the numerical and categorical features from both datasets stands as follows:","metadata":{}},{"cell_type":"code","source":"print(f\"Training set shape: {data_train.shape}\\n\")\nprint(f\"Test set shape: {data_test.shape}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:31.689939Z","iopub.execute_input":"2022-05-06T10:48:31.690452Z","iopub.status.idle":"2022-05-06T10:48:31.695163Z","shell.execute_reply.started":"2022-05-06T10:48:31.690425Z","shell.execute_reply":"2022-05-06T10:48:31.694626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Numerical Data","metadata":{}},{"cell_type":"markdown","source":"Firstly we will focus on the numerical features from both datasets where we will do some exploratory data analysis and some feature engineering to\n\n* Investigate the distribution of these numerical features;\n* Investigate the correlation of these numerical features with \"SalePrice\" and drop any features that have low correlation;\n* Deal with missing values through imputation.\n\nTo start off with, we select only the numerical columns from data_train (including \"SalePrice\") by obtaining the list of numerical columns from data_train in numerical_cols, and using this to take out the numerical columns from data_train and store them in a new DataFrame data_train_num (we also do the same thing with the test dataset).","metadata":{}},{"cell_type":"code","source":"# Select numerical columns from the training dataset\nnumerical_cols = [cname for cname in data_train.columns if \n                  data_train[cname].dtype in ['int64', 'float64']]\n\nnumerical_cols_test = [cname for cname in data_test.columns if \n                      data_test[cname].dtype in ['int64', 'float64']]\n\ndata_train_num = data_train[numerical_cols].copy()\ndata_test_num = data_test[numerical_cols_test].copy()\n\ndata_train_num.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:31.696026Z","iopub.execute_input":"2022-05-06T10:48:31.696428Z","iopub.status.idle":"2022-05-06T10:48:31.731434Z","shell.execute_reply.started":"2022-05-06T10:48:31.6964Z","shell.execute_reply":"2022-05-06T10:48:31.730914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution","metadata":{}},{"cell_type":"code","source":"# Plot the distribution of all the numerical features\nfig_ = data_train_num.hist(figsize=(16, 20), bins=50, color=\"deepskyblue\",\n                           edgecolor=\"black\", xlabelsize=8, ylabelsize=8)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:31.732448Z","iopub.execute_input":"2022-05-06T10:48:31.732757Z","iopub.status.idle":"2022-05-06T10:48:38.937559Z","shell.execute_reply.started":"2022-05-06T10:48:31.732732Z","shell.execute_reply":"2022-05-06T10:48:38.936595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the distriutions of each numerical feature, we see that there are a variety of distributions on show and that we have a mixture of discrete and continuous variables in our dataset. One thing that we can spot from these plots are variables that has small variation (i.e. they have very similar values) - these variables are likely to only have a small impact on the final price of the house. We will drop any variables where 95% of the values are similar or constant.","metadata":{}},{"cell_type":"code","source":"# Drop any quasi-constant features where 95% of the values are similar or constant\nsel = VarianceThreshold(threshold=0.05) # 0.05: drop column where 95% of the values are constant\n\n# The fit finds the features with constant variance\nsel.fit(data_train_num.iloc[:, :-1])\n\n\n# Get the number of features that are not constant\nprint(f\"Number of retained features: {sum(sel.get_support())}\")\nprint(f\"\\nNumber of quasi_constant features: {len(data_train_num.iloc[:, :-1].columns) - sum(sel.get_support())}\")\n\nquasi_constant_features_list = [x for x in data_train_num.iloc[:, :-1].columns if x not in data_train_num.iloc[:, :-1].columns[sel.get_support()]]\n\nprint(f\"\\nQuasi-constant features to be dropped: {quasi_constant_features_list}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:38.939011Z","iopub.execute_input":"2022-05-06T10:48:38.939392Z","iopub.status.idle":"2022-05-06T10:48:38.96322Z","shell.execute_reply.started":"2022-05-06T10:48:38.93935Z","shell.execute_reply":"2022-05-06T10:48:38.962524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this, we see that only one variable \"KitchenAbvGr\" (Kitchens above the grade) has at least 95% of variables that are similar and constant, so this will be removed from both the training and test datasets.","metadata":{}},{"cell_type":"code","source":"# Drop quasi-constant features from both datasets\ndata_train_num.drop(quasi_constant_features_list, axis=1, inplace=True)\ndata_test_num.drop(quasi_constant_features_list, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:38.965363Z","iopub.execute_input":"2022-05-06T10:48:38.965571Z","iopub.status.idle":"2022-05-06T10:48:38.971226Z","shell.execute_reply.started":"2022-05-06T10:48:38.965547Z","shell.execute_reply":"2022-05-06T10:48:38.970279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Training set shape (Numerical features): {data_train_num.shape}\\n\")\nprint(f\"Test set shape (Numerical features): {data_test_num.shape}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:38.972225Z","iopub.execute_input":"2022-05-06T10:48:38.97265Z","iopub.status.idle":"2022-05-06T10:48:38.983701Z","shell.execute_reply.started":"2022-05-06T10:48:38.97261Z","shell.execute_reply":"2022-05-06T10:48:38.982811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation","metadata":{}},{"cell_type":"markdown","source":"We will now produce a correlation heatmap showing the correlation between all of the numerical variables including the correlation of each numerical feature with \"SalePrice\". Any variables that have a low correlation with \"SalePrice\" is likely not to have a huge impact on the final sale price of the house, any variables with correlation less than |0.3| will be replaced by 0 for simplicity.","metadata":{}},{"cell_type":"code","source":"# Heatmap for all the remaining numerical data including the taget 'SalePrice'\n\n# Define the heatmap parameters\npd.options.display.float_format = \"{:,.2f}\".format\n\n# Define correlation matrix\ncorr_matrix = data_train_num.corr()\n\n# Replace any correlation < |0.3| by 0 for a better visibility\ncorr_matrix[(corr_matrix < 0.3) & (corr_matrix > -0.3)] = 0\n\n# Mask the upper part of the heatmap\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n\n# Choose the color map\ncmap = \"viridis\"\n\n# plot the heatmap\nsns.set(rc = {'figure.figsize':(20,15)})\nsns.heatmap(corr_matrix, mask=mask, vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot_kws={\"size\": 9, \"color\": \"black\"}, square=True, cmap=cmap, annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:38.984861Z","iopub.execute_input":"2022-05-06T10:48:38.98516Z","iopub.status.idle":"2022-05-06T10:48:41.976649Z","shell.execute_reply.started":"2022-05-06T10:48:38.985124Z","shell.execute_reply":"2022-05-06T10:48:41.975754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the heatmap above, we see that 18 of these numerical features has some noticeable correlation with \"SalePrice\". There are also a few variables that have a very strong positive correlation (of at least 0.8) including\n\n* \"GarageArea\" and \"GarageCars\" (0.88)\n* \"GarageYrBlt\" and \"YearBuilt\" (0.83)\n* \"TotRmsAbvGrd\" and \"GrLivArea\" (0.83)\n* \"1stFlrSF\" and \"TotalBsmtSF\" (0.82)\n\nThe other correlations will be dealt with later, but first we will focus on features that have a correlaton of more than |0.3| with \"SalePrice\".","metadata":{}},{"cell_type":"code","source":"# Select features where the correlation with 'SalePrice' is higher than |0.3|\n# -1 because the latest row is SalePrice\ndata_num_corr = data_train_num.corr()[\"SalePrice\"][:-1]\n\n# Correlated features (r2 > 0.5)\nhigh_features_list = data_num_corr[abs(data_num_corr) >= 0.5].sort_values(ascending=False)\nprint(f\"{len(high_features_list)} strongly correlated values with SalePrice:\\n{high_features_list}\\n\")\n\n# Correlated features (0.3 < r2 < 0.5)\nlow_features_list = data_num_corr[(abs(data_num_corr) < 0.5) & (abs(data_num_corr) >= 0.3)].sort_values(ascending=False)\nprint(f\"{len(low_features_list)} slightly correlated values with SalePrice:\\n{low_features_list}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:41.977776Z","iopub.execute_input":"2022-05-06T10:48:41.977985Z","iopub.status.idle":"2022-05-06T10:48:41.995351Z","shell.execute_reply.started":"2022-05-06T10:48:41.977961Z","shell.execute_reply":"2022-05-06T10:48:41.994552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features with high correlation (higher than 0.5)\nstrong_features = data_num_corr[abs(data_num_corr) >= 0.5].index.tolist()\nstrong_features.append(\"SalePrice\")\n\ndata_strong_features = data_train_num.loc[:, strong_features]\n\nplt.style.use(\"seaborn-whitegrid\")  # define figures style\nfig, ax = plt.subplots(round(len(strong_features) / 3), 3)\n\nfor i, ax in enumerate(fig.axes):\n    # plot the correlation of each feature with SalePrice\n    if i < len(strong_features)-1:\n        sns.regplot(x=strong_features[i], y=\"SalePrice\", data=data_strong_features, ax=ax, scatter_kws={\n                    \"color\": \"deepskyblue\"}, line_kws={\"color\": \"black\"})","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:41.997747Z","iopub.execute_input":"2022-05-06T10:48:41.998321Z","iopub.status.idle":"2022-05-06T10:48:45.778516Z","shell.execute_reply.started":"2022-05-06T10:48:41.998277Z","shell.execute_reply":"2022-05-06T10:48:45.774923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features with low correlation (between 0.3 and 0.5)\nlow_features = data_num_corr[(abs(data_num_corr) >= 0.3) & (abs(data_num_corr) < 0.5)].index.tolist()\nlow_features.append(\"SalePrice\")\n\ndata_low_features = data_train_num.loc[:, low_features]\n\nplt.style.use(\"seaborn-whitegrid\")  # define figures style\nfig, ax = plt.subplots(round(len(low_features) / 3), 3)\n\nfor i, ax in enumerate(fig.axes):\n    # plot the correlation of each feature with SalePrice\n    if i < len(low_features) - 1:\n        sns.regplot(x=low_features[i], y=\"SalePrice\", data=data_low_features, ax=ax, scatter_kws={\n                    \"color\": \"deepskyblue\"}, line_kws={\"color\": \"black\"},)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:45.779663Z","iopub.execute_input":"2022-05-06T10:48:45.779867Z","iopub.status.idle":"2022-05-06T10:48:49.005326Z","shell.execute_reply.started":"2022-05-06T10:48:45.779843Z","shell.execute_reply":"2022-05-06T10:48:49.004273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at some of these scatter plots related to the surface area of certain rooms in the house (\"1stFlrSF\", \"TotalBsmtSF\", \"GrLivingArea\"), there is at least one house whose price is relatively inexpensive given their surface area (located on the bottom right-hand-side of those graphs) which suggests the presence of outliers in the dataset. These will be dealt with in a future version of this project, but for now we will keep these numerical features shown in the scatter plot above.","metadata":{}},{"cell_type":"code","source":"# Define the list of numerical fetaures to keep\nlist_of_numerical_features = strong_features[:-1] + low_features\n\nprint(\"List of features to be kept in the dataset:\")\nprint(list_of_numerical_features)\n\n# Select these features form our training set\ndata_train_num = data_train_num.loc[:, list_of_numerical_features]\n\n# Select the same features from the test set (-1 -> except 'SalePrice')\ndata_test_num = data_test_num.loc[:, list_of_numerical_features[:-1]]","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:49.006868Z","iopub.execute_input":"2022-05-06T10:48:49.007142Z","iopub.status.idle":"2022-05-06T10:48:49.014191Z","shell.execute_reply.started":"2022-05-06T10:48:49.007111Z","shell.execute_reply":"2022-05-06T10:48:49.013601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the other correlation values beyond \"SalePrice\", since the correlation between \"GarageCars\" and \"GarageArea\" is so high, \"GarageCars\" will be dropped from both datasets as this is not likely to add in any relevant new information with regards to the final price of the house.","metadata":{}},{"cell_type":"code","source":"data_train_num.drop([\"GarageCars\"], axis = 1, inplace = True)\ndata_test_num.drop([\"GarageCars\"], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:49.015241Z","iopub.execute_input":"2022-05-06T10:48:49.015542Z","iopub.status.idle":"2022-05-06T10:48:49.031335Z","shell.execute_reply.started":"2022-05-06T10:48:49.015518Z","shell.execute_reply":"2022-05-06T10:48:49.030658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Training set shape (Numerical features): {data_train_num.shape}\\n\")\nprint(f\"Test set shape (Numerical features): {data_test_num.shape}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:49.032302Z","iopub.execute_input":"2022-05-06T10:48:49.032858Z","iopub.status.idle":"2022-05-06T10:48:49.041526Z","shell.execute_reply.started":"2022-05-06T10:48:49.032829Z","shell.execute_reply":"2022-05-06T10:48:49.040951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Values","metadata":{}},{"cell_type":"code","source":"# Get names of columns with missing values (training set)\ncols_with_missing = [col for col in data_train_num.columns\n                     if data_train_num[col].isnull().any()]\n\nprint(\"Columns with missing (NA) values:\")\nprint(cols_with_missing)\n\n# Count how many NA values are in each of those columns\ncols_nan_count = list(map(lambda col: round(data_train_num[col].isna().sum()*100/len(data_train_num)), cols_with_missing))\n\n\ntab = pd.DataFrame(cols_with_missing, columns=[\"Column\"])\ntab[\"Percent_NaN\"] = cols_nan_count\ntab.sort_values(by=[\"Percent_NaN\"], ascending=False, inplace=True)\n\n\n# Define figure parameters\nsns.set(rc={\"figure.figsize\": (10, 7)})\nsns.set_style(\"whitegrid\")\n\n# Plot results\np = sns.barplot(x=\"Percent_NaN\", y=\"Column\", data=tab,\n                edgecolor=\"black\", color=\"deepskyblue\")\n\np.set_title(\"Percent of NaN per column of the train set\\n\", fontsize=20)\np.set_xlabel(\"\\nPercent of NaN (%)\", fontsize=20)\np.set_ylabel(\"Column Name\\n\", fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:49.042697Z","iopub.execute_input":"2022-05-06T10:48:49.04296Z","iopub.status.idle":"2022-05-06T10:48:49.260641Z","shell.execute_reply.started":"2022-05-06T10:48:49.04293Z","shell.execute_reply":"2022-05-06T10:48:49.259757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the updated numerical features training set, we see that there are three columns with missing values, with \"LotFrontage\" having around 18% missing values. The missing values from these columns will be imputed where they will be replaced with the median value from each column (which is sensible given that one of the columns is \"GarageYrBlt\" where the values are strictly discrete, plus they're less affected by outliers).","metadata":{}},{"cell_type":"code","source":"# Imputation of missing values (NaNs) with SimpleImputer\nmy_imputer = SimpleImputer(strategy=\"median\")\ndata_train_imputed = pd.DataFrame(my_imputer.fit_transform(data_train_num))\ndata_train_imputed.columns = data_train_num.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:49.261775Z","iopub.execute_input":"2022-05-06T10:48:49.262016Z","iopub.status.idle":"2022-05-06T10:48:49.273299Z","shell.execute_reply.started":"2022-05-06T10:48:49.261987Z","shell.execute_reply":"2022-05-06T10:48:49.272624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will also check the distributions to ensure that the distribution of these variables after imputation is not heavily impacted by these changes.","metadata":{}},{"cell_type":"code","source":"# Check the distribution of each imputed feature before and after imputation\n\n# Define figure parameters\nsns.set(rc={\"figure.figsize\": (14, 12)})\nsns.set_style(\"whitegrid\")\nfig, axes = plt.subplots(3, 2)\n\n# Plot the results\nfor feature, fig_pos in zip([\"LotFrontage\", \"GarageYrBlt\", \"MasVnrArea\"], [0, 1, 2]):\n\n    \"\"\"Features distribution before and after imputation\"\"\"\n\n    # before imputation\n    p = sns.histplot(ax=axes[fig_pos, 0], x=data_train_num[feature],\n                     kde=True, bins=30, color=\"deepskyblue\", edgecolor=\"black\")\n    p.set_ylabel(f\"Before imputation\", fontsize=14)\n\n    # after imputation\n    q = sns.histplot(ax=axes[fig_pos, 1], x=data_train_imputed[feature],\n                     kde=True, bins=30, color=\"darkorange\", edgecolor=\"black\")\n    q.set_ylabel(f\"After imputation\", fontsize=14)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:49.274346Z","iopub.execute_input":"2022-05-06T10:48:49.274557Z","iopub.status.idle":"2022-05-06T10:48:50.743153Z","shell.execute_reply.started":"2022-05-06T10:48:49.274534Z","shell.execute_reply":"2022-05-06T10:48:50.742302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the distribution plots, we see that the distributions for \"LotFrontage\" and \"GarageYrBlt\" have been changed by imputation (with heavy bias towards the median for \"LotFrontage\"). Since there is noticeable bias towards the median class for both \"LotFrontage\" and \"GarageYrBlt\", these features will be removed from both datasets. The distribution of \"MasVnrArea\" has not changed that much at all after imputation, and will be kept on.","metadata":{}},{"cell_type":"code","source":"# Drop 'LotFrontage' and 'GarageYrBlt'\ndata_train_imputed.drop([\"LotFrontage\", \"GarageYrBlt\"], axis=1, inplace=True)\ndata_train_imputed.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:50.74456Z","iopub.execute_input":"2022-05-06T10:48:50.745032Z","iopub.status.idle":"2022-05-06T10:48:50.761342Z","shell.execute_reply.started":"2022-05-06T10:48:50.744995Z","shell.execute_reply":"2022-05-06T10:48:50.760306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop these same features from test set\ndata_test_num.drop([\"LotFrontage\", \"GarageYrBlt\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:50.762528Z","iopub.execute_input":"2022-05-06T10:48:50.762733Z","iopub.status.idle":"2022-05-06T10:48:50.773933Z","shell.execute_reply.started":"2022-05-06T10:48:50.762709Z","shell.execute_reply":"2022-05-06T10:48:50.773065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will deal with missing values from the test dataset.","metadata":{}},{"cell_type":"code","source":"# Get names of columns with missing values (test set)\ncols_with_missing_b = [col for col in data_test_num.columns\n                       if data_test_num[col].isnull().any()]\n\nprint(\"Columns with missing (NA) values:\")\nprint(cols_with_missing_b)\n\n# Count how many NA values are in each of those columns\ncols_nan_count_b = list(map(lambda col: round(data_test_num[col].isna().sum()*100/len(data_test_num)), cols_with_missing_b))\n\n\ntab = pd.DataFrame(cols_with_missing_b, columns=[\"Column\"])\ntab[\"Percent_NaN\"] = cols_nan_count_b\ntab.sort_values(by=[\"Percent_NaN\"], ascending=False, inplace=True)\n\n\n# Define figure parameters\nsns.set(rc={\"figure.figsize\": (10, 7)})\nsns.set_style(\"whitegrid\")\n\n# Plot results\np = sns.barplot(x=\"Percent_NaN\", y=\"Column\", data=tab,\n                edgecolor=\"black\", color=\"deepskyblue\")\n\np.set_title(\"Percent of NaN per column of the train set\\n\", fontsize=20)\np.set_xlabel(\"\\nPercent of NaN (%)\", fontsize=20)\np.set_ylabel(\"Column Name\\n\", fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:50.775011Z","iopub.execute_input":"2022-05-06T10:48:50.775234Z","iopub.status.idle":"2022-05-06T10:48:50.99683Z","shell.execute_reply.started":"2022-05-06T10:48:50.775208Z","shell.execute_reply":"2022-05-06T10:48:50.995932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here there are 4 feature variables that have missing values, but they all have a very small percentage of missing values. Each of these values will be filled in with the median from each perspective column.","metadata":{}},{"cell_type":"code","source":"# Imputation of missing values (NaNs) with SimpleImputer\nmy_imputer = SimpleImputer(strategy=\"median\")\ndata_test_imputed = pd.DataFrame(my_imputer.fit_transform(data_test_num))\ndata_test_imputed.columns = data_test_num.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:50.997952Z","iopub.execute_input":"2022-05-06T10:48:50.998159Z","iopub.status.idle":"2022-05-06T10:48:51.010483Z","shell.execute_reply.started":"2022-05-06T10:48:50.998136Z","shell.execute_reply":"2022-05-06T10:48:51.009675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the distribution of each imputed feature before and after imputation\n\n# Define figure parameters\nsns.set(rc={\"figure.figsize\": (20, 18)})\nsns.set_style(\"whitegrid\")\nfig, axes = plt.subplots(4, 2)\n\n# Plot the results\nfor feature, fig_pos in zip(tab[\"Column\"].tolist(), range(0, 6)):\n\n    \"\"\"Features distribution before and after imputation\"\"\"\n\n    # before imputation\n    p = sns.histplot(ax=axes[fig_pos, 0], x=data_test_num[feature],\n                     kde=True, bins=30, color=\"deepskyblue\", edgecolor=\"black\")\n    p.set_ylabel(f\"Before imputation\", fontsize=14)\n\n    # after imputation\n    q = sns.histplot(ax=axes[fig_pos, 1], x=data_test_imputed[feature],\n                     kde=True, bins=30, color=\"darkorange\", edgecolor=\"black\",)\n    q.set_ylabel(f\"After imputation\", fontsize=14)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:51.011661Z","iopub.execute_input":"2022-05-06T10:48:51.012213Z","iopub.status.idle":"2022-05-06T10:48:53.01023Z","shell.execute_reply.started":"2022-05-06T10:48:51.012179Z","shell.execute_reply":"2022-05-06T10:48:53.009217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of each of these variables before and after imputation does not really change at all, so all of these variables will be kept on. The shape of both datasets consisting of numerical features is shown below.","metadata":{}},{"cell_type":"code","source":"print(f\"Training set shape (Numerical features): {data_train_imputed.shape}\\n\")\nprint(f\"Test set shape (Numerical features): {data_test_imputed.shape}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:53.011986Z","iopub.execute_input":"2022-05-06T10:48:53.012296Z","iopub.status.idle":"2022-05-06T10:48:53.018308Z","shell.execute_reply.started":"2022-05-06T10:48:53.012254Z","shell.execute_reply":"2022-05-06T10:48:53.017442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical Data","metadata":{}},{"cell_type":"markdown","source":"Now we focus on the categorical features from each dataset. Firstly we separate the categorical features from both datasets into their own DataFrames data_train_categ and data_test_categ.","metadata":{}},{"cell_type":"code","source":"# Categorical to Quantitative relationship\ncategorical_features = [\n    i for i in data_train.columns if data_train.dtypes[i] == \"object\"]\ncategorical_features.append(\"SalePrice\")\n\n# Train set\ndata_train_categ = data_train[categorical_features]\n\n# Test set (-1 because test set don't have 'Sale Price')\ndata_test_categ = data_test[categorical_features[:-1]]","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:53.0225Z","iopub.execute_input":"2022-05-06T10:48:53.023033Z","iopub.status.idle":"2022-05-06T10:48:53.042678Z","shell.execute_reply.started":"2022-05-06T10:48:53.022993Z","shell.execute_reply":"2022-05-06T10:48:53.041967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Training set shape (Categorical features): {data_train_categ.shape}\\n\")\nprint(f\"Test set shape (Categorical features): {data_test_categ.shape}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:53.044069Z","iopub.execute_input":"2022-05-06T10:48:53.044599Z","iopub.status.idle":"2022-05-06T10:48:53.051836Z","shell.execute_reply.started":"2022-05-06T10:48:53.044555Z","shell.execute_reply":"2022-05-06T10:48:53.050742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution","metadata":{}},{"cell_type":"code","source":"# Countplot for each of the categorical features in the training set\n# Determine which categorical features are dominated by one outcome\nfig, axes = plt.subplots(round(len(data_train_categ.columns) / 3), 3, figsize=(12, 35))\n\nfor i, ax in enumerate(fig.axes):\n    # plot barplot of each feature\n    if i < len(data_train_categ.columns) - 1:\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=90)\n        sns.countplot(x=data_train_categ.columns[i], alpha=0.7, data=data_train_categ, ax=ax)\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:48:53.053024Z","iopub.execute_input":"2022-05-06T10:48:53.053601Z","iopub.status.idle":"2022-05-06T10:49:00.060265Z","shell.execute_reply.started":"2022-05-06T10:48:53.053559Z","shell.execute_reply":"2022-05-06T10:49:00.059453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the Count Plots above, we see that some of these variables are highly dominated by one feature. Since those variables will have minimal impact on the final house prices, those variables dominated by one outcome will be removed from both datasets.","metadata":{}},{"cell_type":"code","source":"cols_to_drop = [\n    'Street',\n    'LandContour',\n    'Utilities',\n    'LandSlope',\n    'Condition2',\n    'RoofMatl',\n    'BsmtCond',\n    'BsmtFinType2',\n    'Heating',\n    'CentralAir',\n    'Electrical',\n    'Functional',\n    'GarageQual',\n    'GarageCond',\n    'PavedDrive'\n]\n\n# Training set\ndata_train_categ.drop(cols_to_drop, axis=1, inplace=True)\n\n# Test set\ndata_test_categ.drop(cols_to_drop, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:00.061679Z","iopub.execute_input":"2022-05-06T10:49:00.062054Z","iopub.status.idle":"2022-05-06T10:49:00.078815Z","shell.execute_reply.started":"2022-05-06T10:49:00.062014Z","shell.execute_reply":"2022-05-06T10:49:00.077735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Training set shape (Categorical features): {data_train_categ.shape}\\n\")\nprint(f\"Test set shape (Categorical features): {data_test_categ.shape}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:00.07996Z","iopub.execute_input":"2022-05-06T10:49:00.080709Z","iopub.status.idle":"2022-05-06T10:49:00.08565Z","shell.execute_reply.started":"2022-05-06T10:49:00.080674Z","shell.execute_reply":"2022-05-06T10:49:00.08485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Variation of target variable with each categorical feature","metadata":{}},{"cell_type":"code","source":"# With the boxplot we can see the variation of the target 'SalePrice' in each of the categorical features\nfig, axes = plt.subplots(\n    round(len(data_train_categ.columns)/3), 3, figsize=(15, 30))\n\nfor i, ax in enumerate(fig.axes):\n    # plot the variation of SalePrice in each feature\n    if i < len(data_train_categ.columns) - 1:\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=75)\n        sns.boxplot(\n            x=data_train_categ.columns[i], y=\"SalePrice\", data=data_train_categ, ax=ax)\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:00.08679Z","iopub.execute_input":"2022-05-06T10:49:00.0872Z","iopub.status.idle":"2022-05-06T10:49:07.175493Z","shell.execute_reply.started":"2022-05-06T10:49:00.087164Z","shell.execute_reply":"2022-05-06T10:49:07.174921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at these box plots, we see that the distribution of sale price for certain categorical variables are similar to each other (like \"Exterior1st\" and \"Exterior2nd\") which suggests that certain categorical variables are co-dependent on each other. There are three pairs of categorical variables for which the distribution of sale price is very similar:\n\n* \"Exterior1st\" and \"Exterior2nd\"\n* \"ExterQual\" and \"MasVnrType\"\n* \"BsmtQual\" and \"BsmtExposure\"\n\nWe will perform a Chi-squared test for each pair of variables at a 5% significance level to determine whether or not there is a strong dependency between these variables.","metadata":{}},{"cell_type":"code","source":"# Plot contingency table\n\nsns.set(rc={\"figure.figsize\": (10, 7)})\n\nX = [\"Exterior1st\", \"ExterQual\", \"BsmtQual\"]\nY = [\"Exterior2nd\", \"MasVnrType\", \"BsmtExposure\"]\n\n# Parameters for Chi-squared test (5% significance level)\nprob = 0.95\nalpha = 1.0 - prob\n\nfor i, j in zip(X, Y):\n\n    # Contingency table\n    cont = data_train_categ[[i, j]].pivot_table(\n        index=i, columns=j, aggfunc=len, margins=True, margins_name=\"Total\")\n    tx = cont.loc[:, [\"Total\"]]\n    ty = cont.loc[[\"Total\"], :]\n    n = len(data_train_categ)\n    indep = tx.dot(ty) / n\n    c = cont.fillna(0)  # Replace NaN with 0 in the contingency table\n    measure = (c - indep) ** 2 / indep\n    xi_n = measure.sum().sum()\n    table = measure / xi_n\n\n    # Plot contingency table\n    p = sns.heatmap(table.iloc[:-1, :-1],\n                    annot=c.iloc[:-1, :-1], fmt=\".0f\", cmap=\"Oranges\")\n    p.set_xlabel(j, fontsize=18)\n    p.set_ylabel(i, fontsize=18)\n    p.set_title(f\"\\nχ² test between groups {i} and groups {j}\\n\", size=18)\n    plt.show()\n\n    # Performing Chi-sq test\n    CrosstabResult = pd.crosstab(\n        index=data_train_categ[i], columns=data_train_categ[j])\n    ChiSqResult = chi2_contingency(CrosstabResult)\n    # P-Value is the Probability of H0 being True\n    print(f\"P-Value of the ChiSq Test bewteen {i} and {j} is: {ChiSqResult[1]}\\n\")\n    print('significance=%.3f, p=%.3f' % (alpha, ChiSqResult[1]))\n    if ChiSqResult[1] <= alpha:\n        print('Dependent (reject H0)')\n    else:\n        print('Independent (fail to reject H0)')","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:07.176354Z","iopub.execute_input":"2022-05-06T10:49:07.176542Z","iopub.status.idle":"2022-05-06T10:49:08.798535Z","shell.execute_reply.started":"2022-05-06T10:49:07.176518Z","shell.execute_reply":"2022-05-06T10:49:08.797584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, the contigency tables for each pair of categorical variables considered is printed as a heatmap, with the colours representing the relative error between the actual value of the table with its expected value. After performing the Chi-squared test for each pairs of variables considered, we see that there is strong co-dependency for each of these variables. Since highly dependent/correlated variables do not add much relevant new information with regards to the value of the target variable, we will drop one of each co-dependent variable from the dataset.","metadata":{}},{"cell_type":"code","source":"# Drop the one of each co-dependent variables\n# Training set\ndata_train_categ.drop(Y, axis=1, inplace=True)\n\n# Test set\ndata_test_categ.drop(Y, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:08.79964Z","iopub.execute_input":"2022-05-06T10:49:08.799869Z","iopub.status.idle":"2022-05-06T10:49:08.807591Z","shell.execute_reply.started":"2022-05-06T10:49:08.799842Z","shell.execute_reply":"2022-05-06T10:49:08.807033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Training set shape (Categorical features): {data_train_categ.shape}\\n\")\nprint(f\"Test set shape (Categorical features): {data_test_categ.shape}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:08.808678Z","iopub.execute_input":"2022-05-06T10:49:08.809151Z","iopub.status.idle":"2022-05-06T10:49:08.8196Z","shell.execute_reply.started":"2022-05-06T10:49:08.80912Z","shell.execute_reply":"2022-05-06T10:49:08.818757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Values","metadata":{}},{"cell_type":"code","source":"# Get names of categorical columns with missing values (training set)\ncat_cols_with_missing = [col for col in data_train_categ.columns\n                         if data_train_categ[col].isnull().any()]\n\nprint(\"Categorical Columns with missing (NA) values:\")\nprint(cat_cols_with_missing)\n\n# Count how many NA values are in each of those columns\ncat_cols_nan_count = list(map(lambda col: round(data_train_categ[col].isna().sum()*100/len(data_train_categ)), \n                              cat_cols_with_missing))\n\n\ntab_cat = pd.DataFrame(cat_cols_with_missing, columns=[\"Column\"])\ntab_cat[\"Percent_NaN\"] = cat_cols_nan_count\ntab_cat.sort_values(by=[\"Percent_NaN\"], ascending=False, inplace=True)\n\n\n# Define figure parameters\nsns.set(rc={\"figure.figsize\": (10, 7)})\nsns.set_style(\"whitegrid\")\n\n# Plot results\np = sns.barplot(x=\"Percent_NaN\", y=\"Column\", data=tab_cat,\n                edgecolor=\"black\", color=\"deepskyblue\")\n\np.set_title(\"Percent of NaN per column of the train set\\n\", fontsize=20)\np.set_xlabel(\"\\nPercent of NaN (%)\", fontsize=20)\np.set_ylabel(\"Column Name\\n\", fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:08.820834Z","iopub.execute_input":"2022-05-06T10:49:08.821056Z","iopub.status.idle":"2022-05-06T10:49:09.12122Z","shell.execute_reply.started":"2022-05-06T10:49:08.821023Z","shell.execute_reply":"2022-05-06T10:49:09.120659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the training set, there are five categorical variables that have a significant amount of missing values. To help reduce the error, we will remove any columns with more than 30% NaN entries from both datasets. Imputation will be used to fill in the missing entries from the remaining columns in the training set using the modal class.","metadata":{}},{"cell_type":"code","source":"# Drop categorical columns that have at least 30% missing values\nlarge_na = [col for col in cat_cols_with_missing if (data_train_categ[col].isna().sum()/data_train_categ.shape[0]) > 0.3]\n\nprint(\"Columns to be dropped:\")\nprint(large_na)\n\ndata_train_categ.drop(large_na, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.122056Z","iopub.execute_input":"2022-05-06T10:49:09.12272Z","iopub.status.idle":"2022-05-06T10:49:09.132416Z","shell.execute_reply.started":"2022-05-06T10:49:09.12269Z","shell.execute_reply":"2022-05-06T10:49:09.131793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill the NaN of each feature by the corresponding modal class\ncateg_fill_null = {\"GarageType\": data_train_categ[\"GarageType\"].mode().iloc[0],\n                   \"GarageFinish\": data_train_categ[\"GarageFinish\"].mode().iloc[0],\n                   \"BsmtQual\": data_train_categ[\"BsmtQual\"].mode().iloc[0],\n                   \"BsmtFinType1\": data_train_categ[\"BsmtFinType1\"].mode().iloc[0]}\n\ndata_train_categ = data_train_categ.fillna(value=categ_fill_null)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.133389Z","iopub.execute_input":"2022-05-06T10:49:09.133992Z","iopub.status.idle":"2022-05-06T10:49:09.145831Z","shell.execute_reply.started":"2022-05-06T10:49:09.133952Z","shell.execute_reply":"2022-05-06T10:49:09.145023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After dropping those same columns from the test dataset, we investigate the categorical variables from that dataset that have missing values.","metadata":{}},{"cell_type":"code","source":"# Drop the same categorical columns from the test set\ndata_test_categ.drop(large_na, axis=1, inplace=True)\n\n# Get names of categorical columns with missing values (test set)\ncat_cols_with_missing_t = [col for col in data_test_categ.columns\n                           if data_test_categ[col].isnull().any()]\n\nprint(\"Categorical Columns with missing (NA) values:\")\nprint(cat_cols_with_missing_t)\n\n# Count how many NA values are in each of those columns\ncat_cols_nan_count_t = list(map(lambda col: round(data_test_categ[col].isna().sum()*100/len(data_test_categ)), \n                              cat_cols_with_missing_t))\n\n\ntab_cat_t = pd.DataFrame(cat_cols_with_missing_t, columns=[\"Column\"])\ntab_cat_t[\"Percent_NaN\"] = cat_cols_nan_count_t\ntab_cat_t.sort_values(by=[\"Percent_NaN\"], ascending=False, inplace=True)\n\n\n# Define figure parameters\nsns.set(rc={\"figure.figsize\": (10, 7)})\nsns.set_style(\"whitegrid\")\n\n# Plot results\np = sns.barplot(x=\"Percent_NaN\", y=\"Column\", data=tab_cat_t,\n                edgecolor=\"black\", color=\"deepskyblue\")\n\np.set_title(\"Percent of NaN per column of the test set\\n\", fontsize=20)\np.set_xlabel(\"\\nPercent of NaN (%)\", fontsize=20)\np.set_ylabel(\"Column Name\\n\", fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.146901Z","iopub.execute_input":"2022-05-06T10:49:09.147142Z","iopub.status.idle":"2022-05-06T10:49:09.444296Z","shell.execute_reply.started":"2022-05-06T10:49:09.147115Z","shell.execute_reply":"2022-05-06T10:49:09.443451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are a few more columns in the test dataset that have missing values, but none of them have more than 5% missing values. Therefore, we will fill in each NaN entry for each feature using it's corresponding modal class like before.","metadata":{}},{"cell_type":"code","source":"# Fill the NaN of each feature by the corresponding modal class\ncateg_fill_null = {\"GarageType\": data_test_categ[\"GarageType\"].mode().iloc[0],\n                   \"GarageFinish\": data_test_categ[\"GarageFinish\"].mode().iloc[0],\n                   \"BsmtQual\": data_test_categ[\"BsmtQual\"].mode().iloc[0],\n                   \"BsmtFinType1\": data_test_categ[\"BsmtFinType1\"].mode().iloc[0],\n                   \"MSZoning\": data_test_categ[\"MSZoning\"].mode().iloc[0],\n                   \"Exterior1st\": data_test_categ[\"Exterior1st\"].mode().iloc[0],\n                   \"KitchenQual\": data_test_categ[\"KitchenQual\"].mode().iloc[0],\n                   \"SaleType\": data_test_categ[\"SaleType\"].mode().iloc[0]}\n\ndata_test_categ = data_test_categ.fillna(value=categ_fill_null)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.445736Z","iopub.execute_input":"2022-05-06T10:49:09.446743Z","iopub.status.idle":"2022-05-06T10:49:09.460599Z","shell.execute_reply.started":"2022-05-06T10:49:09.446697Z","shell.execute_reply":"2022-05-06T10:49:09.459798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The shape of each dataset (containing categorical features only) after imputation is shown below.","metadata":{}},{"cell_type":"code","source":"print(f\"Training set shape (Categorical features): {data_train_categ.shape}\\n\")\nprint(f\"Test set shape (Categorical features): {data_test_categ.shape}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.462155Z","iopub.execute_input":"2022-05-06T10:49:09.462455Z","iopub.status.idle":"2022-05-06T10:49:09.468674Z","shell.execute_reply.started":"2022-05-06T10:49:09.462416Z","shell.execute_reply":"2022-05-06T10:49:09.467837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transformation into numerical values (get_dummies())","metadata":{}},{"cell_type":"markdown","source":"Before we combine the categorical data back with the numerical data, we need to transform the categorical entries into numerical entries. This will be done using the get_dummies() function where each categorical feature will be transformed into a binary feature.","metadata":{}},{"cell_type":"code","source":"# Drop the SalePrice column from the training dataset\ndata_train_categ.drop([\"SalePrice\"], axis = 1, inplace = True)\n\n# Use get_dummies to transform the Categorical features into Binary features (Training dataset)\ndata_train_dummies = pd.get_dummies(data_train_categ)\ndata_train_dummies.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.470569Z","iopub.execute_input":"2022-05-06T10:49:09.471495Z","iopub.status.idle":"2022-05-06T10:49:09.51333Z","shell.execute_reply.started":"2022-05-06T10:49:09.471451Z","shell.execute_reply":"2022-05-06T10:49:09.512436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply get_dummies to the test dataset as well\ndata_test_dummies = pd.get_dummies(data_test_categ)\ndata_test_dummies.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.514824Z","iopub.execute_input":"2022-05-06T10:49:09.515788Z","iopub.status.idle":"2022-05-06T10:49:09.551065Z","shell.execute_reply.started":"2022-05-06T10:49:09.515742Z","shell.execute_reply":"2022-05-06T10:49:09.550159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the first few rows of each modified dataset we see that they do not have the same number of columns. We need to check which columns are missing from the test dataset.","metadata":{}},{"cell_type":"code","source":"# Check if the column headings are the same in both data sets: data_train_dummies and data_test_dummies\ndif_1 = [x for x in data_train_dummies.columns if x not in data_test_dummies.columns]\nprint(f\"Features present in df_train_categ and absent in df_test_categ: {dif_1}\\n\")\n\ndif_2 = [x for x in data_test_dummies.columns if x not in data_test_dummies.columns]\nprint(f\"Features present in df_test_categ set and absent in df_train_categ: {dif_2}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.552426Z","iopub.execute_input":"2022-05-06T10:49:09.552798Z","iopub.status.idle":"2022-05-06T10:49:09.559597Z","shell.execute_reply.started":"2022-05-06T10:49:09.552756Z","shell.execute_reply":"2022-05-06T10:49:09.558647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Three of these columns from the training dataset are not present in the test dataset. Thus they will be dropped from the training dataset to ensure that both datasets have exactly the same features.","metadata":{}},{"cell_type":"code","source":"# Drop the columns listed in dif_1 from data_train_dummies\ndata_train_dummies.drop(dif_1, axis=1, inplace=True)\n\n# Check again if the column headings are the same in both data sets: data_train_dummies and data_test_dummies\ndif_1 = [x for x in data_train_dummies.columns if x not in data_test_dummies.columns]\nprint(f\"Features present in df_train_categ and absent in df_test_categ: {dif_1}\\n\")\n\ndif_2 = [x for x in data_test_dummies.columns if x not in data_test_dummies.columns]\nprint(f\"Features present in df_test_categ set and absent in df_train_categ: {dif_2}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.561302Z","iopub.execute_input":"2022-05-06T10:49:09.561848Z","iopub.status.idle":"2022-05-06T10:49:09.575076Z","shell.execute_reply.started":"2022-05-06T10:49:09.561808Z","shell.execute_reply":"2022-05-06T10:49:09.574132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The shape of both datasets (categorical features only) after all of these changes are given below.","metadata":{}},{"cell_type":"code","source":"print(f\"Training set shape (Categorical features): {data_train_dummies.shape}\\n\")\nprint(f\"Test set shape (Categorical features): {data_test_dummies.shape}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.576121Z","iopub.execute_input":"2022-05-06T10:49:09.576664Z","iopub.status.idle":"2022-05-06T10:49:09.583911Z","shell.execute_reply.started":"2022-05-06T10:49:09.576614Z","shell.execute_reply":"2022-05-06T10:49:09.583129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparing Data for modelling","metadata":{}},{"cell_type":"code","source":"# Join numerical and categorical datasets together\n# Training set\ndata_train_new = pd.concat([data_train_imputed, data_train_dummies], axis = 1)\nprint(f\"Train set: {data_train_new.shape}\")\n\n# Test set\ndata_test_new = pd.concat([data_test_imputed, data_test_dummies], axis = 1)\nprint(f\"Test set: {data_test_new.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.584876Z","iopub.execute_input":"2022-05-06T10:49:09.585184Z","iopub.status.idle":"2022-05-06T10:49:09.599372Z","shell.execute_reply.started":"2022-05-06T10:49:09.585155Z","shell.execute_reply":"2022-05-06T10:49:09.598598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Further Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"The Year of construction and the Year of Remodelling variables will be transformed into new variables representing the Age of the House and the Age since the house was remodelled - this will enable us to apply a log transform to normalize those variables. After the transformation, the variables \"YearBuilt\" and \"YearRemodAdd\" will be removed.","metadata":{}},{"cell_type":"code","source":"# Convert Year of construction to the Age of the house since the construction\ndata_train_new[\"AgeSinceConst\"] = (data_train_new[\"YearBuilt\"].max() - data_train_new[\"YearBuilt\"])\n\ndata_test_new[\"AgeSinceConst\"] = (data_test_new[\"YearBuilt\"].max() - data_test_new[\"YearBuilt\"])\n\n# Drop \"YearBuilt\"\ndata_train_new.drop([\"YearBuilt\"], axis=1, inplace=True)\ndata_test_new.drop([\"YearBuilt\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.600368Z","iopub.execute_input":"2022-05-06T10:49:09.600626Z","iopub.status.idle":"2022-05-06T10:49:09.617055Z","shell.execute_reply.started":"2022-05-06T10:49:09.6006Z","shell.execute_reply":"2022-05-06T10:49:09.616386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert Year of remodeling to the Age of the house since the remodeling\ndata_train_new[\"AgeSinceRemod\"] = (data_train_new[\"YearRemodAdd\"].max() - data_train_new[\"YearRemodAdd\"])\n\ndata_test_new[\"AgeSinceRemod\"] = (data_test_new[\"YearRemodAdd\"].max() - data_test_new[\"YearRemodAdd\"])\n\n# Drop \"YearRemodAdd\"\ndata_train_new.drop([\"YearRemodAdd\"], axis=1, inplace=True)\ndata_test_new.drop([\"YearRemodAdd\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.620137Z","iopub.execute_input":"2022-05-06T10:49:09.621024Z","iopub.status.idle":"2022-05-06T10:49:09.631832Z","shell.execute_reply.started":"2022-05-06T10:49:09.620989Z","shell.execute_reply":"2022-05-06T10:49:09.631253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we consider the continuous numerical variables that are skewed. A Log transformation will be applied to the skewed numerical variables to help mitigate the strong variation of some variables, and to reduce redundancy. The continuous features are defined below.","metadata":{}},{"cell_type":"code","source":"continuous_features = ['AgeSinceConst', 'AgeSinceRemod', 'MasVnrArea', 'BsmtFinSF1',\n                      'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea',\n                      'WoodDeckSF', 'OpenPorchSF']","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.634173Z","iopub.execute_input":"2022-05-06T10:49:09.634467Z","iopub.status.idle":"2022-05-06T10:49:09.639249Z","shell.execute_reply.started":"2022-05-06T10:49:09.634427Z","shell.execute_reply":"2022-05-06T10:49:09.638636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To obtain the skewed features, we take out variables that are more than 50% skewed.","metadata":{}},{"cell_type":"code","source":"data_skew_verify = data_train_new.loc[:, continuous_features]\n\n# Select features with absolute Skew higher than 0.5\nskew_ft = []\n\nfor i in continuous_features:\n    # list of skew for each corresponding feature\n    skew_ft.append(abs(data_skew_verify[i].skew()))\n\ndata_skewed = pd.DataFrame({\"Columns\": continuous_features, \"Abs_Skew\": skew_ft})\n\nsk_features = data_skewed[data_skewed[\"Abs_Skew\"] > 0.5][\"Columns\"].tolist()\nprint(f\"List of skewed features: {sk_features}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.640366Z","iopub.execute_input":"2022-05-06T10:49:09.640807Z","iopub.status.idle":"2022-05-06T10:49:09.656074Z","shell.execute_reply.started":"2022-05-06T10:49:09.640779Z","shell.execute_reply":"2022-05-06T10:49:09.655157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A log transformation is then applied to the skewed features listed above.","metadata":{}},{"cell_type":"code","source":"# Log transformation of the skewed features\nfor i in sk_features:\n    # loop over i (features) to calculate Log of surfaces\n    # Training set\n    data_train_new[i] = np.log((data_train_new[i])+1)\n    \n    # Test set\n    data_test_new[i] = np.log((data_test_new[i])+1)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.657478Z","iopub.execute_input":"2022-05-06T10:49:09.658109Z","iopub.status.idle":"2022-05-06T10:49:09.67475Z","shell.execute_reply.started":"2022-05-06T10:49:09.658072Z","shell.execute_reply":"2022-05-06T10:49:09.674032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the distribution of the numerical features near the start, we noticed that \"SalePrice\" is skewed as well. To help normalize this variable, a log transformation will be applied to \"SalePrice\" as well.","metadata":{}},{"cell_type":"code","source":"# Log transformation of the target variable \"SalePrice\"\ndata_train_new[\"SalePriceLog\"] = np.log(data_train_new.SalePrice)\n\n# Drop the original SalePrice\ndata_train_new.drop([\"SalePrice\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.676153Z","iopub.execute_input":"2022-05-06T10:49:09.67656Z","iopub.status.idle":"2022-05-06T10:49:09.68293Z","shell.execute_reply.started":"2022-05-06T10:49:09.676527Z","shell.execute_reply":"2022-05-06T10:49:09.682282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_cols_new = [cname for cname in data_train_new.columns if \n                      data_train_new[cname].dtype in ['int64', 'float64']]\n\ndata_train_new_num = data_train_new[numerical_cols_new].copy()\n\n# Plot the distribution of all the numerical features\nfig_ = data_train_new_num.hist(figsize=(16, 20), bins=50, color=\"deepskyblue\",\n                               edgecolor=\"black\", xlabelsize=8, ylabelsize=8)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:09.684106Z","iopub.execute_input":"2022-05-06T10:49:09.684456Z","iopub.status.idle":"2022-05-06T10:49:13.791162Z","shell.execute_reply.started":"2022-05-06T10:49:09.684429Z","shell.execute_reply":"2022-05-06T10:49:13.790352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the distribution of the numerical features, we notice that most of the previously skewed variables have a more normal distribution (excluding zero values) with exception of the Age variables, which should result in better predictons.","metadata":{}},{"cell_type":"markdown","source":"### Splitting the data into training and test sets","metadata":{}},{"cell_type":"code","source":"# TRAINING DATASET\n# Feature variables\nX = data_train_new.copy().drop([\"SalePriceLog\"], axis = 1)\n\n# Target Variable\ny = data_train_new.loc[:, \"SalePriceLog\"]\n\nprint(X.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:13.792525Z","iopub.execute_input":"2022-05-06T10:49:13.792723Z","iopub.status.idle":"2022-05-06T10:49:13.800436Z","shell.execute_reply.started":"2022-05-06T10:49:13.792699Z","shell.execute_reply":"2022-05-06T10:49:13.799551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into Training and test sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nprint(f\"X_train:{X_train.shape}\\ny_train:{y_train.shape}\")\nprint(f\"\\nX_test:{X_test.shape}\\ny_test:{y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:13.801512Z","iopub.execute_input":"2022-05-06T10:49:13.801704Z","iopub.status.idle":"2022-05-06T10:49:13.816051Z","shell.execute_reply.started":"2022-05-06T10:49:13.801681Z","shell.execute_reply":"2022-05-06T10:49:13.815232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"markdown","source":"For this project, six supervised learning models will be considered:\n\n* Linear Regression\n* Ridge Regression\n* Lasso Regression\n* Decision Tree Regressor\n* Random Forest Regressor\n* XGBoost Regressor\n\nTo measure model performance and their predicitons the RMSE and R^{2} scores will be used, and 5-fold cross-validation will also be used.","metadata":{}},{"cell_type":"code","source":"# Define models\nmodel_lin = LinearRegression()\nmodel_ridge = Ridge(alpha = 0.001)\nmodel_lasso = Lasso(alpha = 0.001)\nmodel_tree = DecisionTreeRegressor()\nmodel_ran = RandomForestRegressor()\nmodel_xg = XGBRegressor()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:13.817331Z","iopub.execute_input":"2022-05-06T10:49:13.817528Z","iopub.status.idle":"2022-05-06T10:49:13.824583Z","shell.execute_reply.started":"2022-05-06T10:49:13.817507Z","shell.execute_reply":"2022-05-06T10:49:13.823937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\n\n# Define a function for each metric\n# R²\ndef rsqr_score(test, pred):\n    \"\"\"Calculate R squared score \n\n    Args:\n        test -- test data\n        pred -- predicted data\n\n    Returns:\n        R squared score \n    \"\"\"\n    r2_ = r2_score(test, pred)\n    return r2_\n\n\n# RMSE\ndef rmse_score(test, pred):\n    \"\"\"Calculate Root Mean Square Error score \n\n    Args:\n        test -- test data\n        pred -- predicted data\n\n    Returns:\n        Root Mean Square Error score\n    \"\"\"\n    rmse_ = np.sqrt(mean_squared_error(test, pred))\n    return rmse_\n\n\n# Print the scores\ndef print_score(test, pred, model):\n    \"\"\"Print calculated score \n\n    Args:\n        test -- test data\n        pred -- predicted data\n\n    Returns:\n        print the regressor name\n        print the R squared score\n        print Root Mean Square Error score\n    \"\"\"\n\n    print(f\"- Regressor: {model}\")\n    print(f\"R²: {rsqr_score(test, pred)}\")\n    print(f\"RMSE: {rmse_score(test, pred)}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:13.825352Z","iopub.execute_input":"2022-05-06T10:49:13.825571Z","iopub.status.idle":"2022-05-06T10:49:13.835617Z","shell.execute_reply.started":"2022-05-06T10:49:13.825545Z","shell.execute_reply":"2022-05-06T10:49:13.835049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"code","source":"scores_lin = -1 * cross_val_score(model_lin, X_train, y_train,\n                                  cv=5,\n                                  scoring='neg_mean_squared_error')\n\nprint(\"MSE scores (Linear Model):\\n\", scores_lin)\nprint(\"Mean MSE scores:\", scores_lin.mean())","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:13.836807Z","iopub.execute_input":"2022-05-06T10:49:13.837026Z","iopub.status.idle":"2022-05-06T10:49:14.095764Z","shell.execute_reply.started":"2022-05-06T10:49:13.837002Z","shell.execute_reply":"2022-05-06T10:49:14.094999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lin.fit(X_train, y_train)\ny_pred_lin = model_lin.predict(X_test)\nprint_score(y_test, y_pred_lin, \"Linear\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:14.097842Z","iopub.execute_input":"2022-05-06T10:49:14.099288Z","iopub.status.idle":"2022-05-06T10:49:14.161294Z","shell.execute_reply.started":"2022-05-06T10:49:14.099244Z","shell.execute_reply":"2022-05-06T10:49:14.160326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.title(\"Actual vs. Predicted house prices\\n (Linear)\", fontsize=20)\nplt.scatter(np.exp(y_test), np.exp(y_pred_lin),\n            color=\"deepskyblue\", marker=\"o\", facecolors=\"none\")\nplt.plot([0, 800000], [0, 800000], \"darkorange\", lw=2)\nplt.xlim(0, 800000)\nplt.ylim(0, 800000)\nplt.xlabel(\"\\nActual Price\", fontsize=16)\nplt.ylabel(\"Predicted Price\\n\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:14.167283Z","iopub.execute_input":"2022-05-06T10:49:14.170862Z","iopub.status.idle":"2022-05-06T10:49:14.484762Z","shell.execute_reply.started":"2022-05-06T10:49:14.170804Z","shell.execute_reply":"2022-05-06T10:49:14.483839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ridge Regression","metadata":{}},{"cell_type":"code","source":"scores_ridge = -1 * cross_val_score(model_ridge, X_train, y_train,\n                                  cv=5,\n                                  scoring='neg_mean_squared_error')\n\nprint(\"MSE scores (Ridge Model):\\n\", scores_ridge)\nprint(\"Mean MSE scores:\", scores_ridge.mean())","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:14.486188Z","iopub.execute_input":"2022-05-06T10:49:14.486653Z","iopub.status.idle":"2022-05-06T10:49:14.678706Z","shell.execute_reply.started":"2022-05-06T10:49:14.48661Z","shell.execute_reply":"2022-05-06T10:49:14.677827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ridge.fit(X_train, y_train)\ny_pred_ridge = model_ridge.predict(X_test)\nprint_score(y_test, y_pred_ridge, \"Ridge\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:14.680399Z","iopub.execute_input":"2022-05-06T10:49:14.680914Z","iopub.status.idle":"2022-05-06T10:49:14.716714Z","shell.execute_reply.started":"2022-05-06T10:49:14.680854Z","shell.execute_reply":"2022-05-06T10:49:14.715746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.title(\"Actual vs. Predicted house prices\\n (Ridge)\", fontsize=20)\nplt.scatter(np.exp(y_test), np.exp(y_pred_ridge),\n            color=\"deepskyblue\", marker=\"o\", facecolors=\"none\")\nplt.plot([0, 800000], [0, 800000], \"darkorange\", lw=2)\nplt.xlim(0, 800000)\nplt.ylim(0, 800000)\nplt.xlabel(\"\\nActual Price\", fontsize=16)\nplt.ylabel(\"Predicted Price\\n\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:14.718209Z","iopub.execute_input":"2022-05-06T10:49:14.719607Z","iopub.status.idle":"2022-05-06T10:49:15.041131Z","shell.execute_reply.started":"2022-05-06T10:49:14.719558Z","shell.execute_reply":"2022-05-06T10:49:15.040544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparamter Tuning (Ridge)","metadata":{}},{"cell_type":"code","source":"# Define hyperparameters\nalphas = np.linspace(0, 10, 100).tolist()\n\ntuned_parameters = {\"alpha\": alphas}\n\n# GridSearch\nridge_cv = GridSearchCV(Ridge(), tuned_parameters, cv=10, n_jobs=-1, verbose=1)\n\n# fit the GridSearch on train set\nridge_cv.fit(X_train, y_train)\n\n# print best params and the corresponding R²\nprint(f\"Best hyperparameters: {ridge_cv.best_params_}\")\nprint(f\"Best R² (train): {ridge_cv.best_score_}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:15.042344Z","iopub.execute_input":"2022-05-06T10:49:15.04273Z","iopub.status.idle":"2022-05-06T10:49:21.68892Z","shell.execute_reply.started":"2022-05-06T10:49:15.042702Z","shell.execute_reply":"2022-05-06T10:49:21.68795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ridge_opt = Ridge(alpha = ridge_cv.best_params_[\"alpha\"])\nmodel_ridge_opt.fit(X_train, y_train)\ny_pred_ridge_opt = model_ridge_opt.predict(X_test)\nprint_score(y_test, y_pred_ridge_opt, \"Ridge\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:21.694273Z","iopub.execute_input":"2022-05-06T10:49:21.696574Z","iopub.status.idle":"2022-05-06T10:49:21.751153Z","shell.execute_reply.started":"2022-05-06T10:49:21.69651Z","shell.execute_reply":"2022-05-06T10:49:21.750245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.title(\"Actual vs. Predicted house prices\\n (Ridge - Optimal alpha value)\", fontsize=20)\nplt.scatter(np.exp(y_test), np.exp(y_pred_ridge_opt),\n            color=\"deepskyblue\", marker=\"o\", facecolors=\"none\")\nplt.plot([0, 800000], [0, 800000], \"darkorange\", lw=2)\nplt.xlim(0, 800000)\nplt.ylim(0, 800000)\nplt.xlabel(\"\\nActual Price\", fontsize=16)\nplt.ylabel(\"Predicted Price\\n\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:21.75726Z","iopub.execute_input":"2022-05-06T10:49:21.760989Z","iopub.status.idle":"2022-05-06T10:49:22.097152Z","shell.execute_reply.started":"2022-05-06T10:49:21.760933Z","shell.execute_reply":"2022-05-06T10:49:22.096549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lasso Regression","metadata":{}},{"cell_type":"code","source":"scores_lasso = -1 * cross_val_score(model_lasso, X_train, y_train,\n                                  cv=5,\n                                  scoring='neg_mean_squared_error')\n\nprint(\"MSE scores (Lasso Model):\\n\", scores_lasso)\nprint(\"Mean MSE scores:\", scores_lasso.mean())","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:22.098068Z","iopub.execute_input":"2022-05-06T10:49:22.098712Z","iopub.status.idle":"2022-05-06T10:49:22.438071Z","shell.execute_reply.started":"2022-05-06T10:49:22.09868Z","shell.execute_reply":"2022-05-06T10:49:22.437283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lasso.fit(X_train, y_train)\ny_pred_lasso = model_lasso.predict(X_test)\nprint_score(y_test, y_pred_lasso, \"Lasso\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:22.442876Z","iopub.execute_input":"2022-05-06T10:49:22.446628Z","iopub.status.idle":"2022-05-06T10:49:22.553167Z","shell.execute_reply.started":"2022-05-06T10:49:22.446579Z","shell.execute_reply":"2022-05-06T10:49:22.55224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.title(\"Actual vs. Predicted house prices\\n (Lasso)\", fontsize=20)\nplt.scatter(np.exp(y_test), np.exp(y_pred_lasso),\n            color=\"deepskyblue\", marker=\"o\", facecolors=\"none\")\nplt.plot([0, 800000], [0, 800000], \"darkorange\", lw=2)\nplt.xlim(0, 800000)\nplt.ylim(0, 800000)\nplt.xlabel(\"\\nActual Price\", fontsize=16)\nplt.ylabel(\"Predicted Price\\n\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:22.560067Z","iopub.execute_input":"2022-05-06T10:49:22.562857Z","iopub.status.idle":"2022-05-06T10:49:22.895607Z","shell.execute_reply.started":"2022-05-06T10:49:22.562811Z","shell.execute_reply":"2022-05-06T10:49:22.894894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter Tuning (Lasso)","metadata":{}},{"cell_type":"code","source":"# Define hyperparameters\nalphas = np.logspace(-5, 5, 100).tolist()\n\ntuned_parameters = {\"alpha\": alphas}\n\n# GridSearch\nlasso_cv = GridSearchCV(Lasso(), tuned_parameters, cv=10, n_jobs=-1, verbose=1)\n\n# fit the GridSearch on train set\nlasso_cv.fit(X_train, y_train)\n\n# print best params and the corresponding R²\nprint(f\"Best hyperparameters: {lasso_cv.best_params_}\")\nprint(f\"Best R² (train): {lasso_cv.best_score_}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:22.896642Z","iopub.execute_input":"2022-05-06T10:49:22.896838Z","iopub.status.idle":"2022-05-06T10:49:30.363737Z","shell.execute_reply.started":"2022-05-06T10:49:22.896814Z","shell.execute_reply":"2022-05-06T10:49:30.362768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lasso_opt = Lasso(alpha = lasso_cv.best_params_[\"alpha\"])\n\nmodel_lasso_opt.fit(X_train, y_train)\ny_pred_lasso_opt = model_lasso_opt.predict(X_test)\nprint_score(y_test, y_pred_lasso_opt, \"Lasso\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:30.369262Z","iopub.execute_input":"2022-05-06T10:49:30.371837Z","iopub.status.idle":"2022-05-06T10:49:30.418988Z","shell.execute_reply.started":"2022-05-06T10:49:30.371775Z","shell.execute_reply":"2022-05-06T10:49:30.418138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.title(\"Actual vs. Predicted house prices\\n (Lasso - Optimal alpha value)\", fontsize=20)\nplt.scatter(np.exp(y_test), np.exp(y_pred_lasso_opt),\n            color=\"deepskyblue\", marker=\"o\", facecolors=\"none\")\nplt.plot([0, 800000], [0, 800000], \"darkorange\", lw=2)\nplt.xlim(0, 800000)\nplt.ylim(0, 800000)\nplt.xlabel(\"\\nActual Price\", fontsize=16)\nplt.ylabel(\"Predicted Price\\n\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:30.424105Z","iopub.execute_input":"2022-05-06T10:49:30.42654Z","iopub.status.idle":"2022-05-06T10:49:30.738339Z","shell.execute_reply.started":"2022-05-06T10:49:30.426484Z","shell.execute_reply":"2022-05-06T10:49:30.737728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree Regressor","metadata":{}},{"cell_type":"code","source":"scores_tree = -1 * cross_val_score(model_tree, X_train, y_train,\n                                   cv=5,\n                                   scoring='neg_mean_squared_error')\n\nprint(\"MSE scores (Decision Tree Model):\\n\", scores_tree)\nprint(\"Mean MSE scores:\", scores_tree.mean())","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:30.739512Z","iopub.execute_input":"2022-05-06T10:49:30.74037Z","iopub.status.idle":"2022-05-06T10:49:30.885202Z","shell.execute_reply.started":"2022-05-06T10:49:30.740327Z","shell.execute_reply":"2022-05-06T10:49:30.884306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_tree.fit(X_train, y_train)\ny_pred_tree = model_tree.predict(X_test)\nprint_score(y_test, y_pred_tree, \"Decision Tree\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:30.88633Z","iopub.execute_input":"2022-05-06T10:49:30.88657Z","iopub.status.idle":"2022-05-06T10:49:30.922942Z","shell.execute_reply.started":"2022-05-06T10:49:30.886546Z","shell.execute_reply":"2022-05-06T10:49:30.922182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.title(\"Actual vs. Predicted house prices\\n (Decision Tree)\", fontsize=20)\nplt.scatter(np.exp(y_test), np.exp(y_pred_tree),\n            color=\"deepskyblue\", marker=\"o\", facecolors=\"none\")\nplt.plot([0, 800000], [0, 800000], \"darkorange\", lw=2)\nplt.xlim(0, 800000)\nplt.ylim(0, 800000)\nplt.xlabel(\"\\nActual Price\", fontsize=16)\nplt.ylabel(\"Predicted Price\\n\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:30.924004Z","iopub.execute_input":"2022-05-06T10:49:30.92428Z","iopub.status.idle":"2022-05-06T10:49:31.181337Z","shell.execute_reply.started":"2022-05-06T10:49:30.924251Z","shell.execute_reply":"2022-05-06T10:49:31.180634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest Regressor","metadata":{}},{"cell_type":"code","source":"scores_ran = -1 * cross_val_score(model_ran, X_train, y_train,\n                                  cv=5,\n                                  scoring='neg_mean_squared_error')\n\nprint(\"MSE scores (Random Forest Model):\\n\", scores_ran)\nprint(\"Mean MSE scores:\", scores_ran.mean())","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:31.182775Z","iopub.execute_input":"2022-05-06T10:49:31.183162Z","iopub.status.idle":"2022-05-06T10:49:37.42765Z","shell.execute_reply.started":"2022-05-06T10:49:31.183112Z","shell.execute_reply":"2022-05-06T10:49:37.426772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ran.fit(X_train, y_train)\ny_pred_ran = model_ran.predict(X_test)\nprint_score(y_test, y_pred_ran, \"Random Forest\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:37.428696Z","iopub.execute_input":"2022-05-06T10:49:37.428964Z","iopub.status.idle":"2022-05-06T10:49:39.002977Z","shell.execute_reply.started":"2022-05-06T10:49:37.428938Z","shell.execute_reply":"2022-05-06T10:49:39.002384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.title(\"Actual vs. Predicted house prices\\n (Random Forest)\", fontsize=20)\nplt.scatter(np.exp(y_test), np.exp(y_pred_ran),\n            color=\"deepskyblue\", marker=\"o\", facecolors=\"none\")\nplt.plot([0, 800000], [0, 800000], \"darkorange\", lw=2)\nplt.xlim(0, 800000)\nplt.ylim(0, 800000)\nplt.xlabel(\"\\nActual Price\", fontsize=16)\nplt.ylabel(\"Predicted Price\\n\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:39.003976Z","iopub.execute_input":"2022-05-06T10:49:39.004511Z","iopub.status.idle":"2022-05-06T10:49:39.26401Z","shell.execute_reply.started":"2022-05-06T10:49:39.004481Z","shell.execute_reply":"2022-05-06T10:49:39.262982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost Regression","metadata":{}},{"cell_type":"code","source":"# Define hyperparameters\ntuned_parameters_xgb = {\"max_depth\": [3],\n                        \"colsample_bytree\": [0.3, 0.7],\n                        \"learning_rate\": [0.01, 0.05, 0.1],\n                        \"n_estimators\": [100, 500, 1000]}\n\n# GridSearch\nxgbr_cv = GridSearchCV(estimator=XGBRegressor(),\n                       param_grid=tuned_parameters_xgb,\n                       cv=5,\n                       n_jobs=-1,\n                       verbose=1)\n\n# fit the GridSearch on train set\nxgbr_cv.fit(X_train, y_train)\n\n# print best params and the corresponding R²\nprint(f\"Best hyperparameters: {xgbr_cv.best_params_}\\n\")\nprint(f\"Best R²: {xgbr_cv.best_score_}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:49:39.265241Z","iopub.execute_input":"2022-05-06T10:49:39.265461Z","iopub.status.idle":"2022-05-06T10:51:05.155267Z","shell.execute_reply.started":"2022-05-06T10:49:39.265434Z","shell.execute_reply":"2022-05-06T10:51:05.154385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_xgb_opt = XGBRegressor(colsample_bytree = xgbr_cv.best_params_[\"colsample_bytree\"],\n                             learning_rate = xgbr_cv.best_params_[\"learning_rate\"],\n                             max_depth = xgbr_cv.best_params_[\"max_depth\"],\n                             n_estimators = xgbr_cv.best_params_[\"n_estimators\"])\n\nmodel_xgb_opt.fit(X_train, y_train)\ny_pred_xgb_opt = model_xgb_opt.predict(X_test)\nprint_score(y_test, y_pred_xgb_opt, \"XGBoost\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:51:05.162429Z","iopub.execute_input":"2022-05-06T10:51:05.162656Z","iopub.status.idle":"2022-05-06T10:51:08.606581Z","shell.execute_reply.started":"2022-05-06T10:51:05.16263Z","shell.execute_reply":"2022-05-06T10:51:08.605804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.title(\"Actual vs. Predicted house prices\\n (XGBoost Regressor)\", fontsize=20)\nplt.scatter(np.exp(y_test), np.exp(y_pred_xgb_opt),\n            color=\"deepskyblue\", marker=\"o\", facecolors=\"none\")\nplt.plot([0, 800000], [0, 800000], \"darkorange\", lw=2)\nplt.xlim(0, 800000)\nplt.ylim(0, 800000)\nplt.xlabel(\"\\nActual Price\", fontsize=16)\nplt.ylabel(\"Predicted Price\\n\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:51:08.608073Z","iopub.execute_input":"2022-05-06T10:51:08.608712Z","iopub.status.idle":"2022-05-06T10:51:08.885281Z","shell.execute_reply.started":"2022-05-06T10:51:08.608667Z","shell.execute_reply":"2022-05-06T10:51:08.884511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After application of hyperparameter tuning, the Lasso model returned the best R^{2} score and the lowest RMSE value, achieving an accuracy of around 90.7%. Based on this, the optimized Lasso Regression model will be used to predict the Sale Price of houses from the test dataset. The predicted sale prices will be saved into a csv file named \"submission.csv\".","metadata":{}},{"cell_type":"code","source":"# Prediction of House Prices using the Optimal Lasso Regression Model\n\ny_pred = np.exp(model_lasso_opt.predict(data_test_new))\n\noutput = pd.DataFrame({\"Id\": Id_test_list,\n                       \"SalePrice\": y_pred})\n\noutput.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:51:08.886518Z","iopub.execute_input":"2022-05-06T10:51:08.886729Z","iopub.status.idle":"2022-05-06T10:51:08.911634Z","shell.execute_reply.started":"2022-05-06T10:51:08.886704Z","shell.execute_reply":"2022-05-06T10:51:08.910613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the output\noutput.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T10:51:08.913405Z","iopub.execute_input":"2022-05-06T10:51:08.913733Z","iopub.status.idle":"2022-05-06T10:51:08.932369Z","shell.execute_reply.started":"2022-05-06T10:51:08.913693Z","shell.execute_reply":"2022-05-06T10:51:08.931341Z"},"trusted":true},"execution_count":null,"outputs":[]}]}