{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import layers\n\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-29T21:01:15.301089Z","iopub.execute_input":"2022-04-29T21:01:15.301449Z","iopub.status.idle":"2022-04-29T21:01:21.856259Z","shell.execute_reply.started":"2022-04-29T21:01:15.301366Z","shell.execute_reply":"2022-04-29T21:01:21.855536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect hardware\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\nexcept ValueError:\n    tpu = None\n    gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n    \n# Select appropriate distribution strategy\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu) # Going back and forth between TPU and host is expensive. Better to run 128 batches on the TPU before reporting back.\n    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  \nelif len(gpus) > 1:\n    strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n    print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\nelif len(gpus) == 1:\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    print('Running on single GPU ', gpus[0].name)\nelse:\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    print('Running on CPU')\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T21:01:55.111852Z","iopub.execute_input":"2022-04-29T21:01:55.112699Z","iopub.status.idle":"2022-04-29T21:02:00.100277Z","shell.execute_reply.started":"2022-04-29T21:01:55.112658Z","shell.execute_reply":"2022-04-29T21:02:00.099314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size_per_replica = 256\nbatch_size = batch_size_per_replica * strategy.num_replicas_in_sync\nbatch_size","metadata":{"execution":{"iopub.status.busy":"2022-04-29T21:02:00.10184Z","iopub.execute_input":"2022-04-29T21:02:00.102398Z","iopub.status.idle":"2022-04-29T21:02:00.111189Z","shell.execute_reply.started":"2022-04-29T21:02:00.102346Z","shell.execute_reply":"2022-04-29T21:02:00.110226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\ntrain_labels = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')\nsample = pd.read_csv('../input/tabular-playground-series-apr-2022/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T21:22:51.471574Z","iopub.execute_input":"2022-04-29T21:22:51.471948Z","iopub.status.idle":"2022-04-29T21:23:03.082265Z","shell.execute_reply.started":"2022-04-29T21:22:51.471894Z","shell.execute_reply":"2022-04-29T21:23:03.081362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feat_eng(df):\n    \n    seq_df=pd.DataFrame()\n    sensors=[col for col in df if col.startswith('sensor')]\n    print('Processing New DF')\n    \n    temp = df.subject.value_counts().sort_values() // 60\n    subject_count = df.merge(temp, left_on='subject', right_index=True, how='left').iloc[:, -1]\n    df['subject_count'] = subject_count\n    \n    for sensor in tqdm(sensors):\n#         if sensor != 'sensor_02':\n#             df['{}_groupby_sensor_2'.format(sensor)] = df.merge(df.groupby('sensor_02')[sensor].median(), \n#                                                                  left_on='sensor_02', right_index=True, how='left').iloc[:, -1]\n#             df['{}_groupby_sensor_2_diff'.format(sensor)] = df[sensor] - df['{}_groupby_sensor_2'.format(sensor)]\n        df['{}_lag1'.format(sensor)] = df.groupby('sequence')[sensor].shift(1)\n        df['{}_lag1'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n        df['{}_diff'.format(sensor)] = df[sensor] - df['{}_lag1'.format(sensor)] \n        df['{}_roll_mean3'.format(sensor)]=df['{}'.format(sensor)].rolling(window=3).mean()\n        df['{}_roll_mean6'.format(sensor)]=df['{}'.format(sensor)].rolling(window=6).mean()\n        df['{}_roll_mean9'.format(sensor)]=df['{}'.format(sensor)].rolling(window=9).mean()\n        df['{}_roll_mean3'.format(sensor)].fillna(df['{}_roll_mean3'.format(sensor)].median(), inplace=True)\n        df['{}_roll_mean6'.format(sensor)].fillna(df['{}_roll_mean6'.format(sensor)].median(), inplace=True)\n        df['{}_roll_mean9'.format(sensor)].fillna(df['{}_roll_mean9'.format(sensor)].median(), inplace=True)\n        \n        if sensor == 'sensor_02':\n            df['{}_lag2'.format(sensor)] = df.groupby('sequence')[sensor].shift(2)\n            df['{}_lag2'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n            df['{}_diff_lag2'.format(sensor)] = df[sensor] - df['{}_lag2'.format(sensor)]\n            df['{}_lag3'.format(sensor)] = df.groupby('sequence')[sensor].shift(3)\n            df['{}_lag3'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n            df['{}_diff_lag3'.format(sensor)] = df[sensor] - df['{}_lag3'.format(sensor)]\n            df['{}_lag5'.format(sensor)] = df.groupby('sequence')[sensor].shift(5)\n            df['{}_lag5'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n            df['{}_diff_lag5'.format(sensor)] = df[sensor] - df['{}_lag5'.format(sensor)]\n\n            df['{}_lead1'.format(sensor)] = df.groupby('sequence')[sensor].shift(-1)\n            df['{}_lead1'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n            df['{}_diff_lead1'.format(sensor)] = df[sensor] - df['{}_lead1'.format(sensor)]\n            df['{}_lead2'.format(sensor)] = df.groupby('sequence')[sensor].shift(-2)\n            df['{}_lead2'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n            df['{}_diff_lead2'.format(sensor)] = df[sensor] - df['{}_lead2'.format(sensor)]\n            df['{}_lead3'.format(sensor)] = df.groupby('sequence')[sensor].shift(-3)\n            df['{}_lead3'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n            df['{}_diff_lead3'.format(sensor)] = df[sensor] - df['{}_lead3'.format(sensor)]\n            df['{}_lead5'.format(sensor)] = df.groupby('sequence')[sensor].shift(-5)\n            df['{}_lead5'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n            df['{}_diff_lead5'.format(sensor)] = df[sensor] - df['{}_lead5'.format(sensor)]\n        \n        \n        \n#         aa = df.groupby(['sequence','subject'], as_index=False)[sensor].mean()\n#         aa = aa.sort_values(by=['sequence', 'subject'])\n#         train2 = train.copy()\n#         df = df.merge(aa, left_on=['sequence', 'subject'], \n#                       right_on=['sequence', 'subject'], how='left', suffixes=['', '_grouped_by_seq_sub_mean'])\n#         df['diff_'] = df[sensor] - df[f'{sensor}_grouped_by_seq_sub_mean']\n\n        if sensor == 'sensor_02':\n            df['{}_groupby_count'.format(sensor)] = df.merge(df.groupby('subject_count')[sensor].mean(), \n                                                                     left_on='subject_count', right_index=True, how='left').iloc[:, -1]\n\n            df['{}_groupby_count_diff'.format(sensor)] = df[sensor] - df['{}_groupby_count'.format(sensor)]\n        \n        s_diff='{}_diff'.format(sensor)\n        seq_df['{}_mean'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].mean()\n        seq_df['{}_diff_mean'.format(sensor)] = df.groupby(['sequence','subject'])[s_diff].mean()\n        seq_df['{}_med'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].median()\n        seq_df['{}_std'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].std()\n        seq_df['{}_skew'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].skew()\n        seq_df['{}_kurt'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].apply(pd.DataFrame.kurt)\n        seq_df['{}_min'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].min()\n        seq_df['{}_max'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].max()\n    \n    \n    bucketized_00 = pd.qcut(df['sensor_00'], q=10, labels=list(range(10)))\n    df['bucketized_00'] = bucketized_00\n    df['{}_groupby_sensor_00'.format('sensor_02')] = df.merge(df.groupby('bucketized_00')['sensor_02'].mean(), \n                                                                 left_on='bucketized_00', right_index=True, how='left').iloc[:, -1]\n    df['{}_groupby_sensor_00_diff'.format('sensor_02')] = df['sensor_02'] - df['{}_groupby_sensor_00'.format('sensor_02')]\n    df.drop('bucketized_00', axis=1, inplace=True)\n    \n    \n#     bucketized_09 = pd.qcut(df['sensor_09'], q=10, labels=list(range(10)))\n#     df['bucketized_09'] = bucketized_09\n#     df['{}_groupby_sensor_09'.format('sensor_02')] = df.merge(df.groupby('bucketized_09')['sensor_02'].mean(), \n#                                                                  left_on='bucketized_09', right_index=True, how='left').iloc[:, -1]\n#     df['{}_groupby_sensor_09_diff'.format('sensor_02')] = df['sensor_02'] - df['{}_groupby_sensor_09'.format('sensor_02')]\n#     df.drop('bucketized_09', axis=1, inplace=True)\n    \n    return df, seq_df.reset_index()\n\nwarnings.filterwarnings('ignore')\ntrain, train_seq_df =feat_eng(df=train)\ntest, test_seq_df =feat_eng(df=test)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T21:23:03.084139Z","iopub.execute_input":"2022-04-29T21:23:03.084837Z","iopub.status.idle":"2022-04-29T21:25:49.16373Z","shell.execute_reply.started":"2022-04-29T21:23:03.084786Z","shell.execute_reply":"2022-04-29T21:25:49.162749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_seq_df.shape, test_seq_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:50.575369Z","iopub.execute_input":"2022-04-29T13:40:50.576015Z","iopub.status.idle":"2022-04-29T13:40:50.581531Z","shell.execute_reply.started":"2022-04-29T13:40:50.575953Z","shell.execute_reply":"2022-04-29T13:40:50.580586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_seq_df = train_seq_df.values\n# test_seq_df = test_seq_df.values","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:50.583284Z","iopub.execute_input":"2022-04-29T13:40:50.584267Z","iopub.status.idle":"2022-04-29T13:40:50.594281Z","shell.execute_reply.started":"2022-04-29T13:40:50.58422Z","shell.execute_reply":"2022-04-29T13:40:50.593386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.iloc[:, 3:]\ntest = test.iloc[:, 3:]\n\nlen_train = len(train)\nconcat = pd.concat([train, test], axis=0)\ndel train, test\n# time.sleep(10)\n\nscaler = StandardScaler()\nconcat_scaled = scaler.fit_transform(concat)\ndel concat \ntime.sleep(10)\n\n# train_scaled = scaler.fit_transform(train)\n# del train\n# time.sleep(10)\n\n# test_scaled = scaler.transform(test)\n# del test\ntime.sleep(10)\n\ntrain_scaled = concat_scaled[:len_train, :]\ntest_scaled = concat_scaled[len_train:, :]\n\ntrain_scaled = train_scaled.reshape(-1, 60, train_scaled.shape[-1])\ntest_scaled = test_scaled.reshape(-1, 60, train_scaled.shape[-1])\ntrain_scaled.shape, test_scaled.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-29T21:25:49.165376Z","iopub.execute_input":"2022-04-29T21:25:49.165719Z","iopub.status.idle":"2022-04-29T21:26:18.394602Z","shell.execute_reply.started":"2022-04-29T21:25:49.165687Z","shell.execute_reply":"2022-04-29T21:26:18.393664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def transformer_encoder(num_blocks=12, linear_shape=64, num_heads=8, dropout_rate=0.0):\n#     inputs = layers.Input(shape=(train_scaled.shape[-2:]))\n#     random_mask = layers.Input(shape=(num_heads, 60, 60))\n#     indexes = tf.range(inputs.shape[-2])\n#     pos_encoding = layers.Embedding(input_dim=60, output_dim=inputs.shape[-1], trainable=True)(indexes)\n#     encoded_inputs = inputs + pos_encoding\n    \n#     encoder = trasformer_block(encoded_inputs, linear_shape=linear_shape, num_heads=num_heads, attention_mask=random_mask)\n#     encoder = 0.3*encoded_inputs + 0.7*encoder\n#     encoder = layers.Dropout(dropout_rate)(encoder)\n#     if num_blocks > 1:\n#         for i in range(1, num_blocks):\n#             x = encoder\n#             encoder = trasformer_block(x, linear_shape=linear_shape, num_heads=num_heads, attention_mask=random_mask)\n#             encoder = 0.3*x + 0.7*encoder\n#             encoder = layers.Dropout(dropout_rate)(encoder)\n            \n#     pooled = layers.GlobalAveragePooling1D()(encoder)\n#     dropout = layers.Dropout(0.0)(pooled)\n#     output = layers.Dense(1, activation='sigmoid')(dropout)\n#     model = tf.keras.Model(inputs=[inputs, random_mask], outputs=output)\n#     metric1 = tf.keras.metrics.AUC(name='auc')\n#     metric2 = tf.keras.metrics.BinaryAccuracy()\n#     model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n#                   optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=0.0), #tf.keras.optimizers.Adam(learning_rate=5e-4),\n#                   metrics=[metric1, metric2])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.399359Z","iopub.execute_input":"2022-04-29T13:40:59.399598Z","iopub.status.idle":"2022-04-29T13:40:59.404116Z","shell.execute_reply.started":"2022-04-29T13:40:59.399565Z","shell.execute_reply":"2022-04-29T13:40:59.403427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\nLR_START = 1e-4\nLR_MAX = 1e-3\nLR_MIN = 5e-5\nLR_RAMPUP_EPOCHS = 0\nLR_SUSTAIN_EPOCHS = 0\nEPOCHS = 50\nSTEPS = [50]\n\n\ndef lrfn(epoch):\n    if epoch<STEPS[0]:\n        epoch2 = epoch\n        EPOCHS2 = STEPS[0]\n    elif epoch<STEPS[0]+STEPS[1]:\n        epoch2 = epoch-STEPS[0]\n        EPOCHS2 = STEPS[1]\n    elif epoch<STEPS[0]+STEPS[1]+STEPS[2]:\n        epoch2 = epoch-STEPS[0]-STEPS[1]\n        EPOCHS2 = STEPS[2]\n    \n    if epoch2 < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch2 + LR_START\n    elif epoch2 < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        decay_total_epochs = EPOCHS2 - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n        decay_epoch_index = epoch2 - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        phase = math.pi * decay_epoch_index / decay_total_epochs\n        cosine_decay = 0.5 * (1 + math.cos(phase))\n        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN\n    return lr\n\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T21:38:03.023268Z","iopub.execute_input":"2022-04-29T21:38:03.023573Z","iopub.status.idle":"2022-04-29T21:38:03.046936Z","shell.execute_reply.started":"2022-04-29T21:38:03.023544Z","shell.execute_reply":"2022-04-29T21:38:03.046131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def model():\n#     x_input = layers.Input(shape=(train_scaled.shape[-2:]))\n#     x1 = layers.Bidirectional(layers.LSTM(units=512, return_sequences=True))(x_input)\n\n#     l1 = layers.Bidirectional(layers.LSTM(units=384, return_sequences=True))(x1)\n#     l2 = layers.Bidirectional(layers.LSTM(units=384, return_sequences=True))(x_input)\n\n#     c1 = layers.Concatenate(axis=2)([l1,l2])\n\n#     l3 = layers.Bidirectional(layers.LSTM(units=256, return_sequences=True))(c1)\n#     l4 = layers.Bidirectional(layers.LSTM(units=256, return_sequences=True))(l2)\n\n#     c2 = layers.Concatenate(axis=2)([l3,l4])\n\n#     l6 = layers.GlobalMaxPooling1D()(c2)\n#     l7 = layers.Dense(units=128, activation='selu')(l6)\n#     l8 = layers.Dropout(0.05)(l7)\n\n#     output = layers.Dense(1, activation='sigmoid')(l8)\n    \n#     model = tf.keras.Model(inputs=x_input, outputs=output)\n#     model.compile(optimizer='adam', \n#                   loss='binary_crossentropy', \n#                   metrics=[tf.keras.metrics.AUC(name = 'auc')])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.426353Z","iopub.execute_input":"2022-04-29T13:40:59.42671Z","iopub.status.idle":"2022-04-29T13:40:59.437857Z","shell.execute_reply.started":"2022-04-29T13:40:59.426675Z","shell.execute_reply":"2022-04-29T13:40:59.437199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = transformer_encoder(num_blocks=1, linear_shape=64, num_heads=4)\n# model.summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-29T13:40:59.439233Z","iopub.execute_input":"2022-04-29T13:40:59.44018Z","iopub.status.idle":"2022-04-29T13:40:59.4535Z","shell.execute_reply.started":"2022-04-29T13:40:59.440133Z","shell.execute_reply":"2022-04-29T13:40:59.452642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with strategy.scope():\n#     loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n#     train_metric = tf.keras.metrics.AUC(name='auc')\n#     valid_metric = tf.keras.metrics.AUC(name='auc')\n#     optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n\n# @tf.function\n# def train_step(x, y):\n#     with tf.GradientTape() as tape:\n#         preds = model(x, training=True)\n#         loss_value = loss_object(y, preds)\n#     grads = tape.gradient(loss_value, model.trainable_weights)\n#     optimizer.apply_gradients((zip(grads, model.trainable_weights)))\n#     train_metric.update_state(y, preds)\n#     return loss_value\n\n# @tf.function\n# def valid_step(x, y):\n#     preds = model(x, training=True)\n#     loss_value = loss_object(y, preds)\n#     valid_metric.update_state(y, preds)\n#     return loss_value\n\n\n# n_splits = 3\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1443)\n# for fold, (train_idx, valid_idx) in enumerate(skf.split(train_scaled, train_labels.iloc[:, -1].values)):\n#     print('*'*30, f'Fold {fold+1}', '*'*30)\n#     x_train, y_train = train_scaled[train_idx], train_labels.iloc[train_idx, -1].values\n#     x_valid, y_valid = train_scaled[valid_idx], train_labels.iloc[valid_idx, -1].values\n    \n#     train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n#     train_ds = train_ds.shuffle(1024)\n#     train_ds = train_ds.batch(128).prefetch(-1)\n    \n#     valid_ds = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n#     valid_ds = valid_ds.batch(128).prefetch(-1)\n    \n#     with strategy.scope():\n#         model = transformer_encoder(num_blocks=12, linear_shape=64, num_heads=8, dropout_rate=0.2)\n    \n#     for epoch in range(100):\n#         print(f'Epoch --------> {epoch+1}')\n#         train_loss_list = []\n#         epoch_random_mask = tf.random.uniform(shape=(1, 60, 60)) + 0.2\n#         epoch_random_mask = tf.broadcast_to(epoch_random_mask, shape=(8, 60, 60))\n#         epoch_random_mask = tf.where(epoch_random_mask<0.5, 0.0, 1.0)\n#         for x, y in tqdm(train_ds, total=len(train_ds)):\n#             random_mask = tf.broadcast_to(epoch_random_mask, shape=(x.shape[0], 8, 60, 60))\n#             inputs = (x, random_mask)\n#             train_loss_list.append(train_step(inputs, y))\n#         print('Train', 'Loss:', np.mean(train_loss_list), 'AUC:', train_metric.result().numpy())\n#         train_metric.reset_state()\n        \n#         valid_loss_list = []\n#         for x, y in tqdm(valid_ds, total=len(valid_ds)):\n#             ones_mask = tf.ones(shape=(8, 60, 60))\n#             ones_mask = tf.broadcast_to(ones_mask, shape=(x.shape[0], 8, 60, 60))\n#             inputs = (x, ones_mask)\n#             valid_loss_list.append(valid_step(inputs, y))\n#         print('Valid', 'Loss:', np.mean(valid_loss_list), 'AUC:', valid_metric.result().numpy())\n#         valid_metric.reset_state()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.455431Z","iopub.execute_input":"2022-04-29T13:40:59.45586Z","iopub.status.idle":"2022-04-29T13:40:59.466795Z","shell.execute_reply.started":"2022-04-29T13:40:59.455808Z","shell.execute_reply":"2022-04-29T13:40:59.465774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def trasformer_block(inputs, linear_shape=512, num_heads=8, dropout_rate=0.0):\n#     x = layers.MultiHeadAttention(num_heads=num_heads,\n#                                   key_dim=linear_shape,\n#                                   value_dim=linear_shape,\n#                                   dropout=dropout_rate)(inputs, inputs)\n#     x = layers.Add()([inputs, x])\n#     x1 = layers.LayerNormalization()(x)\n    \n#     x = layers.Dense(linear_shape, activation='gelu')(x1)\n#     x = layers.Dense(inputs.shape[-1])(x)\n    \n#     x = layers.Add()([x1, x])\n#     x2 = layers.LayerNormalization()(x)\n#     return x2","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.468347Z","iopub.execute_input":"2022-04-29T13:40:59.470331Z","iopub.status.idle":"2022-04-29T13:40:59.482224Z","shell.execute_reply.started":"2022-04-29T13:40:59.470281Z","shell.execute_reply":"2022-04-29T13:40:59.481422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def transformer_encoder(num_blocks=12, linear_shape=64, num_heads=8, dropout_rate=0.0):\n#     inputs = layers.Input(shape=(train_scaled.shape[-2:]))\n#     indexes = tf.range(inputs.shape[-2])\n#     pos_encoding = layers.Embedding(input_dim=60, output_dim=inputs.shape[-1], trainable=True)(indexes)\n#     encoded_inputs = inputs + pos_encoding\n    \n#     encoder = trasformer_block(encoded_inputs, linear_shape=linear_shape, num_heads=num_heads, dropout_rate=dropout_rate)\n# #     encoder = 0.3*encoded_inputs + 0.7*encoder\n#     encoder = layers.BatchNormalization()(encoder)\n#     encoder = layers.Dropout(dropout_rate)(encoder)\n#     if num_blocks > 1:\n#         for i in range(1, num_blocks):\n#             x = encoder\n#             encoder = trasformer_block(x, linear_shape=linear_shape, num_heads=num_heads, dropout_rate=dropout_rate)\n# #             encoder = 0.3*x + 0.7*encoder\n#             encoder = layers.BatchNormalization()(encoder)\n#             encoder = layers.Dropout(dropout_rate)(encoder)\n            \n#     pooled = layers.GlobalAveragePooling1D()(encoder)\n#     dropout = layers.Dropout(0.5)(pooled)\n#     output = layers.Dense(1, activation='sigmoid')(dropout)\n#     model = tf.keras.Model(inputs=inputs, outputs=output)\n#     metric1 = tf.keras.metrics.AUC(name='auc')\n#     metric2 = tf.keras.metrics.BinaryAccuracy()\n#     model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n#                   optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=5e-4), #tf.keras.optimizers.Adam(learning_rate=5e-4),\n#                   metrics=[metric1, metric2])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.483689Z","iopub.execute_input":"2022-04-29T13:40:59.483956Z","iopub.status.idle":"2022-04-29T13:40:59.499143Z","shell.execute_reply.started":"2022-04-29T13:40:59.483925Z","shell.execute_reply":"2022-04-29T13:40:59.498187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def transformer_encoder(num_blocks=12, linear_shape=64, num_heads=8, dropout_rate=0.0):\n#     inputs = layers.Input(shape=(train_scaled.shape[-2:]))\n# #     indexes = tf.range(inputs.shape[-2])\n# #     pos_encoding = layers.Embedding(input_dim=60, output_dim=inputs.shape[-1], trainable=True)(indexes)\n# #     encoded_inputs = inputs + pos_encoding\n    \n#     lstm = layers.Bidirectional(layers.LSTM(512, return_sequences=True))(inputs)\n#     lstm = layers.Bidirectional(layers.LSTM(512, return_sequences=True))(lstm)\n    \n#     encoder = trasformer_block(lstm, linear_shape=linear_shape, num_heads=num_heads, dropout_rate=dropout_rate)\n# #     encoder = lstm + encoder\n#     encoder = layers.BatchNormalization()(encoder)\n#     encoder = layers.Dropout(dropout_rate)(encoder)\n#     if num_blocks > 1:\n#         for i in range(1, num_blocks):\n#             x = encoder\n#             encoder = trasformer_block(x, linear_shape=linear_shape, num_heads=num_heads, dropout_rate=dropout_rate)\n# #             encoder = x + encoder\n#             encoder = layers.BatchNormalization()(encoder)\n#             encoder = layers.Dropout(dropout_rate)(encoder)\n        \n#     pooled = layers.GlobalAveragePooling1D()(encoder)\n#     dropout = layers.Dropout(0.5)(pooled)\n#     output = layers.Dense(1, activation='sigmoid')(dropout)\n#     model = tf.keras.Model(inputs=inputs, outputs=output)\n#     metric1 = tf.keras.metrics.AUC(name='auc')\n#     metric2 = tf.keras.metrics.BinaryAccuracy()\n#     model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n#                   optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=5e-4), #tf.keras.optimizers.Adam(learning_rate=5e-4),\n#                   metrics=[metric1, metric2])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.503071Z","iopub.execute_input":"2022-04-29T13:40:59.503332Z","iopub.status.idle":"2022-04-29T13:40:59.513793Z","shell.execute_reply.started":"2022-04-29T13:40:59.503303Z","shell.execute_reply":"2022-04-29T13:40:59.512886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = transformer_encoder(num_blocks=2, linear_shape=64, num_heads=4, dropout_rate=0.0)\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.515466Z","iopub.execute_input":"2022-04-29T13:40:59.515709Z","iopub.status.idle":"2022-04-29T13:40:59.528816Z","shell.execute_reply.started":"2022-04-29T13:40:59.515681Z","shell.execute_reply":"2022-04-29T13:40:59.528106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# n_splits = 3\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1443)\n# for fold, (train_idx, valid_idx) in enumerate(skf.split(train_scaled, train_labels.iloc[:, -1].values)):\n#     print('*'*30, f'Fold {fold+1}', '*'*30)\n#     x_train, y_train = train_scaled[train_idx], train_labels.iloc[train_idx, -1].values\n#     x_valid, y_valid = train_scaled[valid_idx], train_labels.iloc[valid_idx, -1].values\n    \n#     file_path = f'Fold_{fold+1}_weights.h5'\n#     ckpt = tf.keras.callbacks.ModelCheckpoint(filepath=file_path,\n#                                               monitor='val_auc',\n#                                               mode='max',\n#                                               save_best_only=True,\n#                                               save_weights_only=True,\n#                                              verbose=1)\n    \n#     lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.4,  patience=10, verbose=True)\n#     with strategy.scope():\n#         model = transformer_encoder(num_blocks=2, linear_shape=32, num_heads=8, dropout_rate=0.0)\n#     model.fit(x_train, y_train, \n#               validation_data=(x_valid, y_valid),\n#               epochs=120, batch_size=256, callbacks=[ckpt])\n#     break","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-29T13:40:59.530356Z","iopub.execute_input":"2022-04-29T13:40:59.530851Z","iopub.status.idle":"2022-04-29T13:40:59.541985Z","shell.execute_reply.started":"2022-04-29T13:40:59.530806Z","shell.execute_reply":"2022-04-29T13:40:59.540923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# n_splits = 5\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1443)\n# for fold, (train_idx, valid_idx) in enumerate(skf.split(train_scaled, train_labels.iloc[:, -1].values)):\n#     print('*'*30, f'Fold {fold+1}', '*'*30)\n#     x_train, y_train = train_scaled[train_idx], train_labels.iloc[train_idx, -1].values\n#     x_valid, y_valid = train_scaled[valid_idx], train_labels.iloc[valid_idx, -1].values\n    \n#     file_path = f'Fold_{fold+1}_weights.h5'\n#     ckpt = tf.keras.callbacks.ModelCheckpoint(filepath=file_path,\n#                                               monitor='val_auc',\n#                                               mode='max',\n#                                               save_best_only=True,\n#                                               save_weights_only=True,\n#                                              verbose=1)\n    \n#     lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.4,  patience=4, verbose=True, mode='max')\n#     with strategy.scope():\n#         model = transformer_encoder(num_blocks=4, linear_shape=64, num_heads=4, dropout_rate=0.0)\n#     model.fit(x_train, y_train, \n#               validation_data=(x_valid, y_valid),\n#               epochs=20, batch_size=256, callbacks=[ckpt, lr])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-29T13:40:59.543493Z","iopub.execute_input":"2022-04-29T13:40:59.544525Z","iopub.status.idle":"2022-04-29T13:40:59.558475Z","shell.execute_reply.started":"2022-04-29T13:40:59.544483Z","shell.execute_reply":"2022-04-29T13:40:59.557566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def dense_block(inputs):\n#     x = layers.Dense(256, activation='gelu')(inputs)\n#     x = layers.Dense(256, activation='gelu')(x)\n#     x = layers.Dense(128, activation='gelu')(x)\n#     x = layers.Dense(128, activation='gelu')(x)\n#     x = layers.Dense(64, activation='gelu')(x)\n#     return x","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.560331Z","iopub.execute_input":"2022-04-29T13:40:59.560921Z","iopub.status.idle":"2022-04-29T13:40:59.574601Z","shell.execute_reply.started":"2022-04-29T13:40:59.560876Z","shell.execute_reply":"2022-04-29T13:40:59.573858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.layers import Input, Bidirectional, LSTM, Concatenate, GlobalMaxPooling1D, Dense, Dropout\n\n# def lstm_model():\n#     inputs = Input(shape=(train_scaled.shape[-2:]))\n#     dense = Dense(128, activation='gelu')(inputs)\n#     dense = Dense(128, activation='gelu')(dense)\n#     dropout = Dropout(0.35)(dense)\n#     dense = Dense(128, activation='linear')(dropout)\n# #     dense = layers.Add()([x_input, dense])\n#     x1 = Bidirectional(LSTM(units=512, return_sequences=True))(dense)\n\n#     l1 = Bidirectional(LSTM(units=384, return_sequences=True))(x1)\n#     l2 = Bidirectional(LSTM(units=384, return_sequences=True))(inputs)\n\n#     c1 = Concatenate(axis=2)([l1,l2])\n\n#     l3 = Bidirectional(LSTM(units=256, return_sequences=True))(c1)\n#     l4 = Bidirectional(LSTM(units=256, return_sequences=True))(l2)\n\n#     c2 = Concatenate(axis=2)([l3,l4])\n\n#     l6 = GlobalMaxPooling1D()(c2)\n    \n#     l7 = Dense(units=128, activation='gelu')(l6)\n#     l8 = Dropout(0.0)(l7)\n\n#     output = Dense(1, activation='sigmoid')(l8)\n    \n    \n#     model = tf.keras.Model(inputs=inputs, outputs=output)\n#     metric1 = tf.keras.metrics.AUC(name='auc')\n#     metric2 = tf.keras.metrics.BinaryAccuracy()\n#     model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n#                   optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-3), #tf.keras.optimizers.Adam(learning_rate=5e-4),\n#                   metrics=[metric1, metric2])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.576344Z","iopub.execute_input":"2022-04-29T13:40:59.577197Z","iopub.status.idle":"2022-04-29T13:40:59.585856Z","shell.execute_reply.started":"2022-04-29T13:40:59.577147Z","shell.execute_reply":"2022-04-29T13:40:59.58514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cnn_block(inputs, filters=128, dropout=0.4):\n    x = layers.Conv2D(filters=filters, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(dropout)(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-04-29T21:32:23.631635Z","iopub.execute_input":"2022-04-29T21:32:23.632Z","iopub.status.idle":"2022-04-29T21:32:23.63879Z","shell.execute_reply.started":"2022-04-29T21:32:23.631963Z","shell.execute_reply":"2022-04-29T21:32:23.637892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GeMPoolingLayer(tf.keras.layers.Layer):\n    def __init__(self, p=1., train_p=False):\n        super().__init__()\n        if train_p:\n            self.p = tf.Variable(p, dtype=tf.float32)\n        else:\n            self.p = p\n        self.eps = 1e-6\n\n    def call(self, inputs: tf.Tensor, **kwargs):\n        inputs = tf.clip_by_value(inputs, clip_value_min=1e-6, clip_value_max=tf.reduce_max(inputs))\n        inputs = tf.pow(inputs, self.p)\n        inputs = tf.reduce_mean(inputs, axis=[1], keepdims=False)\n        inputs = tf.pow(inputs, 1./self.p)\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2022-04-29T21:32:23.961003Z","iopub.execute_input":"2022-04-29T21:32:23.961834Z","iopub.status.idle":"2022-04-29T21:32:23.970424Z","shell.execute_reply.started":"2022-04-29T21:32:23.961785Z","shell.execute_reply":"2022-04-29T21:32:23.96948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Bidirectional, LSTM, Concatenate, GlobalMaxPooling1D, Dense, Dropout, GRU\n\ndef lstm_model():\n    inputs = Input(shape=(train_scaled.shape[-2:]))\n#     dense = Dense(128, activation='gelu')(inputs)\n#     dense = Dense(128, activation='gelu')(dense)\n#     dropout = Dropout(0.35)(dense)\n#     dense = Dense(128, activation='linear')(dropout)\n    processed = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(inputs)\n    cnn = cnn_block(processed, filters=128, dropout=0.40)\n    cnn = cnn_block(cnn, filters=64, dropout=0.35)\n    cnn = cnn_block(cnn, filters=32, dropout=0.20)\n    cnn = cnn_block(cnn, filters=1, dropout=0.0)\n    cnn = layers.MaxPooling2D()(cnn)\n    cnn = layers.Lambda(lambda x: tf.squeeze(x, axis=-1))(cnn)\n    \n    x1 = Bidirectional(GRU(units=512, return_sequences=True))(cnn)\n\n    l1 = Bidirectional(GRU(units=384, return_sequences=True))(x1)\n    l2 = Bidirectional(GRU(units=384, return_sequences=True))(cnn)\n\n    c1 = Concatenate(axis=2)([l1,l2])\n\n    l3 = Bidirectional(GRU(units=256, return_sequences=True))(c1)\n    l4 = Bidirectional(GRU(units=256, return_sequences=True))(l2)\n\n    c2 = Concatenate(axis=2)([l3,l4])\n#     c2 = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(c2)\n#     c2 = layers.Conv2D(filters=1, kernel_size=3, strides=1, activation='gelu', padding='valid')(c2)\n#     c2 = layers.Lambda(lambda x: tf.squeeze(x, axis=-1))(c2)\n    l6 = GlobalMaxPooling1D()(c2)\n#     l6 = GeMPoolingLayer(p=1., train_p=True)(c2)\n    \n    l7 = Dense(units=128, activation='gelu')(l6)\n    l8 = Dropout(0.0)(l7)\n\n    output = Dense(1, activation='sigmoid')(l8)\n    \n    \n    model = tf.keras.Model(inputs=inputs, outputs=output)\n    metric1 = tf.keras.metrics.AUC(name='auc')\n    metric2 = tf.keras.metrics.BinaryAccuracy()\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n                  optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-3), #tf.keras.optimizers.Adam(learning_rate=5e-4),\n                  metrics=[metric1, metric2])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:08:23.536171Z","iopub.execute_input":"2022-04-29T22:08:23.536464Z","iopub.status.idle":"2022-04-29T22:08:23.553318Z","shell.execute_reply.started":"2022-04-29T22:08:23.536434Z","shell.execute_reply":"2022-04-29T22:08:23.552298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.layers import Input, Bidirectional, LSTM, Concatenate, GlobalMaxPooling1D, Dense, Dropout, GRU\n\n# def lstm_model():\n#     inputs = Input(shape=(train_scaled.shape[-2:]))\n# #     dense = Dense(128, activation='gelu')(inputs)\n# #     dense = Dense(128, activation='gelu')(dense)\n# #     dropout = Dropout(0.35)(dense)\n# #     dense = Dense(128, activation='linear')(dropout)\n#     processed = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(inputs)\n#     cnn = cnn_block(processed, filters=128, dropout=0.40)\n#     cnn = cnn_block(cnn, filters=64, dropout=0.35)\n#     cnn = cnn_block(cnn, filters=32, dropout=0.20)\n#     cnn = cnn_block(cnn, filters=1, dropout=0.0)\n#     cnn = layers.MaxPooling2D()(cnn)\n#     cnn = layers.Lambda(lambda x: tf.squeeze(x, axis=-1))(cnn)\n    \n#     x1 = Bidirectional(LSTM(units=512, return_sequences=True))(cnn)\n    \n#     l1 = Bidirectional(LSTM(units=384, return_sequences=True))(x1)\n#     l2 = Bidirectional(LSTM(units=384, return_sequences=True))(cnn)\n\n#     c1 = Concatenate(axis=2)([l1,l2])\n\n#     l3 = layers.MultiHeadAttention(num_heads=4,\n#                                   key_dim=64,\n#                                   value_dim=64,\n#                                   dropout=0.0)(c1, c1)\n    \n#     l3 = layers.MultiHeadAttention(num_heads=4,\n#                                   key_dim=64,\n#                                   value_dim=64,\n#                                   dropout=0.0)(l3, l3)\n    \n#     l4 = Bidirectional(LSTM(units=256, return_sequences=True))(l3)\n\n# #     c2 = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(c2)\n# #     c2 = layers.Conv2D(filters=1, kernel_size=3, strides=1, activation='gelu', padding='valid')(c2)\n# #     c2 = layers.Lambda(lambda x: tf.squeeze(x, axis=-1))(c2)\n#     l6 = GeMPoolingLayer(p=1., train_p=True)(l4)\n    \n#     l7 = Dense(units=128, activation='gelu')(l6)\n#     l8 = Dropout(0.0)(l7)\n\n#     output = Dense(1, activation='sigmoid')(l8)\n    \n    \n#     model = tf.keras.Model(inputs=inputs, outputs=output)\n#     metric1 = tf.keras.metrics.AUC(name='auc')\n#     metric2 = tf.keras.metrics.BinaryAccuracy()\n#     model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n#                   optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-3), #tf.keras.optimizers.Adam(learning_rate=5e-4),\n#                   metrics=[metric1, metric2])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.635657Z","iopub.execute_input":"2022-04-29T13:40:59.635989Z","iopub.status.idle":"2022-04-29T13:40:59.649463Z","shell.execute_reply.started":"2022-04-29T13:40:59.635944Z","shell.execute_reply":"2022-04-29T13:40:59.648538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.layers import Input, Bidirectional, LSTM, Concatenate, GlobalMaxPooling1D, Dense, Dropout, GRU\n\n# def lstm_model():\n#     inputs = Input(shape=(train_scaled.shape[-2:]))\n# #     dense = Dense(128, activation='gelu')(inputs)\n# #     dense = Dense(128, activation='gelu')(dense)\n# #     dropout = Dropout(0.35)(dense)\n# #     dense = Dense(128, activation='linear')(dropout)\n#     processed = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(inputs)\n#     cnn = cnn_block(processed, filters=128, dropout=0.20)\n#     cnn = cnn_block(cnn, filters=64, dropout=0.10)\n#     cnn = cnn_block(cnn, filters=32, dropout=0.05)\n#     cnn = cnn_block(cnn, filters=1, dropout=0.0)\n#     cnn = layers.MaxPooling2D()(cnn)\n#     cnn = layers.Lambda(lambda x: tf.squeeze(x, axis=-1))(cnn) \n    \n#     x1 = Bidirectional(LSTM(units=512, return_sequences=True))(cnn)\n\n#     l1 = Bidirectional(LSTM(units=384, return_sequences=True))(x1)\n#     l2 = Bidirectional(LSTM(units=384, return_sequences=True))(cnn)\n\n#     c1 = Concatenate(axis=2)([l1,l2])\n    \n#     processed = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(c1)\n#     cnn = cnn_block(processed, filters=128, dropout=0.30)\n#     cnn = cnn_block(cnn, filters=128, dropout=0.25)\n#     cnn = cnn_block(cnn, filters=64, dropout=0.10)\n#     cnn = cnn_block(cnn, filters=64, dropout=0.10)\n#     cnn = layers.MaxPooling2D()(cnn)\n\n#     l6 = layers.GlobalMaxPooling2D()(cnn)\n    \n#     l7 = Dense(units=128, activation='gelu')(l6)\n#     l8 = Dropout(0.0)(l7)\n\n#     output = Dense(1, activation='sigmoid')(l8)\n    \n    \n#     model = tf.keras.Model(inputs=inputs, outputs=output)\n#     metric1 = tf.keras.metrics.AUC(name='auc')\n#     metric2 = tf.keras.metrics.BinaryAccuracy()\n#     model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n#                   optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-3), #tf.keras.optimizers.Adam(learning_rate=5e-4),\n#                   metrics=[metric1, metric2])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.651234Z","iopub.execute_input":"2022-04-29T13:40:59.651734Z","iopub.status.idle":"2022-04-29T13:40:59.665713Z","shell.execute_reply.started":"2022-04-29T13:40:59.6517Z","shell.execute_reply":"2022-04-29T13:40:59.664906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nmodel = lstm_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:40:59.666805Z","iopub.execute_input":"2022-04-29T13:40:59.667755Z","iopub.status.idle":"2022-04-29T13:41:02.57086Z","shell.execute_reply.started":"2022-04-29T13:40:59.667711Z","shell.execute_reply":"2022-04-29T13:41:02.569844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.layers import Input, Bidirectional, LSTM, Concatenate, GlobalMaxPooling1D, Dense, Dropout\n\n# def lstm_model():\n#     inputs_1 = Input(shape=(train_scaled.shape[-2:]))\n#     inputs_2 = Input(shape=(train_seq_df.shape[-1],))\n#     dense = Dense(128, activation='gelu')(inputs_1)\n#     dense = Dense(128, activation='gelu')(dense)\n#     dropout = Dropout(0.35)(dense)\n#     dense = Dense(128, activation='linear')(dropout)\n# #     dense = layers.Add()([x_input, dense])\n#     x1 = Bidirectional(LSTM(units=512, return_sequences=True))(dense)\n\n#     l1 = Bidirectional(LSTM(units=384, return_sequences=True))(x1)\n#     l2 = Bidirectional(LSTM(units=384, return_sequences=True))(inputs_1)\n\n#     c1 = Concatenate(axis=2)([l1,l2])\n\n#     l3 = Bidirectional(LSTM(units=256, return_sequences=True))(c1)\n#     l4 = Bidirectional(LSTM(units=256, return_sequences=True))(l2)\n\n#     c2 = Concatenate(axis=2)([l3,l4])\n\n#     l6 = GlobalMaxPooling1D()(c2)\n    \n#     dense_outputs = dense_block(inputs_2)\n#     c3 = Concatenate()([l6, dense_outputs])\n    \n#     l7 = Dense(units=128, activation='gelu')(c3)\n#     l8 = Dropout(0.0)(l7)\n\n#     output = Dense(1, activation='sigmoid')(l8)\n    \n    \n#     model = tf.keras.Model(inputs=[inputs_1, inputs_2], outputs=output)\n#     metric1 = tf.keras.metrics.AUC(name='auc')\n#     metric2 = tf.keras.metrics.BinaryAccuracy()\n#     model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n#                   optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-3), #tf.keras.optimizers.Adam(learning_rate=5e-4),\n#                   metrics=[metric1, metric2])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:41:02.572347Z","iopub.execute_input":"2022-04-29T13:41:02.572605Z","iopub.status.idle":"2022-04-29T13:41:02.578168Z","shell.execute_reply.started":"2022-04-29T13:41:02.572573Z","shell.execute_reply":"2022-04-29T13:41:02.577324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = lstm_model()\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:41:02.581976Z","iopub.execute_input":"2022-04-29T13:41:02.582248Z","iopub.status.idle":"2022-04-29T13:41:02.589592Z","shell.execute_reply.started":"2022-04-29T13:41:02.582217Z","shell.execute_reply":"2022-04-29T13:41:02.58895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1443)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_scaled, train_labels.iloc[:, -1].values)):\n    for rep in range(3):\n        print('*'*30, f'Fold {fold+1} Trial {rep+1}', '*'*30)\n        x_train, y_train = train_scaled[train_idx], train_labels.iloc[train_idx, -1].values\n        x_valid, y_valid = train_scaled[valid_idx], train_labels.iloc[valid_idx, -1].values\n\n        file_path = f'Fold_{fold+1}_{rep+1}_weights.h5'\n        ckpt = tf.keras.callbacks.ModelCheckpoint(filepath=file_path,\n                                                  monitor='val_auc',\n                                                  mode='max',\n                                                  save_best_only=True,\n                                                  save_weights_only=True,\n                                                 verbose=1)\n\n        tf.keras.backend.clear_session()\n        lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5,  patience=4, verbose=True, mode='max')\n        with strategy.scope():\n            model = lstm_model()\n        model.fit(x_train, y_train, \n                  validation_data=(x_valid, y_valid),\n                  epochs=40, batch_size=32*8, callbacks=[ckpt, lr])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-29T22:08:32.927503Z","iopub.execute_input":"2022-04-29T22:08:32.92781Z","iopub.status.idle":"2022-04-29T23:20:52.830854Z","shell.execute_reply.started":"2022-04-29T22:08:32.927779Z","shell.execute_reply":"2022-04-29T23:20:52.829934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best = [1, 2, 2, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2022-04-29T23:20:58.64318Z","iopub.execute_input":"2022-04-29T23:20:58.644511Z","iopub.status.idle":"2022-04-29T23:20:58.650338Z","shell.execute_reply.started":"2022-04-29T23:20:58.644351Z","shell.execute_reply":"2022-04-29T23:20:58.649426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# n_splits = 5\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1443)\n# for fold, (train_idx, valid_idx) in enumerate(skf.split(train_scaled, train_labels.iloc[:, -1].values)):\n#     print('*'*30, f'Fold {fold+1}', '*'*30)\n#     x_train_1, x_train_2, y_train = train_scaled[train_idx], train_seq_df[train_idx], train_labels.iloc[train_idx, -1].values\n#     x_valid_1, x_valid_2, y_valid = train_scaled[valid_idx], train_seq_df[valid_idx], train_labels.iloc[valid_idx, -1].values\n    \n#     file_path = f'Fold_{fold+1}_weights.h5'\n#     ckpt = tf.keras.callbacks.ModelCheckpoint(filepath=file_path,\n#                                               monitor='val_auc',\n#                                               mode='max',\n#                                               save_best_only=True,\n#                                               save_weights_only=True,\n#                                              verbose=1)\n    \n#     lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.4,  patience=4, verbose=True, mode='max')\n#     with strategy.scope():\n#         model = lstm_model()\n#     model.fit((x_train_1, x_train_2), y_train, \n#               validation_data=((x_valid_1, x_valid_2), y_valid),\n#               epochs=20, batch_size=256, callbacks=[ckpt, lr])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-29T14:37:54.268397Z","iopub.execute_input":"2022-04-29T14:37:54.26889Z","iopub.status.idle":"2022-04-29T14:37:54.280123Z","shell.execute_reply.started":"2022-04-29T14:37:54.268843Z","shell.execute_reply":"2022-04-29T14:37:54.279411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_preds_array = np.zeros(len(train_scaled))\ntest_preds_array = np.zeros(len(test_scaled))\n\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1443)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_scaled, train_labels.iloc[:, -1].values)):\n    print('*'*30, f'Fold {fold+1}', '*'*30)\n    x_train, y_train = train_scaled[train_idx], train_labels.iloc[train_idx, -1].values\n    x_valid, y_valid = train_scaled[valid_idx], train_labels.iloc[valid_idx, -1].values\n    \n    tf.keras.backend.clear_session()\n    with strategy.scope():\n        model = lstm_model()\n        print('Weights:', f'./Fold_{fold+1}_{best[fold]}_weights.h5')\n        model.load_weights(f'./Fold_{fold+1}_{best[fold]}_weights.h5')\n    valid_preds = model.predict(x_valid, batch_size=256, verbose=True)\n    model.evaluate(x=x_valid, y=y_valid, batch_size=256, verbose=True)\n    valid_preds_array[valid_idx] = np.squeeze(valid_preds)\n\n    test_preds = model.predict(test_scaled, batch_size=256, verbose=True)\n    test_preds_array += np.squeeze(test_preds) / n_splits","metadata":{"execution":{"iopub.status.busy":"2022-04-29T23:21:09.40144Z","iopub.execute_input":"2022-04-29T23:21:09.402594Z","iopub.status.idle":"2022-04-29T23:23:16.150268Z","shell.execute_reply.started":"2022-04-29T23:21:09.402509Z","shell.execute_reply":"2022-04-29T23:23:16.14864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean([0.9851, 0.9861, 0.9874, 0.9873, 0.9878])","metadata":{"execution":{"iopub.status.busy":"2022-04-29T23:23:44.221682Z","iopub.execute_input":"2022-04-29T23:23:44.222616Z","iopub.status.idle":"2022-04-29T23:23:44.233725Z","shell.execute_reply.started":"2022-04-29T23:23:44.222568Z","shell.execute_reply":"2022-04-29T23:23:44.232975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({'preds': valid_preds_array}).to_csv('valid_preds_array_9867.csv', index=False)\npd.DataFrame({'preds': test_preds_array}).to_csv('test_preds_array_9867.csv', index=False)\n# # !rm ./Fold*","metadata":{"execution":{"iopub.status.busy":"2022-04-29T23:24:04.641767Z","iopub.execute_input":"2022-04-29T23:24:04.642047Z","iopub.status.idle":"2022-04-29T23:24:04.79309Z","shell.execute_reply.started":"2022-04-29T23:24:04.642017Z","shell.execute_reply":"2022-04-29T23:24:04.792246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 256 ----> 0.9591\n# 512 ----> 0.9612\n# 512, 4, 64, 4, 0.0 ----> 0.9637\n# 0.9689","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:37:54.327092Z","iopub.execute_input":"2022-04-29T14:37:54.327424Z","iopub.status.idle":"2022-04-29T14:37:54.336916Z","shell.execute_reply.started":"2022-04-29T14:37:54.327381Z","shell.execute_reply":"2022-04-29T14:37:54.335975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.load_weights('./Fold_1_weights.h5')\n# preds = model.predict(test_scaled, batch_size=256, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:37:54.338247Z","iopub.execute_input":"2022-04-29T14:37:54.338483Z","iopub.status.idle":"2022-04-29T14:37:54.348961Z","shell.execute_reply.started":"2022-04-29T14:37:54.338456Z","shell.execute_reply":"2022-04-29T14:37:54.347741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample.iloc[:, -1] = preds.reshape(-1,)\n# sample.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:37:54.350757Z","iopub.execute_input":"2022-04-29T14:37:54.35143Z","iopub.status.idle":"2022-04-29T14:37:54.360588Z","shell.execute_reply.started":"2022-04-29T14:37:54.351381Z","shell.execute_reply":"2022-04-29T14:37:54.359979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}