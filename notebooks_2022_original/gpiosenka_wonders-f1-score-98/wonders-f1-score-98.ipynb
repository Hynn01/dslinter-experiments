{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## [1. Import Needed Modules](#import) ##\n## [2. Read in images and create a dataframe of image paths and class labels](#makedf) ## \n## [3. Trim the train_df dataframe](#trim) ##\n## [4. Create train, test and validation generators](#generators) ## \n## [5. Create a function to show Training Image Samples](#show) ## \n## [6. Create the Model](#model) ## \n## [7. Create a custom Keras callback to continue or halt training](#callback) ## \n## [8. Instantiate custom callback and create callbacks to control learning rate and early stopping](#callbacks) ##\n## [9. Train the model](#train) ##\n## [10. Define a function to plot the training data](#plot) ##\n## [11. Make predictions on test set, create Confusion Matrix and Classification Report](#result) ##\n## [12. Save the model](#save) ##","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\n# <center>Import Need Modules</center>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport matplotlib.pyplot as plt\nimport cv2\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport shutil\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Model\n# pprevent annoying tensorflow warning\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\nimport warnings\npd.set_option('max_columns', None)\npd.set_option('max_rows', 90)\nwarnings.simplefilter(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-05-04T02:01:18.020371Z","iopub.execute_input":"2022-05-04T02:01:18.0207Z","iopub.status.idle":"2022-05-04T02:01:24.509408Z","shell.execute_reply.started":"2022-05-04T02:01:18.020613Z","shell.execute_reply":"2022-05-04T02:01:24.508593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"makedf\"></a>\n# <center>Read in images and create a dataframe of image paths and class labels</center>\n","metadata":{}},{"cell_type":"code","source":"sdir=r'../input/wonders-of-the-world-image-classification/Wonders of World/Wonders of World'\nht=0\nwt=0\nsamples=0\nsample_count=10\nfilepaths=[]\nlabels=[]\nclasslist=os.listdir(sdir)\nfor klass in classlist:\n    classpath=os.path.join(sdir, klass)\n    flist=os.listdir(classpath)    \n    for i, f in enumerate(flist):\n        fpath=os.path.join(classpath,f)\n        filepaths.append(fpath)\n        labels.append(klass)\n        #  get image shape to use for averaging of image shapes\n        if i < sample_count:\n            img=plt.imread(fpath)               \n            ht += img.shape[0]\n            wt += img.shape[1]\n            samples +=1        \nFseries=pd.Series(filepaths, name='filepaths')\nLseries=pd.Series(labels, name='labels')\ndf=pd.concat([Fseries, Lseries], axis=1)\nclass_count= len(df['labels'].unique())\nprint('The dataframe has ', class_count, ' classes')\n# split df into train_df, test_df and valid_df\ntrain_df, dummy_df=train_test_split(df, train_size=.8, shuffle=True, random_state=123, stratify=df['labels']) \nvalid_df, test_df = train_test_split(dummy_df, train_size=.5, shuffle=True, random_state=123, stratify=dummy_df['labels'])    \nprint('train_df lenght: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df))\nclasses=list(train_df['labels'].unique())\nclass_count = len(classes)\ngroups=train_df.groupby('labels')\nprint('{0:^30s} {1:^13s}'.format('CLASS', 'IMAGE COUNT'))\nfor label in train_df['labels'].unique():\n      group=groups.get_group(label)      \n      print('{0:^30s} {1:^13s}'.format(label, str(len(group))))\nwave=wt/samples\nhave=ht/samples\naspect_ratio= have/wave\nprint ('Average Image Height: ' ,have, '  Average Image Width: ', wave, '  Aspect ratio: ', aspect_ratio)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T02:01:24.512912Z","iopub.execute_input":"2022-05-04T02:01:24.513145Z","iopub.status.idle":"2022-05-04T02:01:26.367444Z","shell.execute_reply.started":"2022-05-04T02:01:24.513108Z","shell.execute_reply":"2022-05-04T02:01:26.366661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The dataset is not balanced and has more images per class than is necessary. Limit the number of images in each class\n### to 150 using the trim function defined below","metadata":{}},{"cell_type":"markdown","source":"<a id=\"trim\"></a>\n# <center>Trim the df dataframe to max and min samples</center>","metadata":{}},{"cell_type":"code","source":"def trim (df, max_size, min_size, column):\n    df=df.copy()\n    original_class_count= len(list(df[column].unique()))    \n    sample_list=[] \n    groups=df.groupby(column)\n    for label in df[column].unique():        \n        group=groups.get_group(label)\n        sample_count=len(group)         \n        if sample_count> max_size :\n            strat=group[column]\n            samples,_=train_test_split(group, train_size=max_size, shuffle=True, random_state=123, stratify=strat)            \n            sample_list.append(samples)\n        elif sample_count>= min_size:\n            sample_list.append(group)\n    df=pd.concat(sample_list, axis=0).reset_index(drop=True)\n    final_class_count= len(list(df[column].unique())) \n    if final_class_count != original_class_count:\n        print ('*** WARNING***  dataframe has a reduced number of classes from ', original_class_count,' to ', final_class_count )\n    groups=df.groupby('labels')\n    print('{0:^30s} {1:^13s}'.format('CLASS', 'IMAGE COUNT'))\n    for label in train_df['labels'].unique():\n          group=groups.get_group(label)      \n          print('{0:^30s} {1:^13s}'.format(label, str(len(group))))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-04T02:01:26.368674Z","iopub.execute_input":"2022-05-04T02:01:26.369041Z","iopub.status.idle":"2022-05-04T02:01:26.378699Z","shell.execute_reply.started":"2022-05-04T02:01:26.369003Z","shell.execute_reply":"2022-05-04T02:01:26.37773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_samples=150\nmin_samples=0\ncolumn='labels'\ntrain_df=trim(train_df, max_samples, min_samples, column)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T02:01:26.381456Z","iopub.execute_input":"2022-05-04T02:01:26.382187Z","iopub.status.idle":"2022-05-04T02:01:26.414611Z","shell.execute_reply.started":"2022-05-04T02:01:26.382147Z","shell.execute_reply":"2022-05-04T02:01:26.413922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"generators\"></a>\n# <center>Create the train_gen, test_gen and valid_gen</center>","metadata":{}},{"cell_type":"code","source":"working_dir=r'./'\nimg_size=(200,250)\nbatch_size=40 # We will use and EfficientetB3 model, with image size of (200, 250) this size should not cause resource error\ntrgen=ImageDataGenerator(horizontal_flip=True,rotation_range=20, width_shift_range=.2,\n                                  height_shift_range=.2, zoom_range=.2 )\nt_and_v_gen=ImageDataGenerator()\ntrain_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size,\n                                   class_mode='categorical', color_mode='rgb', shuffle=True, batch_size=batch_size)\nvalid_gen=t_and_v_gen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size,\n                                   class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=batch_size)\n# for the test_gen we want to calculate the batch size and test steps such that batch_size X test_steps= number of samples in test set\n# this insures that we go through all the sample in the test set exactly once.\nlength=len(test_df)\ntest_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]  \ntest_steps=int(length/test_batch_size)\ntest_gen=t_and_v_gen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size,\n                                   class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n# from the generator we can get information we will need later\nclasses=list(train_gen.class_indices.keys())\nclass_indices=list(train_gen.class_indices.values())\nclass_count=len(classes)\nlabels=test_gen.labels\nprint ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps, ' number of classes : ', class_count)\nprint ('{0:^25s}{1:^12s}'.format('class name', 'class index'))\nfor klass, index in zip(classes, class_indices):\n    print(f'{klass:^25s}{str(index):^12s}')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T02:01:26.415943Z","iopub.execute_input":"2022-05-04T02:01:26.416349Z","iopub.status.idle":"2022-05-04T02:01:26.47001Z","shell.execute_reply.started":"2022-05-04T02:01:26.416311Z","shell.execute_reply":"2022-05-04T02:01:26.469319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"show\"></a>\n# <center>Create a function to show example training images</center>","metadata":{}},{"cell_type":"code","source":"def show_image_samples(gen ):\n    t_dict=gen.class_indices\n    classes=list(t_dict.keys())    \n    images,labels=next(gen) # get a sample batch from the generator \n    plt.figure(figsize=(20, 20))\n    length=len(labels)\n    if length<25:   #show maximum of 25 images\n        r=length\n    else:\n        r=25\n    for i in range(r):        \n        plt.subplot(5, 5, i + 1)\n        image=images[i] /255       \n        plt.imshow(image)\n        index=np.argmax(labels[i])\n        class_name=classes[index]\n        plt.title(class_name, color='blue', fontsize=12)\n        plt.axis('off')\n    plt.show()\n    \nshow_image_samples(train_gen )","metadata":{"execution":{"iopub.status.busy":"2022-05-04T02:01:26.472628Z","iopub.execute_input":"2022-05-04T02:01:26.47282Z","iopub.status.idle":"2022-05-04T02:01:29.893518Z","shell.execute_reply.started":"2022-05-04T02:01:26.472797Z","shell.execute_reply":"2022-05-04T02:01:29.892711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"model\"></a>\n# <center>Create a model using transfer learning with EfficientNetB3</center>\n### NOTE experts advise you make the base model initially not trainable. Then train for some number of epochs\n### then fine tune model by making base model trainable and run more epochs\n### I have found this to be WRONG!!!!\n### Making the base model trainable from the outset leads to faster convegence and a lower validation loss\n### for the same number of total epochs!","metadata":{}},{"cell_type":"code","source":"img_shape=(img_size[0], img_size[1], 3)\nmodel_name='EfficientNetB3'\nbase_model=tf.keras.applications.efficientnet.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \n# Note you are always told NOT to make the base model trainable initially- that is WRONG you get better results leaving it trainable\nbase_model.trainable=True\nx=base_model.output\nx=BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\nx = Dense(1024, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\nx=Dropout(rate=.3, seed=123)(x)\nx = Dense(128, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\nx=Dropout(rate=.45, seed=123)(x)        \noutput=Dense(class_count, activation='softmax')(x)\nmodel=Model(inputs=base_model.input, outputs=output)\nlr=.001 # start with this learning rate\nmodel.compile(Adamax(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy']) ","metadata":{"execution":{"iopub.status.busy":"2022-05-04T02:01:29.894647Z","iopub.execute_input":"2022-05-04T02:01:29.894917Z","iopub.status.idle":"2022-05-04T02:01:35.326784Z","shell.execute_reply.started":"2022-05-04T02:01:29.894876Z","shell.execute_reply":"2022-05-04T02:01:35.326086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"callback\"></a>\n# <center>Create a custom Keras callback to continue or halt training</center>","metadata":{}},{"cell_type":"code","source":"class ASK(keras.callbacks.Callback):\n    def __init__ (self, model, epochs,  ask_epoch): # initialization of the callback\n        super(ASK, self).__init__()\n        self.model=model               \n        self.ask_epoch=ask_epoch\n        self.epochs=epochs\n        self.ask=True # if True query the user on a specified epoch\n        \n    def on_train_begin(self, logs=None): # this runs on the beginning of training\n        if self.ask_epoch == 0: \n            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n            self.ask_epoch=1\n        if self.ask_epoch >= self.epochs: # you are running for epochs but ask_epoch>epochs\n            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n            self.ask=False # do not query the user\n        if self.epochs == 1:\n            self.ask=False # running only for 1 epoch so do not query user\n        else:\n            print('Training will proceed until epoch', ask_epoch,' then you will be asked to') \n            print(' enter H to halt training or enter an integer for how many more epochs to run then be asked again')  \n        self.start_time= time.time() # set the time at which training started\n        \n    def on_train_end(self, logs=None):   # runs at the end of training     \n        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n        hours = tr_duration // 3600\n        minutes = (tr_duration - (hours * 3600)) // 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print (msg, flush=True) # print out training duration time\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        if self.ask: # are the conditions right to query the user?\n            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n                print('\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again')\n                ans=input()\n                \n                if ans == 'H' or ans =='h' or ans == '0': # quit training for these conditions\n                    print ('you entered ', ans, ' Training halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n                    self.model.stop_training = True # halt training\n                else: # user wants to continue training\n                    self.ask_epoch += int(ans)\n                    if self.ask_epoch > self.epochs:\n                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush =True)\n                    else:\n                        print ('you entered ', ans, ' Training will continue to epoch ', self.ask_epoch, flush=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T02:01:35.328011Z","iopub.execute_input":"2022-05-04T02:01:35.328275Z","iopub.status.idle":"2022-05-04T02:01:35.341847Z","shell.execute_reply.started":"2022-05-04T02:01:35.328242Z","shell.execute_reply":"2022-05-04T02:01:35.340879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"callbacks\"></a>\n# <center>Instantiate custom callback and create 2 callbacks to control learning rate and early stop","metadata":{}},{"cell_type":"code","source":"epochs=40\nask_epoch=10\nask=ASK(model, epochs,  ask_epoch)\nrlronp=tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2,verbose=1)\nestop=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, verbose=1,restore_best_weights=True)\ncallbacks=[rlronp, estop, ask]","metadata":{"execution":{"iopub.status.busy":"2022-05-04T02:01:35.343184Z","iopub.execute_input":"2022-05-04T02:01:35.34365Z","iopub.status.idle":"2022-05-04T02:01:35.35406Z","shell.execute_reply.started":"2022-05-04T02:01:35.343613Z","shell.execute_reply":"2022-05-04T02:01:35.353411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"train\"></a>\n# <center>Train the model\n### Note unlike how you are told it is BETTER to make the base model trainable from the outset\n### It will converge faster and have a lower validation losss","metadata":{}},{"cell_type":"code","source":"history=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T02:01:35.35686Z","iopub.execute_input":"2022-05-04T02:01:35.357115Z","iopub.status.idle":"2022-05-04T02:15:08.636763Z","shell.execute_reply.started":"2022-05-04T02:01:35.357083Z","shell.execute_reply":"2022-05-04T02:15:08.636055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"plot\"></a>\n# <center>Define a function to plot the training data","metadata":{}},{"cell_type":"code","source":"def tr_plot(tr_data, start_epoch):\n    #Plot the training and validation data\n    tacc=tr_data.history['accuracy']\n    tloss=tr_data.history['loss']\n    vacc=tr_data.history['val_accuracy']\n    vloss=tr_data.history['val_loss']\n    Epoch_count=len(tacc)+ start_epoch\n    Epochs=[]\n    for i in range (start_epoch ,Epoch_count):\n        Epochs.append(i+1)   \n    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n    val_lowest=vloss[index_loss]\n    index_acc=np.argmax(vacc)\n    acc_highest=vacc[index_acc]\n    plt.style.use('fivethirtyeight')\n    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].set_xlabel('Epochs')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    plt.tight_layout    \n    plt.show()\n    \ntr_plot(history,0)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T02:15:08.63811Z","iopub.execute_input":"2022-05-04T02:15:08.638363Z","iopub.status.idle":"2022-05-04T02:15:16.979212Z","shell.execute_reply.started":"2022-05-04T02:15:08.638329Z","shell.execute_reply":"2022-05-04T02:15:16.978249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"result\"></a>\n# <center>Make predictions on test set, create Confusion Matrix and Classification Results","metadata":{}},{"cell_type":"code","source":"y_pred= []\ny_true=test_gen.labels\nerrors=0\npreds=model.predict(test_gen, steps=test_steps, verbose=1) # predict on the test set\ntests=len(preds)\nfor i, p in enumerate(preds):\n        pred_index=np.argmax(p)         \n        true_index=test_gen.labels[i]  # labels are integer values\n        if pred_index != true_index: # a misclassification has occurred                                           \n            errors=errors + 1\n        y_pred.append(pred_index)\nacc=( 1-errors/tests) * 100\nprint(f'there were {errors} in {tests} tests for an accuracy of {acc:6.2f}')\nypred=np.array(y_pred)\nytrue=np.array(y_true)\ncm = confusion_matrix(ytrue, ypred )\n# plot the confusion matrix\nplt.figure(figsize=(16, 10))\nsns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \nplt.xticks(np.arange(class_count)+.5, classes, rotation=90)\nplt.yticks(np.arange(class_count)+.5, classes, rotation=0)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\nclr = classification_report(y_true, y_pred, target_names=classes, digits= 4) # create classification report\nprint(\"Classification Report:\\n----------------------\\n\", clr)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T02:15:16.980564Z","iopub.execute_input":"2022-05-04T02:15:16.98095Z","iopub.status.idle":"2022-05-04T02:15:25.4272Z","shell.execute_reply.started":"2022-05-04T02:15:16.980911Z","shell.execute_reply":"2022-05-04T02:15:25.426306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"save\"></a>\n# <center>Save the model","metadata":{}},{"cell_type":"code","source":"subject='world wonders' \nacc=str(( 1-errors/tests) * 100)\nindex=acc.rfind('.')\nacc=acc[:index + 3]\nsave_id= subject + '_' + str(acc) + '.h5' \nmodel_save_loc=os.path.join(working_dir, save_id)\nmodel.save(model_save_loc)\nprint ('model was saved as ' , model_save_loc ) \n   ","metadata":{"execution":{"iopub.status.busy":"2022-05-04T02:15:25.428659Z","iopub.execute_input":"2022-05-04T02:15:25.42898Z","iopub.status.idle":"2022-05-04T02:15:26.606133Z","shell.execute_reply.started":"2022-05-04T02:15:25.428943Z","shell.execute_reply":"2022-05-04T02:15:26.605364Z"},"trusted":true},"execution_count":null,"outputs":[]}]}