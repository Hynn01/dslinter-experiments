{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Quick and dirty EDA\n==\n\nI'm going to do a fairly minimal EDA before I do any modelling. First, let's read in the data and verify that pandas infers the data types for the train and test sets to be the same:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\n\nsns.set(\n    style='darkgrid', context='notebook', rc={'figure.figsize': (12, 8), 'figure.frameon': False, 'legend.frameon': False}\n)\ndef compress_mem(df):\n    floats = df.columns[df.dtypes == np.float64]\n    ints = df.columns[df.dtypes == np.int64]\n    return df.astype(\n        {col: np.float32 for col in floats}\n    ).astype({col: np.int32 for col in ints})\n\ntrain = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv').pipe(compress_mem)\ntest = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv').pipe(compress_mem)\n\n(train.dtypes.drop('target') == test.dtypes).all()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Are the train test and test set similar?\n==\n\nOkay, with that out of the way, I want to start by finding out whether the train and test sets have big differences. I'll take a look at everything that's non-float first, and check whether we have some feature values that are present only in test, or vice-versa. Let's start by checking the number of possible values for the non-numerical columns:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"df = pd.concat([train.drop(columns=['target']).assign(ds='train'), test.assign(ds='test')]).set_index('id')\n\nsummary = df.select_dtypes(exclude=np.float32).groupby('ds').nunique().reset_index().melt(id_vars=['ds'])\n\nsns.catplot(\n    data=summary, y='variable', x='value', col='ds', kind='bar', orient='horizontal'\n);\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`f_27` needs special treatment\n--\n\nOkay, we'll need to set aside some time for `f_27` later, that's clearly ready to be used as a feature yet. With this many possible values, it's highly likely that some values only occur in test, but maybe we can interpret the data to make it more valuable. Let's check out the rest:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"sns.catplot(\n    data=summary.loc[summary.variable != 'f_27'], y='variable', x='value', hue='ds', kind='bar', orient='horizontal'\n);","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some feature values occur only in test\n--\n\nThere are columns where test has more values than train, but it's not the common case. Let's try to check which features that have values that only occur in the test set or only in the train set.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"possible = df.select_dtypes(exclude=np.float32).drop(columns=['f_27']).melt(\n    id_vars=['ds']\n).drop_duplicates()\n\nin_train = possible.loc[possible.ds == 'train'].drop(columns=['ds'])\nin_test = possible.loc[possible.ds == 'test'].drop(columns=['ds'])\njoined = in_train.merge(in_test, how='outer', indicator=True)\njoined = joined.loc[joined._merge != 'both']\njoined = joined.replace({'left_only': 'train', 'right_only': 'test'}).rename(columns={'_merge': 'unique_in'})\njoined\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Maybe just discrete numbers, not categoricals?\n--\n\nOkay, there are actually a few cases here. Let's try to find out whether we can treat any of these as numerical columns.\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"sns.catplot(\n    data=df.loc[:, joined.variable.to_list() + ['ds']].melt(id_vars=['ds']),\n    col='variable', x='value', hue='ds', kind='count', col_wrap=3\n);","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks plausible that these are just discrete numerical data, and we might get away with just scaling them, instead of treating them as categoricals. This data set is supposed look like a manufacturing problem of some sort, so it might be that some sensors output discrete measurements or settings of some type. At a glance, there are no huge differences between train and test for these columns.\n\nLooking into `f_27`\n==\n\nThis column is very different from anything we've seen so far. Let's check same possible values:\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"df.f_27.sample(n=10)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, my first thought is that this is \"more than 1\" variable. My second thought is that maybe this is a single variable because the order matters, e.g. this is some sort of instruction sequence. Let's break this column apart to start with:\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"f_27 = df.f_27.str.split('', expand=True).drop(columns=[0, 11]) # Empty string in first and last column\nf_27.nunique().plot.bar(title='Unique values by string position').set_xlabel('position');","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This looks like it could plausibly be used as categorical variables, one way or the other. We should probably also check if some of these occur only in the train or test set.\n\nReally hard to say anything meaningful about what this represents. We'll need to check the distributions of the various positions, these could easily represent numbers from some sensor or equipment. We're on a mission to find out if train and test have glaring differences, so let's check if we're in trouble here:\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"f_27 = f_27.join(df.ds).melt(id_vars=['ds'], var_name=['position'])\nsns.catplot(\n    data=f_27,\n    col='position', x='value', kind='count', hue='ds', col_wrap=3\n);","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seemingly no big differences between train and test here. It's probably still a good idea to check if any of these value/position pairs occur only in one of the data sets, so let's work on that:\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"unique = f_27.drop_duplicates()\nunique_train = unique.loc[unique.ds == 'train'].drop(columns='ds')\nunique_test = unique.loc[unique.ds == 'test'].drop(columns='ds')\njoined = unique_train.merge(unique_test, how='outer', indicator=True)\njoined = joined.loc[joined._merge != 'both'].replace(\n    {'left_only': 'train_only', 'right_only': 'test_only'}\n).rename(columns={'_merge': 'in'})\njoined\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, so a possible pitfall if we were to try using these as categoricals is that not all position/value combinations occur in both data sets. From the distributions we looked at above, it seems likely that position 2, 5 and 9 might be numerical values anyway.\n\nLet's quickly check if it is also the case that the floating point values follow roughly the same distributions between train/test, then we'll stop looking at the test set and start looking at the target instead.\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"sns.displot(\n    data=df.select_dtypes(np.float32).join(df.ds).melt(id_vars=['ds']),\n    col='variable', hue='ds', x='value', kind='hist', col_wrap=3,\n    facet_kws={'sharex': False}, bins=50, common_bins=False,\n);\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Concluding the differences between train and test\n==\n\nAll of these look very similar. To sum up:\n\n- `f_27` needs to be handled separately, either by splitting it into constituent parts, or possibly by treating it as a sequence (RNN?)\n- When split, `f_27` has 3 positions where not all values occur in both train and test. We can probably treat positions 2, 5 and 9 as numbers, though.\n- The discrete variables we've got have values that occur in only train, or only test, which could create problems if we introduce categorical encoding for them.\n- The distributions we've looked at appear to be very similar between train and test. We've no reason (this time...) to expect local CV and LB score to be wildly different.\n\nInvestigating how features relate to target\n==\n\nNow, let's stop snooping in the test set and start looking only at the train data. We already know that we'll need to split apart the `f_27` feature, so we'll just get that out of the way right away, then translate the character values in those values to numbers.\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"del df\ndel f_27\ndf = train.set_index('id').drop(columns=['f_27'])\nf_27 = train.set_index('id').f_27.str.split('', expand=True).drop(columns=[0, 11]).rename(columns='f_27_{}'.format)\nf_27 = f_27.applymap(ord) - ord('A')\ndf = df.join(f_27)\ntest = test.set_index('id')\ntest = test.drop(columns=['f_27']).join(\n    test.f_27.str.split('', expand=True).drop(columns=[0, 11]).rename(columns='f_27_{}'.format).pipe(\n        lambda df: df.applymap(ord) - ord('A')\n    )\n)\ndf.head().T\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's first check the size of train vs test, and the target balance:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"print('Train samples:', len(train), 'test samples:', len(test), 'mean(target) =', train.target.mean())","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, train is about the same size as test, and the training data is nearly balanced.\n\nLet's check if a correlation map suggests anything at all:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"sns.heatmap(df.select_dtypes(np.float32).join(df.target).corr(), cmap='coolwarm', annot=True, fmt='.2f');","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are some correlated features, but the correlations aren't very strong. There are mostly weak correlations with the target:\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"df.corr()['target'].drop('target').plot.bar(title='Correlation with target');","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Candidates for categorical interpretation\n--\n\nCan any of our low-ordinal columns predict the target well?","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"cats = df.columns[df.nunique() <= 30]\nsns.catplot(\n    data=df[cats].melt(id_vars=['target']),\n    col='variable', x='value', kind='count', hue='target',\n    col_wrap=3, sharex=False, sharey=False\n);\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nothing that is super-strong, but the binary features definitely seem useful, as do several of the `f_27` string positions. Looking at it right now, I think we're probably OK with encoding many of the string positions simply as numbers.\n\nDistribution of numerical features\n--\n\nLet's check if there are any distribution differences for the float features that seem promising:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"sns.displot(\n    data=df.select_dtypes(np.float32).join(df.target).melt(id_vars='target'),\n    x='value', col='variable', hue='target', col_wrap=3, kind='ecdf', facet_kws={'sharex': False}\n);\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is easy to see that a model should be able to exploit some of these. They're all fairly normally distributed and should work fine with any kind of scaler with no big adjustment needed. Because we know that some integer features have values that occur only in test, I am tempted to just send them through a scaler as well. I'm thinking we'll focus on tree models and NN here, and we can start playing with trees without doing any transformations at all.\n\nLet's keep the bools, turn everything else into floats and get the ball rolling, that way we won't have to deal with values that don't exist in train:\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"train = df.astype({col: np.float32 for col in df.columns[(df.dtypes == np.int64) | (df.dtypes == np.int32)]})\ntrain = train.astype({col: np.bool_ for col in df.columns[df.nunique() == 2]})\ntest = test.astype(train.dtypes.drop('target'))\ntrain.to_parquet('train.pq')\ntest.to_parquet('test.pq')\n\ntrain.info()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check sklearn models\n==\n\nAt this point I feel ready to do a simple grid search to check if any particular kind of model seems more promising than others. Let's do some imports and set up seeds, CV splits.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"import os\nimport random\nfrom sklearn import metrics, model_selection, ensemble, pipeline, linear_model, neural_network, preprocessing\n\nrandom.seed(42)\nnp.random.seed(42)\ncv = model_selection.KFold(shuffle=True, random_state=42)\n\nn_jobs = os.cpu_count()\nif n_jobs > 4:\n    n_jobs = n_jobs // 2 # hyper-threading\n\npipe = pipeline.Pipeline([\n    ('scaler', preprocessing.StandardScaler()),\n    ('model', linear_model.LogisticRegression())\n])\nscalers = [[preprocessing.StandardScaler()], [preprocessing.RobustScaler()]]\n\ngrid = model_selection.ParameterGrid([\n    {'scaler': scalers, 'model': [[linear_model.LogisticRegression()], [linear_model.SGDClassifier()]]},\n    {'scaler': scalers, 'model': [[neural_network.MLPClassifier(hidden_layer_sizes=(64, 64), early_stopping=True)]]},\n    {'scaler': [[None]], 'model': [\n        [ensemble.RandomForestClassifier()],\n        [ensemble.HistGradientBoostingClassifier()],\n    ]}\n])\nsearch = model_selection.GridSearchCV(pipe, grid, scoring='roc_auc', n_jobs=n_jobs)\nsearch.fit(train.drop(columns=['target']), train.target)\npd.DataFrame(search.cv_results_)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NN looks promising\n==\n\nAt first glance, it looks like we should be focusing on NN, and linear models are possibly a waste of time. It's probably worth looking at gradient boosters + additional feature engineering on `f_27` too.\n\nThere's very little spread in score across the CVs, for hyper-parameter tuning we could probably get away with using less than the whole data set or simply not using all the splits.\n\nLet's invest a little bit of time training an MLP, and submit before we round off. In another notebook, we're going to tune an NN with pytorch and optuna instead of guesstimating parameters.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"pipe.set_params(model=neural_network.MLPClassifier(\n    hidden_layer_sizes=(64, 64, 32), batch_size=4096, alpha=1e-3, max_iter=50 * 200, # roughly 50 epochs\n    early_stopping=True, learning_rate_init=5e-3, n_iter_no_change=25\n))\npipe.fit(train.drop(columns=['target']), train.target)\nproba = pipe.predict_proba(test)[:, 1]\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.assign(target=proba)[['target']].to_csv('submission.csv')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]}]}