{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background:#2b6684   ;font-family:'Times';font-size:35px;color:  #F0CB8E\" >&ensp;TPS - MAY2022</div>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-06T14:39:13.893624Z","iopub.execute_input":"2022-05-06T14:39:13.89467Z","iopub.status.idle":"2022-05-06T14:39:13.912206Z","shell.execute_reply.started":"2022-05-06T14:39:13.894537Z","shell.execute_reply":"2022-05-06T14:39:13.911102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport heapq\nimport scipy\nfrom imblearn.under_sampling import (\n            RandomUnderSampler,\n            OneSidedSelection,\n            InstanceHardnessThreshold,\n        )\nimport lightgbm as lgbm\nfrom sklearn.metrics import classification_report\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif,f_classif\nstyle.use('fivethirtyeight')","metadata":{"execution":{"iopub.status.busy":"2022-05-06T14:39:14.050268Z","iopub.execute_input":"2022-05-06T14:39:14.05086Z","iopub.status.idle":"2022-05-06T14:39:15.90615Z","shell.execute_reply.started":"2022-05-06T14:39:14.050821Z","shell.execute_reply":"2022-05-06T14:39:15.905236Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-06T14:39:15.908166Z","iopub.execute_input":"2022-05-06T14:39:15.908601Z","iopub.status.idle":"2022-05-06T14:39:25.327022Z","shell.execute_reply.started":"2022-05-06T14:39:15.908553Z","shell.execute_reply":"2022-05-06T14:39:25.326216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['f_27']","metadata":{"execution":{"iopub.status.busy":"2022-05-06T14:39:25.328154Z","iopub.execute_input":"2022-05-06T14:39:25.328647Z","iopub.status.idle":"2022-05-06T14:39:25.33774Z","shell.execute_reply.started":"2022-05-06T14:39:25.328611Z","shell.execute_reply":"2022-05-06T14:39:25.336791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nmapper = dict(zip(string.ascii_uppercase,np.arange(0,len(string.ascii_lowercase))))\nfor i in range(10):\n    train[str(i) +'_27']=train.f_27.str[i]\n    train[str(i) +'_27_num']=train[str(i) +'_27'].map(mapper)\n    test[str(i) +'_27']=test.f_27.str[i]\n    test[str(i) +'_27_num']=test[str(i) +'_27'].map(mapper)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T14:39:25.339838Z","iopub.execute_input":"2022-05-06T14:39:25.340106Z","iopub.status.idle":"2022-05-06T14:39:37.769593Z","shell.execute_reply.started":"2022-05-06T14:39:25.340076Z","shell.execute_reply":"2022-05-06T14:39:37.768647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\" role=\"alert\">\n<ul style=\"font-family:cursive;font-size:18px; color:#A20404\">Observations: \n<li>We observe that not just the combination but the permutations can perform differently as well ! .</li>\n<li>8_27_num+_7_27_num performs better than 7_27_num+_8_27_num.</li>\n</ul>\n</div>","metadata":{}},{"cell_type":"code","source":"train['7_27_num+_8_27_num']=(train['7_27_num'].astype(str) + train['8_27_num'].astype(str)).astype(int)\ntest['7_27_num+_8_27_num'] = (test['7_27_num'].astype(str) + test['8_27_num'].astype(str)).astype(int)\n\ntrain['8_27_num+_7_27_num']=(train['8_27_num'].astype(str) + train['7_27_num'].astype(str)).astype(int)\ntest['8_27_num+_7_27_num'] = (test['8_27_num'].astype(str) + test['7_27_num'].astype(str)).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T14:39:37.770808Z","iopub.execute_input":"2022-05-06T14:39:37.771129Z","iopub.status.idle":"2022-05-06T14:39:45.571074Z","shell.execute_reply.started":"2022-05-06T14:39:37.771098Z","shell.execute_reply":"2022-05-06T14:39:45.570374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_columns = [str(x)+'_27_num' for x in range(0,10)] +['7_27_num+_8_27_num','8_27_num+_7_27_num']\nfs = SelectKBest(score_func=chi2, k='all')\nfs.fit(train[categorical_columns], train['target'])\nplt.figure(figsize=(10,7))\nsns.barplot(x='feat',y='imp',data=pd.DataFrame({'feat':categorical_columns,'imp':fs.scores_}).sort_values(['imp'],ascending=False))\nplt.xticks(rotation=70)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T14:39:45.572153Z","iopub.execute_input":"2022-05-06T14:39:45.572557Z","iopub.status.idle":"2022-05-06T14:39:46.728191Z","shell.execute_reply.started":"2022-05-06T14:39:45.572511Z","shell.execute_reply":"2022-05-06T14:39:46.727327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfeature_list = [f for f in train.columns if train[f].dtype in ['float64','int64'] and f not in ['target','id','7_27_num+_8_27_num','8_27_num+_7_27_num']]\nX_train, X_test, y_train, y_test = train_test_split( train[feature_list], train['target'], test_size=0.2, random_state=42, stratify=train['target'])","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:14:44.495431Z","iopub.execute_input":"2022-05-06T19:14:44.496601Z","iopub.status.idle":"2022-05-06T19:14:45.990899Z","shell.execute_reply.started":"2022-05-06T19:14:44.496548Z","shell.execute_reply":"2022-05-06T19:14:45.989948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background:#2b6684   ;font-family:'Times';font-size:35px;color:  #F0CB8E\" >&ensp;f_27 Feature combination search using Genetic algo</div>\n<div class=\"alert alert-warning\" role=\"alert\">\n<p style=\"font-family:cursive;font-size:20px;color:  #A20404\"> We can use two modes of fitness calculation</p>\n\n<li style=\"font-family:cursive;font-size:18px; color:#A20404\">Chi2 value as fitness.</li>\n<li style=\"font-family:cursive;font-size:18px; color:#A20404\">Feature importance using a model fit on Undersampled Data</li>","metadata":{"execution":{"iopub.status.busy":"2022-05-06T12:12:32.399Z","iopub.execute_input":"2022-05-06T12:12:32.399772Z","iopub.status.idle":"2022-05-06T12:12:32.407319Z","shell.execute_reply.started":"2022-05-06T12:12:32.3997Z","shell.execute_reply":"2022-05-06T12:12:32.405471Z"}}},{"cell_type":"code","source":"from typing import Union,TypeVar\n\nclass GeneticAlgo:\n    def __init__(\n        self,\n        populationsize:int,\n        genelength:int,\n        elitism:int,\n        X_train:pd.DataFrame,\n        y_train:Union[pd.DataFrame,pd.Series],\n        data:pd.DataFrame,\n        mutation_rate:float,\n        n_generations:int,\n        non_num_columns:list,\n    ):\n        \"\"\"__init__ \n\n        Parameters\n        ----------\n        populationsize : int\n            population size \n        genelength : int\n            number of combinations you want to work with \n        elitism : int\n            number of fittest individuals to consider\n        X_train : pd.DataFrame\n            training data , required for feature importance method\n        y_train : Union[pd.DataFrame,pd.Series]\n            data , required for feature importance method\n        data : pd.DataFrame\n            data , required for chi2 method\n        mutation_rate : float\n            rate of mutation required for genetic algo\n        n_generations : int\n            number of iterations\n        non_num_columns : list\n            columns excluding f_27\n        \"\"\"\n        self.populationsize = populationsize\n        self.population = []\n        self.genelength = genelength # if i want feature combinations up to 3 eg.[4,5,6],then genelength =3\n        self.elitism = elitism\n        self.data = data\n        self.mutation_rate = mutation_rate\n        self.n_generations = n_generations\n        self.non_num_columns = non_num_columns\n        \n        # heap will get us best k features\n        self.li = [(-np.inf, [0])] * 10\n        heapq.heapify(self.li)\n\n        # Undersampling for feature importance method\n        from imblearn.under_sampling import RandomUnderSampler\n        nm = RandomUnderSampler(sampling_strategy={0: 100000, 1: 100000})\n        self.X_res, self.y_res = nm.fit_resample(X_train, y_train)\n\n    def _check_individuals(self):\n        \"\"\" -1 represent empty choice.\n            This functions checks and replaces all individuals which are completely empty \"\"\"\n        all_empty_individuals = np.where(self.population.sum(axis=1) == -self.genelength)[0]\n        for inx in all_empty_individuals:\n            pop = np.random.choice(\n                np.arange(0, 10),\n                size=np.random.choice(np.arange(1, self.genelength + 1)),\n                replace=False,\n            )\n            pop = np.concatenate([pop, np.array([-1] * (self.genelength - len(pop)))])\n            self.population[inx] = pop\n\n    def initialize_population(self):\n        \"\"\" initialize the first population\"\"\"\n        for _ in range(self.populationsize):\n            # we choose without replacement to avoid repetition of same feature\n            pop = np.random.choice(\n                np.arange(0, 10, dtype=np.int64),\n                size=np.random.choice(np.arange(1, self.genelength + 1, dtype=np.int64)),\n                replace=False,\n            )\n            pop = np.concatenate([pop, np.array([-1] * (self.genelength - len(pop)))])\n            self.population.append(pop)\n        self.population = np.array(self.population)\n\n    def fitness_via_model_split(self):\n        \"\"\" Calculate fitness value via feature importance \"\"\"\n        self._all = pd.Series(dtype=\"float64\")\n        for individual in self.population:\n            self._all = pd.concat(\n                [\n                    self._all,\n                    self.X_res[[str(int(x)) + \"_27_num\" for x in individual if x != -1]]\n                    .astype(str)\n                    .sum(axis=1)\n                    .astype(int)\n                    .rename( '+'.join([str(int(x))+'_27_num' for x in individual if x!=-1])),\n                ],\n                axis=1,\n            )\n\n        self._all = self._all.drop(columns=[0])\n        all_tomap = list(self._all.columns)\n\n        self._all = self._all.loc[:,~self._all.columns.duplicated()]\n        self._all = pd.concat([self._all, self.X_res[self.non_num_columns]], axis=1)\n\n        params = {\n            \"n_estimators\": 1000,\n            \"reg_lambda\": 0.0015,\n            \"learning_rate\": 0.09,\n            \"max_depth\": 11,\n            \"min_child_weight\": 135,\n        }\n        model = lgbm.LGBMClassifier(**params)\n        model.fit(self._all, self.y_res)\n        #dd=classification_report(self.y_res, model.predict(self._all),output_dict=True)\n        \n        # iteration to only include fitness of our generated features\n        rank_variable = 1/(1 + len(model.feature_importances_) - scipy.stats.rankdata(model.feature_importances_))\n        _dict = dict(zip(self._all.columns, rank_variable ) )\n        self.population_fitness = [ _dict[key] for key in all_tomap ]\n\n        for individual, _fit in zip(self.population, self.population_fitness):\n            if list(individual) in [_iter[1] for _iter in self.li]:\n                ix  = [_iter[1] for _iter in self.li].index(list(individual))\n                if _fit>self.li[ix][0]:\n                    self.li[ix]=(_fit,list(individual))\n                    heapq.heapify(self.li)\n            else:   \n                # push new individual to heap\n                heapq.heappushpop(self.li, (_fit, list(individual)))\n\n        self.ranks = scipy.stats.rankdata(self.population_fitness, method=\"average\")\n        self.fitness_ranks = 2 * self.ranks\n\n        if np.max(self.population_fitness) > self.best_fitess:\n            self.best_fitess = np.max(self.population_fitness)\n            self.best_individual = self.population[np.argmax(self.population_fitness)]\n\n    def fitness(self):\n        self._all = pd.Series(dtype=\"float64\")\n        for individual in self.population:\n            self._all = pd.concat(\n                [\n                    self._all,\n                    self.data[[str(int(x)) + \"_27_num\" for x in individual if x != -1]]\n                    .astype(str)\n                    .sum(axis=1)\n                    .astype(int),\n                ],\n                axis=1,\n            )\n\n        self._all.columns = np.arange(0, self._all.shape[1])\n        self._all = self._all.drop(columns=[0])\n        fs = SelectKBest(score_func=chi2, k=\"all\")\n        fs.fit(self._all, self.data[\"target\"])\n        self.population_fitness = fs.scores_\n\n        for individual, _fit in zip(self.population, self.population_fitness):\n            if (_fit, list(individual)) not in self.li:\n                heapq.heappushpop(self.li, (_fit, list(individual)))\n\n        self.ranks = scipy.stats.rankdata(self.population_fitness, method=\"average\")\n        self.fitness_ranks = 2 * self.ranks\n\n        if np.max(self.population_fitness) > self.best_fitess:\n            self.best_fitess = np.max(fs.scores_)\n            self.best_individual = self.population[np.argmax(fs.scores_)]\n\n    def _select_individuals(self):\n        # self.fitness()\n        self.fitness_via_model_split()\n\n        sorted_individuals_fitness = sorted(\n            zip(self.population, self.fitness_ranks), key=lambda x: x[1], reverse=True\n        )\n        elite_individuals = np.array(\n            [individual for individual, fitness in sorted_individuals_fitness[: self.elitism]]\n        )\n        non_elite_individuals = np.array(\n            [individual[0] for individual in sorted_individuals_fitness[self.elitism :]]\n        )\n\n        non_elite_individuals_fitness = [\n            individual[1] for individual in sorted_individuals_fitness[self.elitism :]\n        ]\n        selection_probability = non_elite_individuals_fitness / np.sum(\n            non_elite_individuals_fitness\n        )\n\n        selected_indices = np.random.choice(\n            range(len(non_elite_individuals)), self.populationsize // 2, p=selection_probability\n        )\n        selected_individuals = non_elite_individuals[selected_indices, :]\n        self.fit_individuals = np.vstack((elite_individuals, selected_individuals))\n\n    def _mutate(self, array):\n        mutated_array = np.copy(array)\n        for idx, gene in enumerate(array):\n            if np.random.random() < self.mutation_rate:\n                array[idx] = np.random.choice(np.arange(-1, 10))\n\n        return mutated_array\n\n    def fix_repeatition(self, ind):\n        s = set()\n        ind_copy = ind\n        for i, ix in enumerate(ind):\n            if ix in s:\n                ind_copy[i] = -1\n            s.add(ix)\n        return ind_copy\n\n    def _produce_next_generation(self):\n        new_population = np.empty(shape=(self.populationsize, self.genelength), dtype=np.float64)\n\n        for i in range(0, self.populationsize, 2):\n            parents = self.fit_individuals[\n                np.random.choice(self.fit_individuals.shape[0], 2, replace=False), :\n            ]\n\n            crossover_index = np.random.randint(0, len(self.population[0]))\n            new_population[i] = np.hstack(\n                (parents[0][:crossover_index], parents[1][crossover_index:])\n            )\n\n            new_population[i + 1] = np.hstack(\n                (parents[1][:crossover_index], parents[0][crossover_index:])\n            )\n\n            new_population[i] = self.fix_repeatition(self._mutate(new_population[i]))\n            new_population[i + 1] = self.fix_repeatition(self._mutate(new_population[i + 1]))\n        self.population = new_population\n\n    def fit(self):\n        self.initialize_population()\n\n        self.best_fitess = -np.inf\n        self.best_individual = [-1] * self.genelength\n        for i in range(self.n_generations):\n            self._check_individuals()\n\n            self._select_individuals()\n\n            self._produce_next_generation()\n\n            print(\n                \"Iteration-->\",\n                i,\n                \" Best feature-->\",\n                self.best_individual,\n                \" Best fitness-->\",\n                self.best_fitess,\n            )\n\ncolz = ['f_00', 'f_01', 'f_02', 'f_03', 'f_04', 'f_05', 'f_06', 'f_07',\n       'f_08', 'f_09', 'f_10', 'f_11', 'f_12', 'f_13', 'f_14', 'f_15', 'f_16',\n       'f_17', 'f_18', 'f_19', 'f_20', 'f_21', 'f_22', 'f_23', 'f_24', 'f_25',\n       'f_26', 'f_28', 'f_29', 'f_30']\n\nobj = GeneticAlgo(\n    populationsize=50,\n    genelength=3,\n    elitism=2,\n    X_train=X_train,\n    y_train=y_train,\n    data=train,\n    mutation_rate=0.1,\n    n_generations=10,\n    non_num_columns=colz,\n)\nobj.fit()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:14:47.85979Z","iopub.execute_input":"2022-05-06T19:14:47.860115Z","iopub.status.idle":"2022-05-06T19:24:49.968711Z","shell.execute_reply.started":"2022-05-06T19:14:47.860079Z","shell.execute_reply":"2022-05-06T19:24:49.967599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sorted(obj.li, key=lambda x:x[0],reverse=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:29:38.666096Z","iopub.execute_input":"2022-05-06T19:29:38.666391Z","iopub.status.idle":"2022-05-06T19:29:38.674972Z","shell.execute_reply.started":"2022-05-06T19:29:38.66636Z","shell.execute_reply":"2022-05-06T19:29:38.673971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ll=[]\nfor i,ix in enumerate(obj.li):\n    if '+'.join([str(int(x))+'_27_num' for x in obj.li[i][1] if x!=-1]) not in ll:\n        ll.append('+'.join([str(int(x))+'_27_num' for x in obj.li[i][1] if x!=-1]))\n        X_train['+'.join([str(int(x))+'_27_num' for x in obj.li[i][1] if x!=-1])]=X_train[[str(int(x))+'_27_num' for x in obj.li[i][1] if x!=-1]].astype(str).sum(axis=1).astype(int)\n        X_test['+'.join([str(int(x))+'_27_num' for x in obj.li[i][1] if x!=-1])]=X_test[[str(int(x))+'_27_num' for x in obj.li[i][1] if x!=-1]].astype(str).sum(axis=1).astype(int)\n        test['+'.join([str(int(x))+'_27_num' for x in obj.li[i][1] if x!=-1])]=test[[str(int(x))+'_27_num' for x in obj.li[i][1] if x!=-1]].astype(str).sum(axis=1).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:29:50.20835Z","iopub.execute_input":"2022-05-06T19:29:50.208981Z","iopub.status.idle":"2022-05-06T19:30:48.032235Z","shell.execute_reply.started":"2022-05-06T19:29:50.208928Z","shell.execute_reply":"2022-05-06T19:30:48.031182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_list = [f for f in X_train.columns if X_train[f].dtype in ['float64','int64'] and f not in ['target','id']]\nimport lightgbm as lgbm\nparams = {'n_estimators': 10000,\n          'lambda_l2': 0.0015, \n          'alpha': 9.82, \n          'learning_rate': 0.02, \n          'max_depth': 11, \n          'min_child_weight': 135}\nmodel = lgbm.LGBMClassifier(**params)\nmodel.fit(X_train[colz+ll], y_train)\n\nprint(classification_report(y_test, model.predict(X_test[colz+ll])))\nprint(classification_report(y_train, model.predict(X_train[colz+ll])))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:30:48.034397Z","iopub.execute_input":"2022-05-06T19:30:48.034803Z","iopub.status.idle":"2022-05-06T19:48:24.126792Z","shell.execute_reply.started":"2022-05-06T19:30:48.034755Z","shell.execute_reply":"2022-05-06T19:48:24.125566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 10))\nlgbm.plot_importance(model,ignore_zero=False,ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:48:26.073371Z","iopub.execute_input":"2022-05-06T19:48:26.073737Z","iopub.status.idle":"2022-05-06T19:48:26.796455Z","shell.execute_reply.started":"2022-05-06T19:48:26.073692Z","shell.execute_reply":"2022-05-06T19:48:26.795477Z"},"trusted":true},"execution_count":null,"outputs":[]}]}