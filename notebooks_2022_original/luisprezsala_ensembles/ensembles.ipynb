{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Repositorio","metadata":{}},{"cell_type":"markdown","source":"Se puede consultar la metodología de desarrollo del proyecto y el histórico de imágenes en el siguiente repositorio:\n\n- https://github.com/luperezsal/DM-Classification-Tree","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport time","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:27:46.026567Z","iopub.execute_input":"2022-05-07T11:27:46.02725Z","iopub.status.idle":"2022-05-07T11:27:46.053501Z","shell.execute_reply.started":"2022-05-07T11:27:46.027118Z","shell.execute_reply":"2022-05-07T11:27:46.052858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Directory and version specifications","metadata":{"tags":[]}},{"cell_type":"code","source":"from datetime import datetime\n\nMODEL_TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d__%H-%M-%S\")\n\nDATA_PATH = '../input/atp-matches/'\n\nREPORTS_PATH = 'reports/ensembles/'\nSAMPLE_GRAPH_RESULTS_PATH  = 'sample_graph_result/ensembles/'\nTREE_PATH = 'tree/'\n\n# Resolución de imágenes\nresolution = 300\nrandom_state = 2","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:27:46.095929Z","iopub.execute_input":"2022-05-07T11:27:46.09621Z","iopub.status.idle":"2022-05-07T11:27:46.100676Z","shell.execute_reply.started":"2022-05-07T11:27:46.096178Z","shell.execute_reply":"2022-05-07T11:27:46.100138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download and Store Data","metadata":{"tags":[]}},{"cell_type":"code","source":"# for index in range(0,22):\n#     index_str = str(index)\n\n#     print(index_str)\n    \n#     if len(index_str) == 1:\n#         index_str = '0' + index_str\n\n#     print(index_str)\n\n#     url = \"https://raw.githubusercontent.com/JeffSackmann/tennis_atp/master/atp_matches_20{}.csv\".format(index_str)\n#     print(url)\n\n#     FILE_NAME = \"atp_matches_20{}.csv\".format(index_str)\n\n#     df = pd.read_csv(url, index_col=0, parse_dates=[0])\n#     df.to_csv(DATA_PATH + FILE_NAME)\n\n# # data_frame = pd.read_csv(DATA_PATH + FILE_NAME)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-05-07T11:27:46.102026Z","iopub.execute_input":"2022-05-07T11:27:46.102372Z","iopub.status.idle":"2022-05-07T11:27:46.114616Z","shell.execute_reply.started":"2022-05-07T11:27:46.102344Z","shell.execute_reply":"2022-05-07T11:27:46.113804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{"tags":[],"toc-hr-collapsed":true}},{"cell_type":"code","source":"atp = pd.DataFrame()\n\nyears_index_20_22 = range(0,22)\n\nfor index in years_index_20_22:\n    index_str = str(index)\n\n    if len(index_str) == 1:\n        index_str = '0' + index_str\n\n    FILE_NAME = \"atp_matches_20{}.csv\".format(index_str)\n\n    data_frame_iter = pd.read_csv(DATA_PATH + FILE_NAME)\n    atp = pd.concat([atp, data_frame_iter])\n\npd.set_option('display.max_columns', None)\natp","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:27:46.119329Z","iopub.execute_input":"2022-05-07T11:27:46.119811Z","iopub.status.idle":"2022-05-07T11:27:47.362897Z","shell.execute_reply.started":"2022-05-07T11:27:46.119779Z","shell.execute_reply":"2022-05-07T11:27:47.362071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# COLUMNS_TO_REMOVE = [\"tourney_id\", \"tourney_name\", \"tourney_date\",\n#                      \"match_num\",\n#                      \"winner_id\", \"loser_id\",\n#                      \"winner_seed\", \"loser_seed\",\n#                      \"winner_name\", \"loser_name\",\n#                      \"winner_ioc\", \"loser_loc\",\n#                      \"winner_rank\", \"loser_rank\",\n#                      \"winner_rank_points\", \"loser_rank_points\",\n#                      \"round\"]","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:27:47.364876Z","iopub.execute_input":"2022-05-07T11:27:47.365436Z","iopub.status.idle":"2022-05-07T11:27:47.369605Z","shell.execute_reply.started":"2022-05-07T11:27:47.365391Z","shell.execute_reply":"2022-05-07T11:27:47.368961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clean Dataset","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"code","source":"# Vamos a eliminar las variables que son identificadores, nombres etc\n# Incluimos en el drop las siguientes variables que tienen muchos registros NaN\n# quitaremos las columna de score\ndf_regression = atp\n\n\nCOLUMNS_TO_REMOVE = ['tourney_id', 'tourney_name', 'tourney_date',\n                     'winner_name', 'loser_name',\n                     'winner_entry', 'loser_entry',\n                     'winner_seed', 'loser_seed',\n                     'winner_id', 'loser_id',\n                     'score']\n\ndf_regression = df_regression.drop(COLUMNS_TO_REMOVE, axis = 1) \ndf_regression = df_regression.dropna()\ndf_regression = df_regression.drop_duplicates()\n\n# Crearemos dos formulas para calculos del ganador y el perdedor para evitar la correlación de estas variables, tambien haremos un drop de estas variables.\ndf_regression['w_calculation'] = df_regression['w_svpt'] + df_regression['w_1stIn'] + df_regression['w_1stWon'] + df_regression['w_2ndWon'] + df_regression['w_SvGms']\ndf_regression['l_calculation'] = df_regression['l_svpt'] + df_regression['l_1stIn'] + df_regression['l_1stWon'] + df_regression['l_2ndWon'] + df_regression['l_SvGms']\n\ndf_regression = df_regression.drop(['w_svpt', 'w_1stIn', 'w_1stWon', 'w_2ndWon', 'w_SvGms', 'l_svpt', 'l_1stIn', 'l_1stWon', 'l_2ndWon', 'l_SvGms'], axis = 1) \n\ndf_regression = df_regression._get_numeric_data() #drop non-numeric cols","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:27:47.370843Z","iopub.execute_input":"2022-05-07T11:27:47.371288Z","iopub.status.idle":"2022-05-07T11:27:47.608661Z","shell.execute_reply.started":"2022-05-07T11:27:47.371256Z","shell.execute_reply":"2022-05-07T11:27:47.607718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split Data","metadata":{"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df_regression.drop('minutes', axis = 1) \ny = df_regression['minutes']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_state)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:27:47.61069Z","iopub.execute_input":"2022-05-07T11:27:47.610936Z","iopub.status.idle":"2022-05-07T11:27:48.668258Z","shell.execute_reply.started":"2022-05-07T11:27:47.610908Z","shell.execute_reply":"2022-05-07T11:27:48.667375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensembles","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Los ensembles son técnicas que permiten combinar las predicciones de distintos modelos con el objetivo de aumentar la prediccción global de los resultados.\nEn esta sección aplicaremos distintas técnicas de ensembles y ejecutaremos cada una de ellas al dataset, con el objetivo de predecir la duración de los partidos como se hizo en el apartado de Regresión:\n\n- Ensemble Bagging\n- Ensembles Boosting\n    - AdaBoost\n    - Gradient Boosting Regressor\n- Ensemble Stacking","metadata":{}},{"cell_type":"markdown","source":"Lo primero será buscar el Árbol de regresión que más accuracy nos dé mediante cross-validation para usarlo en el proceso de Bagging.","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\n\n\nbest_params = {'criterion': 'absolute_error',\n               'max_depth': 7,\n               'max_features': None,\n               'min_weight_fraction_leaf': 0.0,\n               'splitter': 'best'}\n\ndecision_tree = DecisionTreeRegressor(random_state = random_state)\ndecision_tree.set_params(**best_params)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:27:48.669458Z","iopub.execute_input":"2022-05-07T11:27:48.669685Z","iopub.status.idle":"2022-05-07T11:27:48.811282Z","shell.execute_reply.started":"2022-05-07T11:27:48.669658Z","shell.execute_reply":"2022-05-07T11:27:48.810449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inicializamos un diccionario `info` donde almacenaremos toda la información relacionada con los ensembles, sus resultados, su tiempo de ejecución, el nombre del ensemble, etc.","metadata":{}},{"cell_type":"code","source":"info = {}","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:27:48.812764Z","iopub.execute_input":"2022-05-07T11:27:48.813202Z","iopub.status.idle":"2022-05-07T11:27:48.817236Z","shell.execute_reply.started":"2022-05-07T11:27:48.813169Z","shell.execute_reply":"2022-05-07T11:27:48.816233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bagging","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Bootstrap Aggregation o Bagging es una técnica que permite utilizar el resampling Bootstrap para consturir ensembles y poder utilizar varios conjuntos de datos para cada uno de los modelos pertenecientes a la arquitectura ensemble diseñada. Los modelos que se utilzarán en esta arquitectura serán todos el mismo, con la única diferencia de haberlos entrenado con distintos conjuntos de datos.\n\nEn el caso de la regresión, esta técnica permite calcular la media de las prediccciones de cada uno de los modelos entrenados mediante Bootstrap para obtener un resultado final que dependa de todos los modelos.\n\n\n[Referencia](https://machinelearningmastery.com/bagging-ensemble-with-python/)","metadata":{}},{"cell_type":"markdown","source":"### Definition","metadata":{}},{"cell_type":"markdown","source":"Crearemos un Ensemble de tipo Bagging con 10 estimadores, es decir, se entrenarán diez árboles de decisión que predecirán las muestras del conjunto de test y el resultado que ofrecerá será la media de todas las prediciones.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingRegressor\n\nMODEL_NAME = 'bagging'\ninfo[MODEL_NAME] = {}\ninfo[MODEL_NAME]['model_name'] = MODEL_NAME\n\nnum_models = 10\nbagging = BaggingRegressor(decision_tree,\n                           n_estimators = num_models,\n                           random_state = random_state)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:27:48.818539Z","iopub.execute_input":"2022-05-07T11:27:48.818787Z","iopub.status.idle":"2022-05-07T11:27:48.893046Z","shell.execute_reply.started":"2022-05-07T11:27:48.818759Z","shell.execute_reply":"2022-05-07T11:27:48.892357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Entrenamos el ensemble Bagging y almacenamos el tiempo de ejecución para analizar en apartados posteriores los rendimientos de los modelos.","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\nbagging_regressor = bagging.fit(X_train, y_train)\n\nend = time.time()\n\nellapsed_time = round(end - start, 2)\n\ninfo[MODEL_NAME]['time'] = ellapsed_time\n\nprint(f\"Done in {ellapsed_time} (s)\")","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:27:48.894469Z","iopub.execute_input":"2022-05-07T11:27:48.89486Z","iopub.status.idle":"2022-05-07T11:33:31.505571Z","shell.execute_reply.started":"2022-05-07T11:27:48.894831Z","shell.execute_reply":"2022-05-07T11:33:31.504474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metrics","metadata":{}},{"cell_type":"markdown","source":"Las métricas que utilizaremos para medir los errores del modelos serán:\n- **Mean Squared Error (MSE)**:  error cuadrático medio, es la suma al cuadrado de los residuos dividido entre el número de muestras totales. Es deseable minimizar este estadístico.\n- **Root Mean Squared Error (RMSE)**: raíz del error cuadrático medio, es la raíz cuadrada de la suma de los residuos al cuadrado entre el número de muestras totales. Es deseable minimizar este estadístico.\n- **R-Squared**: indica la cantidad de varianza en los datos explicada por el modelo actual. Es deseable maximizar este estadístico.","metadata":{}},{"cell_type":"code","source":"y_pred = bagging_regressor.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\ninfo[MODEL_NAME]['mse'] = mse\n\nprint(\"MSE: \", info[MODEL_NAME]['mse'])\n\nrmse = mean_squared_error(y_true  = y_test,\n                          y_pred  = y_pred,\n                          squared = False\n                        )\ninfo[MODEL_NAME]['rmse'] = rmse\n\nprint(\"RMSE: \", info[MODEL_NAME]['rmse'])\n\nscore = bagging_regressor.score(X_test, y_test)\ninfo[MODEL_NAME]['score'] = score\n\nprint(\"R-squared:\", info[MODEL_NAME]['score']) ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:33:31.507224Z","iopub.execute_input":"2022-05-07T11:33:31.507921Z","iopub.status.idle":"2022-05-07T11:33:31.556859Z","shell.execute_reply.started":"2022-05-07T11:33:31.507874Z","shell.execute_reply":"2022-05-07T11:33:31.555813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Graphic Results","metadata":{}},{"cell_type":"markdown","source":"A continuación graficaremos un ejemplo de cómo el modelo se ajusta a los valores verdaderos sobre las muestras del conjunto de test. Es deseable tener un modelo que prediga valores lo más cercanos posibles a las muestras verdaderas.","metadata":{}},{"cell_type":"code","source":"n_samples = range(len(y_test[:50]))\n\ninfo[MODEL_NAME]['y_pred'] = y_pred[:50]\n\nplt.figure(figsize=(15,10))\n\nplt.scatter(n_samples, y_test[:50], s = 15, color = 'red', label = \"original\")\nplt.plot(n_samples, y_pred[:50], linewidth = 1.1, color = 'blue', label = \"predicted\")\n\nplt.title(\"Y-true / Y-predicted (minutes)\")\nplt.xlabel('Sample')\nplt.ylabel('Minutes')\n\nplt.legend(loc = 'best',\n           fancybox = True,\n           shadow = True)\n\nGRAPH_PATH = f\"{SAMPLE_GRAPH_RESULTS_PATH}{MODEL_NAME}/\"\nFILE_NAME  = f\"{MODEL_NAME}_{MODEL_TIMESTAMP}.png\"\n\nplt.grid(True)\n# plt.savefig(GRAPH_PATH + FILE_NAME, dpi = resolution)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:33:31.559927Z","iopub.execute_input":"2022-05-07T11:33:31.56018Z","iopub.status.idle":"2022-05-07T11:33:31.852836Z","shell.execute_reply.started":"2022-05-07T11:33:31.56015Z","shell.execute_reply":"2022-05-07T11:33:31.851996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Boosting","metadata":{"tags":[],"toc-hr-collapsed":true}},{"cell_type":"markdown","source":"### ADABoost","metadata":{"tags":[]}},{"cell_type":"markdown","source":"AdaBoost utiliza múltiples weak learners (árboles de decisión de un nivel) que son agregados secuencialmente al conjunto de modelos, con el objetivo de que cada uno de estos árboles minimice el error producido por el anterior modelo.\n\nEsto se consigue asignando una serie de pesos (ponderación) a cada una de las muestras que estén clasificadas erróneamente (clasificación) o que tengan un error alto (regresión).\n\n[Referencia](https://machinelearningmastery.com/adaboost-ensemble-in-python/)","metadata":{}},{"cell_type":"markdown","source":"#### Definition","metadata":{}},{"cell_type":"markdown","source":"Crearemos un Ensemble de tipo AdaBoost con 10 estimadores, es decir, se entrenarán diez árboles de decisión que minimizarán el error de los anteriores árboles.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.datasets import make_regression\n\nMODEL_NAME = 'adaboost'\ninfo[MODEL_NAME] = {}\ninfo[MODEL_NAME]['model_name'] = MODEL_NAME\n\nnum_models = 10\n\nada_boosting_regresor = AdaBoostRegressor(random_state = random_state)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:33:31.854193Z","iopub.execute_input":"2022-05-07T11:33:31.854439Z","iopub.status.idle":"2022-05-07T11:33:31.932393Z","shell.execute_reply.started":"2022-05-07T11:33:31.85441Z","shell.execute_reply":"2022-05-07T11:33:31.931366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"markdown","source":"Entrenamos el ensemble Ada Boost y anotamos el tiempo de ejecución.","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\nada_boosting_regresor.fit(X, y)\n\nend = time.time()\n\nellapsed_time = round(end - start, 2)\ninfo[MODEL_NAME]['time'] = ellapsed_time\n\nprint(f\"Done in {ellapsed_time} (s)\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-05-07T11:33:31.933567Z","iopub.execute_input":"2022-05-07T11:33:31.933828Z","iopub.status.idle":"2022-05-07T11:33:37.713319Z","shell.execute_reply.started":"2022-05-07T11:33:31.9338Z","shell.execute_reply":"2022-05-07T11:33:37.712723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Metrics","metadata":{}},{"cell_type":"markdown","source":"Calculamos los estadísticos **MSE**, **RMSE** y **R-Squared**.","metadata":{}},{"cell_type":"code","source":"y_pred = ada_boosting_regresor.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\ninfo[MODEL_NAME]['mse'] = mse\n\nprint(\"MSE: \", info[MODEL_NAME]['mse'])\n\nrmse = mean_squared_error(y_true  = y_test,\n                          y_pred  = y_pred,\n                          squared = False\n                        )\ninfo[MODEL_NAME]['rmse'] = rmse\n\nprint(\"RMSE: \", info[MODEL_NAME]['rmse'])\n\nscore = ada_boosting_regresor.score(X_test, y_test)\ninfo[MODEL_NAME]['score'] = score\n\nprint(\"R-squared:\", info[MODEL_NAME]['score']) ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:33:37.714343Z","iopub.execute_input":"2022-05-07T11:33:37.715007Z","iopub.status.idle":"2022-05-07T11:33:37.851384Z","shell.execute_reply.started":"2022-05-07T11:33:37.714971Z","shell.execute_reply":"2022-05-07T11:33:37.850337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Graphic Results","metadata":{}},{"cell_type":"markdown","source":"Graficamos un interavalo de predicciones de 50 muestras de test.","metadata":{}},{"cell_type":"code","source":"n_samples = range(len(y_test[:50]))\n\ninfo[MODEL_NAME]['y_pred'] = y_pred[:50]\n\nplt.figure(figsize=(15,10))\n\nplt.scatter(n_samples, y_test[:50], s = 15, color = 'red', label = \"original\")\nplt.plot(n_samples, y_pred[:50], linewidth = 1.1, color = 'blue', label = \"predicted\")\n\nplt.title(\"Y-true / Y-predicted (minutes)\")\nplt.xlabel('Sample')\nplt.ylabel('Minutes')\n\nplt.legend(loc='best',fancybox = True, shadow = True)\n\nGRAPH_PATH = f\"{SAMPLE_GRAPH_RESULTS_PATH}boosting/{MODEL_NAME}/\"\nFILE_NAME  = f\"{MODEL_NAME}_{MODEL_TIMESTAMP}.png\"\n\nplt.grid(True)\n# plt.savefig(GRAPH_PATH + FILE_NAME, dpi = resolution)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:33:37.853126Z","iopub.execute_input":"2022-05-07T11:33:37.853546Z","iopub.status.idle":"2022-05-07T11:33:38.110849Z","shell.execute_reply.started":"2022-05-07T11:33:37.853497Z","shell.execute_reply":"2022-05-07T11:33:38.109968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Boosting Regressor","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Al igual que en el caso del AdaBoost, utiliza la técnica Boosting para entrenar los modelos, es decir, trata de minimizar los residuos de los modelos anteriores. Sin embargo, utiliza el método de Descenso por Gradiente en lugar de la asignación de pesos como en el caso anterior.\n\n[Referencia](https://www.cienciadedatos.net/documentos/py09_gradient_boosting_python.html)","metadata":{}},{"cell_type":"markdown","source":"#### Definition","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\nMODEL_NAME = 'gradientboost'\ninfo[MODEL_NAME] = {}\ninfo[MODEL_NAME]['model_name'] = MODEL_NAME\n\nnum_models = 10\n\ngradient_boosting = GradientBoostingRegressor(criterion = best_params['criterion'],\n                                              max_depth = best_params['max_depth'],\n                                              n_estimators  = num_models,\n                                              random_state = random_state)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:33:38.112471Z","iopub.execute_input":"2022-05-07T11:33:38.113056Z","iopub.status.idle":"2022-05-07T11:33:38.11967Z","shell.execute_reply.started":"2022-05-07T11:33:38.11301Z","shell.execute_reply":"2022-05-07T11:33:38.119121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"markdown","source":"Entrenamos el ensemble Gradient Boosting y anotamos el tiempo de ejecución.","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\ngradient_boosting_regressor = gradient_boosting.fit(X_train, y_train)\n\nend = time.time()\n\nellapsed_time = round(end - start, 2)\ninfo[MODEL_NAME]['time'] = ellapsed_time\n\nprint(f\"Done in {ellapsed_time} (s)\")","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:33:38.121519Z","iopub.execute_input":"2022-05-07T11:33:38.12222Z","iopub.status.idle":"2022-05-07T11:47:42.296185Z","shell.execute_reply.started":"2022-05-07T11:33:38.122177Z","shell.execute_reply":"2022-05-07T11:47:42.295353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Metrics","metadata":{}},{"cell_type":"markdown","source":"Calculamos los estadísticos **MSE**, **RMSE** y **R-Squared**.","metadata":{}},{"cell_type":"code","source":"y_pred = gradient_boosting.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\ninfo[MODEL_NAME]['mse'] = mse\n\nprint(\"MSE: \", info[MODEL_NAME]['mse'])\n\nrmse = mean_squared_error(y_true  = y_test,\n                          y_pred  = y_pred,\n                          squared = False\n                        )\ninfo[MODEL_NAME]['rmse'] = rmse\n\nprint(\"RMSE: \", info[MODEL_NAME]['rmse'])\n\nscore = gradient_boosting.score(X_test, y_test)\ninfo[MODEL_NAME]['score'] = score\n\nprint(\"R-squared:\", info[MODEL_NAME]['score']) ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:47:42.297309Z","iopub.execute_input":"2022-05-07T11:47:42.297984Z","iopub.status.idle":"2022-05-07T11:47:42.333454Z","shell.execute_reply.started":"2022-05-07T11:47:42.29795Z","shell.execute_reply":"2022-05-07T11:47:42.332421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Graphic Results","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Graficamos un interavalo de predicciones de 50 muestras de test.","metadata":{}},{"cell_type":"code","source":"n_samples = range(len(y_test[:50]))\n\ninfo[MODEL_NAME]['y_pred'] = y_pred[:50]\n\nplt.figure(figsize=(15,10))\n\nplt.scatter(n_samples, y_test[:50], s = 15, color = 'red', label = \"original\")\nplt.plot(n_samples, y_pred[:50], linewidth = 1.1, color = 'blue', label = \"predicted\")\n\nplt.title(\"Y-true / Y-predicted (minutes)\")\nplt.xlabel('Sample')\nplt.ylabel('Minutes')\n\nplt.legend(loc='best',fancybox = True, shadow = True)\n\nGRAPH_PATH = f\"{SAMPLE_GRAPH_RESULTS_PATH}boosting/{MODEL_NAME}/\"\nFILE_NAME  = f\"{MODEL_NAME}_{MODEL_TIMESTAMP}.png\"\n\nplt.grid(True)\n# plt.savefig(GRAPH_PATH + FILE_NAME, dpi = resolution)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:47:42.334544Z","iopub.execute_input":"2022-05-07T11:47:42.334879Z","iopub.status.idle":"2022-05-07T11:47:42.536325Z","shell.execute_reply.started":"2022-05-07T11:47:42.33485Z","shell.execute_reply":"2022-05-07T11:47:42.535239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacking","metadata":{"toc-hr-collapsed":true}},{"cell_type":"markdown","source":"Stacking permite utilizar distintas tipologías de modelos para crear una arquitectura que combine las predicciones de éstos.\n\nA diferencia del Bagging, el Stacking nos da la flexibilidad de explorar distintos modelos en lugar de siempre el mismo.\n\nEs por esto que en esta sección crearemos un Ensemble Stacking con los siguientes modelos:\n- **KNeighborsRegressor**: configurado con tres vecinos cercanos para calcular la regresión. \n- **DecisionTreeRegressor**: con los parámetros óptimos calculados en el apartado de Regresión.\n- **SVR**: con los paráemtros por defecto.\n\nDebido a motivos de tiempo de ejecución, no ha sido posible buscar los mejores hiperparámetros para cada uno de los modelos que pertenecen a la arquitectura de Stacking, por lo que un posible trabajo a futuro sería encontrar estos parámetros que logren minimizar los residuos.\n\n[Referencia](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/)","metadata":{}},{"cell_type":"markdown","source":"### Definition","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import StackingRegressor\n\n\nMODEL_NAME = 'stacking'\ninfo[MODEL_NAME] = {}\ninfo[MODEL_NAME]['model_name'] = MODEL_NAME\n\nbase_models = list()\nbase_models.append(('knn', KNeighborsRegressor(n_neighbors = 3)))\nbase_models.append(('cart', DecisionTreeRegressor(criterion = best_params['criterion'],\n                                                  max_depth = best_params['max_depth'],\n                                                  random_state = random_state)))\nbase_models.append(('svm', SVR()))\n\n\nmeta_learner = DecisionTreeRegressor(criterion = best_params['criterion'],\n                                     max_depth = best_params['max_depth'],\n                                     random_state = random_state)\n\nstacking = StackingRegressor(estimators = base_models,\n                             final_estimator = meta_learner,\n                             cv = 5)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T11:47:42.537852Z","iopub.execute_input":"2022-05-07T11:47:42.538118Z","iopub.status.idle":"2022-05-07T11:47:42.54707Z","shell.execute_reply.started":"2022-05-07T11:47:42.538064Z","shell.execute_reply":"2022-05-07T11:47:42.546205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"markdown","source":"Entrenamos el ensemble Stacking y anotamos el tiempo de ejecución.","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\nstacking_regressor = stacking.fit(X_train, y_train)\n\nend = time.time()\n\nellapsed_time = round(end - start, 2)\ninfo[MODEL_NAME]['time'] = ellapsed_time\n\nprint(f\"Done in {ellapsed_time} (s)\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-05-07T11:47:42.548745Z","iopub.execute_input":"2022-05-07T11:47:42.549121Z","iopub.status.idle":"2022-05-07T12:07:51.309983Z","shell.execute_reply.started":"2022-05-07T11:47:42.54906Z","shell.execute_reply":"2022-05-07T12:07:51.30901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metrics","metadata":{}},{"cell_type":"markdown","source":"Calculamos los estadísticos **MSE**, **RMSE** y **R-Squared**.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ny_pred = stacking_regressor.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\ninfo[MODEL_NAME]['mse'] = mse\n\nprint(\"MSE: \", info[MODEL_NAME]['mse'])\n\nrmse = mean_squared_error(y_true  = y_test,\n                          y_pred  = y_pred,\n                          squared = False\n                        )\ninfo[MODEL_NAME]['rmse'] = rmse\n\nprint(\"RMSE: \", info[MODEL_NAME]['rmse'])\n\n\nscore = stacking_regressor.score(X_test, y_test)\ninfo[MODEL_NAME]['score'] = score\n\nprint(\"R-squared:\", info[MODEL_NAME]['score'])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T12:07:51.311583Z","iopub.execute_input":"2022-05-07T12:07:51.311826Z","iopub.status.idle":"2022-05-07T12:09:07.987713Z","shell.execute_reply.started":"2022-05-07T12:07:51.311799Z","shell.execute_reply":"2022-05-07T12:09:07.98684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Graphic Results","metadata":{}},{"cell_type":"markdown","source":"Graficamos un interavalo de predicciones de 50 muestras de test.","metadata":{}},{"cell_type":"code","source":"n_samples = range(len(y_test[:50]))\n\ninfo[MODEL_NAME]['y_pred'] = y_pred[:50]\n\nplt.figure(figsize=(15,10))\n\nplt.scatter(n_samples, y_test[:50], s = 15, color = 'red', label = \"original\")\nplt.plot(n_samples, y_pred[:50], linewidth = 1.1, color = 'blue', label = \"predicted\")\n\nplt.title(\"Y-true / Y-predicted (minutes)\")\nplt.xlabel('Sample')\nplt.ylabel('Minutes')\n\nplt.legend(loc = 'best',\n           fancybox = True,\n           shadow = True)\n\nGRAPH_PATH = f\"{SAMPLE_GRAPH_RESULTS_PATH}{MODEL_NAME}/\"\nFILE_NAME  = f\"{MODEL_NAME}_{MODEL_TIMESTAMP}.png\"\n\nplt.grid(True)\n# plt.savefig(GRAPH_PATH + FILE_NAME, dpi = resolution)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T12:12:20.380587Z","iopub.execute_input":"2022-05-07T12:12:20.380903Z","iopub.status.idle":"2022-05-07T12:12:20.600204Z","shell.execute_reply.started":"2022-05-07T12:12:20.380871Z","shell.execute_reply":"2022-05-07T12:12:20.599373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reports","metadata":{}},{"cell_type":"markdown","source":"A continuación analizaremos los resultados de los ensembles obtenidos, centrándonos concretamente en:\n\n- **Tiempo**: tiempo empleado en entrenar el ensemble.\n- **Mean Squared Error (MSE)**: Error cuadrático medio de las predicciones respecto a su valor verdadero.\n- **Root-mean-square deviation (RMSE)**: Raíz cuadrada del error cuadrático medio o MSE.\n- **Score**: Media de la precisión de los modelos.","metadata":{}},{"cell_type":"code","source":"FEATURES = ['model_name', 'time', 'mse', 'rmse', 'score']\nsummary_dataframe = pd.DataFrame(columns = FEATURES)\n\nfor key in info:\n    row = info[key]\n    fields = []\n    for feature in row:\n        if (feature in FEATURES):\n            fields.append(row[feature])\n\n    row_series = pd. Series(fields, index = summary_dataframe.columns)\n    summary_dataframe = summary_dataframe.append(row_series, ignore_index = True)\n\nSAVE_PATH =  f\"{REPORTS_PATH}{MODEL_TIMESTAMP}.csv\"\n\n# summary_dataframe.to_csv(SAVE_PATH, index = True)\nsummary_dataframe.style.highlight_min(subset = ['time', 'mse', 'rmse'], color = 'green')\\\n                       .highlight_max(subset = ['score'], color = 'green')\\\n                       .highlight_max(subset = ['time', 'mse', 'rmse'], color = 'red')\\\n                       .highlight_min(subset = ['score'], color = 'red')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T12:12:24.431232Z","iopub.execute_input":"2022-05-07T12:12:24.431509Z","iopub.status.idle":"2022-05-07T12:12:24.541172Z","shell.execute_reply.started":"2022-05-07T12:12:24.431477Z","shell.execute_reply":"2022-05-07T12:12:24.540364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos observar que cada uno de los modelos emplea un tiempo de entrenamiento notablemente diferente. Esto, junto a la distinta precisión entre los modelos, genera la necesidad de analizar cada modelo individualmente.\n\nComo vemos en el reporte, el modelo que mejor resultados nos ofrece es el Bagging, además con un tiempo de ejecución razonable con respecto a los modelos Gradient Boosting y Stacking, no siendo así con respecto al Ensemble AdaBoost, sin embargo, debido a la diferencia de precisión, este modelo queda descartado para este problema. \n\nPor lo que, con esta configuración de hiperparámetros con los modelos actuales, eligiríamos el modelo Ensemble Bagging como solución a este problema.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\n\nplt.scatter(n_samples, y_test[:50], s = 20, color = 'red', label = \"original\")\n\nfor key in info:\n    y_pred = info[key]['y_pred']\n    model_name = info[key]['model_name']\n    plt.plot(n_samples, y_pred, linewidth = 1.1, label = model_name)\n\nplt.title(\"Y-true / Y-predicted (minutes)\")\nplt.xlabel('Sample')\nplt.ylabel('Minutes')\n\nplt.legend(loc = 'best',\n           fancybox = True,\n           shadow = True)\n\nGRAPH_PATH = f\"{SAMPLE_GRAPH_RESULTS_PATH}/\"\nFILE_NAME  = f\"{MODEL_TIMESTAMP}.png\"\n\nplt.grid(True)\n# plt.savefig(GRAPH_PATH + FILE_NAME, dpi = resolution)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T12:12:27.102709Z","iopub.execute_input":"2022-05-07T12:12:27.103063Z","iopub.status.idle":"2022-05-07T12:12:27.352037Z","shell.execute_reply.started":"2022-05-07T12:12:27.103018Z","shell.execute_reply":"2022-05-07T12:12:27.351071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observando la gráfica podemos apreciar dos casos contrapuestos en lo que a valores predichos se refiere.\n\nSi ponemos el foco en el ensemble Stacking (rojo) y Bagging (azul), podemos comprobar que siguen una predicción bastante cercana a los valores reales en comparación a los métodos de Ensembles AdaBoost (naranja) y GradientBoost (verde).\n\nVisualizando los valores de los resultados de la sección anterior comprobamos que tanto el modelo Bagging como el modelo Stacking tienen mayores precisiones con respecto a AdaBoost y GraidentBoost.","metadata":{}},{"cell_type":"markdown","source":"# Conclusiones","metadata":{}},{"cell_type":"markdown","source":"Como hemos podido comprobar, es posible aplicar distintas técnicas de ensembles a un problema, concretamente:\n- Bagging\n- Boosting\n    - AdaBoost:\n    - Gradient Boosting Regressor\n- Stacking\n\nDespués de haber probado las distintas arquitecturas, los mejores resultados se han obtenido en Bagging con un R-Squared de 0.8586 con respecto al 0.849 del ensemble Stacking, 0.752 del Gradient Boost y 0.723 del AdaBoost respectivamente.\n    \n\nPor motivos de tiempo de ejecución no se han podido testar tantas combinaciones distintas de modelos e hiperparámetros deseadas a la hora de construir los ensembles.\n\nPor lo que se propone como trabajos futuros la implementación y el estudio de distintos modelos en las arquitecturas de los ensembles y realizar una búsqueda más profunda de hiperparámetros que logren minimizar aún más el error producido en la regresión y, por ende, mejorar la precisión de los resultados.","metadata":{}}]}