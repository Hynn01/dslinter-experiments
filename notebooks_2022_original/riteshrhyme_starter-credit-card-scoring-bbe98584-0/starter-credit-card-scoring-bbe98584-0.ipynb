{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Laoding Basic data analysis libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic programming and data visualization libraries\nimport sys\nimport os\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n%matplotlib inline\n\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\npy.init_notebook_mode(connected=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def hist_perc(df_col,bin_size,rng_st,rng_end):\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import matplotlib.ticker as mticker\n    #plt.hist(x = train_df.RevolvingUtilizationOfUnsecuredLines,bins=10,range=(,1))\n    plt.hist(x = df_col,bins=bin_size,range=(rng_st,rng_end))\n    formatter = mticker.FuncFormatter(lambda v, pos: str(round((v*100/train_df.shape[0]),2)))\n    plt.gca().yaxis.set_major_formatter(formatter)\n    plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotHist(df,nameOfFeature):\n    cls_train = df[nameOfFeature]\n    data_array = cls_train\n    hist_data = np.histogram(data_array)\n    binsize = .5\n\n    trace1 = go.Histogram(\n        x=data_array,\n        histnorm=\"\",\n        name='Histogram of Wind Speed',\n        autobinx=False,\n        xbins=dict(\n            start=df[nameOfFeature].min()-1,\n            end=df[nameOfFeature].max()+1,\n            size=binsize\n        )\n    )\n\n    trace_data = [trace1]\n    layout = go.Layout(\n        bargroupgap=0.3,\n         title='The distribution of ' + nameOfFeature,\n        xaxis=dict(\n            title=nameOfFeature,\n            titlefont=dict(\n                family='Courier New, monospace',\n                size=18,\n                color='#7f7f7f'\n            )\n        ),\n        yaxis=dict(\n            title='Number of labels',\n            titlefont=dict(\n                family='Courier New, monospace',\n                size=19,\n                color='#7f7f7f'\n            )\n        )\n    )\n    fig = go.Figure(data=trace_data, layout=layout)\n    py.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import skew\nfrom scipy.stats import kurtosis\ndef plotBarCat(df,feature,target):\n    x0 = df[df[target]==0][feature]\n    x1 = df[df[target]==1][feature]\n\n    trace1 = go.Histogram(\n        x=x0,\n        opacity=0.75\n    )\n    trace2 = go.Histogram(\n        x=x1,\n        opacity=0.75\n    )\n\n    data = [trace1, trace2]\n    layout = go.Layout(barmode='overlay',\n                      title=feature,\n                       yaxis=dict(title='Count'\n        ))\n    fig = go.Figure(data=data, layout=layout)\n\n    py.iplot(fig, filename='overlaid histogram')\n    \n    def DescribeFloatSkewKurt(df,target):\n        \"\"\"\n            A fundamental task in many statistical analyses is to characterize\n            the location and variability of a data set. A further\n            characterization of the data includes skewness and kurtosis.\n            Skewness is a measure of symmetry, or more precisely, the lack\n            of symmetry. A distribution, or data set, is symmetric if it\n            looks the same to the left and right of the center point.\n            Kurtosis is a measure of whether the data are heavy-tailed\n            or light-tailed relative to a normal distribution. That is,\n            data sets with high kurtosis tend to have heavy tails, or\n            outliers. Data sets with low kurtosis tend to have light\n            tails, or lack of outliers. A uniform distribution would\n            be the extreme case\n        \"\"\"\n        print('-*-'*25)\n        print(\"{0} mean : \".format(target), np.mean(df[feature]))\n        print(\"{0} var  : \".format(target), np.var(df[feature]))\n        print(\"{0} skew : \".format(target), skew(df[feature]))\n        print(\"{0} kurt : \".format(target), kurtosis(df[feature]))\n        print('-*-'*25)\n    \n    DescribeFloatSkewKurt(df,feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def OutLiersBox(df,nameOfFeature):\n    \n    trace0 = go.Box(\n        y = df[nameOfFeature],\n        name = \"All Points\",\n        jitter = 0.3,\n        pointpos = -1.8,\n        boxpoints = 'all',\n        marker = dict(\n            color = 'rgb(7,40,89)'),\n        line = dict(\n            color = 'rgb(7,40,89)')\n    )\n\n    trace1 = go.Box(\n        y = df[nameOfFeature],\n        name = \"Only Whiskers\",\n        boxpoints = False,\n        marker = dict(\n            color = 'rgb(9,56,125)'),\n        line = dict(\n            color = 'rgb(9,56,125)')\n    )\n\n    trace2 = go.Box(\n        y = df[nameOfFeature],\n        name = \"Suspected Outliers\",\n        boxpoints = 'suspectedoutliers',\n        marker = dict(\n            color = 'rgb(8,81,156)',\n            outliercolor = 'rgba(219, 64, 82, 0.6)',\n            line = dict(\n                outliercolor = 'rgba(219, 64, 82, 0.6)',\n                outlierwidth = 2)),\n        line = dict(\n            color = 'rgb(8,81,156)')\n    )\n\n    trace3 = go.Box(\n        y = df[nameOfFeature],\n        name = \"Whiskers and Outliers\",\n        boxpoints = 'outliers',\n        marker = dict(\n            color = 'rgb(107,174,214)'),\n        line = dict(\n            color = 'rgb(107,174,214)')\n    )\n\n    data = [trace0,trace1,trace2,trace3]\n\n    layout = go.Layout(\n        title = \"{} Outliers\".format(nameOfFeature)\n    )\n\n    fig = go.Figure(data=data,layout=layout)\n    py.iplot(fig, filename = \"Outliers\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\n\nfrom sklearn import svm\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n\ndef OutLierDetection(df,feature1,feature2,outliers_fraction=.1):\n    \n    new_df = df.copy()\n    rng = np.random.RandomState(42)\n\n    # Example settings\n    n_samples = new_df.shape[0]\n#     outliers_fraction = 0.2 # ************************************** imp\n    clusters_separation = [0]#, 1, 2]\n\n    # define two outlier detection tools to be compared\n    classifiers = {\n        \"One-Class SVM\": svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05,\n                                         kernel=\"rbf\", gamma=0.1),\n        \"Robust covariance\": EllipticEnvelope(contamination=outliers_fraction),\n        \"Isolation Forest\": IsolationForest(max_samples=n_samples,\n                                            contamination=outliers_fraction,\n                                            random_state=rng),\n        \"Local Outlier Factor\": LocalOutlierFactor(\n            n_neighbors=35,\n            contamination=outliers_fraction)}\n\n    \n    xx, yy = np.meshgrid(np.linspace(new_df[feature1].min()-new_df[feature1].min()*10/100, \n                                     new_df[feature1].max()+new_df[feature1].max()*10/100, 50),\n                         np.linspace(new_df[feature2].min()-new_df[feature2].min()*10/100,\n                                     new_df[feature2].max()+new_df[feature2].max()*10/100, 50))\n\n\n    n_inliers = int((1. - outliers_fraction) * n_samples)\n    n_outliers = int(outliers_fraction * n_samples)\n    ground_truth = np.ones(n_samples, dtype=int)\n    ground_truth[-n_outliers:] = -1\n\n    # Fit the problem with varying cluster separation\n    for i, offset in enumerate(clusters_separation):\n        np.random.seed(42)\n        # Data generation\n\n        X = new_df[[feature1,feature2]].values.tolist()\n\n        # Fit the model\n        plt.figure(figsize=(9, 7))\n        for i, (clf_name, clf) in enumerate(classifiers.items()):\n            # fit the data and tag outliers\n            if clf_name == \"Local Outlier Factor\":\n                y_pred = clf.fit_predict(X)\n                scores_pred = clf.negative_outlier_factor_\n            else:\n                clf.fit(X)\n                scores_pred = clf.decision_function(X)\n                y_pred = clf.predict(X)\n            threshold = stats.scoreatpercentile(scores_pred,\n                                                100 * outliers_fraction)\n            n_errors = (y_pred != ground_truth).sum()\n            \n            unique, counts = np.unique(y_pred,return_counts=True)\n            print(clf_name,dict(zip(unique, counts)))\n            \n            new_df[feature1+'_'+feature2+clf_name] = y_pred\n#             print(clf_name,y_pred) \n            # plot the levels lines and the points\n            if clf_name == \"Local Outlier Factor\":\n                # decision_function is private for LOF\n                Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])\n            else:\n                Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n            Z = Z.reshape(xx.shape)\n            subplot = plt.subplot(2, 2, i + 1)\n            subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),\n                             cmap=plt.cm.Blues_r)\n            a = subplot.contour(xx, yy, Z, levels=[threshold],\n                                linewidths=2, colors='red')\n            subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],\n                             colors='orange')\n            b = plt.scatter(new_df[feature1], new_df[feature2], c='white',\n                     s=20, edgecolor='k')\n\n            subplot.axis('tight')\n\n            subplot.set_xlabel(\"%s\" % (feature1))\n \n            plt.ylabel(feature2)#, fontsize=18)\n            plt.title(\"%d. %s (errors: %d)\" % (i + 1, clf_name, n_errors))\n\n        plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)\n#         plt.suptitle(\"Outlier detection\")\n\n    plt.show()\n    return new_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Declaration of Varibles\nSEED = 7\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# IMporting the dataset into memory\n\nnp.random.seed(SEED)\ntrain_df = pd.read_csv(\"cs-training.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df = train_df.sample(frac=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop the serial number column\ntrain_df.drop(train_df[['Unnamed: 0']],axis=1,inplace=True)\n#test_df.drop(test_df[['Unnamed: 0']],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train_df['SeriousDlqin2yrs']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df.drop(train_df[['SeriousDlqin2yrs']],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Identify NA values\n\ntrain_df.isna().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df[(train_df['MonthlyIncome'].isna()) & (train_df['SeriousDlqin2yrs']!=0)]\ntrain_df[(train_df['MonthlyIncome'].isna())].SeriousDlqin2yrs.value_counts().plot.bar()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[(train_df['MonthlyIncome']==0)].SeriousDlqin2yrs.value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[(train_df['MonthlyIncome']==0) | (train_df['MonthlyIncome'].isna()) ].SeriousDlqin2yrs.value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.hist(x = train_df.MonthlyIncome,range=(0,500),bins=50)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\ntrain_df.MonthlyIncome.plot.kde()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(train_df[train_df['MonthlyIncome'].isna()==True].index,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(train_df[train_df['MonthlyIncome']==0].index,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.NumberOfDependents = train_df.NumberOfDependents.astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#sns.pairplot(train_df,hue=\"SeriousDlqin2yrs\", palette=\"husl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train_df.corr(),cmap = \"Greens\")\n\n\n#sns.heatmap(df, cmap=\"YlGnBu\")\n#sns.heatmap(df, cmap=\"Blues\")\n#sns.heatmap(df, cmap=\"BuPu\")\n#sns.heatmap(df, cmap=\"Greens\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OutLiersBox(train_df,\"NumberOfOpenCreditLinesAndLoans\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this implies that the people who defaulted later than 30 days, most of them ran their accounts into bad debt as \n#they were termed as bad loans after 90+ days of default\n\n# EDA\n# 1. Revolving Utillization of unsecured lines\n#plt.hist(x = train_df.RevolvingUtilizationOfUnsecuredLines,bins=10,range=(0,1))\n#train_df[train_df.RevolvingUtilizationOfUnsecuredLines>0.4 & train_df.SeriousDlqin2yrs=1]\nhist_perc(train_df.RevolvingUtilizationOfUnsecuredLines,10,0,1.5)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Anamolies analysis in the data\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The consolidated description confirms the presence of outliers in the data.\n# Investigating further to detect outliers and visuallize them wrt different predictors.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#OutLierDetection(train_df,'RevolvingUtilizationOfUnsecuredLines','MonthlyIncome',outliers_fraction=.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotHist(train_df,'MonthlyIncome')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,Y_train,X_test,Y_test = train_test_split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Definition data cleaning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_df.iloc[:,1:]\nY_train = train_df.iloc[:,0:1]\n\nprint(X_train.shape,Y_train.shape)\nScoreCard = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ScoreCard.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import set_option\nfrom pandas.tools.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def GetBasedModel():\n    basedModels = []\n    basedModels.append(('LR'   , LogisticRegression()))\n    basedModels.append(('LDA'  , LinearDiscriminantAnalysis()))\n    basedModels.append(('KNN'  , KNeighborsClassifier()))\n    basedModels.append(('CART' , DecisionTreeClassifier()))\n    basedModels.append(('NB'   , GaussianNB()))\n    basedModels.append(('SVM'  , SVC(probability=True)))\n    basedModels.append(('AB'   , AdaBoostClassifier()))\n    basedModels.append(('GBM'  , GradientBoostingClassifier()))\n    basedModels.append(('RF'   , RandomForestClassifier()))\n    basedModels.append(('ET'   , ExtraTreesClassifier()))\n\n    \n    return basedModels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def BasedLine2(X_train, Y_train,models,scoring_type):\n    # Test options and evaluation metric\n    num_folds = 10\n    scoring = scoring_type\n\n    results = []\n    names = []\n    for name, model in models:\n        kfold = StratifiedKFold(n_splits=num_folds, random_state=SEED)\n        cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n    return names, results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PlotBoxR(object):\n    \n    \n    def __Trace(self,nameOfFeature,value): \n    \n        trace = go.Box(\n            y=value,\n            name = nameOfFeature,\n            marker = dict(\n                color = 'rgb(0, 128, 128)',\n            )\n        )\n        return trace\n\n    def PlotResult(self,names,results):\n        \n        data = []\n\n        for i in range(len(names)):\n            data.append(self.__Trace(names[i],results[i]))\n\n\n        py.iplot(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = GetBasedModel()\nnames,results = BasedLine2(X_train, Y_train,models,'f1_weighted')\nPlotBoxR().PlotResult(names,results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ScoreDataFrame(names,results,score_name):\n    def floatingDecimals(f_val, dec=3):\n        prc = \"{:.\"+str(dec)+\"f}\" \n    \n        return float(prc.format(f_val))\n\n    scores = []\n    for r in results:\n        scores.append(floatingDecimals(r.mean(),4))\n\n    scoreDataFrame = pd.DataFrame({'Model':names, score_name: scores})\n    return scoreDataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basedLineF1Score = ScoreDataFrame(names,results,'baseline_f1_Score')\nbasedLineF1Score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = GetBasedModel()\nnames,results = BasedLine2(X_train, Y_train,models,'accuracy')\nPlotBoxR().PlotResult(names,results)\n\nbasedLineAccuracyScore = ScoreDataFrame(names,results,'baseline_accuracy')\nbasedLineAccuracyScore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ScoreCard = pd.concat([basedLineAccuracyScore,\n                       basedLineF1Score], axis=1)\nScoreCard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndef GetScaledModel(nameOfScaler):\n    \n    if nameOfScaler == 'standard':\n        scaler = StandardScaler()\n    elif nameOfScaler =='minmax':\n        scaler = MinMaxScaler()\n\n    pipelines = []\n    pipelines.append((nameOfScaler+'LR'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression())])))\n    pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))\n    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier())])))\n    pipelines.append((nameOfScaler+'CART', Pipeline([('Scaler', scaler),('CART', DecisionTreeClassifier())])))\n    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))\n    pipelines.append((nameOfScaler+'SVM' , Pipeline([('Scaler', scaler),('SVM' , SVC())])))\n    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostClassifier())])  ))\n    pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingClassifier())])  ))\n    pipelines.append((nameOfScaler+'RF'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier())])  ))\n    pipelines.append((nameOfScaler+'ET'  , Pipeline([('Scaler', scaler),('ET'  , ExtraTreesClassifier())])  ))\n    \n\n    return pipelines ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = GetScaledModel('standard')\nnames,results = BasedLine2(X_train, Y_train,models,'f1_weighted')\nPlotBoxR().PlotResult(names,results)\nscaledScoreStandard = ScoreDataFrame(names,results,'standard_f1_score')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ScoreCard = pd.concat([ScoreCard,\n                           scaledScoreStandard], axis=1)\nScoreCard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = GetScaledModel('minmax')\nnames,results = BasedLine2(X_train, Y_train,models,'f1_weighted')\nPlotBoxR().PlotResult(names,results)\n\nscaledScoreMinMax = ScoreDataFrame(names,results,'minmax_f1_score')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ScoreCard = pd.concat([ScoreCard,\n                          scaledScoreMinMax], axis=1)\nScoreCard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndef GetScaledModel(nameOfScaler):\n    \n    if nameOfScaler == 'standard':\n        scaler = StandardScaler()\n    elif nameOfScaler =='minmax':\n        scaler = MinMaxScaler()\n\n    pipelines = []\n    pipelines.append((nameOfScaler+'LR'+'CW'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression(class_weight='balanced'))])))\n    pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))\n    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier())])))\n    pipelines.append((nameOfScaler+'CART'+'CW', Pipeline([('Scaler', scaler),('CART', DecisionTreeClassifier(class_weight='balanced'))])))\n    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))\n    pipelines.append((nameOfScaler+'SVM'+'CW' , Pipeline([('Scaler', scaler),('SVM' , SVC(class_weight='balanced'))])))\n    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostClassifier())])  ))\n    pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingClassifier())])  ))\n    pipelines.append((nameOfScaler+'RF'+'CW'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier(class_weight='balanced'))])  ))\n    pipelines.append((nameOfScaler+'ET'+'CW'  , Pipeline([('Scaler', scaler),('ET'  , ExtraTreesClassifier(class_weight='balanced'))])  ))\n    \n\n    return pipelines ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = GetScaledModel('standard')\nnames,results = BasedLine2(X_train, Y_train,models,'f1_weighted')\nPlotBoxR().PlotResult(names,results)\nscaledScoreStandardCW = ScoreDataFrame(names,results,'standard_f1_score')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ScoreCard = pd.concat([ScoreCard,\n                           scaledScoreStandardCW], axis=1)\nScoreCard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = GetScaledModel('minmax')\nnames,results = BasedLine2(X_train, Y_train,models,'f1_weighted')\nPlotBoxR().PlotResult(names,results)\n\nscaledScoreMinMaxCW = ScoreDataFrame(names,results,'minmax_f1_score')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ScoreCard = pd.concat([ScoreCard,\n                          scaledScoreMinMaxCW], axis=1)\nScoreCard","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Detect and Remove Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.reset_index(drop=True)\ndf_t = train_df.copy()\ndf_t_name = df_t.columns\ndf_name = train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def TurkyOutliers(df_out,nameOfFeature,drop=False):\n\n    #feature_number = 1\n    #df_out = df_t\n    #nameOfFeature = df_name[feature_number]\n    #drop = True\n    \n    valueOfFeature = df_out[nameOfFeature]\n    # Calculate Q1 (25th percentile of the data) for the given feature\n    Q1 = np.percentile(valueOfFeature, 25.)\n\n    # Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = np.percentile(valueOfFeature, 75.)\n\n    # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = (Q3-Q1)*1.5\n    # print \"Outlier step:\", step\n    outliers = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].index.tolist()\n    feature_outliers = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].values\n    # df[~((df[nameOfFeature] >= Q1 - step) & (df[nameOfFeature] <= Q3 + step))]\n\n\n    # Remove the outliers, if any were specified\n    print (\"Number of outliers (inc duplicates): {} and outliers: {}\".format(len(outliers), feature_outliers))\n    if drop:\n        good_data = df_out.drop(df_out.index[outliers]).reset_index(drop = True)\n        print (\"New dataset with removed outliers has {} samples with {} features each.\".format(*good_data.shape))\n        return good_data\n    else: \n        print (\"Nothing happens, df.shape = \",df_out.shape)\n        return df_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_number  = 1\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = TurkyOutliers(df_t,df_name[feature_number],True)\nOutLiersBox(df_clean,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_number  = 2\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_number  = 3\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.drop(df_clean[(df_clean['NumberOfTime30-59DaysPastDueNotWorse']>50) &(df_clean.SeriousDlqin2yrs==0)].index,inplace=True)\ndf_clean.shape\n#df_clean.drop(df_clean[df_clean['NumberOfTime30-59DaysPastDueNotWorse']>50],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_number  = 4\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_perc(df_clean.DebtRatio,100,5,3000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.drop(df_clean[(df_clean.DebtRatio>5)&(df_clean.SeriousDlqin2yrs==0)].index,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_number  = 5\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_perc(df_clean.MonthlyIncome,1000,0,100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.drop(df_clean[(df_clean.MonthlyIncome>30000)&(df_clean.SeriousDlqin2yrs==0)].index,inplace=True)\n#df_clean[(df_clean.MonthlyIncome>30000)&(df_clean.SeriousDlqin2yrs==0)].SeriousDlqin2yrs.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_number  = 6\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_perc(df_clean.NumberOfOpenCreditLinesAndLoans,10,20,50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.drop(df_clean[(df_clean.NumberOfOpenCreditLinesAndLoans>20)&(df_clean.SeriousDlqin2yrs==0)].index,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_number  = 7\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_clean.drop(df_clean[(df_clean.NumberOfTimes90DaysLate>90)&(df_clean.SeriousDlqin2yrs==0)].index,inplace=True)\ndf_clean[(df_clean.NumberOfTimes90DaysLate>10)].SeriousDlqin2yrs.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_number  = 8\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean[(df_clean['NumberRealEstateLoansOrLines']>10)].SeriousDlqin2yrs.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_number  = 9\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean[(df_clean['NumberOfTime60-89DaysPastDueNotWorse']>10)].SeriousDlqin2yrs.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_number  = 10\nprint(df_name[feature_number])\nOutLiersBox(df_t,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = TurkyOutliers(df_clean,df_name[feature_number],False)\nOutLiersBox(df_clean,df_name[feature_number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean[(df_clean['NumberOfDependents']>6)].SeriousDlqin2yrs.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.drop(df_clean[(df_clean.NumberOfDependents>6)&(df_clean.SeriousDlqin2yrs==0)].index,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.SeriousDlqin2yrs.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New X_train Y Train with cleaned dataset\n\nX_train = df_clean.iloc[:,1:]\nY_train = df_clean.iloc[:,0:1]\n\nprint(X_train.shape,Y_train.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ScoreCard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = GetScaledModel('standard')\nnames,results = BasedLine2(X_train, Y_train,models,'f1_weighted')\nPlotBoxR().PlotResult(names,results)\nscaledScoreStandardCW_Clean = ScoreDataFrame(names,results,'standard_f1_score_clean')\nScoreCard = pd.concat([ScoreCard,\n                           scaledScoreStandardCW_Clean], axis=1)\nScoreCard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = GetScaledModel('minmax')\nnames,results = BasedLine2(X_train, Y_train,models,'f1_weighted')\nPlotBoxR().PlotResult(names,results)\n\nscaledScoreMinMaxCW_clean = ScoreDataFrame(names,results,'minmax_f1_score_clean')\nScoreCard = pd.concat([ScoreCard,\n                          scaledScoreMinMaxCW_clean], axis=1)\nScoreCard","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### FEATURE SELECTION"},{"metadata":{"trusted":true},"cell_type":"code","source":"def HeatMap(df,x=True):\n        correlations = train_df.corr()\n        ## Create color map ranging between two colors\n        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n        fig, ax = plt.subplots(figsize=(10, 10))\n        fig = sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',square=True, linewidths=.5, annot=x, cbar_kws={\"shrink\": .75})\n        fig.set_xticklabels(fig.get_xticklabels(), rotation = 90, fontsize = 10)\n        fig.set_yticklabels(fig.get_yticklabels(), rotation = 0, fontsize = 10)\n        plt.tight_layout()\n        plt.show()\n\nHeatMap(train_df,x=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Importance\n\nclf = ExtraTreesClassifier(n_estimators=250,\n                              random_state=SEED)\n\nclf.fit(X_train, Y_train)\n\n# #############################################################################\n# Plot feature importance\nfeature_importance = clf.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, train_df.columns[sorted_idx])#boston.feature_names[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_clean[['NumberOfTime30-59DaysPastDueNotWorse','DebtRatio','MonthlyIncome','RevolvingUtilizationOfUnsecuredLines','NumberOfOpenCreditLinesAndLoans','age']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape,Y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"models = GetScaledModel('standard')\nnames,results = BasedLine2(X_train, Y_train,models,'f1_weighted')\nPlotBoxR().PlotResult(names,results)\nscaledScoreStandardCW_Clean_VI = ScoreDataFrame(names,results,'standard_f1_score_clean_vi')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ScoreCard = pd.concat([ScoreCard,\n                           scaledScoreStandardCW_Clean_VI], axis=1)\nScoreCard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ScoreCard = pd.concat([ScoreCard.iloc[0:11,0:15],ScoreCard.iloc[0:11,19:20]],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ScoreCard","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### HYPER PARAMETER TUNING"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom scipy.stats import uniform","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_test = test_df.sample(frac=0.1)[['SeriousDlqin2yrs','NumberOfTime30-59DaysPastDueNotWorse','DebtRatio','MonthlyIncome','RevolvingUtilizationOfUnsecuredLines','NumberOfOpenCreditLinesAndLoans','age']]\nX_test.drop(test_df[['SeriousDlqin2yrs']],axis=1,inplace=True)\nX_test = X_test.dropna()\n#Y_test = X_test['SeriousDlqin2yrs']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RandomSearch(object):\n    \n    def __init__(self,X_train,y_train,model,hyperparameters):\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n        \n    def RandomSearch(self):\n        # Create randomized search 10-fold cross validation and 100 iterations\n        cv = 10\n        clf = RandomizedSearchCV(self.model,\n                                 self.hyperparameters,\n                                 random_state=1,\n                                 n_iter=100,\n                                 cv=cv,\n                                 verbose=0,\n                                 n_jobs=-1,\n                                 )\n        # Fit randomized search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n    \n    def BestModelPridict(self,X_test):\n        \n        best_model,_ = self.RandomSearch()\n        pred = best_model.predict(X_test)\n        return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GridSearch(object):\n    \n    def __init__(self,X_train,y_train,model,hyperparameters):\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n        \n    def GridSearch(self):\n        # Create randomized search 10-fold cross validation and 100 iterations\n        cv = 10\n        clf = GridSearchCV(self.model,\n                                 self.hyperparameters,\n                                 cv=cv,\n                                 verbose=0,\n                                 n_jobs=-1,\n                                 )\n        # Fit randomized search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n    \n    def BestModelPridict(self,X_test):\n        \n        best_model,_ = self.GridSearch()\n        pred = best_model.predict(X_test)\n        return pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### LOGISTIC REGRESSION"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model\nmodel = LogisticRegression()\n# Create regularization penalty space\npenalty = ['l1', 'l2']\n\n# Create regularization hyperparameter distribution using uniform distribution\nC = uniform(loc=0, scale=4)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_RandSearch = RandomSearch(X_train,Y_train,model,hyperparameters)\nLR_best_model,LR_best_params = LR_RandSearch.RandomSearch()\nPrediction_LR = LR_RandSearch.BestModelPridict(X_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cp = classification_report(Y_train,Prediction_LR)\nprint(cp)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hpt = [['hptuningLR',floatingDecimals(0.929252,4),0.91]]\nhpt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_KNN = KNeighborsClassifier()\n\nneighbors = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\nparam_grid = dict(n_neighbors=neighbors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN_GridSearch = GridSearch(X_train,Y_train,model_KNN,param_grid)\nPrediction_KNN = KNN_GridSearch.BestModelPridict(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cp = classification_report(Y_train,Prediction_KNN)\nprint(cp)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hpt.append(['modehptuningKNN',floatingDecimals(0.926401,4),0.89])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### SVC Model ####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\nkernel_values = [ 'linear' , 'poly' , 'rbf' , 'sigmoid' ]\nparam_grid = dict(C=c_values, kernel=kernel_values)\nmodel_SVC = SVC()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SVC_GridSearch = GridSearch(X_train,Y_train,model_SVC,param_grid,scoring='f1_weighted')\nPrediction_SVC = SVC_GridSearch.BestModelPridict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Decision Tree ####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import randint\nmax_depth_value = [3, None]\nmax_features_value =  randint(1, 4)\nmin_samples_leaf_value = randint(1, 4)\ncriterion_value = [\"gini\", \"entropy\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = dict(max_depth = max_depth_value,\n                  max_features = max_features_value,\n                  min_samples_leaf = min_samples_leaf_value,\n                  criterion = criterion_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_CART = DecisionTreeClassifier()\nCART_RandSearch = RandomSearch(X_train,y_train,model_CART,param_grid)\nPrediction_CART = CART_RandSearch.BestModelPridict(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### ADABOOST CLASSIFIER ###","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate_value = [.01,.05,.1,.5,1]\nn_estimators_value = [50,100,150,200,250,300]\n\nparam_grid = dict(learning_rate=learning_rate_value, n_estimators=n_estimators_value)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_Ad = AdaBoostClassifier()\nAd_GridSearch = GridSearch(X_train,Y_train,model_Ad,param_grid)\nPrediction_Ad = Ad_GridSearch.BestModelPridict(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### RANDOM FOREST ####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators_value = [50,100,150,200,250,300]\ncriterion_val = [\"gini\", \"entropy\"]\n\nparam_grid = dict(criterion=criterion_val, n_estimators=n_estimators_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_RF = RandomForestClassifier()\nGB_GridSearch = GridSearch(X_train,Y_train,model_RF,param_grid)\nPrediction_GB = GB_GridSearch.BestModelPridict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### VOTING ENSEMBLE ####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {'C': 0.7678243129497218, 'penalty': 'l1'}\nmodel1 = LogisticRegression(**param)\n\nparam = {'n_neighbors': 15}\nmodel2 = KNeighborsClassifier(**param)\n\nparam = {'C': 1.7, 'kernel': 'linear'}\nmodel3 = SVC(**param)\n\nparam = {'criterion': 'gini', 'max_depth': 3, 'max_features': 2, 'min_samples_leaf': 3}\nmodel4 = DecisionTreeClassifier(**param)\n\nparam = {'learning_rate': 0.05, 'n_estimators': 150}\nmodel5 = AdaBoostClassifier(**param)\n\nparam = {'learning_rate': 0.01, 'n_estimators': 100}\nmodel6 = GradientBoostingClassifier(**param)\n\nmodel7 = GaussianNB()\n\nmodel8 = RandomForestClassifier()\n\nmodel9 = ExtraTreesClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the sub models\nestimators = [('LR',model1), ('KNN',model2), ('SVC',model3),\n              ('DT',model4), ('ADa',model5), ('GB',model6),\n              ('NB',model7), ('RF',model8),  ('ET',model9)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the ensemble model\nkfold = StratifiedKFold(n_splits=10, random_state=SEED)\nensemble = VotingClassifier(estimators)\nresults = cross_val_score(ensemble, X_train,Y_train, cv=kfold)\nprint('Accuracy on train: ',results.mean())\nensemble_model = ensemble.fit(X_train,Y_train)\npred = ensemble_model.predict(X_test)\n;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_models():\n    \"\"\"Generate a library of base learners.\"\"\"\n    param = {'C': 0.7678243129497218, 'penalty': 'l1'}\n    model1 = LogisticRegression(**param)\n\n    param = {'n_neighbors': 15}\n    model2 = KNeighborsClassifier(**param)\n\n    param = {'C': 1.7, 'kernel': 'linear', 'probability':True}\n    model3 = SVC(**param)\n\n    param = {'criterion': 'gini', 'max_depth': 3, 'max_features': 2, 'min_samples_leaf': 3}\n    model4 = DecisionTreeClassifier(**param)\n\n    param = {'learning_rate': 0.05, 'n_estimators': 150}\n    model5 = AdaBoostClassifier(**param)\n\n    param = {'learning_rate': 0.01, 'n_estimators': 100}\n    model6 = GradientBoostingClassifier(**param)\n\n    model7 = GaussianNB()\n\n    model8 = RandomForestClassifier()\n\n    model9 = ExtraTreesClassifier()\n\n    models = {'LR':model1, 'KNN':model2, 'SVC':model3,\n              'DT':model4, 'ADa':model5, 'GB':model6,\n              'NB':model7, 'RF':model8,  'ET':model9\n              }\n\n    return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### ERROR CORRELATION ####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_predict(model_list,xtrain, xtest, ytrain, ytest):\n    \"\"\"Fit models in list on training set and return preds\"\"\"\n    P = np.zeros((ytest.shape[0], len(model_list)))\n    P = pd.DataFrame(P)\n\n    print(\"Fitting models.\")\n    cols = list()\n    for i, (name, m) in enumerate(models.items()):\n        print(\"%s...\" % name, end=\" \", flush=False)\n        m.fit(xtrain, ytrain)\n        P.iloc[:, i] = m.predict_proba(xtest)[:, 1]\n        cols.append(name)\n        print(\"done\")\n\n    P.columns = cols\n    print(\"Done.\\n\")\n    return P","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = get_models()\nP = train_predict(models,X_train,X_test,Y_train,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install mlens\nfrom mlens.visualization import corrmat\n\ncorrmat(P.corr(), inflate=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_learners = get_models()\nmeta_learner = GradientBoostingClassifier(\n    n_estimators=1000,\n    loss=\"exponential\",\n    max_features=6,\n    max_depth=3,\n    subsample=0.5,\n    learning_rate=0.001, \n    random_state=SEED\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_learners","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlens.ensemble import SuperLearner\n\n# Instantiate the ensemble with 10 folds\nsl = SuperLearner(\n    folds=10,\n    random_state=SEED,\n    verbose=2,\n    backend=\"multiprocessing\"\n)\n\n# Add the base learners and the meta learner\nsl.add(list(base_learners.values()), proba=True) \nsl.add_meta(meta_learner, proba=True)\n\n# Train the ensemble\nsl.fit(X_train, Y_train)\n\n# Predict the test set\np_sl = sl.predict_proba(X_test)\n\n# print(\"\\nSuper Learner ROC-AUC score: %.3f\" % roc_auc_score(y_test_sc, p_sl[:, 1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pp = []\nfor p in p_sl[:, 1]:\n    if p>0.5:\n        pp.append(1.)\n    else:\n        pp.append(0.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(Y_test == pp).mean()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python36","display_name":"Python 3.6","language":"python"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","version":"3.6.6","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"}}},"nbformat":4,"nbformat_minor":1}