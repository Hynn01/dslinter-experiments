{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 align=center><font size = 5>CNN transfer learning detect cracks in concrete</font></h1>","metadata":{}},{"cell_type":"markdown","source":"The data and the estrucutre of the notebook is obtained from the IBM AI engineering course.This notebook is part of the final project of the course. \nIf you are another student of the course, please be careful to copy\n\n<a href=\"https://www.coursera.org/learn/ai-deep-learning-capstone?specialization=ai-engineer\">Link to the course</a>\n\n","metadata":{}},{"cell_type":"markdown","source":"Detect concrete cracks using convolutional neural networks pre trained. \nIn this case use ResNet50 and VGG16\n\n0. <a href=\"https://keras.io/api/applications/\">Keras documentation for pre-trained models</a>\n1. <a href=\"https://keras.io/api/applications/vgg/\">Keras documentation for VGG16</a>\n2. <a href=\"https://towardsdatascience.com/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c\">Example for VGG16</a>\n3. <a href=\"https://keras.io/api/applications/resnet/#resnet50-function\">Keras documentation for ResNet50</a>","metadata":{}},{"cell_type":"markdown","source":"<h2 align=center><font size = 5>VGG16 Architecture</font></h2>\n<img align= center src = \"https://www.researchgate.net/profile/Max-Ferguson/publication/322512435/figure/fig3/AS:697390994567179@1543282378794/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only.png\" width = 400> </a>\n","metadata":{}},{"cell_type":"markdown","source":"## 1.-Import Libraries and packages","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\nimport matplotlib as plt \nimport numpy as np \nimport tensorflow.keras.callbacks as keras_callback\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.applications import VGG16\nfrom keras.applications.vgg16 import preprocess_input as preprocess_input_vgg16\nimport tensorflow","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-08T11:54:03.63847Z","iopub.execute_input":"2022-05-08T11:54:03.638732Z","iopub.status.idle":"2022-05-08T11:54:03.64492Z","shell.execute_reply.started":"2022-05-08T11:54:03.638702Z","shell.execute_reply":"2022-05-08T11:54:03.643824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tensorflow.__version__","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:54:05.130359Z","iopub.execute_input":"2022-05-08T11:54:05.130968Z","iopub.status.idle":"2022-05-08T11:54:05.136401Z","shell.execute_reply.started":"2022-05-08T11:54:05.130932Z","shell.execute_reply":"2022-05-08T11:54:05.135473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.__version__","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:53:52.995945Z","iopub.execute_input":"2022-05-08T11:53:52.996236Z","iopub.status.idle":"2022-05-08T11:53:53.004892Z","shell.execute_reply.started":"2022-05-08T11:53:52.996207Z","shell.execute_reply":"2022-05-08T11:53:53.004077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.-Get the data ","metadata":{}},{"cell_type":"code","source":"!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:54:08.446035Z","iopub.execute_input":"2022-05-08T11:54:08.446588Z","iopub.status.idle":"2022-05-08T11:54:19.006651Z","shell.execute_reply.started":"2022-05-08T11:54:08.446548Z","shell.execute_reply":"2022-05-08T11:54:19.005698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -qq concrete_data_week3.zip","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:54:19.008718Z","iopub.execute_input":"2022-05-08T11:54:19.008977Z","iopub.status.idle":"2022-05-08T11:54:26.139379Z","shell.execute_reply.started":"2022-05-08T11:54:19.008947Z","shell.execute_reply":"2022-05-08T11:54:26.136645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.-Create the data generator ","metadata":{}},{"cell_type":"code","source":"#Data augmentation isn't necesarie for this type and quantity of images.\n#train_datagen_vgg16 = ImageDataGenerator(rescale = 1./255.,\n#                                   rotation_range = 40,\n#                                   width_shift_range = 0.2,\n#                                   height_shift_range = 0.2,\n#                                   #shear_range = 0.2,\n#                                   #zoom_range = 0.2,\n#                                   horizontal_flip = True)\ntrain_datagen_vgg16 = ImageDataGenerator(rescale = 1./255.,\n                                   preprocessing_function = preprocess_input_vgg16)\n\ntest_datagen_vgg16 = ImageDataGenerator( rescale = 1.0/255.,\n                                   preprocessing_function = preprocess_input_vgg16)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:54:44.471177Z","iopub.execute_input":"2022-05-08T11:54:44.47146Z","iopub.status.idle":"2022-05-08T11:54:44.477061Z","shell.execute_reply.started":"2022-05-08T11:54:44.471428Z","shell.execute_reply":"2022-05-08T11:54:44.476078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size_training = 100\nbatch_size_validation = 100\nimage_resize = 224\nnum_classes = 2","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:54:46.235855Z","iopub.execute_input":"2022-05-08T11:54:46.236474Z","iopub.status.idle":"2022-05-08T11:54:46.240783Z","shell.execute_reply.started":"2022-05-08T11:54:46.236432Z","shell.execute_reply":"2022-05-08T11:54:46.239717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator_vgg16 = train_datagen_vgg16.flow_from_directory(\n    './concrete_data_week3/train',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_training,\n    class_mode='categorical')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:54:47.178537Z","iopub.execute_input":"2022-05-08T11:54:47.178837Z","iopub.status.idle":"2022-05-08T11:54:48.686717Z","shell.execute_reply.started":"2022-05-08T11:54:47.178793Z","shell.execute_reply":"2022-05-08T11:54:48.685945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_generator_vgg16 = test_datagen_vgg16.flow_from_directory(\n    './concrete_data_week3/valid',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_training,\n    class_mode='categorical')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:54:48.688187Z","iopub.execute_input":"2022-05-08T11:54:48.689021Z","iopub.status.idle":"2022-05-08T11:54:49.117063Z","shell.execute_reply.started":"2022-05-08T11:54:48.688979Z","shell.execute_reply":"2022-05-08T11:54:49.116274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_datagen_resnet = ImageDataGenerator(rescale = 1./255.,\n                                   preprocessing_function = preprocess_input)\n\ntest_datagen_resnet = ImageDataGenerator( rescale = 1.0/255.,\n                                   preprocessing_function = preprocess_input)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:54:50.988248Z","iopub.execute_input":"2022-05-08T11:54:50.988563Z","iopub.status.idle":"2022-05-08T11:54:50.998219Z","shell.execute_reply.started":"2022-05-08T11:54:50.988526Z","shell.execute_reply":"2022-05-08T11:54:50.997337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator_resnet = train_datagen_resnet.flow_from_directory(\n    './concrete_data_week3/train',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_training,\n    class_mode='categorical')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:54:51.272506Z","iopub.execute_input":"2022-05-08T11:54:51.27329Z","iopub.status.idle":"2022-05-08T11:54:52.713432Z","shell.execute_reply.started":"2022-05-08T11:54:51.273237Z","shell.execute_reply":"2022-05-08T11:54:52.712643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_generator_resnet = test_datagen_resnet.flow_from_directory(\n    './concrete_data_week3/valid',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_training,\n    class_mode='categorical')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:54:53.24219Z","iopub.execute_input":"2022-05-08T11:54:53.243098Z","iopub.status.idle":"2022-05-08T11:54:53.887609Z","shell.execute_reply.started":"2022-05-08T11:54:53.243048Z","shell.execute_reply":"2022-05-08T11:54:53.886698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.-Create the neural net","metadata":{}},{"cell_type":"code","source":"model_vgg16 = Sequential([\n    VGG16(include_top=False,pooling='avg',weights='imagenet'),\n    Dense(num_classes, activation='softmax')])\nmodel_vgg16.layers[0].trainable = False\nmodel_vgg16.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel_vgg16.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:54:56.330589Z","iopub.execute_input":"2022-05-08T11:54:56.331188Z","iopub.status.idle":"2022-05-08T11:54:59.280641Z","shell.execute_reply.started":"2022-05-08T11:54:56.33115Z","shell.execute_reply":"2022-05-08T11:54:59.279911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_resnet = Sequential([\n    ResNet50(include_top=False,pooling='avg',weights='imagenet'),\n    Dense(num_classes, activation='softmax')])\nmodel_resnet.layers[0].trainable = False\nmodel_resnet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel_resnet.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:54:59.282189Z","iopub.execute_input":"2022-05-08T11:54:59.282433Z","iopub.status.idle":"2022-05-08T11:55:02.470707Z","shell.execute_reply.started":"2022-05-08T11:54:59.282398Z","shell.execute_reply":"2022-05-08T11:55:02.469926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.-Train the model \n","metadata":{}},{"cell_type":"code","source":"steps_per_epoch_training = len(train_generator_vgg16) #train_generator\nsteps_per_epoch_validation = len(valid_generator_vgg16)\nnum_epochs = 5\n\nclass myCallback(keras_callback.Callback):\n      def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_accuracy')>0.99):\n          print(\"\\nReached 99% accuracy in validation data so cancelling training!\")\n          self.model.stop_training = True\nclass BCP(keras_callback.Callback):  \n    def __init__(self):\n        self.batch_accuracy = [] # accuracy at given batch\n        self.batch_loss = [] # loss at given batch \n    def on_batch_end(self, batch, logs={}):                \n        self.batch_accuracy.append(logs.get('accuracy'))\n        self.batch_loss.append(logs.get('loss'))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:56:18.282965Z","iopub.execute_input":"2022-05-08T11:56:18.2839Z","iopub.status.idle":"2022-05-08T11:56:18.291927Z","shell.execute_reply.started":"2022-05-08T11:56:18.283833Z","shell.execute_reply":"2022-05-08T11:56:18.291096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = myCallback()\nBCP_vgg16 = BCP()\nfit_history_vgg16 = model_vgg16.fit_generator(\n    train_generator_vgg16,\n    steps_per_epoch=steps_per_epoch_training,\n    epochs=num_epochs,\n    validation_data=valid_generator_vgg16,\n    validation_steps=steps_per_epoch_validation,\n    verbose=1,\n    callbacks=[callbacks,BCP_vgg16])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T11:56:20.022267Z","iopub.execute_input":"2022-05-08T11:56:20.022542Z","iopub.status.idle":"2022-05-08T12:03:04.854785Z","shell.execute_reply.started":"2022-05-08T11:56:20.022513Z","shell.execute_reply":"2022-05-08T12:03:04.854068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = myCallback()\nBCP_resnet = BCP()\nfit_history_resnet = model_resnet.fit_generator(\n    train_generator_resnet,\n    steps_per_epoch=steps_per_epoch_training,\n    epochs=num_epochs,\n    validation_data=valid_generator_resnet,\n    validation_steps=steps_per_epoch_validation,\n    verbose=1,\n    callbacks=[callbacks,BCP_resnet])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:09:48.305832Z","iopub.execute_input":"2022-05-08T12:09:48.306583Z","iopub.status.idle":"2022-05-08T12:21:02.973096Z","shell.execute_reply.started":"2022-05-08T12:09:48.306544Z","shell.execute_reply":"2022-05-08T12:21:02.972325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.1 save the model","metadata":{}},{"cell_type":"code","source":"model_vgg16.save('model_vgg16.h5')\n#model_resnet.save('model_resnet.h')\n\nimport pandas as pd\n\n#hist_df_resnet = pd.DataFrame(fit_history_resnet.history) \n#hist_df_resnet.to_csv('history_resnet')\nhist_df_vgg16 = pd.DataFrame(fit_history_vgg16.history)\nhist_df_vgg16.to_csv('history_vgg16')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:23:18.961793Z","iopub.execute_input":"2022-05-08T12:23:18.962456Z","iopub.status.idle":"2022-05-08T12:23:19.10202Z","shell.execute_reply.started":"2022-05-08T12:23:18.962419Z","shell.execute_reply":"2022-05-08T12:23:19.101188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_history = pd.DataFrame (BCP_vgg16.batch_accuracy, columns = ['batch accuracy'])\ndf_history.head()\ndf_history.to_csv('df_history.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:23:22.034974Z","iopub.execute_input":"2022-05-08T12:23:22.035519Z","iopub.status.idle":"2022-05-08T12:23:22.054411Z","shell.execute_reply.started":"2022-05-08T12:23:22.035469Z","shell.execute_reply":"2022-05-08T12:23:22.053566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Guardar configuraciÃ³n JSON en el disco\njson_config = model_vgg16.to_json()\nwith open('model_vgg16_config.json', 'w') as json_file:\n    json_file.write(json_config)\n# Guardar pesos en el disco\nmodel_vgg16.save_weights('model_vgg16_weights.h5')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:23:22.454064Z","iopub.execute_input":"2022-05-08T12:23:22.454614Z","iopub.status.idle":"2022-05-08T12:23:22.557457Z","shell.execute_reply.started":"2022-05-08T12:23:22.454575Z","shell.execute_reply":"2022-05-08T12:23:22.556714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('model_config.json') as json_file:\n    json_config = json_file.read()\nnew_model = keras.models.model_from_json(json_config)\nnew_model.load_weights('path_to_my_weights.h5')\n\nconfig = model.get_config()\nweights = model.get_weights()\n\nnew_model = keras.Model.from_config(config)\nnew_model.set_weights(weights)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.-Visualize the learning curves and compare models","metadata":{}},{"cell_type":"code","source":"valid_generator = test_datagen_vgg16.flow_from_directory(\n    './concrete_data_week3/valid',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_training,\n    shuffle = False,\n    class_mode='categorical')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:27:32.92207Z","iopub.execute_input":"2022-05-08T12:27:32.922732Z","iopub.status.idle":"2022-05-08T12:27:33.349246Z","shell.execute_reply.started":"2022-05-08T12:27:32.922693Z","shell.execute_reply":"2022-05-08T12:27:33.348454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_vgg16 = model_vgg16.evaluate_generator(valid_generator_vgg16)\nprint(\"%s%s: %.2f%%\" % (\"VGG16 evaluate_generator \",model_vgg16.metrics_names[1], scores_vgg16[1]*100))\n    \nscores_resnet = model_resnet.evaluate_generator(valid_generator_vgg16)    \nprint(\"%s%s: %.2f%%\" % (\"ResNet50 evaluate_generator \",model_resnet.metrics_names[1], scores_resnet[1]*100))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:28:53.022184Z","iopub.execute_input":"2022-05-08T12:28:53.022476Z","iopub.status.idle":"2022-05-08T12:29:59.69666Z","shell.execute_reply.started":"2022-05-08T12:28:53.022446Z","shell.execute_reply":"2022-05-08T12:29:59.695939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames = valid_generator_vgg16.filenames\nnb_samples = len(filenames)\npredict_vgg16 = model_vgg16.predict_generator(valid_generator_vgg16,steps = nb_samples)\npredict_vgg16[0:6]","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:30:15.279602Z","iopub.execute_input":"2022-05-08T12:30:15.279882Z","iopub.status.idle":"2022-05-08T12:30:43.807197Z","shell.execute_reply.started":"2022-05-08T12:30:15.279831Z","shell.execute_reply":"2022-05-08T12:30:43.806507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in predict_vgg16[0:6,0]:\n    if i >0.5:\n        print(\"Positive\")\n    else: \n        print(\"Negative\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:31:40.40616Z","iopub.execute_input":"2022-05-08T12:31:40.406903Z","iopub.status.idle":"2022-05-08T12:31:40.412464Z","shell.execute_reply.started":"2022-05-08T12:31:40.406841Z","shell.execute_reply":"2022-05-08T12:31:40.411735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc_v = BCP_vgg16.batch_accuracy\nacc_res =  BCP_resnet.batch_accuracy\n\nepochs = range(len(acc_v))\nplt.plot(epochs, acc_v, 'r', label='Training accuracy VGG16')\nplt.plot(range(len(acc_res)), acc_res, 'b', label='Training accuracy ResNet50')\nplt.title('Training accuracy VGG16 and ResNet50')\nplt.legend(loc=0)\nplt.figure()\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:34:42.885567Z","iopub.execute_input":"2022-05-08T12:34:42.885827Z","iopub.status.idle":"2022-05-08T12:34:43.083373Z","shell.execute_reply.started":"2022-05-08T12:34:42.885796Z","shell.execute_reply":"2022-05-08T12:34:43.082714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_v = fit_history_vgg16.history['accuracy']\nval_acc_v = fit_history_vgg16.history['val_accuracy']\nacc_res =  fit_history_resnet.history['accuracy']\nval_acc_res =  fit_history_resnet.history['val_accuracy']\nepochs = range(len(acc_v))\nplt.plot(epochs, acc_v, 'r', label='Training accuracy VGG16')\nplt.plot(epochs, val_acc_v, 'b', label='Validation accuracy VGG16')\nplt.plot(range(len(acc_res)), acc_res, 'g', marker='o', label='Trainign accuracy ResNet50')\nplt.plot(range(len(acc_res)), val_acc_res, 'y', marker='o', label='Validation accuracy ResNet50')\nplt.title('Training and validation accuracy ')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:35:13.799128Z","iopub.execute_input":"2022-05-08T12:35:13.799395Z","iopub.status.idle":"2022-05-08T12:35:14.02609Z","shell.execute_reply.started":"2022-05-08T12:35:13.799365Z","shell.execute_reply":"2022-05-08T12:35:14.025144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.-Make prediction and show mismatching","metadata":{}},{"cell_type":"code","source":"import os\nfrom keras.preprocessing import image","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:35:21.632482Z","iopub.execute_input":"2022-05-08T12:35:21.632792Z","iopub.status.idle":"2022-05-08T12:35:21.636771Z","shell.execute_reply.started":"2022-05-08T12:35:21.632758Z","shell.execute_reply":"2022-05-08T12:35:21.635828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_neg = os.listdir('./concrete_data_week3/valid/negative/')\nimages_pos = os.listdir('./concrete_data_week3/valid/positive/')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:35:22.092024Z","iopub.execute_input":"2022-05-08T12:35:22.092334Z","iopub.status.idle":"2022-05-08T12:35:22.106081Z","shell.execute_reply.started":"2022-05-08T12:35:22.092289Z","shell.execute_reply":"2022-05-08T12:35:22.10528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_neg = []\nmissclassified_neg = []\ncontador = 0\nn_images = 1000\nfor i in images_neg:\n    path = './concrete_data_week3/valid/negative/'+i\n    img = image.load_img(path, target_size=(224, 224))\n    x = image.img_to_array(img)/255.\n    x = np.expand_dims(x, axis=0)\n    images = np.vstack([x])\n    classes = model_vgg16.predict(images, batch_size=10)\n    prediction_neg.append(classes)\n    contador = contador +1\n    if classes[0][0] < 0.5:\n        missclassified_neg.append(path)\n        print(\"mistake  in classification\")\n        plt.imshow(image.img_to_array(img).astype(np.uint8))\n        plt.title('negative probability:'+str(classes[0][0])+str(path))\n        plt.show()\n    if contador == n_images:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:35:27.008285Z","iopub.execute_input":"2022-05-08T12:35:27.008549Z","iopub.status.idle":"2022-05-08T12:37:06.783685Z","shell.execute_reply.started":"2022-05-08T12:35:27.008518Z","shell.execute_reply":"2022-05-08T12:37:06.782923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_pos = []\nmissclassified_pos = []\ncontador = 0\nn_images = 1000\nfor i in images_pos:\n    path = './concrete_data_week3/valid/positive/'+i\n    img = image.load_img(path, target_size=(224, 224))\n    x = image.img_to_array(img)/255.\n    x = np.expand_dims(x, axis=0)\n    images = np.vstack([x])\n    classes = model_vgg16.predict(images, batch_size=10)\n    prediction_pos.append(classes)\n    contador = contador +1\n    if classes[0][1] < 0.5:\n        missclassified_pos.append(path)\n        print(\"mistake  in classification\")\n        plt.imshow(image.img_to_array(img).astype(np.uint8))\n        plt.title('positive probability:'+str(classes[0][1])+str(path))\n        plt.show()\n    if contador == n_images:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:37:55.319664Z","iopub.execute_input":"2022-05-08T12:37:55.320394Z","iopub.status.idle":"2022-05-08T12:38:47.201292Z","shell.execute_reply.started":"2022-05-08T12:37:55.320337Z","shell.execute_reply":"2022-05-08T12:38:47.200549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9.-Retrain the model","metadata":{}},{"cell_type":"code","source":"steps_per_epoch_training = len(train_generator_vgg16) #train_generator\nsteps_per_epoch_validation = len(valid_generator_vgg16)\nnum_epochs = 10\n\nBCP_vgg16 = BCP()\nfit_history_vgg16 = model_vgg16.fit_generator(\n    train_generator_vgg16,\n    steps_per_epoch=steps_per_epoch_training,\n    epochs=num_epochs,\n    validation_data=valid_generator_vgg16,\n    validation_steps=steps_per_epoch_validation,\n    verbose=1,\n    callbacks=[BCP_vgg16])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:40:48.154822Z","iopub.execute_input":"2022-05-08T12:40:48.155543Z","iopub.status.idle":"2022-05-08T13:03:39.98464Z","shell.execute_reply.started":"2022-05-08T12:40:48.155505Z","shell.execute_reply":"2022-05-08T13:03:39.983754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this case may be acceptable misclassified a negative case as positive. Because we need detect all cracks. \n\nWe can modify the probability in predictions for classified as positive the case with a probability major to 0.4","metadata":{}},{"cell_type":"code","source":"for path in missclassified_neg:\n    img = image.load_img(path, target_size=(224, 224))\n    x = image.img_to_array(img)/255.\n    x = np.expand_dims(x, axis=0)\n    images = np.vstack([x])\n    classes = model_vgg16.predict(images, batch_size=10)\n    if classes[0][0] < 0.4:\n        print(\"mistake  in classification\")\n        plt.imshow(image.img_to_array(img).astype(np.uint8))\n        plt.title('negative probability:'+str(classes[0][0])+str(path))\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:03:39.986713Z","iopub.execute_input":"2022-05-08T13:03:39.986981Z","iopub.status.idle":"2022-05-08T13:03:40.525018Z","shell.execute_reply.started":"2022-05-08T13:03:39.986944Z","shell.execute_reply":"2022-05-08T13:03:40.524246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for path in missclassified_pos:\n    img = image.load_img(path, target_size=(224, 224))\n    x = image.img_to_array(img)/255.\n    x = np.expand_dims(x, axis=0)\n    images = np.vstack([x])\n    classes = model_vgg16.predict(images, batch_size=10)\n    if classes[0][1] < 0.4:\n        print(\"mistake  in classification\")\n        plt.imshow(image.img_to_array(img).astype(np.uint8))\n        plt.title('negative probability:'+str(classes[0][1])+str(path))\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:03:40.526533Z","iopub.execute_input":"2022-05-08T13:03:40.526773Z","iopub.status.idle":"2022-05-08T13:03:41.957362Z","shell.execute_reply.started":"2022-05-08T13:03:40.526736Z","shell.execute_reply":"2022-05-08T13:03:41.956635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}