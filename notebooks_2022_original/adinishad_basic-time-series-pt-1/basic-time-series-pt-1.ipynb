{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Let a time forecasting problem equation is, \n\n$T_{t} = 300 + 0.2t + 5\\sin (\\frac{t}{5}) + 20\\cos (\\frac{t}{24}) + 100\\sin (\\frac{t}{120}) + 20R_{t} $\n\n**This equation has a Trend, random deviation and Three Seasonable Period**","metadata":{}},{"cell_type":"code","source":"import random\nimport copy\nfrom math import sin, cos\nfrom scipy import interpolate\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom sklearn import model_selection\nimport torch.nn.functional as F\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-23T05:18:15.927347Z","iopub.execute_input":"2022-04-23T05:18:15.928143Z","iopub.status.idle":"2022-04-23T05:18:15.936795Z","shell.execute_reply.started":"2022-04-23T05:18:15.928109Z","shell.execute_reply":"2022-04-23T05:18:15.93603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_time_series_data(length):\n    a = 0.2\n    b = 300\n    c = 20\n    ls = 5\n    ms = 20\n    gs = 100\n    ts = []\n    for i in range(length):\n        ts.append(b + a * i + ls * sin(i / 5) + ms * cos(i / 24) + gs * sin(i / 120) + c * random.random())\n    return ts\n\nif __name__ == \"__main__\":\n    data = get_time_series_data(3000)\n    plt.figure(figsize=(10, 8))\n    plt.plot(data)\n    plt.title(\"Dataset\")\n    plt.grid()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T05:25:47.605254Z","iopub.execute_input":"2022-04-23T05:25:47.605921Z","iopub.status.idle":"2022-04-23T05:25:47.803653Z","shell.execute_reply.started":"2022-04-23T05:25:47.605868Z","shell.execute_reply":"2022-04-23T05:25:47.803055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now we have to preprare a time series dataset as the series of inputs and outputs, and it is done using sliding window technique.","metadata":{}},{"cell_type":"code","source":"def get_time_series_datasets(features, ts_len):\n    X = []\n    Y = []\n    for i in range(features + 1, ts_len):\n        ts = get_time_series_data(ts_len)\n        X.append(ts[i-(features+1):i-1])\n        Y.append([ts[i]])\n\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n        X, Y, test_size=0.3, shuffle=False\n    )\n\n    X_val, X_test, y_val, y_test = model_selection.train_test_split(\n        X_test, y_test, test_size=0.5, shuffle=False\n    )\n    \n    X_train = torch.tensor(data=X_train)\n    X_test = torch.tensor(data=X_test)\n    y_train = torch.tensor(data=y_train)\n    y_test = torch.tensor(data=y_test)\n    X_val = torch.tensor(data=X_val)\n    y_val = torch.tensor(data=y_val)\n\n    return X_train, X_val, X_test, y_train, y_val, y_test","metadata":{"execution":{"iopub.status.busy":"2022-04-23T05:12:12.369676Z","iopub.execute_input":"2022-04-23T05:12:12.370437Z","iopub.status.idle":"2022-04-23T05:12:12.377727Z","shell.execute_reply.started":"2022-04-23T05:12:12.370393Z","shell.execute_reply":"2022-04-23T05:12:12.377036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create a fully connected neural network","metadata":{}},{"cell_type":"code","source":"class FCNN(torch.nn.Module):\n    def __init__(self, n_inp, l_1, l_2, n_out):\n        super(FCNN, self).__init__()\n        self.lin1 = nn.Linear(n_inp, l_1)\n        self.lin2 = nn.Linear(l_1, l_2)\n        self.lin3 = nn.Linear(l_2, n_out)\n\n    def forward(self, x):\n        x = F.relu(self.lin1(x))\n        x = F.relu(self.lin2(x))\n        x = self.lin3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-04-23T05:12:26.250902Z","iopub.execute_input":"2022-04-23T05:12:26.2512Z","iopub.status.idle":"2022-04-23T05:12:26.257631Z","shell.execute_reply.started":"2022-04-23T05:12:26.251168Z","shell.execute_reply":"2022-04-23T05:12:26.256954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Better to compare with another model and understand how effective it is.\n\n## A straight forward model, which always predict last observer value.","metadata":{}},{"cell_type":"code","source":"class DummyPredictor(nn.Module):\n    def forward(self, x):\n        last_values = []\n        for r in x.tolist():\n            last_values.append(r[-1])\n        return torch.tensor(data=last_values)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T05:12:45.663554Z","iopub.execute_input":"2022-04-23T05:12:45.663958Z","iopub.status.idle":"2022-04-23T05:12:45.669492Z","shell.execute_reply.started":"2022-04-23T05:12:45.663929Z","shell.execute_reply":"2022-04-23T05:12:45.668819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Another one is linear interpolation","metadata":{}},{"cell_type":"code","source":"class InterpolationPredictor(nn.Module):\n    def forward(self, x):\n        last_values = []\n        values = x.tolist()\n        for v in values:\n            x = np.arange(0, len(v))\n            y = interpolate.interp1d(x, v, fill_value='extrapolate')\n            last_values.append([y(len(v)).tolist()])\n        return torch.tensor(data=last_values)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T05:13:09.132222Z","iopub.execute_input":"2022-04-23T05:13:09.132709Z","iopub.status.idle":"2022-04-23T05:13:09.139706Z","shell.execute_reply.started":"2022-04-23T05:13:09.132672Z","shell.execute_reply":"2022-04-23T05:13:09.139099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classical HWES method","metadata":{}},{"cell_type":"code","source":"class HwesPredictor(nn.Module):\n    def forward(self, x):\n        last_value = []\n        for r in x.tolist():\n            model = ExponentialSmoothing(r)\n            results = model.fit()\n            forecast = results.forecast()\n            last_value.append([forecast[0]])\n        return torch.tensor(data=last_value)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T05:13:23.091874Z","iopub.execute_input":"2022-04-23T05:13:23.092343Z","iopub.status.idle":"2022-04-23T05:13:23.09753Z","shell.execute_reply.started":"2022-04-23T05:13:23.092304Z","shell.execute_reply":"2022-04-23T05:13:23.096945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.seed(1)\ntorch.manual_seed(1)\n\n# We will use 256 sliding window and 3000 as time series length\nfeatures = 256\nts_len = 3000\n\n# Dataset for training, validation and testing\nx_train, x_val, x_test, y_train, y_val, y_test = get_time_series_datasets(\n    features, ts_len\t\n)\n\n# initialize prediction models\nnet = FCNN(n_inp=features, l_1=64, l_2=32, n_out=1)\nnet.train()\n\ndummy_predictor = DummyPredictor()\ninterpolation_predictor = InterpolationPredictor()\nhwes_predictor = HwesPredictor()\n\noptimizer = torch.optim.Adam(net.parameters())\nloss_fn = nn.MSELoss()\n\n# we will choose the model that shown the best results on validation set\nbest_model = None\nmin_val_loss = 1000000\n\n# Trainning Process\ntrainning_loss = []\nvalidation_loss = []\n\n# Start Training\nfor t in range(10000):\n    prediction = net(x_train)\n    loss = loss_fn(prediction, y_train)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    val_prediction = net(x_val)\n    val_loss = loss_fn(val_prediction, y_val)\n    trainning_loss.append(loss.item())\n    validation_loss.append(val_loss.item())\n\n    if val_loss.item() < min_val_loss:\n        best_model = copy.deepcopy(net)\n        min_val_loss = val_loss.item()\n    if t % 1000 == 0:\n        print(f'EPOCH - {t}: \\\n            train - {round(loss.item(), 4)} \\\n            val - {round(val_loss.item(), 4)}')\nnet.eval()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T05:16:15.574531Z","iopub.execute_input":"2022-04-23T05:16:15.575412Z","iopub.status.idle":"2022-04-23T05:16:51.739622Z","shell.execute_reply.started":"2022-04-23T05:16:15.575361Z","shell.execute_reply":"2022-04-23T05:16:51.738717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's visualize results. Our FCNN shows the best result","metadata":{}},{"cell_type":"code","source":"print('Testing')\nprint(f'FCNN Loss: {loss_fn(best_model(x_test), y_test).item()}')\nprint(f'Dummy Loss: {loss_fn(dummy_predictor(x_test), y_test).item()}')\nprint(f'Linear Interpolation Loss:{loss_fn(interpolation_predictor(x_test), y_test).item()}')\nprint(f\"HWES Loss: {loss_fn(hwes_predictor(x_test), y_test).item()}\")\n\nplt.figure(figsize=(12, 8))\nplt.title(\"trainning progress\")\nplt.yscale(\"log\")\nplt.plot(trainning_loss, label=\"trainning loss\")\nplt.plot(validation_loss, label=\"validation loss\")\nplt.grid()\nplt.show()\n\nplt.figure(figsize=(12, 8))\nplt.title(\"FCNN on Train Data\")\nplt.plot(y_test, label=\"actual\")\nplt.plot(best_model(x_test).tolist(), label=\"FCNN\")\nplt.plot(hwes_predictor(x_test).tolist(), label=\"HWES\")\nplt.plot()\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T05:24:37.648578Z","iopub.execute_input":"2022-04-23T05:24:37.649109Z","iopub.status.idle":"2022-04-23T05:24:42.426691Z","shell.execute_reply.started":"2022-04-23T05:24:37.64907Z","shell.execute_reply":"2022-04-23T05:24:42.425916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}