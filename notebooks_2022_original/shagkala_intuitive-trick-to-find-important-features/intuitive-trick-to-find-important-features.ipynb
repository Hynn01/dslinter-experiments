{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hi Kagglers, \n\nIn this short notebook I will showcase a simple yet intuitive trick to check the relevance of features in the model. I have learned this trick from the top grandmasters on this platform. Using this technique, we can quickly check the significance of each feature and try to increase their significance by applying feature transformations or even drop them if nothing works.\n\n# Intuition:\n\n#### Ideally, any relevant feature should add some value to help the model in making the right predictions. It should be at least more significant than a variable containing randomÂ noise.\n\nLet's try to implement this intuition and see if any of the features are performing worse than random noise in our data!","metadata":{}},{"cell_type":"markdown","source":"# Loading Libraries","metadata":{}},{"cell_type":"code","source":"#Importing Required Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom statistics import mean\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('display.max_columns', None)\n\nsns.set_palette(\"muted\")","metadata":{"execution":{"iopub.status.busy":"2022-05-02T17:12:45.085657Z","iopub.execute_input":"2022-05-02T17:12:45.086107Z","iopub.status.idle":"2022-05-02T17:12:46.571414Z","shell.execute_reply.started":"2022-05-02T17:12:45.085989Z","shell.execute_reply":"2022-05-02T17:12:46.570368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the data files","metadata":{}},{"cell_type":"code","source":"#Reading the data files\n\ntrain = pd.read_csv('../input/titanic/train.csv')\ntest = pd.read_csv('../input/titanic/test.csv')\nsample = pd.read_csv('../input/titanic/gender_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T17:12:46.573731Z","iopub.execute_input":"2022-05-02T17:12:46.574247Z","iopub.status.idle":"2022-05-02T17:12:46.612153Z","shell.execute_reply.started":"2022-05-02T17:12:46.574199Z","shell.execute_reply":"2022-05-02T17:12:46.611117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data Overview\n\nprint(f'Shape of train data: {train.shape}')\nprint(f'Shape of train data: {test.shape}')\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T17:12:46.613945Z","iopub.execute_input":"2022-05-02T17:12:46.614598Z","iopub.status.idle":"2022-05-02T17:12:46.644654Z","shell.execute_reply.started":"2022-05-02T17:12:46.614544Z","shell.execute_reply":"2022-05-02T17:12:46.64402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"# Filling missing values\ntest['Embarked'].fillna((train['Embarked'].mode()), inplace=True)\ntrain['Embarked'].fillna((train['Embarked'].mode()), inplace=True)\n\ntest['Fare'].fillna((train['Fare'].median()), inplace=True)\ntrain['Fare'].fillna((train['Fare'].median()), inplace=True)\n\ntest['Age'].fillna((train['Age'].median()), inplace=True)\ntrain['Age'].fillna((train['Age'].median()), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T17:12:46.647235Z","iopub.execute_input":"2022-05-02T17:12:46.647762Z","iopub.status.idle":"2022-05-02T17:12:46.669373Z","shell.execute_reply.started":"2022-05-02T17:12:46.647711Z","shell.execute_reply":"2022-05-02T17:12:46.668494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping unimportant features\ntrain.drop(['Name','Ticket','Cabin', 'PassengerId'], axis=1, inplace=True)\ntest.drop(['Name','Ticket','Cabin', 'PassengerId'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T17:12:46.670777Z","iopub.execute_input":"2022-05-02T17:12:46.671264Z","iopub.status.idle":"2022-05-02T17:12:46.682265Z","shell.execute_reply.started":"2022-05-02T17:12:46.671218Z","shell.execute_reply":"2022-05-02T17:12:46.681293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encodeing categorical features\nobj_cols = train.select_dtypes(include=['object']).columns.tolist()\nfor col in obj_cols:\n    le = LabelEncoder()\n    le.fit(train[col])\n    train[col] = le.transform(train[col])\n    test[col] = le.transform(test[col])","metadata":{"execution":{"iopub.status.busy":"2022-05-02T17:12:46.683724Z","iopub.execute_input":"2022-05-02T17:12:46.683973Z","iopub.status.idle":"2022-05-02T17:12:46.699768Z","shell.execute_reply.started":"2022-05-02T17:12:46.683939Z","shell.execute_reply":"2022-05-02T17:12:46.69902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding a random noise feature","metadata":{}},{"cell_type":"code","source":"np.random.seed(44)\n\ntrain['Noise'] = np.random.normal(0,1,train.shape[0])\ntest['Noise'] = np.random.normal(0,1,test.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-05-02T17:13:10.716306Z","iopub.execute_input":"2022-05-02T17:13:10.716676Z","iopub.status.idle":"2022-05-02T17:13:10.722408Z","shell.execute_reply.started":"2022-05-02T17:13:10.716644Z","shell.execute_reply":"2022-05-02T17:13:10.721725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"# Storing the target variable separately\n\nX_train = StandardScaler().fit_transform(train.drop('Survived', axis = 1))\nX_test = StandardScaler().fit_transform(test)\ny_train = train['Survived']\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('y_train shape: {}'.format(y_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T17:13:11.139594Z","iopub.execute_input":"2022-05-02T17:13:11.140541Z","iopub.status.idle":"2022-05-02T17:13:11.157084Z","shell.execute_reply.started":"2022-05-02T17:13:11.140489Z","shell.execute_reply":"2022-05-02T17:13:11.156108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Finding model importance using Logistic Regression and K Fold Cross Validation","metadata":{}},{"cell_type":"code","source":"#Stratified K fold Cross Validation\n\ndef train_and_validate(model, N):\n    \n    regex = '^[^\\(]+'\n    match = re.findall(regex, str(model))\n    print(f'Running {N} Fold CV with {match[0]} Model.')\n    \n    probs = pd.DataFrame(np.zeros((len(X_test), N * 2)), columns=['Fold_{}_Prob_{}'.format(i, j) for i in range(1, N + 1) for j in range(2)])\n    importances = pd.DataFrame(np.zeros((X_train.shape[1], N)), columns=['Fold_{}'.format(i) for i in range(1, N + 1)], index=train.drop('Survived', axis = 1).columns)\n    fprs, tprs, scores = [], [], []\n\n    skf = StratifiedKFold(n_splits=N, random_state=N, shuffle=True)\n\n    for fold, (trn_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n        print('Fold {}\\n'.format(fold))\n        \n        # Fitting the model\n        model.fit(X_train[trn_idx], y_train[trn_idx])\n\n        # Computing Train AUC score\n        trn_fpr, trn_tpr, trn_thresholds = roc_curve(y_train[trn_idx], model.predict_proba(X_train[trn_idx])[:, 1])\n        trn_auc_score = auc(trn_fpr, trn_tpr)\n        # Computing Validation AUC score\n        val_fpr, val_tpr, val_thresholds = roc_curve(y_train[val_idx], model.predict_proba(X_train[val_idx])[:, 1])\n        val_auc_score = auc(val_fpr, val_tpr)  \n\n        scores.append((trn_auc_score, val_auc_score))\n        fprs.append(val_fpr)\n        tprs.append(val_tpr)\n\n        # X_test probabilities\n        probs.loc[:, 'Fold_{}_Prob_0'.format(fold)] = model.predict_proba(X_test)[:, 0]\n        probs.loc[:, 'Fold_{}_Prob_1'.format(fold)] = model.predict_proba(X_test)[:, 1]\n        importances.iloc[:, fold - 1] = model.coef_[0] \n        \n        print(scores[-1])    \n    \n    trauc = mean([i[0] for i in scores])\n    cvauc = mean([i[1] for i in scores])\n    print(f'\\nAverage Training AUC: {trauc}, Average CV AUC: {cvauc}')\n    \n    return trauc, cvauc, importances, probs","metadata":{"execution":{"iopub.status.busy":"2022-05-02T17:13:11.522797Z","iopub.execute_input":"2022-05-02T17:13:11.52347Z","iopub.status.idle":"2022-05-02T17:13:11.536681Z","shell.execute_reply.started":"2022-05-02T17:13:11.523405Z","shell.execute_reply":"2022-05-02T17:13:11.535652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Testing multiple ML models using stratified K fold CV\n\ndf_row = []\nN = 3\nmodel = LogisticRegression()\n    \ntrauc, cvauc, importances, probs = train_and_validate(model, N)\n\nregex = '^[^\\(]+'\nmatch = re.findall(regex, str(model))\n\ndf_row.append([match[0], trauc, cvauc])\n\ndf = pd.DataFrame(df_row, columns = ['Model', f'{N} Fold Training AUC', f'{N} Fold CV AUC'])\ndf","metadata":{"execution":{"iopub.status.busy":"2022-05-02T17:13:11.661748Z","iopub.execute_input":"2022-05-02T17:13:11.662535Z","iopub.status.idle":"2022-05-02T17:13:11.712304Z","shell.execute_reply.started":"2022-05-02T17:13:11.662485Z","shell.execute_reply":"2022-05-02T17:13:11.711515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Importance","metadata":{}},{"cell_type":"code","source":"#Plotting the feature importance\n\nimportances['Mean_Importance'] = abs(importances.mean(axis=1))\nimportances.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n\nplt.figure(figsize=(8,8))\nbar = sns.barplot(x='Mean_Importance', y=importances.index, data=importances, palette=['orange' if x!='Noise' else 'black' for x in importances.index])\n\nplt.xlabel('Feature Importance')\nplt.ylabel('Features')\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.title('Logistic Regression Mean Feature Importance Between Folds', size=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T17:13:11.892766Z","iopub.execute_input":"2022-05-02T17:13:11.893088Z","iopub.status.idle":"2022-05-02T17:13:12.116712Z","shell.execute_reply.started":"2022-05-02T17:13:11.893056Z","shell.execute_reply":"2022-05-02T17:13:12.115739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can observe that Fare, Embarked, and Parch variables are less important than a random noise feature. Hence, these features can be considered insignificant for the model predictions because their importance is lower than the importance of a random noise feature containing randomly generated numbers.","metadata":{}},{"cell_type":"markdown","source":"# Points to note:","metadata":{}},{"cell_type":"markdown","source":"1. Please note that it is not a good idea to blindly drop the lesser significant variables.\n2. Instead, we should preprocess, and transform the insignificant features to make them add value to predictions.\n3. Internally every machine learning model works differently. Hence each model can give different order of feature importance. So before taking any decision we should first check the importance of a few models.\n4. The noise feature can lower the accuracy. Hence, do not forget to remove the Noise feature before training the final model for submission.","metadata":{}},{"cell_type":"markdown","source":"## The End!\n\nThank you for reading this notebook. I hope you have learned something new today.\nKindly share feedback if you find any flaws or have a better approach.\n\nPlease upvote the notebook if you liked this kernel!\n\nHave a good day!","metadata":{}}]}