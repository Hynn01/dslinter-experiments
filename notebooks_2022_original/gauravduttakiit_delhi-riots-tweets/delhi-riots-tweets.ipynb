{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-07T19:02:37.864192Z","iopub.execute_input":"2022-05-07T19:02:37.865127Z","iopub.status.idle":"2022-05-07T19:02:37.896805Z","shell.execute_reply.started":"2022-05-07T19:02:37.864983Z","shell.execute_reply":"2022-05-07T19:02:37.895927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets=pd.read_csv('../input/delhi-riots-tweets/tweets.csv')\ntweets.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:02:37.939167Z","iopub.execute_input":"2022-05-07T19:02:37.939754Z","iopub.status.idle":"2022-05-07T19:02:40.995097Z","shell.execute_reply.started":"2022-05-07T19:02:37.9397Z","shell.execute_reply":"2022-05-07T19:02:40.994268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:02:40.996582Z","iopub.execute_input":"2022-05-07T19:02:40.99683Z","iopub.status.idle":"2022-05-07T19:02:41.205067Z","shell.execute_reply.started":"2022-05-07T19:02:40.996799Z","shell.execute_reply":"2022-05-07T19:02:41.204113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets['DateTime']=tweets['date']+' '+tweets['time']\ntweets.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:02:41.206453Z","iopub.execute_input":"2022-05-07T19:02:41.206911Z","iopub.status.idle":"2022-05-07T19:02:41.266583Z","shell.execute_reply.started":"2022-05-07T19:02:41.206864Z","shell.execute_reply":"2022-05-07T19:02:41.265702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets['date']=pd.to_datetime(tweets['date'], infer_datetime_format=True)\ntweets['DateTime']=pd.to_datetime(tweets['DateTime'], infer_datetime_format=True)\ntweets.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:02:41.268739Z","iopub.execute_input":"2022-05-07T19:02:41.269002Z","iopub.status.idle":"2022-05-07T19:02:41.560941Z","shell.execute_reply.started":"2022-05-07T19:02:41.268972Z","shell.execute_reply":"2022-05-07T19:02:41.560086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:02:41.562404Z","iopub.execute_input":"2022-05-07T19:02:41.56275Z","iopub.status.idle":"2022-05-07T19:02:41.591626Z","shell.execute_reply.started":"2022-05-07T19:02:41.562686Z","shell.execute_reply":"2022-05-07T19:02:41.590781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets = tweets[(tweets['date'] >= '2020-02-15') & (tweets['date'] <= '2020-03-15')]\ntweets.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:02:41.592845Z","iopub.execute_input":"2022-05-07T19:02:41.593146Z","iopub.status.idle":"2022-05-07T19:02:41.65833Z","shell.execute_reply.started":"2022-05-07T19:02:41.593115Z","shell.execute_reply":"2022-05-07T19:02:41.657508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nunique_train=tweets.nunique().reset_index()\nremove_col=nunique_train[(nunique_train[0]==0) | (nunique_train[0]==1) ]['index'].tolist()\nremove_col","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:02:41.659665Z","iopub.execute_input":"2022-05-07T19:02:41.659895Z","iopub.status.idle":"2022-05-07T19:02:42.128774Z","shell.execute_reply.started":"2022-05-07T19:02:41.659867Z","shell.execute_reply":"2022-05-07T19:02:42.128012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\ntweets=tweets.drop(remove_col,axis=1)\ntweets.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:02:42.130093Z","iopub.execute_input":"2022-05-07T19:02:42.130311Z","iopub.status.idle":"2022-05-07T19:02:42.179696Z","shell.execute_reply.started":"2022-05-07T19:02:42.130284Z","shell.execute_reply":"2022-05-07T19:02:42.178878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:02:42.181269Z","iopub.execute_input":"2022-05-07T19:02:42.181748Z","iopub.status.idle":"2022-05-07T19:02:42.331827Z","shell.execute_reply.started":"2022-05-07T19:02:42.181691Z","shell.execute_reply":"2022-05-07T19:02:42.330946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets.place.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:02:42.335548Z","iopub.execute_input":"2022-05-07T19:02:42.33589Z","iopub.status.idle":"2022-05-07T19:02:42.347643Z","shell.execute_reply.started":"2022-05-07T19:02:42.335856Z","shell.execute_reply":"2022-05-07T19:02:42.346802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets.user_id.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:02:42.349477Z","iopub.execute_input":"2022-05-07T19:02:42.350059Z","iopub.status.idle":"2022-05-07T19:02:42.372365Z","shell.execute_reply.started":"2022-05-07T19:02:42.349998Z","shell.execute_reply":"2022-05-07T19:02:42.371409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:02:42.373517Z","iopub.execute_input":"2022-05-07T19:02:42.374213Z","iopub.status.idle":"2022-05-07T19:02:43.552401Z","shell.execute_reply.started":"2022-05-07T19:02:42.374178Z","shell.execute_reply":"2022-05-07T19:02:43.551437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_df_1min = tweets.groupby(pd.Grouper(key='DateTime', freq='1Min', convention='start')).size()\ntweet_df_1min.plot(figsize=(18,6))\nplt.ylabel('1 Minute Tweet Count')\nplt.title('Delhi Riots Tweet Freq. Count, February 15 to March 15, 2020')\nplt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:02:43.554349Z","iopub.execute_input":"2022-05-07T19:02:43.554719Z","iopub.status.idle":"2022-05-07T19:02:44.506178Z","shell.execute_reply.started":"2022-05-07T19:02:43.554671Z","shell.execute_reply":"2022-05-07T19:02:44.50532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wordcloud","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-07T19:02:44.507563Z","iopub.execute_input":"2022-05-07T19:02:44.507869Z","iopub.status.idle":"2022-05-07T19:02:56.550401Z","shell.execute_reply.started":"2022-05-07T19:02:44.507833Z","shell.execute_reply":"2022-05-07T19:02:56.549467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyspellchecker","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-07T19:02:56.552184Z","iopub.execute_input":"2022-05-07T19:02:56.552941Z","iopub.status.idle":"2022-05-07T19:03:07.725318Z","shell.execute_reply.started":"2022-05-07T19:02:56.55289Z","shell.execute_reply":"2022-05-07T19:03:07.72431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom nltk.tokenize import word_tokenize\nfrom spellchecker import SpellChecker\nimport time\nfrom multiprocessing import  Pool\nfrom nltk.stem import WordNetLemmatizer ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:07.728187Z","iopub.execute_input":"2022-05-07T19:03:07.728471Z","iopub.status.idle":"2022-05-07T19:03:08.473283Z","shell.execute_reply.started":"2022-05-07T19:03:07.728438Z","shell.execute_reply":"2022-05-07T19:03:08.472296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def untokenize(words):\n    \"\"\"Untokenizing a text undoes the tokenizing operation, restoring\n    punctuation and spaces to the places that people expect them to be.\n    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n    except for line breaks.\n    \"\"\"\n    text = ' '.join(words)\n    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .', '...')\n    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n        \"can not\", \"cannot\")\n    step6 = step5.replace(\" ` \", \" '\")\n    return step6.strip()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:08.474421Z","iopub.execute_input":"2022-05-07T19:03:08.47464Z","iopub.status.idle":"2022-05-07T19:03:08.482648Z","shell.execute_reply.started":"2022-05-07T19:03:08.474613Z","shell.execute_reply":"2022-05-07T19:03:08.481456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decontracted(phrase):\n    \"\"\"Convert contractions like \"can't\" into \"can not\"\n    \"\"\"\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    #phrase = re.sub(r\"n't\", \" not\", phrase) # resulted in \"ca not\" when sentence started with \"can't\"\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:08.484265Z","iopub.execute_input":"2022-05-07T19:03:08.484606Z","iopub.status.idle":"2022-05-07T19:03:08.498588Z","shell.execute_reply.started":"2022-05-07T19:03:08.48457Z","shell.execute_reply":"2022-05-07T19:03:08.497746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"slang_abbrev_dict = {\n    'AFAIK': 'As Far As I Know',\n    'AFK': 'Away From Keyboard',\n    'ASAP': 'As Soon As Possible',\n    'ATK': 'At The Keyboard',\n    'ATM': 'At The Moment',\n    'A3': 'Anytime, Anywhere, Anyplace',\n    'BAK': 'Back At Keyboard',\n    'BBL': 'Be Back Later',\n    'BBS': 'Be Back Soon',\n    'BFN': 'Bye For Now',\n    'B4N': 'Bye For Now',\n    'BRB': 'Be Right Back',\n    'BRT': 'Be Right There',\n    'BTW': 'By The Way',\n    'B4': 'Before',\n    'B4N': 'Bye For Now',\n    'CU': 'See You',\n    'CUL8R': 'See You Later',\n    'CYA': 'See You',\n    'FAQ': 'Frequently Asked Questions',\n    'FC': 'Fingers Crossed',\n    'FWIW': 'For What It\\'s Worth',\n    'FYI': 'For Your Information',\n    'GAL': 'Get A Life',\n    'GG': 'Good Game',\n    'GN': 'Good Night',\n    'GMTA': 'Great Minds Think Alike',\n    'GR8': 'Great!',\n    'G9': 'Genius',\n    'IC': 'I See',\n    'ICQ': 'I Seek you',\n    'ILU': 'I Love You',\n    'IMHO': 'In My Humble Opinion',\n    'IMO': 'In My Opinion',\n    'IOW': 'In Other Words',\n    'IRL': 'In Real Life',\n    'KISS': 'Keep It Simple, Stupid',\n    'LDR': 'Long Distance Relationship',\n    'LMAO': 'Laugh My Ass Off',\n    'LOL': 'Laughing Out Loud',\n    'LTNS': 'Long Time No See',\n    'L8R': 'Later',\n    'MTE': 'My Thoughts Exactly',\n    'M8': 'Mate',\n    'NRN': 'No Reply Necessary',\n    'OIC': 'Oh I See',\n    'OMG': 'Oh My God',\n    'PITA': 'Pain In The Ass',\n    'PRT': 'Party',\n    'PRW': 'Parents Are Watching',\n    'QPSA?': 'Que Pasa?',\n    'ROFL': 'Rolling On The Floor Laughing',\n    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n    'ROTFLMAO': 'Rolling On The Floor Laughing My Ass Off',\n    'SK8': 'Skate',\n    'STATS': 'Your sex and age',\n    'ASL': 'Age, Sex, Location',\n    'THX': 'Thank You',\n    'TTFN': 'Ta-Ta For Now!',\n    'TTYL': 'Talk To You Later',\n    'U': 'You',\n    'U2': 'You Too',\n    'U4E': 'Yours For Ever',\n    'WB': 'Welcome Back',\n    'WTF': 'What The Fuck',\n    'WTG': 'Way To Go!',\n    'WUF': 'Where Are You From?',\n    'W8': 'Wait',\n    '7K': 'Sick:-D Laugher'\n}\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:08.500053Z","iopub.execute_input":"2022-05-07T19:03:08.500635Z","iopub.status.idle":"2022-05-07T19:03:08.514309Z","shell.execute_reply.started":"2022-05-07T19:03:08.500589Z","shell.execute_reply":"2022-05-07T19:03:08.513662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unslang(text):\n    \"\"\"Converts text like \"OMG\" into \"Oh my God\"\n    \"\"\"\n    if text.upper() in slang_abbrev_dict.keys():\n        return slang_abbrev_dict[text.upper()]\n    else:\n        return text","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:08.515508Z","iopub.execute_input":"2022-05-07T19:03:08.515714Z","iopub.status.idle":"2022-05-07T19:03:08.531594Z","shell.execute_reply.started":"2022-05-07T19:03:08.515689Z","shell.execute_reply":"2022-05-07T19:03:08.530673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords = [\n    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\",\n    \"an\", \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\",\n    \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\",\n    \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\",\n    \"doesn't\", \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\",\n    \"from\", \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\",\n    \"haven\", \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\",\n    \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\",\n    \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\",\n    \"mightn't\", \"more\", \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\",\n    \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\",\n    \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\n    \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\",\n    \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\",\n    \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\",\n    \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\",\n    \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\",\n    \"weren't\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\",\n    \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\",\n    \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n    \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\",\n    \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\",\n    \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\",\n    \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\", \"able\", \"abst\",\n    \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\",\n    \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\",\n    \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n    \"among\", \"amongst\", \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\",\n    \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\",\n    \"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\",\n    \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"became\", \"become\",\n    \"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\",\n    \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\",\n    \"brief\", \"briefly\", \"c\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\",\n    \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\",\n    \"containing\", \"contains\", \"couldnt\", \"date\", \"different\", \"done\",\n    \"downwards\", \"due\", \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\",\n    \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\",\n    \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\",\n    \"everywhere\", \"ex\", \"except\", \"f\", \"far\", \"ff\", \"fifth\", \"first\", \"five\",\n    \"fix\", \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\",\n    \"found\", \"four\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\",\n    \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\",\n    \"h\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\",\n    \"heres\", \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\",\n    \"however\", \"hundred\", \"id\", \"ie\", \"im\", \"immediate\", \"immediately\",\n    \"importance\", \"important\", \"inc\", \"indeed\", \"index\", \"information\",\n    \"instead\", \"invention\", \"inward\", \"itd\", \"it'll\", \"j\", \"k\", \"keep\", \"keeps\",\n    \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\", \"last\",\n    \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\",\n    \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\",\n    \"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\",\n    \"maybe\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\",\n    \"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\",\n    \"must\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\",\n    \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\",\n    \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\", \"none\",\n    \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\",\n    \"obtain\", \"obtained\", \"obviously\", \"often\", \"oh\", \"ok\", \"okay\", \"old\",\n    \"omitted\", \"one\", \"ones\", \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\",\n    \"overall\", \"owing\", \"p\", \"page\", \"pages\", \"part\", \"particular\",\n    \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\",\n    \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\",\n    \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\",\n    \"provides\", \"put\", \"q\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\",\n    \"rather\", \"rd\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\",\n    \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\",\n    \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"said\",\n    \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\",\n    \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\",\n    \"several\", \"shall\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\",\n    \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\",\n    \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"somethan\",\n    \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\",\n    \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\",\n    \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\",\n    \"suggest\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\",\n    \"thank\", \"thanks\", \"thanx\", \"thats\", \"that've\", \"thence\", \"thereafter\",\n    \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\",\n    \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"theyd\", \"theyre\",\n    \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"throughout\",\n    \"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\",\n    \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\",\n    \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\",\n    \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\",\n    \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\",\n    \"vs\", \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\",\n    \"werent\", \"whatever\", \"what'll\", \"whats\", \"whence\", \"whenever\",\n    \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\",\n    \"wherever\", \"whether\", \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\",\n    \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\", \"willing\", \"wish\",\n    \"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\", \"x\", \"yes\",\n    \"yet\", \"youd\", \"youre\", \"z\", \"zero\", \"a's\", \"ain't\", \"allow\", \"allows\",\n    \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"associated\", \"best\",\n    \"better\", \"c'mon\", \"c's\", \"cant\", \"changes\", \"clearly\", \"concerning\",\n    \"consequently\", \"consider\", \"considering\", \"corresponding\", \"course\",\n    \"currently\", \"definitely\", \"described\", \"despite\", \"entirely\", \"exactly\",\n    \"example\", \"going\", \"greetings\", \"hello\", \"help\", \"hopefully\", \"ignored\",\n    \"inasmuch\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\",\n    \"it'd\", \"keep\", \"keeps\", \"novel\", \"presumably\", \"reasonably\", \"second\",\n    \"secondly\", \"sensible\", \"serious\", \"seriously\", \"sure\", \"t's\", \"third\",\n    \"thorough\", \"thoroughly\", \"three\", \"well\", \"wonder\", \"a\", \"about\", \"above\",\n    \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\",\n    \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\",\n    \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\",\n    \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\",\n    \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\",\n    \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\",\n    \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\",\n    \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\",\n    \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\",\n    \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\",\n    \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\",\n    \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\",\n    \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\",\n    \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\",\n    \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\",\n    \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\",\n    \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\",\n    \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n    \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\",\n    \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\",\n    \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\",\n    \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\",\n    \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\",\n    \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\n    \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\n    \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\",\n    \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\",\n    \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\",\n    \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\",\n    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\",\n    \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n    \"the\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\",\n    \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\",\n    \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\",\n    \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"co\", \"op\", \"research-articl\",\n    \"pagecount\", \"cit\", \"ibid\", \"les\", \"le\", \"au\", \"que\", \"est\", \"pas\", \"vol\",\n    \"el\", \"los\", \"pp\", \"u201d\", \"well-b\", \"http\", \"volumtype\", \"par\", \"0o\",\n    \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"ac\",\n    \"ad\", \"ae\", \"af\", \"ag\", \"aj\", \"al\", \"an\", \"ao\", \"ap\", \"ar\", \"av\", \"aw\",\n    \"ax\", \"ay\", \"az\", \"b1\", \"b2\", \"b3\", \"ba\", \"bc\", \"bd\", \"be\", \"bi\", \"bj\",\n    \"bk\", \"bl\", \"bn\", \"bp\", \"br\", \"bs\", \"bt\", \"bu\", \"bx\", \"c1\", \"c2\", \"c3\",\n    \"cc\", \"cd\", \"ce\", \"cf\", \"cg\", \"ch\", \"ci\", \"cj\", \"cl\", \"cm\", \"cn\", \"cp\",\n    \"cq\", \"cr\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d2\", \"da\", \"dc\",\n    \"dd\", \"de\", \"df\", \"di\", \"dj\", \"dk\", \"dl\", \"do\", \"dp\", \"dr\", \"ds\", \"dt\",\n    \"du\", \"dx\", \"dy\", \"e2\", \"e3\", \"ea\", \"ec\", \"ed\", \"ee\", \"ef\", \"ei\", \"ej\",\n    \"el\", \"em\", \"en\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"et\", \"eu\", \"ev\", \"ex\",\n    \"ey\", \"f2\", \"fa\", \"fc\", \"ff\", \"fi\", \"fj\", \"fl\", \"fn\", \"fo\", \"fr\", \"fs\",\n    \"ft\", \"fu\", \"fy\", \"ga\", \"ge\", \"gi\", \"gj\", \"gl\", \"go\", \"gr\", \"gs\", \"gy\",\n    \"h2\", \"h3\", \"hh\", \"hi\", \"hj\", \"ho\", \"hr\", \"hs\", \"hu\", \"hy\", \"i\", \"i2\", \"i3\",\n    \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ic\", \"ie\", \"ig\", \"ih\", \"ii\", \"ij\",\n    \"il\", \"in\", \"io\", \"ip\", \"iq\", \"ir\", \"iv\", \"ix\", \"iy\", \"iz\", \"jj\", \"jr\",\n    \"js\", \"jt\", \"ju\", \"ke\", \"kg\", \"kj\", \"km\", \"ko\", \"l2\", \"la\", \"lb\", \"lc\",\n    \"lf\", \"lj\", \"ln\", \"lo\", \"lr\", \"ls\", \"lt\", \"m2\", \"ml\", \"mn\", \"mo\", \"ms\",\n    \"mt\", \"mu\", \"n2\", \"nc\", \"nd\", \"ne\", \"ng\", \"ni\", \"nj\", \"nl\", \"nn\", \"nr\",\n    \"ns\", \"nt\", \"ny\", \"oa\", \"ob\", \"oc\", \"od\", \"of\", \"og\", \"oi\", \"oj\", \"ol\",\n    \"om\", \"on\", \"oo\", \"oq\", \"or\", \"os\", \"ot\", \"ou\", \"ow\", \"ox\", \"oz\", \"p1\",\n    \"p2\", \"p3\", \"pc\", \"pd\", \"pe\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"pm\",\n    \"pn\", \"po\", \"pq\", \"pr\", \"ps\", \"pt\", \"pu\", \"py\", \"qj\", \"qu\", \"r2\", \"ra\",\n    \"rc\", \"rd\", \"rf\", \"rh\", \"ri\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\",\n    \"rs\", \"rt\", \"ru\", \"rv\", \"ry\", \"s2\", \"sa\", \"sc\", \"sd\", \"se\", \"sf\", \"si\",\n    \"sj\", \"sl\", \"sm\", \"sn\", \"sp\", \"sq\", \"sr\", \"ss\", \"st\", \"sy\", \"sz\", \"t1\",\n    \"t2\", \"t3\", \"tb\", \"tc\", \"td\", \"te\", \"tf\", \"th\", \"ti\", \"tj\", \"tl\", \"tm\",\n    \"tn\", \"tp\", \"tq\", \"tr\", \"ts\", \"tt\", \"tv\", \"tx\", \"ue\", \"ui\", \"uj\", \"uk\",\n    \"um\", \"un\", \"uo\", \"ur\", \"ut\", \"va\", \"wa\", \"vd\", \"wi\", \"vj\", \"vo\", \"wo\",\n    \"vq\", \"vt\", \"vu\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\",\n    \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y2\", \"yj\", \"yl\", \"yr\", \"ys\", \"yt\", \"zi\", \"zz\"\n]","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:08.535115Z","iopub.execute_input":"2022-05-07T19:03:08.535867Z","iopub.status.idle":"2022-05-07T19:03:08.610959Z","shell.execute_reply.started":"2022-05-07T19:03:08.535827Z","shell.execute_reply":"2022-05-07T19:03:08.609883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\n        \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        \"]+\",\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:08.613172Z","iopub.execute_input":"2022-05-07T19:03:08.614136Z","iopub.status.idle":"2022-05-07T19:03:08.637054Z","shell.execute_reply.started":"2022-05-07T19:03:08.614093Z","shell.execute_reply":"2022-05-07T19:03:08.636186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spell = SpellChecker()\n\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:08.638801Z","iopub.execute_input":"2022-05-07T19:03:08.639518Z","iopub.status.idle":"2022-05-07T19:03:08.801553Z","shell.execute_reply.started":"2022-05-07T19:03:08.639468Z","shell.execute_reply":"2022-05-07T19:03:08.800688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_urls(text):\n    text = clean(r\"http\\S+\", text)\n    text = clean(r\"www\\S+\", text)\n    text = clean(r\"pic.twitter.com\\S+\", text)\n\n    return text\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:08.80297Z","iopub.execute_input":"2022-05-07T19:03:08.803335Z","iopub.status.idle":"2022-05-07T19:03:08.809401Z","shell.execute_reply.started":"2022-05-07T19:03:08.803291Z","shell.execute_reply":"2022-05-07T19:03:08.80846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(reg_exp, text):\n    text = re.sub(reg_exp, \" \", text)\n\n    # replace multiple spaces with one.\n    text = re.sub('\\s{2,}', ' ', text)\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:08.81063Z","iopub.execute_input":"2022-05-07T19:03:08.810985Z","iopub.status.idle":"2022-05-07T19:03:08.826031Z","shell.execute_reply.started":"2022-05-07T19:03:08.81095Z","shell.execute_reply":"2022-05-07T19:03:08.825222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n\ndef clean_all(t, correct_spelling=False, remove_stopwords=False, lemmatize=False):\n    \n    # first do bulk cleanup on tokens that don't depend on word tokenization\n\n    # remove xml tags\n    t = clean(r\"<[^>]+>\", t)\n    t = clean(r\"&lt;\", t)\n    t = clean(r\"&gt;\", t)\n\n    # remove URLs\n    t = remove_urls(t)\n\n    # https://stackoverflow.com/a/35041925\n    # replace multiple punctuation with single. Ex: !?!?!? would become ?\n    t = clean(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', t)\n\n    t = remove_emoji(t)\n\n    # expand common contractions like \"I'm\" \"he'll\"\n    t = decontracted(t)\n\n    # now remove/expand bad patterns per word\n    words = word_tokenize(t)\n\n    # remove stopwords\n    if remove_stopwords is True:\n        words = [w for w in words if not w in stopwords]\n\n    clean_words = []\n\n    for w in words:\n        # normalize punctuation\n        w = re.sub(r'&', 'and', w)\n\n        # expand slang like OMG = Oh my God\n        w = unslang(w)\n\n        if lemmatize is True:\n            w = lemmatizer.lemmatize(w)\n        \n        clean_words.append(w)\n\n    # join the words back into a full string\n    t = untokenize(clean_words)\n\n    if correct_spelling is True:\n        # this resulted in lots of lost punctuation - omitting for now. Also greatly speeds things up\n        t = correct_spellings(t)\n\n    # finally, remove any non ascii and special characters that made it through\n    t = clean(r\"[^A-Za-z0-9\\.\\'!\\?,\\$]\", t)\n\n    return t\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:08.827191Z","iopub.execute_input":"2022-05-07T19:03:08.827835Z","iopub.status.idle":"2022-05-07T19:03:08.84015Z","shell.execute_reply.started":"2022-05-07T19:03:08.827778Z","shell.execute_reply":"2022-05-07T19:03:08.839324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_dataframe(df, correct_spelling=False, remove_stopwords=False):\n    df['tweet'] = df.apply(lambda x: clean_all(x['tweet'], correct_spelling=correct_spelling, remove_stopwords=remove_stopwords), axis=1)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:08.841871Z","iopub.execute_input":"2022-05-07T19:03:08.842422Z","iopub.status.idle":"2022-05-07T19:03:08.854047Z","shell.execute_reply.started":"2022-05-07T19:03:08.842379Z","shell.execute_reply":"2022-05-07T19:03:08.853093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parallelize_dataframe(\n        df, func, n_cores=4): \n    df_split = np.array_split(df, n_cores)\n    pool = Pool(n_cores)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:08.85821Z","iopub.execute_input":"2022-05-07T19:03:08.858983Z","iopub.status.idle":"2022-05-07T19:03:08.865571Z","shell.execute_reply.started":"2022-05-07T19:03:08.858946Z","shell.execute_reply":"2022-05-07T19:03:08.864911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" import numpy as np\nstart_time = time.time()\ntweets = parallelize_dataframe(tweets, clean_dataframe)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\ntweets.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:08.866617Z","iopub.execute_input":"2022-05-07T19:03:08.867138Z","iopub.status.idle":"2022-05-07T19:03:39.881258Z","shell.execute_reply.started":"2022-05-07T19:03:08.867087Z","shell.execute_reply":"2022-05-07T19:03:39.880627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets['tweet']=tweets['tweet'].str.lower()\ntweets['tweet']=tweets['tweet'].str.replace('[^\\w\\s]','')\ntweets['tweet']=tweets['tweet'].str.replace('\\d+', '')\ntweets['tweet']=tweets['tweet'].str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')\ntweets['tweet']=tweets['tweet'].str.strip()\ntweets.tail()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:39.883244Z","iopub.execute_input":"2022-05-07T19:03:39.884321Z","iopub.status.idle":"2022-05-07T19:03:43.19162Z","shell.execute_reply.started":"2022-05-07T19:03:39.884276Z","shell.execute_reply":"2022-05-07T19:03:43.190493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing all necessary modules\nfrom wordcloud import WordCloud, STOPWORDS\n \ncomment_words = ''\nstopwords = set(STOPWORDS)\n \n# iterate through the csv file\nfor val in tweets.tweet:\n     \n    # typecaste each val to string\n    val = str(val)\n \n    # split the value\n    tokens = val.split()\n     \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n     \n    comment_words += \" \".join(tokens)+\" \"\n \nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(comment_words)\n \n# plot the WordCloud image                      \nplt.figure(figsize = (20, 20), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:43.193117Z","iopub.execute_input":"2022-05-07T19:03:43.193356Z","iopub.status.idle":"2022-05-07T19:03:56.968757Z","shell.execute_reply.started":"2022-05-07T19:03:43.193329Z","shell.execute_reply":"2022-05-07T19:03:56.967786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the lexicon \nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# Create an instance of SentimentIntensityAnalyzer\nsent_analyzer = SentimentIntensityAnalyzer()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:56.970356Z","iopub.execute_input":"2022-05-07T19:03:56.970879Z","iopub.status.idle":"2022-05-07T19:03:57.003657Z","shell.execute_reply.started":"2022-05-07T19:03:56.970841Z","shell.execute_reply":"2022-05-07T19:03:57.00297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_output(output_dict):\n  \n  polarity = \"neutral\"\n\n  if(output_dict['compound']>= 0.05):\n    polarity = \"positive\"\n\n  elif(output_dict['compound']<= -0.05):\n    polarity = \"negative\"\n\n  return polarity\n\ndef predict_sentiment(text):\n  \n  output_dict =  sent_analyzer.polarity_scores(text)\n  return format_output(output_dict)\n\n# Run the predictions\ntweets[\"sentiment\"] = tweets['tweet'].apply(predict_sentiment)\n\n# Show 5 random rows of the data\ntweets.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:03:57.004888Z","iopub.execute_input":"2022-05-07T19:03:57.005509Z","iopub.status.idle":"2022-05-07T19:04:30.735734Z","shell.execute_reply.started":"2022-05-07T19:03:57.005477Z","shell.execute_reply":"2022-05-07T19:04:30.734874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_df_1min = tweets[tweets.sentiment=='neutral'].groupby(pd.Grouper(key='DateTime', freq='1Min', convention='start')).size()\ntweet_df_1min.plot(figsize=(18,6))\nplt.ylabel('1 Minute Tweet Count')\nplt.title('Neutral Tweet Freq. Count during Delhi Riots , February 15 to March 15, 2020')\nplt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:04:30.736915Z","iopub.execute_input":"2022-05-07T19:04:30.737133Z","iopub.status.idle":"2022-05-07T19:04:31.287852Z","shell.execute_reply.started":"2022-05-07T19:04:30.737106Z","shell.execute_reply":"2022-05-07T19:04:31.287049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_df_1min = tweets[tweets.sentiment=='positive'].groupby(pd.Grouper(key='DateTime', freq='1Min', convention='start')).size()\ntweet_df_1min.plot(figsize=(18,6))\nplt.ylabel('1 Minute Tweet Count')\nplt.title('Positive Tweet Freq. Count during Delhi Riots , February 15 to March 15, 2020')\nplt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:04:31.289387Z","iopub.execute_input":"2022-05-07T19:04:31.289708Z","iopub.status.idle":"2022-05-07T19:04:32.084553Z","shell.execute_reply.started":"2022-05-07T19:04:31.289669Z","shell.execute_reply":"2022-05-07T19:04:32.083763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_df_1min = tweets[tweets.sentiment=='positive'].groupby(pd.Grouper(key='DateTime', freq='1Min', convention='start')).size()\ntweet_df_1min.plot(figsize=(18,6))\nplt.ylabel('1 Minute Tweet Count')\nplt.title('Positive Tweet Freq. Count during Delhi Riots , February 15 to March 15, 2020')\nplt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:04:32.086012Z","iopub.execute_input":"2022-05-07T19:04:32.086249Z","iopub.status.idle":"2022-05-07T19:04:32.699671Z","shell.execute_reply.started":"2022-05-07T19:04:32.08622Z","shell.execute_reply":"2022-05-07T19:04:32.69908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped = tweets.groupby(by='date')['sentiment'].value_counts()\ngrouped ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:04:32.700807Z","iopub.execute_input":"2022-05-07T19:04:32.70142Z","iopub.status.idle":"2022-05-07T19:04:32.735486Z","shell.execute_reply.started":"2022-05-07T19:04:32.701389Z","shell.execute_reply":"2022-05-07T19:04:32.734847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unstacked = grouped.unstack(level=1)\nunstacked","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:04:32.736635Z","iopub.execute_input":"2022-05-07T19:04:32.73702Z","iopub.status.idle":"2022-05-07T19:04:32.758697Z","shell.execute_reply.started":"2022-05-07T19:04:32.736981Z","shell.execute_reply":"2022-05-07T19:04:32.757786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unstacked.plot.bar(figsize = (20, 5))\nplt.grid()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:05:27.601315Z","iopub.execute_input":"2022-05-07T19:05:27.60174Z","iopub.status.idle":"2022-05-07T19:05:28.267813Z","shell.execute_reply.started":"2022-05-07T19:05:27.601675Z","shell.execute_reply":"2022-05-07T19:05:28.266907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:11:07.822106Z","iopub.execute_input":"2022-05-07T19:11:07.822471Z","iopub.status.idle":"2022-05-07T19:11:07.849871Z","shell.execute_reply.started":"2022-05-07T19:11:07.822438Z","shell.execute_reply":"2022-05-07T19:11:07.84884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# iterate through the csv file\nfor val in tweets[tweets.sentiment=='negative'].tweet:\n     \n    # typecaste each val to string\n    val = str(val)\n \n    # split the value\n    tokens = val.split()\n     \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n     \n    comment_words += \" \".join(tokens)+\" \"\n \nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(comment_words)\n \n# plot the WordCloud image                      \nplt.figure(figsize = (20, 20), facecolor = None)\nplt.title('Negative keywords in tweet')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:13:34.091873Z","iopub.execute_input":"2022-05-07T19:13:34.092418Z","iopub.status.idle":"2022-05-07T19:14:05.22279Z","shell.execute_reply.started":"2022-05-07T19:13:34.092364Z","shell.execute_reply":"2022-05-07T19:14:05.220529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# iterate through the csv file\nfor val in tweets[tweets.sentiment=='neutral'].tweet:\n     \n    # typecaste each val to string\n    val = str(val)\n \n    # split the value\n    tokens = val.split()\n     \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n     \n    comment_words += \" \".join(tokens)+\" \"\n \nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(comment_words)\n \n# plot the WordCloud image                      \nplt.figure(figsize = (20, 20), facecolor = None)\nplt.title('Neutral keywords in tweet')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:14:11.949411Z","iopub.execute_input":"2022-05-07T19:14:11.950636Z","iopub.status.idle":"2022-05-07T19:14:40.680704Z","shell.execute_reply.started":"2022-05-07T19:14:11.950569Z","shell.execute_reply":"2022-05-07T19:14:40.678899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# iterate through the csv file\nfor val in tweets[tweets.sentiment=='positive'].tweet:\n     \n    # typecaste each val to string\n    val = str(val)\n \n    # split the value\n    tokens = val.split()\n     \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n     \n    comment_words += \" \".join(tokens)+\" \"\n \nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(comment_words)\n \n# plot the WordCloud image                      \nplt.figure(figsize = (20, 20), facecolor = None)\nplt.title('Positive keywords in tweet')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:15:45.85917Z","iopub.execute_input":"2022-05-07T19:15:45.85956Z","iopub.status.idle":"2022-05-07T19:16:15.76936Z","shell.execute_reply.started":"2022-05-07T19:15:45.859518Z","shell.execute_reply":"2022-05-07T19:16:15.766827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round(tweets.sentiment.value_counts()*100/len(tweets),2)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:22:11.862786Z","iopub.execute_input":"2022-05-07T19:22:11.863275Z","iopub.status.idle":"2022-05-07T19:22:11.885112Z","shell.execute_reply.started":"2022-05-07T19:22:11.863238Z","shell.execute_reply":"2022-05-07T19:22:11.884317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets.sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T19:24:22.180789Z","iopub.execute_input":"2022-05-07T19:24:22.181231Z","iopub.status.idle":"2022-05-07T19:24:22.204996Z","shell.execute_reply.started":"2022-05-07T19:24:22.181186Z","shell.execute_reply":"2022-05-07T19:24:22.204044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets.to_csv('./DelhiRiotstweets.csv',index=False)","metadata":{},"execution_count":null,"outputs":[]}]}