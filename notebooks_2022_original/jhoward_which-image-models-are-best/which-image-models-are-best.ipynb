{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"*The data, concept, and initial implementation of this notebook was done in Colab by Ross Wightman, the creator of timm. I (Jeremy Howard) did some refactoring, curating, and expanding of the analysis, and added prose.*","metadata":{}},{"cell_type":"markdown","source":"## timm\n\n[PyTorch Image Models](https://timm.fast.ai/) (timm) is a wonderful library by Ross Wightman which provides state-of-the-art pre-trained computer vision models. It's like Huggingface Transformers, but for computer vision instead of NLP (and it's not restricted to transformers-based models)!\n\nRoss has been kind enough to help me understand how to best take advantage of this library by identifying the top models. I'm going to share here so of what I've learned from him, plus some additional ideas.","metadata":{}},{"cell_type":"markdown","source":"## The data\n\nRoss regularly benchmarks new models as they are added to timm, and puts the results in a CSV in the project's GitHub repo. To analyse the data, we'll first clone the repo:","metadata":{}},{"cell_type":"code","source":"! git clone --depth 1 https://github.com/rwightman/pytorch-image-models.git\n%cd pytorch-image-models/results","metadata":{"execution":{"iopub.status.busy":"2022-05-02T21:24:23.537994Z","iopub.execute_input":"2022-05-02T21:24:23.538354Z","iopub.status.idle":"2022-05-02T21:24:27.141699Z","shell.execute_reply.started":"2022-05-02T21:24:23.538264Z","shell.execute_reply":"2022-05-02T21:24:27.140668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using Pandas, we can read the two CSV files we need, and merge them together.\n\nWe'll also add a \"family\" column that will allow us to group architectures into categories with similar characteristics:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('benchmark-infer-amp-nchw-pt111-cu113-rtx3090.csv').merge(\n     pd.read_csv('results-imagenet.csv'), on='model')\ndf['secs'] = 1. / df['infer_samples_per_sec']\ndf['family'] = df.model.str.extract('^([a-z]+?(?:v2)?)(?:\\d|_|$)')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T21:24:27.14362Z","iopub.execute_input":"2022-05-02T21:24:27.143856Z","iopub.status.idle":"2022-05-02T21:24:28.802849Z","shell.execute_reply.started":"2022-05-02T21:24:27.143827Z","shell.execute_reply":"2022-05-02T21:24:28.801849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ross has told me which models he's found the most usable in practice, so I'll limit the charts to just look at these.","metadata":{}},{"cell_type":"code","source":"df2 = df[df.family.str.match('re[sg]net|beit|convnext|levit|efficient|vit')]","metadata":{"execution":{"iopub.status.busy":"2022-05-02T22:02:31.933435Z","iopub.execute_input":"2022-05-02T22:02:31.93377Z","iopub.status.idle":"2022-05-02T22:02:31.941497Z","shell.execute_reply.started":"2022-05-02T22:02:31.933738Z","shell.execute_reply":"2022-05-02T22:02:31.940237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The results\n\nHere's the results. In this chart, the x axis shows how many seconds it takes to process one image (**note**: it's a log scale), and the y axis is the accuracy on Imagenet.\n\nThe size of each bubble is proportional to the size of images used in testing.\n\nThe color shows what \"family\" the architecture is from.\n\nJust hover your mouse over a marker to see details about the model.\n\n**Note**: on my screen, Kaggle cuts off the family selector and some plotly functionality -- to see the whole thing, collapse the table of contents on the right by clicking the little arrow to the right of \"*Contents*\".","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nw,h = 1000,800\n\npx.scatter(df2, width=w, height=h, size=df2.infer_img_size**2,\n    x='secs',  y='top1', log_x=True, color='family',\n    hover_name='model', hover_data=['infer_samples_per_sec', 'infer_img_size']\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T22:02:33.919539Z","iopub.execute_input":"2022-05-02T22:02:33.920262Z","iopub.status.idle":"2022-05-02T22:02:34.081612Z","shell.execute_reply.started":"2022-05-02T22:02:33.920213Z","shell.execute_reply":"2022-05-02T22:02:34.0808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this, we can see that the *levit* family models are extremely fast for image recognition, and clearly the most accurate amongst the faster models. That's not surprising, since these models are a hybrid of the best ideas from CNNs and transformers, so get the benefit of each. In fact, we see a similar thing even in the middle category of speeds -- the best is the ConvNeXt, which is a pure CNN, but which takes advantage of ideas from the transformers literature.\n\nFor the slowest models, *beit* is the most accurate -- although we need to be a bit careful of interpreting this, since it's trained on a larger dataset (ImageNet-21k, which is also used for *vit* models).\n\nI'll add one other plot, which is of speed vs parameter count. Often, parameter count is used in papers as a proxy for speed. However, as we see, there is a wide variation in speeds at each level of parameter count, so it's really not a useful proxy.\n\n(Parameter count may be be useful for identifying how much memory a model needs, but even for that it's not always a great proxy.)","metadata":{}},{"cell_type":"code","source":"px.scatter(df2, width=w, height=h,\n    x='param_count_x',  y='secs', log_x=True, log_y=True, color='infer_img_size',\n    hover_name='model', hover_data=['infer_samples_per_sec', 'family']\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T21:55:57.199906Z","iopub.execute_input":"2022-05-02T21:55:57.200253Z","iopub.status.idle":"2022-05-02T21:55:57.279589Z","shell.execute_reply.started":"2022-05-02T21:55:57.200216Z","shell.execute_reply":"2022-05-02T21:55:57.278561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we should remember that speed depends on hardware. If you're using something other than a modern NVIDIA GPU, your results may be different. In particular, I suspect that transformers-based models might have worse performance in general on CPUs (although I need to study this more to be sure).","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}