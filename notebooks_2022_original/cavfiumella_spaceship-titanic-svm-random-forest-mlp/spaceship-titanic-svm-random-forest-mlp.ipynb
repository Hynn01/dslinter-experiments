{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nSEED = 1234 # global seed for random operations\n\ndf = pd.read_csv(\"/kaggle/input/spaceship-titanic-data-cleaning/train.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T20:44:21.718815Z","iopub.execute_input":"2022-05-06T20:44:21.719226Z","iopub.status.idle":"2022-05-06T20:44:21.823577Z","shell.execute_reply.started":"2022-05-06T20:44:21.719118Z","shell.execute_reply":"2022-05-06T20:44:21.822595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"In this notebook data from [Spaceship Titanic competition](https://www.kaggle.com/competitions/spaceship-titanic), previously preprocessed in another [notebook](https://www.kaggle.com/code/cavfiumella/spaceship-titanic-data-cleaning), are predicted using a SVM, a Random Forest and a Multilayer Perceptron.","metadata":{}},{"cell_type":"markdown","source":"## Data normalization","metadata":{}},{"cell_type":"markdown","source":"SVM, Decision Trees and Multilayer Perceptrons work better on normalized data. There is not a unique normalization; in this notebook _standard normalization_ is used:\n\n<center>$x' = (x - \\langle x \\rangle)\\ /\\ \\sigma$</center>\n\nData has to be normalized so that features have a similar range for their values.","metadata":{}},{"cell_type":"markdown","source":"## Train and test data","metadata":{}},{"cell_type":"markdown","source":"To avoid supervised model adapting too much to training data, loosing precision in generic data prediction (i.e. _overfitting_) let's split our data into one subset for training and one for testing. Only training data are going to be used to build the model, while testing subset is going to be used to determine the most accurate object.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(df, train_size = 0.8, random_state = SEED, stratify = df.Transported)\n\n# let's split x from y (i.e. features from targets)\nx_train, y_train = train.drop(columns=\"Transported\").values, train.Transported.values\nx_test, y_test = test.drop(columns=\"Transported\").values, test.Transported.values","metadata":{"execution":{"iopub.status.busy":"2022-05-06T20:44:24.891973Z","iopub.execute_input":"2022-05-06T20:44:24.892584Z","iopub.status.idle":"2022-05-06T20:44:26.149079Z","shell.execute_reply.started":"2022-05-06T20:44:24.89254Z","shell.execute_reply":"2022-05-06T20:44:26.147923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {} # all built models","metadata":{"execution":{"iopub.status.busy":"2022-05-06T20:44:47.886372Z","iopub.execute_input":"2022-05-06T20:44:47.887Z","iopub.status.idle":"2022-05-06T20:44:47.891356Z","shell.execute_reply.started":"2022-05-06T20:44:47.886962Z","shell.execute_reply":"2022-05-06T20:44:47.890491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Support Vector Machine","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\nmodels[\"svm\"] = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"svm\", SVC())\n])\n\nmodels[\"svm\"] = models[\"svm\"].fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T20:44:49.714125Z","iopub.execute_input":"2022-05-06T20:44:49.714455Z","iopub.status.idle":"2022-05-06T20:44:50.855821Z","shell.execute_reply.started":"2022-05-06T20:44:49.714418Z","shell.execute_reply":"2022-05-06T20:44:50.854814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"markdown","source":"Random Forest use multiple Decision Trees to improve the performance of a single tree.","metadata":{}},{"cell_type":"code","source":"#from sklearn.pipeline import Pipeline\n#from sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodels[\"tree\"] = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"tree\", RandomForestClassifier(n_jobs = -1, random_state = SEED))\n])\n\nmodels[\"tree\"] = models[\"tree\"].fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T20:44:52.306867Z","iopub.execute_input":"2022-05-06T20:44:52.307206Z","iopub.status.idle":"2022-05-06T20:44:52.98881Z","shell.execute_reply.started":"2022-05-06T20:44:52.307171Z","shell.execute_reply":"2022-05-06T20:44:52.987997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multilayer perceptron","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Input, BatchNormalization, Dense\n\nmodels[\"mlp\"] = Sequential([\n    Input((x_train.shape[-1],), name = \"input\"),\n    BatchNormalization(name = \"norm\"),\n    Dense(2048, activation = \"relu\", name = \"dense1\"),\n    Dense(1, name = \"dense2\")\n])\n\nmodels[\"mlp\"].summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:13:58.377652Z","iopub.execute_input":"2022-05-06T21:13:58.377995Z","iopub.status.idle":"2022-05-06T21:13:58.425323Z","shell.execute_reply.started":"2022-05-06T21:13:58.377956Z","shell.execute_reply":"2022-05-06T21:13:58.424305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's compile the model choosing optimizer, loss and metric to be used during training.","metadata":{}},{"cell_type":"code","source":"models[\"mlp\"].compile(\n    optimizer = \"adam\",                # tensorflow.keras.optimizers.Adam\n    loss      = \"binary_crossentropy\", # tensorflow.keras.losses.BinaryCrossentropy\n    metrics   = \"accuracy\"             # tensorflow.keras.metrics.accuracy\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:14:00.175321Z","iopub.execute_input":"2022-05-06T21:14:00.175677Z","iopub.status.idle":"2022-05-06T21:14:00.188254Z","shell.execute_reply.started":"2022-05-06T21:14:00.175636Z","shell.execute_reply":"2022-05-06T21:14:00.186969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 1000\nEPOCHS = 30\n\nhistory = models[\"mlp\"].fit(\n    x_train.astype(float), y_train.astype(float),\n    validation_data = (x_test.astype(float), y_test.astype(float)),\n    batch_size = BATCH_SIZE, epochs = EPOCHS,\n    workers = 4, use_multiprocessing = True\n).history","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:14:01.120329Z","iopub.execute_input":"2022-05-06T21:14:01.121112Z","iopub.status.idle":"2022-05-06T21:14:05.147658Z","shell.execute_reply.started":"2022-05-06T21:14:01.121065Z","shell.execute_reply":"2022-05-06T21:14:05.14679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(2,1, sharex = True, figsize = (8,8))\n\nfor i, key in enumerate([\"loss\", \"accuracy\"]):\n    ax[i].plot(np.arange(1, EPOCHS+1), history[key], label = \"train\")\n    ax[i].plot(np.arange(1, EPOCHS+1), history[f\"val_{key}\"], label = \"test\")\n    ax[i].legend()\n    ax[i].grid(True)\n    if i != 0: ax[i].set_xlabel(\"epoch\")\n    ax[i].set_ylabel(key)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:14:05.149233Z","iopub.execute_input":"2022-05-06T21:14:05.149497Z","iopub.status.idle":"2022-05-06T21:14:05.573724Z","shell.execute_reply.started":"2022-05-06T21:14:05.149464Z","shell.execute_reply":"2022-05-06T21:14:05.571235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model evaluation","metadata":{}},{"cell_type":"markdown","source":"Let's compare different models accuracy on testing data.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nscores = []\nfor key, model in models.items():\n    if key == \"mlp\":\n        scores += [[\n            key, accuracy_score(y_true = y_test, y_pred = [round(a) for a in model.predict(x_test.astype(float)).ravel()])\n        ]]\n    else:\n        scores += [[\n            key, accuracy_score(y_true = y_test, y_pred = model.predict(x_test))\n        ]]\n\npd.DataFrame(scores, columns = [\"model\", \"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:14:05.696221Z","iopub.execute_input":"2022-05-06T21:14:05.696539Z","iopub.status.idle":"2022-05-06T21:14:06.232857Z","shell.execute_reply.started":"2022-05-06T21:14:05.696508Z","shell.execute_reply":"2022-05-06T21:14:06.23173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}