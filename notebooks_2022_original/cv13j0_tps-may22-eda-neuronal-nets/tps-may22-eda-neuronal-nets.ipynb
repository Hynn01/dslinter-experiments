{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting States of Manufacturing Control Data; Using Neuronal Nets ⚙️","metadata":{}},{"cell_type":"markdown","source":"**Note: Use GPU for Training...**\n\n**Objective:** Build a powerfull NN Model that can provide a good estimation.\n\n**Strategy:** I think I will follow this strategy:\n\n**Level 1 Getting Started**\n\n* Quick EDA to identify potential opportunities.\n* Simple pre-processing step to encode categorical features.\n* A basic CV strategy using 90% for TRaining and 10% for Testing.\n* Looking at the feature importances.\n* Creating a submission file.\n* Submit the file to Kaggle.\n\n**Level 2 Feature Engineering**\n\n* Feature engineering using text information. (Massive boost in the score)\n* Cross validation loop.\n\n**Level 3 Model Optimization**\n* Work in Progress...\n\n---\n**Other Similar Implementations**\nI been working on other architechtures at the same time, to see what works more effiently\n\nXGBoost and LGBM Models\n\nhttps://www.kaggle.com/code/cv13j0/tps-may22-eda-gbdt\n\n---\n\n\n\n\n**Data Description**\n\nFor this challenge, you are given (simulated) manufacturing control data and are tasked to predict whether the machine is in state 0 or state 1. \nThe data has various feature interactions that may be important in determining the machine state.\n\nGood luck!\n\n**Files**\n* train.csv - the training data, which includes normalized continuous data and categorical data\n* test.csv - the test set; your task is to predict binary target variable which represents the state of a manufacturing process\n* sample_submission.csv - a sample submission file in the correct format\n\n---\n**Notebooks Ideas and Credits**\n\nI took ideas or inspiration from the following notebooks, if you enjoy my work, please take a look to the notebooks that inspire my work.\n\nTPSMAY22 Gradient-Boosting Quickstart: https://www.kaggle.com/code/ambrosm/tpsmay22-gradient-boosting-quickstart/notebook","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 1. Loading the Requiered Libraries","metadata":{}},{"cell_type":"code","source":"%%time\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-08T02:27:46.558318Z","iopub.execute_input":"2022-05-08T02:27:46.558916Z","iopub.status.idle":"2022-05-08T02:27:46.572712Z","shell.execute_reply.started":"2022-05-08T02:27:46.558821Z","shell.execute_reply":"2022-05-08T02:27:46.571894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport datetime","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:27:46.575186Z","iopub.execute_input":"2022-05-08T02:27:46.575713Z","iopub.status.idle":"2022-05-08T02:27:46.582707Z","shell.execute_reply.started":"2022-05-08T02:27:46.575674Z","shell.execute_reply":"2022-05-08T02:27:46.581737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n","metadata":{}},{"cell_type":"markdown","source":"# 2. Setting the Notebook","metadata":{}},{"cell_type":"code","source":"%%time\n# I like to disable my Notebook Warnings.\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:27:46.584121Z","iopub.execute_input":"2022-05-08T02:27:46.584408Z","iopub.status.idle":"2022-05-08T02:27:46.591677Z","shell.execute_reply.started":"2022-05-08T02:27:46.584369Z","shell.execute_reply":"2022-05-08T02:27:46.590875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Notebook Configuration...\n\n# Amount of data we want to load into the Model...\nDATA_ROWS = None\n# Dataframe, the amount of rows and cols to visualize...\nNROWS = 50\nNCOLS = 15\n# Main data location path...\nBASE_PATH = '...'","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:27:46.594306Z","iopub.execute_input":"2022-05-08T02:27:46.594778Z","iopub.status.idle":"2022-05-08T02:27:46.601321Z","shell.execute_reply.started":"2022-05-08T02:27:46.59474Z","shell.execute_reply":"2022-05-08T02:27:46.600529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Configure notebook display settings to only use 2 decimal places, tables look nicer.\npd.options.display.float_format = '{:,.5f}'.format\npd.set_option('display.max_columns', NCOLS) \npd.set_option('display.max_rows', NROWS)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:27:46.602856Z","iopub.execute_input":"2022-05-08T02:27:46.603408Z","iopub.status.idle":"2022-05-08T02:27:46.612947Z","shell.execute_reply.started":"2022-05-08T02:27:46.603371Z","shell.execute_reply":"2022-05-08T02:27:46.612134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 3. Loading the Information (CSV) Into A Dataframe","metadata":{}},{"cell_type":"code","source":"%%time\n# Load the CSV information into a Pandas DataFrame...\ntrn_data = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/train.csv')\ntst_data = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/test.csv')\n\nsub = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:27:46.614197Z","iopub.execute_input":"2022-05-08T02:27:46.614659Z","iopub.status.idle":"2022-05-08T02:27:54.572296Z","shell.execute_reply.started":"2022-05-08T02:27:46.614621Z","shell.execute_reply":"2022-05-08T02:27:54.570785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 4. Exploring the Information Available","metadata":{}},{"cell_type":"markdown","source":"## 4.1. Analysing the Trian Dataset","metadata":{}},{"cell_type":"code","source":"%%time\n# Explore the shape of the DataFrame...\ntrn_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:27:54.573844Z","iopub.execute_input":"2022-05-08T02:27:54.574118Z","iopub.status.idle":"2022-05-08T02:27:54.582883Z","shell.execute_reply.started":"2022-05-08T02:27:54.57408Z","shell.execute_reply":"2022-05-08T02:27:54.582079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display simple information of the variables in the dataset...\ntrn_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:27:54.58448Z","iopub.execute_input":"2022-05-08T02:27:54.585075Z","iopub.status.idle":"2022-05-08T02:27:54.727681Z","shell.execute_reply.started":"2022-05-08T02:27:54.585036Z","shell.execute_reply":"2022-05-08T02:27:54.726927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display the first few rows of the DataFrame...\ntrn_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:27:54.729888Z","iopub.execute_input":"2022-05-08T02:27:54.730453Z","iopub.status.idle":"2022-05-08T02:27:54.750519Z","shell.execute_reply.started":"2022-05-08T02:27:54.73041Z","shell.execute_reply":"2022-05-08T02:27:54.749783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Generate a simple statistical summary of the DataFrame, Only Numerical...\ntrn_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:27:54.751899Z","iopub.execute_input":"2022-05-08T02:27:54.752397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Calculates the total number of missing values...\ntrn_data.isnull().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display the number of missing values by variable...\ntrn_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display the number of unique values for each variable...\ntrn_data.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the number of unique values for each variable, sorted by quantity...\ntrn_data.nunique().sort_values(ascending = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Check some of the categorical variables\ncateg_cols = ['f_29','f_30','f_13', 'f_18','f_17','f_14','f_11','f_10','f_09','f_15','f_07','f_12','f_16','f_08','f_27']\ntrn_data[categ_cols].sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Generate a quick correlation matrix to understand the dataset better\ncorrelation = trn_data.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Diplay the correlation matrix\ncorrelation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Check the most correlated variables to the target\ncorrelation['target'].sort_values(ascending = False)[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Check the least correlated variables to the target\ncorrelation['target'].sort_values(ascending = True)[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 4.2. Analysing the Trian Labels Dataset","metadata":{}},{"cell_type":"code","source":"%%time\n# Check how well balanced is the dataset\ntrn_data['target'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Check some statistics on the target variable\ntrn_data['target'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 5. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Text Base Features","metadata":{}},{"cell_type":"code","source":"%%time\n# The idea is to create a simple funtion to count the amount of letters on feature 27.\n# feature 27 seems quite important \n\ndef count_sequence(df, field):\n    '''\n    For each letter of the provided suquence it return new feature with the number of occurences.\n    '''\n    alphabet = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']    \n    \n    for letter in alphabet:\n        df[letter + '_count'] = df[field].str.count(letter)\n    \n    df[\"unique_characters\"] = df['f_27'].apply(lambda s: len(set(s)))\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Utilizes the new created funtions to generate more features.\n# trn_data = count_sequence(trn_data, 'f_27')\n# tst_data = count_sequence(tst_data, 'f_27')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef count_chars(df, field):\n    '''\n    Describe something...\n    '''\n    \n    for i in range(10):\n        df[f'ch_{i}'] = df[field].str.get(i).apply(ord) - ord('A')\n        \n    df[\"unique_characters\"] = df[field].apply(lambda s: len(set(s)))\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Utilizes the new created funtions to generate more features.\ntrn_data = count_chars(trn_data, 'f_27')\ntst_data = count_chars(tst_data, 'f_27')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stats Features","metadata":{}},{"cell_type":"code","source":"%%time\ncontinuous_feat = ['f_00', 'f_01', 'f_02', 'f_03', 'f_04', 'f_05', 'f_06', 'f_19', 'f_20', 'f_21', 'f_22', 'f_23', 'f_24', 'f_25', 'f_26', 'f_28']\n\ndef stat_features(df, cols = continuous_feat):\n    '''\n    \n    '''\n    \n    df['f_sum']  = df[continuous_feat].sum(axis=1)\n    df['f_min']  = df[continuous_feat].min(axis=1)\n    df['f_max']  = df[continuous_feat].max(axis=1)\n    df['f_std']  = df[continuous_feat].std(axis=1)    \n    df['f_mad']  = df[continuous_feat].mad(axis=1)\n    df['f_mean'] = df[continuous_feat].mean(axis=1)\n    df['f_kurt'] = df[continuous_feat].kurt(axis=1)\n    df['f_count_pos']  = df[continuous_feat].gt(0).count(axis=1)\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_data = stat_features(trn_data, continuous_feat)\ntst_data = stat_features(tst_data, continuous_feat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 7. Pre-Processing Labels","metadata":{}},{"cell_type":"code","source":"%%time\n# Define a label encoding function\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndef encode_features(df, cols = ['f_27']):\n    for col in cols:\n        df[col + '_enc'] = encoder.fit_transform(df[col])\n    return df\n\ntrn_data = encode_features(trn_data)\ntst_data = encode_features(tst_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the results of the transformation\ntrn_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.2 - One-Hot Encode","metadata":{}},{"cell_type":"code","source":"# We will process to One-Hot encode all this variables...\n# f_29           2\n# f_30           3\n# f_13          13\n# f_18          14\n# f_17          14\n# f_14          14\n# f_11          14\n# f_10          15\n# f_09          15\n# f_15          15\n# f_07          16\n# f_12          16\n# f_16          16\n# f_08          16","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef one_hot_encoder(df_trn, df_tst, var_list):\n    '''\n    '''\n    df_trn['is_train'] = 1\n    df_tst['is_train'] = 0\n\n    combined = df_trn.append(df_tst)\n    combined = pd.get_dummies(combined, columns = var_list)\n    return combined[combined['is_train'] == 1], combined[combined['is_train'] == 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#trn_data, tst_data = one_hot_encoder(trn_data,tst_data, [\n                                                         #'f_29',\n                                                         #'f_30',\n                                                         #'f_13',\n                                                         #'f_18',\n                                                         #'f_17',\n                                                         #'f_14',\n                                                         #'f_11',\n                                                         #'f_10',\n                                                         #'f_09',\n                                                         #'f_15',\n                                                         #'f_07',\n                                                         #'f_12',\n                                                         #'f_16',\n                                                         #'f_08'\n                                                         #])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 8. Feature Selection for Baseline Model","metadata":{}},{"cell_type":"code","source":"%%time\n# Define what will be used in the training stage\nignore = ['id', \n          'f_27', \n          'f_27_enc', \n          'is_train', \n          'target'] # f_27 has been label encoded...\n\nfeatures = [feat for feat in trn_data.columns if feat not in ignore]\ntarget_feature = 'target'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 9. Creating a Simple Train / Test Split Strategy","metadata":{}},{"cell_type":"code","source":"%%time\n# Creates a simple train split breakdown for baseline model\nfrom sklearn.model_selection import train_test_split\ntest_size_pct = 0.20\nX_train, X_valid, y_train, y_valid = train_test_split(trn_data[features], trn_data[target_feature], test_size = test_size_pct, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 10. Building a Baseline NN Model, Simple Split","metadata":{}},{"cell_type":"code","source":"%%time\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\nfrom tensorflow.keras.layers import Dense, Input, InputLayer, Add, BatchNormalization, Dropout\n\nfrom sklearn.preprocessing import StandardScaler\nimport random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nnp.random.seed(1)\nrandom.seed(1)\ntf.random.set_seed(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef nn_model():\n    '''\n    '''\n    \n    activation_func = 'swish'\n    inputs = Input(shape = (len(features)))\n    \n    x = Dense(64, \n              #use_bias  = True, \n              kernel_regularizer = tf.keras.regularizers.l2(30e-6), \n              activation = activation_func)(inputs)\n    \n    #x = BatchNormalization()(x)\n    \n    x = Dense(64, \n              #use_bias  = True, \n              kernel_regularizer = tf.keras.regularizers.l2(30e-6), \n              activation = activation_func)(x)\n    \n    #x = BatchNormalization()(x)\n    \n    x = Dense(64, \n          #use_bias  = True, \n          kernel_regularizer = tf.keras.regularizers.l2(30e-6), \n          activation = activation_func)(x)\n    \n    #x = BatchNormalization()(x)\n    \n    x = Dense(64, \n              #use_bias  = True, \n              kernel_regularizer = tf.keras.regularizers.l2(30e-6), \n              activation = activation_func)(x)\n    \n    #x = BatchNormalization()(x)\n\n    x = Dense(16, \n              #use_bias  = True, \n              kernel_regularizer = tf.keras.regularizers.l2(30e-6), \n              activation = activation_func)(x)\n    \n    #x = BatchNormalization()(x)\n\n    x = Dense(1 , \n              #use_bias  = True, \n              #kernel_regularizer = tf.keras.regularizers.l2(30e-6),\n              activation = 'sigmoid')(x)\n    \n    model = Model(inputs, x)\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\narchitecture = nn_model()\narchitecture.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Defining model parameters...\nBATCH_SIZE         = 4096\nEPOCHS             = 200 \nEPOCHS_COSINEDECAY = 300 \nDIAGRAMS           = True\nUSE_PLATEAU        = False\nINFERENCE          = False\nVERBOSE            = 0 \nTARGET             = 'target'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" %%time\n# Defining model training function...\ndef fit_model(X_train, y_train, X_val, y_val, run = 0):\n    '''\n    '''\n    lr_start = 0.01\n    start_time = datetime.datetime.now()\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n\n    epochs = EPOCHS    \n    lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.7, patience = 4, verbose = VERBOSE)\n    es = EarlyStopping(monitor = 'val_loss',patience = 12, verbose = 1, mode = 'min', restore_best_weights = True)\n    tm = tf.keras.callbacks.TerminateOnNaN()\n    callbacks = [lr, es, tm]\n    \n    # Cosine Learning Rate Decay\n    if USE_PLATEAU == False:\n        epochs = EPOCHS_COSINEDECAY\n        lr_end = 0.0002\n\n        def cosine_decay(epoch):\n            if epochs > 1:\n                w = (1 + math.cos(epoch / (epochs - 1) * math.pi)) / 2\n            else:\n                w = 1\n            return w * lr_start + (1 - w) * lr_end\n        \n        lr = LearningRateScheduler(cosine_decay, verbose = 0)\n        callbacks = [lr, tm]\n        \n    model = nn_model()\n    optimizer_func = tf.keras.optimizers.Adam(learning_rate = lr_start)\n    loss_func = tf.keras.losses.BinaryCrossentropy()\n    model.compile(optimizer = optimizer_func, loss = loss_func)\n    \n    X_val = scaler.transform(X_val)\n    validation_data = (X_val, y_val)\n    \n    history = model.fit(X_train, \n                        y_train, \n                        validation_data = validation_data, \n                        epochs          = epochs,\n                        verbose         = VERBOSE,\n                        batch_size      = BATCH_SIZE,\n                        shuffle         = True,\n                        callbacks       = callbacks\n                       )\n    \n    history_list.append(history.history)\n    print(f'Training loss:{history_list[-1][\"loss\"][-1]:.3f}')\n    callbacks, es, lr, tm, history = None, None, None, None, None\n    \n    \n    y_val_pred = model.predict(X_val, batch_size = BATCH_SIZE, verbose =VERBOSE)\n    score = roc_auc_score(y_val, y_val_pred)\n    print(f'Fold {run}.{fold} | {str(datetime.datetime.now() - start_time)[-12:-7]}'\n          f'| AUC: {score:.5f}')\n    \n    score_list.append(score)\n    \n    tst_data_scaled = scaler.transform(tst_data[features])\n    tst_pred = model.predict(tst_data_scaled)\n    predictions.append(tst_pred)\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport math\n\n# Create empty lists to store NN information...\nhistory_list = []\nscore_list   = []\npredictions  = []\n\n# Define kfolds for training purposes...\nkf = KFold(n_splits = 5)\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(trn_data)):\n    X_train, X_val = trn_data.iloc[trn_idx][features], trn_data.iloc[val_idx][features]\n    y_train, y_val = trn_data.iloc[trn_idx][TARGET], trn_data.iloc[val_idx][TARGET]\n    \n    fit_model(X_train, y_train, X_val, y_val)\n    \nprint(f'OOF AUC: {np.mean(score_list):.5f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# OOF AUC: 0.99658... 10 Folds, Batch Normalization, Using One-Hot Features, Epochs = 150, [64,64,64,16,1]...\n# OOF AUC: 0.99653... 05 Folds, No Batch Normalization, Using Partial One-Hot Features, Epochs = 150, [64,64,64,16,1]...\n# OOF AUC: 0.99757... 05 Folds, No Batch Normalization, No One-Hot Features, Epochs = 150, [64,64,64,16,1]...\n# OOF AUC: 0.99766... 05 Folds, No Batch Normalization, No One-Hot Features, Epochs = 200, [64,64,64,16,1]...\n# OOF AUC: 0.99771... 05 Folds, No Batch Normalization, No One-Hot Features, Epochs = 300, [64,64,64,16,1]...\n# OOF AUC: 0.99759... 05 Folds, No Batch Normalization, No One-Hot Features, Epochs = 300, [96,64,64,16,1]...\n# OOF AUC: 0.99772... 05 Folds, No Batch Normalization, No One-Hot Features, Epochs = 300, [64,64,32,16,1]...\n# OOF AUC: 0.99772... 05 Folds, No Batch Normalization, No One-Hot Features, Epochs = 300, [64,64,32,16,1]...\n# OOF AUC: 0.99769... 05 Folds, No Batch Normalization, No One-Hot Features, Epochs = 300, [64,64,64,16,1], Stat Features = Yes\n# OOF AUC: 0.99769... 05 Folds, No Batch Normalization, No One-Hot Features, Epochs = 300, [256,64,64,16,1], Stat Features = Yes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 11. Undertanding Model Behavior, Feature Importance","metadata":{}},{"cell_type":"code","source":"# Work in Progress...","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 12. Baseline Model Submission File Generation","metadata":{}},{"cell_type":"code","source":"%%time\n# Review the format of the submission file\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Populated the prediction on the submission dataset and creates an output file\nsub['target'] = np.array(predictions).mean(axis = 0)\nsub.to_csv('my_submission_050722.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Create submission\nprint(f\"{len(features)} features\")\n\npred_list = []\nfor seed in range(10):\n    model = fit_model(X_tr, y_tr, run = seed)\n    model.fit(X_tr.values, y_tr)\n    pred_list.append(scipy.stats.rankdata(model.predict(tst_data[features].values, batch_size = BATCH_SIZE)))\n    print(f\"{seed:2}\", pred_list[-1])\nprint()\n\nsubmission = tst_data[['id']].copy()\nsubmission[TARGET] = np.array(pred_list).mean(axis = 0)\n\nsubmission.to_csv('submission_nn_05012022.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Review the submission file as a final step to upload to Kaggle.\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}}]}