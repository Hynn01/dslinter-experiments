{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Machine Learning model to Retail Industry","metadata":{}},{"cell_type":"markdown","source":"# Section 1: Business understanding","metadata":{}},{"cell_type":"markdown","source":"This project presents a Machine Learning model (Random Forest) to predict whether or not a customer will use an offer sent by a business of the Retail Industry.","metadata":{}},{"cell_type":"markdown","source":"The project presents answers to the following questions:\n\nQuestion 1: Will a customer respond to an offer sent by the commerce?","metadata":{}},{"cell_type":"markdown","source":"# Section 2: Data Understanding","metadata":{}},{"cell_type":"markdown","source":"### Gather process","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\n# read in the json files\nportfolio = pd.read_json('../input/starbucks-app-customer-reward-program-data/portfolio.json', orient='records', lines=True)\nprofile = pd.read_json('../input/starbucks-app-customer-reward-program-data/profile.json', orient='records', lines=True)\ntranscript = pd.read_json('../input/starbucks-app-customer-reward-program-data/transcript.json', orient='records', lines=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:19.960154Z","iopub.execute_input":"2022-04-18T21:36:19.960502Z","iopub.status.idle":"2022-04-18T21:36:23.548421Z","shell.execute_reply.started":"2022-04-18T21:36:19.96041Z","shell.execute_reply":"2022-04-18T21:36:23.547641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Sets\n\nThe data is contained in three files:\n\n* portfolio.json - containing offer ids and meta data about each offer (duration, type, etc.)\n* profile.json - demographic data for each customer\n* transcript.json - records for transactions, offers received, offers viewed, and offers completed\n\nHere is the schema and explanation of each variable in the files:\n\n**portfolio.json**\n* id (string) - offer id\n* offer_type (string) - type of offer ie BOGO, discount, informational\n* difficulty (int) - minimum required spend to complete an offer\n* reward (int) - reward given for completing an offer\n* duration (int) - time for offer to be open, in days\n* channels (list of strings)\n\n**profile.json**\n* age (int) - age of the customer \n* became_member_on (int) - date when customer created an app account\n* gender (str) - gender of the customer (note some entries contain 'O' for other rather than M or F)\n* id (str) - customer id\n* income (float) - customer's income\n\n**transcript.json**\n* event (str) - record description (ie transaction, offer received, offer viewed, etc.)\n* person (str) - customer id\n* time (int) - time in hours since start of test. The data begins at time t=0\n* value - (dict of strings) - either an offer id or transaction amount depending on the record","metadata":{}},{"cell_type":"markdown","source":"# Section 3: Prepare Data","metadata":{}},{"cell_type":"markdown","source":"### Explore: Transcript (transactions)","metadata":{}},{"cell_type":"code","source":"transcript.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:23.551281Z","iopub.execute_input":"2022-04-18T21:36:23.551809Z","iopub.status.idle":"2022-04-18T21:36:23.570213Z","shell.execute_reply.started":"2022-04-18T21:36:23.551764Z","shell.execute_reply":"2022-04-18T21:36:23.569528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we have dictionaries in the value column\nlist(transcript.value.iloc[12658].keys())","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:23.571533Z","iopub.execute_input":"2022-04-18T21:36:23.57182Z","iopub.status.idle":"2022-04-18T21:36:23.579213Z","shell.execute_reply.started":"2022-04-18T21:36:23.571785Z","shell.execute_reply":"2022-04-18T21:36:23.578348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there are 4 types of events\ntranscript.event.unique()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:23.580933Z","iopub.execute_input":"2022-04-18T21:36:23.581651Z","iopub.status.idle":"2022-04-18T21:36:23.644432Z","shell.execute_reply.started":"2022-04-18T21:36:23.581594Z","shell.execute_reply":"2022-04-18T21:36:23.64375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare data: clean process (transcript df)","metadata":{}},{"cell_type":"markdown","source":"In this dataframe we can apply the following modifications:\n\n- we can separate the column value into three more columns: \"offer_id\", \"amount\" e \"reward\"\n- we can separate the column value into three more columns (one hot encoding): \"offer_received\", \"offer_viewed\", \"transaction\" e \"offer_completed\"","metadata":{}},{"cell_type":"code","source":"# we have three different keys in the value column (dictionary): offer_id, amount and reward\n# let's create functions to extract data from these dictionaries' keys\n\ndef expand_transcript_offer_id():\n    '''\n    this function extract the data: \"offer_id\" from the value column\n    '''\n    offer_id_list = []\n\n    for dic in transcript.value.iloc[0:]:\n        if 'offer id' in list(dic.keys()):\n            offer_id_list.append(dic['offer id'])\n        elif 'offer_id' in list(dic.keys()):\n            offer_id_list.append(dic['offer_id'])\n        else:\n            offer_id_list.append(np.nan)\n    \n    return offer_id_list\n    \ndef expand_transcript_amount(entry):\n    '''\n    this function extract the data: \"amount\" from the value column\n    '''\n    try:\n        if type(entry['amount']) == float or int:\n            return entry['amount']\n    except:\n        return np.nan\n    \ndef expand_transcript_reward(entry):\n    '''\n    this function extract the data: \"reward\" from the value column\n    '''\n    try:\n        if type(entry['reward']) == float or int:\n            return entry['reward']\n    except:\n        return np.nan","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:23.646677Z","iopub.execute_input":"2022-04-18T21:36:23.64739Z","iopub.status.idle":"2022-04-18T21:36:23.65683Z","shell.execute_reply.started":"2022-04-18T21:36:23.647347Z","shell.execute_reply":"2022-04-18T21:36:23.656092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def expand_transcript_all(transcript=transcript):\n    '''\n    this function use the following functions:\n    - expand_transcript_offer_id()\n    - expand_transcript_amount(entry)\n    - expand_transcript_reward(entry)\n    to create new columns in the transcript dataframe from the data in the value column\n    '''\n    \n    # then we create three new columns     \n    offer_id_list = expand_transcript_offer_id()\n    transcript['offer_id'] = offer_id_list\n    transcript['amount'] = transcript['value'].apply(lambda x: expand_transcript_amount(x))\n    transcript['reward'] = transcript['value'].apply(lambda x: expand_transcript_reward(x))\n    \n    # and then drop the \"value\" column\n    transcript.drop(columns=[\"value\"], inplace=True)\n    return transcript\n\ntranscript = expand_transcript_all(transcript=transcript)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:23.659198Z","iopub.execute_input":"2022-04-18T21:36:23.659897Z","iopub.status.idle":"2022-04-18T21:36:24.524994Z","shell.execute_reply.started":"2022-04-18T21:36:23.659857Z","shell.execute_reply":"2022-04-18T21:36:24.524214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transcript.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:24.526268Z","iopub.execute_input":"2022-04-18T21:36:24.526513Z","iopub.status.idle":"2022-04-18T21:36:24.538745Z","shell.execute_reply.started":"2022-04-18T21:36:24.52648Z","shell.execute_reply":"2022-04-18T21:36:24.537504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transcript.hist(bins=100, figsize=(10, 5));","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:24.540143Z","iopub.execute_input":"2022-04-18T21:36:24.540463Z","iopub.status.idle":"2022-04-18T21:36:25.638625Z","shell.execute_reply.started":"2022-04-18T21:36:24.540426Z","shell.execute_reply":"2022-04-18T21:36:25.637925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we can see that one person can have more than one transaction (event) and there are 17000 clients\nlen(transcript), len(transcript.person.unique())","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:25.639883Z","iopub.execute_input":"2022-04-18T21:36:25.640132Z","iopub.status.idle":"2022-04-18T21:36:25.700083Z","shell.execute_reply.started":"2022-04-18T21:36:25.640095Z","shell.execute_reply":"2022-04-18T21:36:25.698966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this dataset we have information of the user's transactions and the offers the users receive. So, we can inquire which offers the users received, viewed and completed and which they just received and didn't used. It can be interesting if we apply one hot encoding to each event: received, viewed and completed. So, we will separate the dataset by event and then merge to apply one hot encoding.\n\nWe also have the event: transaction. Using this event we can see the number of transactions of each user and the total amount the users have spent. This information can be valuable to complement the profile dataset.","metadata":{}},{"cell_type":"markdown","source":"### events: received, viewed, completed, transaction","metadata":{}},{"cell_type":"code","source":"def separate_transcript(transcript=transcript):\n    '''\n    this function separates the transcript df into 4 df according to\n    the 4 types of events\n    '''\n    # the received df just tells us that a user received an offer\n    # so, it doesn't have amount and reward information, let's drop\n    received = transcript[transcript['event'] == \"offer received\"]\n    received.drop(columns=['amount', 'reward'], inplace=True)\n    \n    # the received df just tells us that a user viewed an offer\n    # so, it doesn't have amount and reward information, let's drop\n    viewed = transcript[transcript['event'] == \"offer viewed\"]\n    viewed.drop(columns=['amount', 'reward'], inplace=True)\n    \n    # the completed df tells us that a user completed (responded to) an offer\n    # so, it just have reward information, let's drop amount column\n    completed = transcript[transcript['event'] == \"offer completed\"]\n    completed.drop(columns=['amount'], inplace=True)\n    \n    # the transaction df tells us the amount of each transaction\n    # so, itjust have amount information, let's drop offer_id and reward columns \n    transaction = transcript[transcript['event'] == \"transaction\"]\n    transaction.drop(columns=['offer_id', 'reward'], inplace=True)\n    \n    #change the columns' names\n    received.columns = ['customer_id', 'event', 'time', 'offer_id']\n    viewed.columns = ['customer_id', 'event', 'time', 'offer_id']\n    completed.columns = ['customer_id', 'event', 'time', 'offer_id', 'reward']\n    transaction.columns = ['customer_id', 'event', 'time', 'amount']\n    \n    return received, viewed, completed, transaction\n\nreceived, viewed, completed, transaction = separate_transcript(transcript=transcript)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:25.701424Z","iopub.execute_input":"2022-04-18T21:36:25.701674Z","iopub.status.idle":"2022-04-18T21:36:25.95377Z","shell.execute_reply.started":"2022-04-18T21:36:25.70164Z","shell.execute_reply":"2022-04-18T21:36:25.95301Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are going to apply one hot encoding to the events: received, viewed and completed by merging the dataframes with a new column with a 1 value depending on the event","metadata":{}},{"cell_type":"code","source":"def one_hot_events(received=received, viewed=viewed, completed=completed):\n    '''\n    this function creates a new column with one values according to the event\n    and then merge the dataframes\n    '''\n    # column with 1 value\n    received['offer_received'] = 1\n    viewed['offer_viewed'] = 1\n    completed['offer_completed'] = 1\n    \n    # merge the three dfs\n    event_df = received.merge(\n        viewed, how=\"left\", on=['customer_id', \"offer_id\"]).merge(\n        completed, how=\"left\", on=['customer_id', \"offer_id\"])\n    event_df.drop(columns=[\"event_x\", \"event_y\", \"event\"], inplace=True)\n    event_df.columns = ['customer_id', 'time_received', 'offer_id', 'offer_received',\n                        'time_viewed', 'offer_viewed', 'time_completed', 'reward', 'offer_completed']\n    \n    return event_df","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:25.954966Z","iopub.execute_input":"2022-04-18T21:36:25.955793Z","iopub.status.idle":"2022-04-18T21:36:25.96253Z","shell.execute_reply.started":"2022-04-18T21:36:25.955753Z","shell.execute_reply":"2022-04-18T21:36:25.961755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"event_df = one_hot_events(received=received, viewed=viewed, completed=completed)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:25.964037Z","iopub.execute_input":"2022-04-18T21:36:25.96449Z","iopub.status.idle":"2022-04-18T21:36:26.203122Z","shell.execute_reply.started":"2022-04-18T21:36:25.964449Z","shell.execute_reply":"2022-04-18T21:36:26.202377Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we are interested in whether or not someone will respond to an offer, we are going to drop offers that weren't seen.","metadata":{}},{"cell_type":"markdown","source":"Also, we can see that there are some offers that were completed before viewed, so the customer completed the offer but he didn't know it exists. Since we are just interested in the offers that the customers responded to, we are going to drop offers completed before they were seen.","metadata":{}},{"cell_type":"code","source":"def clean_event_df(event_df=event_df):\n    '''\n    this function will create a dataframe that indicate whether or not the\n    customer responded to an offer with binary data. Columns:\n    *customer_responded:\n    1 - the customer viewed and completed the offer\n    0 - the customer viewed but didn't used the offer\n    *customer_id\n    *offer_id\n    '''\n    event_df = event_df[event_df['offer_viewed'] == 1]\n    \n    # we replace the nan with 1000 to then apply the customer_responded\n    # function\n    event_df.time_completed = event_df.time_completed.fillna(1000)\n    \n    def customer_responded(event_df=event_df):\n        '''\n        This function returns the values 0, 1 or nan depending on the values\n        of the 'time_completed' and 'time_viewed' columns\n        '''\n        if event_df['time_completed'] == 1000:\n            return 0\n        elif event_df['time_completed'] > event_df['time_viewed']:\n            return 1\n        else:\n            return np.nan\n    \n    # we create a new column applying the customer_responded function\n    event_df['customer_responded'] = event_df.apply(customer_responded, axis=1)\n    \n    # drop customers who viewed the offer after completed it\n    event_df.dropna(subset=['customer_responded'], inplace=True)\n    \n    # replace 1000 with nan again\n    event_df.time_completed = event_df.time_completed.replace(1000, np.nan)\n    \n    # drop the columns that we won't use to build the ML model\n    event_df.drop(columns=['time_received','offer_received','time_viewed','offer_viewed',\n                          'time_completed','reward','offer_completed'], inplace=True)\n    \n    return event_df","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:26.20448Z","iopub.execute_input":"2022-04-18T21:36:26.204747Z","iopub.status.idle":"2022-04-18T21:36:26.213143Z","shell.execute_reply.started":"2022-04-18T21:36:26.204703Z","shell.execute_reply":"2022-04-18T21:36:26.212319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"event_df = clean_event_df(event_df=event_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:26.218168Z","iopub.execute_input":"2022-04-18T21:36:26.218453Z","iopub.status.idle":"2022-04-18T21:36:28.437135Z","shell.execute_reply.started":"2022-04-18T21:36:26.218417Z","shell.execute_reply":"2022-04-18T21:36:28.436218Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"event_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:28.438586Z","iopub.execute_input":"2022-04-18T21:36:28.439104Z","iopub.status.idle":"2022-04-18T21:36:28.451087Z","shell.execute_reply.started":"2022-04-18T21:36:28.439064Z","shell.execute_reply":"2022-04-18T21:36:28.450311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explore: Portfolio (offers' data) ","metadata":{}},{"cell_type":"code","source":"portfolio.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:28.452464Z","iopub.execute_input":"2022-04-18T21:36:28.452728Z","iopub.status.idle":"2022-04-18T21:36:28.464418Z","shell.execute_reply.started":"2022-04-18T21:36:28.452677Z","shell.execute_reply":"2022-04-18T21:36:28.463496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"portfolio.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:28.466279Z","iopub.execute_input":"2022-04-18T21:36:28.466601Z","iopub.status.idle":"2022-04-18T21:36:28.473528Z","shell.execute_reply.started":"2022-04-18T21:36:28.466565Z","shell.execute_reply":"2022-04-18T21:36:28.472738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"portfolio[['difficulty', 'duration', 'reward']].hist(bins=50, figsize=(10, 5));","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:28.474973Z","iopub.execute_input":"2022-04-18T21:36:28.475301Z","iopub.status.idle":"2022-04-18T21:36:29.106151Z","shell.execute_reply.started":"2022-04-18T21:36:28.475257Z","shell.execute_reply":"2022-04-18T21:36:29.1055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare data: clean process (Portfolio df)","metadata":{}},{"cell_type":"markdown","source":"In this dataframe we can make the following modifications:\n\n- binary data of each channel (one hot encoding)\n- binary data of each offer_type (one hot encoding)","metadata":{}},{"cell_type":"code","source":"def one_hot_channels_offer_type(portfolio=portfolio):\n    '''\n    this function applies one hot encoding in the channels column of the portfolio df\n    '''\n    channels_list = ['web', 'email', 'mobile', 'social']\n    portfolio['web_channel'] = portfolio['channels'].apply(lambda x: 1 if channels_list[0] in x else 0)\n    portfolio['email_channel'] = portfolio['channels'].apply(lambda x: 1 if channels_list[1] in x else 0)\n    portfolio['mobile_channel'] = portfolio['channels'].apply(lambda x: 1 if channels_list[2] in x else 0)\n    portfolio['social_channel'] = portfolio['channels'].apply(lambda x: 1 if channels_list[3] in x else 0)\n    \n    # drop the channels columns\n    portfolio.drop(columns=[\"channels\"], inplace=True)\n    \n    # one hot offer_type\n    try:\n        portfolio = pd.get_dummies(portfolio, columns=['offer_type'], prefix=\"offer_type_\")\n    except:\n        pass\n    \n    portfolio.columns = ['reward', 'difficulty', 'duration', 'offer_id', 'web_channel', 'email_channel', \n                     'mobile_channel', 'social_channel', 'offer_type__bogo','offer_type__discount',\n                     'offer_type__informational']\n    \n    return portfolio\n\nportfolio = one_hot_channels_offer_type(portfolio=portfolio)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:29.107395Z","iopub.execute_input":"2022-04-18T21:36:29.108139Z","iopub.status.idle":"2022-04-18T21:36:29.124333Z","shell.execute_reply.started":"2022-04-18T21:36:29.108098Z","shell.execute_reply":"2022-04-18T21:36:29.123567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"portfolio[['difficulty', 'duration', 'reward']].hist(bins=50, figsize=(10, 5));","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:29.125539Z","iopub.execute_input":"2022-04-18T21:36:29.125816Z","iopub.status.idle":"2022-04-18T21:36:29.790232Z","shell.execute_reply.started":"2022-04-18T21:36:29.125779Z","shell.execute_reply":"2022-04-18T21:36:29.78954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"portfolio.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:29.791669Z","iopub.execute_input":"2022-04-18T21:36:29.792143Z","iopub.status.idle":"2022-04-18T21:36:29.804631Z","shell.execute_reply.started":"2022-04-18T21:36:29.792099Z","shell.execute_reply":"2022-04-18T21:36:29.803845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explore: Profile (customer's data)","metadata":{}},{"cell_type":"code","source":"profile.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:29.806127Z","iopub.execute_input":"2022-04-18T21:36:29.806657Z","iopub.status.idle":"2022-04-18T21:36:29.821033Z","shell.execute_reply.started":"2022-04-18T21:36:29.806617Z","shell.execute_reply":"2022-04-18T21:36:29.820277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this dataframe we can do the following modifications:\n\n- total time using the app\n- one hot encoding of gender","metadata":{}},{"cell_type":"code","source":"profile.age.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:29.822323Z","iopub.execute_input":"2022-04-18T21:36:29.822591Z","iopub.status.idle":"2022-04-18T21:36:29.834096Z","shell.execute_reply.started":"2022-04-18T21:36:29.822558Z","shell.execute_reply":"2022-04-18T21:36:29.833366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that probably customers who didn't inform their age, are registered as 118 years old, which don't make sense.","metadata":{}},{"cell_type":"code","source":"(profile['age'] == 118).sum()/len(profile), profile['income'].isna().sum()/len(profile)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:29.835491Z","iopub.execute_input":"2022-04-18T21:36:29.83618Z","iopub.status.idle":"2022-04-18T21:36:29.845529Z","shell.execute_reply.started":"2022-04-18T21:36:29.836138Z","shell.execute_reply":"2022-04-18T21:36:29.844637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare data: clean process (profile df)","metadata":{}},{"cell_type":"markdown","source":"We can see that the percentage of people who didn't inform age is the same as people who didn't inform their income and their gender as well. And, since this percentage is relatively small (13%) it makes sense if we drop the these rows, because we're not going to lose too much information.","metadata":{}},{"cell_type":"code","source":"def modify_profile(profile=profile):\n    try:\n        profile = pd.get_dummies(profile, columns=['gender'], dummy_na=False, prefix=\"gender_\")\n    except:\n        pass\n    \n    profile.became_member_on = pd.to_datetime(profile.became_member_on.apply(str), format='%Y%m%d')\n    \n    profile['now'] = pd.to_datetime('now')\n    profile['days_as_member'] = (profile['now'] - profile['became_member_on']).dt.days\n    profile.drop(columns=['became_member_on', 'now'], inplace=True)\n    \n    # drop nan\n    profile.dropna(subset=['income'], inplace=True)\n    \n    return profile\n\nprofile = modify_profile()\nprofile.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:29.846968Z","iopub.execute_input":"2022-04-18T21:36:29.847433Z","iopub.status.idle":"2022-04-18T21:36:29.899438Z","shell.execute_reply.started":"2022-04-18T21:36:29.84739Z","shell.execute_reply":"2022-04-18T21:36:29.898748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"profile[['age', 'income', 'days_as_member']].hist(bins=30, figsize=(10, 5));","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:29.900844Z","iopub.execute_input":"2022-04-18T21:36:29.901306Z","iopub.status.idle":"2022-04-18T21:36:30.517187Z","shell.execute_reply.started":"2022-04-18T21:36:29.901266Z","shell.execute_reply":"2022-04-18T21:36:30.516518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we can use the transactions df that we extracted from the transcript df to add valuable \n# information to the profile df, which contains customer's data\ntransaction.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:30.518707Z","iopub.execute_input":"2022-04-18T21:36:30.51918Z","iopub.status.idle":"2022-04-18T21:36:30.530556Z","shell.execute_reply.started":"2022-04-18T21:36:30.51914Z","shell.execute_reply":"2022-04-18T21:36:30.529722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare data: merge data (transcript df)","metadata":{}},{"cell_type":"markdown","source":"We're going to calculate the number of transactions per user and the total amount per user. This information will be valuable to add to the profile df:\n\n- Number of transactions\n- Total amount","metadata":{}},{"cell_type":"code","source":"def merge_profile_df(profile=profile, transaction=transaction):\n    '''\n    this function merge the profile df with data from transaction df. Columns:\n    - age: customer's age\n    - customer_id\n    - income: customer's income\n    - gender: M, F, O with one-hot-encoding\n    - days_as_member: time in days using the app\n    - number_of_transactions: total transactions using the app\n    - total_amount: total amount of money spent on the app\n    '''\n    # number of transactions per customer\n    number_of_transactions = pd.DataFrame(\n        transaction.groupby(['customer_id']).size(), columns=['number_of_transactions']).reset_index(level=0)\n    number_of_transactions.columns = ['customer_id', 'number_of_transactions']\n    \n    # merge the number of transactions on customer_id\n    profile.columns = ['age', 'customer_id', 'income', 'gender__F', 'gender__M', 'gender__O', 'days_as_member']\n    profile = profile.merge(number_of_transactions, how='left', on='customer_id')\n    \n    # total amount spent per customer\n    total_amount = pd.DataFrame(transaction.groupby(['customer_id']).sum()['amount'])\n    total_amount = total_amount.reset_index(level=0)\n    total_amount.columns = ['customer_id', 'total_amount']\n    \n    # merge the total_amount on customer_id\n    profile = profile.merge(total_amount, how='left', on='customer_id')\n    \n    return profile","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:30.532326Z","iopub.execute_input":"2022-04-18T21:36:30.532573Z","iopub.status.idle":"2022-04-18T21:36:30.542263Z","shell.execute_reply.started":"2022-04-18T21:36:30.532537Z","shell.execute_reply":"2022-04-18T21:36:30.541579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we have just 2.2% of missing values after the merge operation, it does make sense to drop the rows with missing values.\n\nThe profile df after the merge provides interesting information of the consumption of the customers based on demographic data. So, we can make some visuals with this df.","metadata":{}},{"cell_type":"code","source":"profile = merge_profile_df(profile=profile, transaction=transaction)\nprofile.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:30.544144Z","iopub.execute_input":"2022-04-18T21:36:30.545824Z","iopub.status.idle":"2022-04-18T21:36:30.714295Z","shell.execute_reply.started":"2022-04-18T21:36:30.545788Z","shell.execute_reply":"2022-04-18T21:36:30.713536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"profile[['age', 'income', 'days_as_member', 'number_of_transactions', 'total_amount']].hist(bins=50, figsize=(10, 8));","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:30.715831Z","iopub.execute_input":"2022-04-18T21:36:30.716332Z","iopub.status.idle":"2022-04-18T21:36:31.928073Z","shell.execute_reply.started":"2022-04-18T21:36:30.716287Z","shell.execute_reply":"2022-04-18T21:36:31.927393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Merge all dataframes","metadata":{}},{"cell_type":"markdown","source":"Let's add the customer's information to the event_df dataframe","metadata":{}},{"cell_type":"code","source":"df = event_df.merge(profile, how='left', on='customer_id').dropna()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:31.929181Z","iopub.execute_input":"2022-04-18T21:36:31.929759Z","iopub.status.idle":"2022-04-18T21:36:31.992413Z","shell.execute_reply.started":"2022-04-18T21:36:31.929709Z","shell.execute_reply":"2022-04-18T21:36:31.991657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And then, let's add the offer's information","metadata":{}},{"cell_type":"code","source":"df = df.merge(portfolio, how='left', on='offer_id')","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:31.993586Z","iopub.execute_input":"2022-04-18T21:36:31.993865Z","iopub.status.idle":"2022-04-18T21:36:32.018402Z","shell.execute_reply.started":"2022-04-18T21:36:31.99383Z","shell.execute_reply":"2022-04-18T21:36:32.017619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-18T21:36:32.020794Z","iopub.execute_input":"2022-04-18T21:36:32.021223Z","iopub.status.idle":"2022-04-18T21:36:32.048397Z","shell.execute_reply.started":"2022-04-18T21:36:32.021183Z","shell.execute_reply":"2022-04-18T21:36:32.047603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see if the data is balanced\ndf.customer_responded.sum() / len(df)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:32.049666Z","iopub.execute_input":"2022-04-18T21:36:32.049988Z","iopub.status.idle":"2022-04-18T21:36:32.057731Z","shell.execute_reply.started":"2022-04-18T21:36:32.049947Z","shell.execute_reply":"2022-04-18T21:36:32.056845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section 4: Data Modeling ","metadata":{}},{"cell_type":"markdown","source":"Then, let's separate the df into X and Y (target) variables","metadata":{}},{"cell_type":"code","source":"int_cols = ['age', 'income', 'gender__F', 'gender__M', 'gender__O', 'days_as_member',\n     'number_of_transactions', 'reward', 'difficulty', 'duration', 'web_channel',\n     'email_channel', 'mobile_channel', 'social_channel', 'offer_type__bogo', \n     'offer_type__discount', 'offer_type__informational']","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:32.059439Z","iopub.execute_input":"2022-04-18T21:36:32.059978Z","iopub.status.idle":"2022-04-18T21:36:32.065649Z","shell.execute_reply.started":"2022-04-18T21:36:32.059937Z","shell.execute_reply":"2022-04-18T21:36:32.064607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns[3:]","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:32.067349Z","iopub.execute_input":"2022-04-18T21:36:32.068717Z","iopub.status.idle":"2022-04-18T21:36:32.078825Z","shell.execute_reply.started":"2022-04-18T21:36:32.068667Z","shell.execute_reply":"2022-04-18T21:36:32.078098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df[df.columns[3:]]\nX[int_cols] = X[int_cols].astype(int)\ny = df['customer_responded'].astype('category')","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:32.080169Z","iopub.execute_input":"2022-04-18T21:36:32.08165Z","iopub.status.idle":"2022-04-18T21:36:32.11379Z","shell.execute_reply.started":"2022-04-18T21:36:32.08162Z","shell.execute_reply":"2022-04-18T21:36:32.112978Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:32.115166Z","iopub.execute_input":"2022-04-18T21:36:32.115976Z","iopub.status.idle":"2022-04-18T21:36:32.138204Z","shell.execute_reply.started":"2022-04-18T21:36:32.115932Z","shell.execute_reply":"2022-04-18T21:36:32.13742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nclf = RandomForestClassifier()\nclf.fit(train_X, train_y)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:32.13952Z","iopub.execute_input":"2022-04-18T21:36:32.140063Z","iopub.status.idle":"2022-04-18T21:36:36.769388Z","shell.execute_reply.started":"2022-04-18T21:36:32.14002Z","shell.execute_reply":"2022-04-18T21:36:36.76875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = clf.predict(val_X)\npred","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:36.770652Z","iopub.execute_input":"2022-04-18T21:36:36.771071Z","iopub.status.idle":"2022-04-18T21:36:37.046383Z","shell.execute_reply.started":"2022-04-18T21:36:36.771029Z","shell.execute_reply":"2022-04-18T21:36:37.045752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section 5: Evaluate the Model","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(val_y, pred)\nprint('Accuracy: ', accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:37.050719Z","iopub.execute_input":"2022-04-18T21:36:37.051366Z","iopub.status.idle":"2022-04-18T21:36:37.059633Z","shell.execute_reply.started":"2022-04-18T21:36:37.051322Z","shell.execute_reply":"2022-04-18T21:36:37.058741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(val_y,pred))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:37.061265Z","iopub.execute_input":"2022-04-18T21:36:37.061613Z","iopub.status.idle":"2022-04-18T21:36:37.108147Z","shell.execute_reply.started":"2022-04-18T21:36:37.061575Z","shell.execute_reply":"2022-04-18T21:36:37.107173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section 6: Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 500, num = 4)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 4)]\nmax_depth.append(None)\n# Method of selecting samples for training each tree\nbootstrap = [True, False]# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'bootstrap': bootstrap}\n\nprint(random_grid)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:37.109699Z","iopub.execute_input":"2022-04-18T21:36:37.110063Z","iopub.status.idle":"2022-04-18T21:36:37.11899Z","shell.execute_reply.started":"2022-04-18T21:36:37.110019Z","shell.execute_reply":"2022-04-18T21:36:37.117985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=10, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(train_X, train_y)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:36:37.120383Z","iopub.execute_input":"2022-04-18T21:36:37.12098Z","iopub.status.idle":"2022-04-18T21:39:59.265462Z","shell.execute_reply.started":"2022-04-18T21:36:37.120938Z","shell.execute_reply":"2022-04-18T21:39:59.264631Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate the tuned model","metadata":{}},{"cell_type":"code","source":"pred2 = np.around(rf_random.predict(val_X))\npred2","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:39:59.267075Z","iopub.execute_input":"2022-04-18T21:39:59.267815Z","iopub.status.idle":"2022-04-18T21:40:00.224741Z","shell.execute_reply.started":"2022-04-18T21:39:59.267768Z","shell.execute_reply":"2022-04-18T21:40:00.22397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy2 = accuracy_score(val_y, pred2)\nprint('Accuracy of tuned model: ', accuracy2)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:40:00.226193Z","iopub.execute_input":"2022-04-18T21:40:00.226634Z","iopub.status.idle":"2022-04-18T21:40:00.236814Z","shell.execute_reply.started":"2022-04-18T21:40:00.22659Z","shell.execute_reply":"2022-04-18T21:40:00.235855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(val_y,pred2))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:40:00.238517Z","iopub.execute_input":"2022-04-18T21:40:00.239162Z","iopub.status.idle":"2022-04-18T21:40:00.287611Z","shell.execute_reply.started":"2022-04-18T21:40:00.239121Z","shell.execute_reply":"2022-04-18T21:40:00.28675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy improvement of: {0:.3g} % with hyperparameter tuning (GridSearchCV)'.format((accuracy2-accuracy)*100))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T21:40:00.289048Z","iopub.execute_input":"2022-04-18T21:40:00.289389Z","iopub.status.idle":"2022-04-18T21:40:00.295584Z","shell.execute_reply.started":"2022-04-18T21:40:00.289349Z","shell.execute_reply":"2022-04-18T21:40:00.294653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section 6: Conclusions","metadata":{}},{"cell_type":"markdown","source":"- The data preprocessing methods allowed us to build a highly accurate model\n- The Random Forest Regressor classifier algorithm showed to be effective for the binary classification in this business case since the base model showed an accuracy of 92.5% and precision, recall and f1-score metrics almost all over 90%\n- Performing hyperparameter tuning improved the model accuracy in 0.344%","metadata":{}}]}