{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Module 1: Data Exploration\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n#------------------------------------------------------------------------------------------------------------------------------\ndf = pd.read_csv(\"../input/creditcard-test-train/creditcard_train.csv\")\n\n#Check DateFrame\nprint('\\33[91m'+\"DateFrame Preview\")\ndisplay(df.head())\nprint(\"\")\n\n#Check General information of df\nprint('\\33[91m'+\"Information of DateFrame\")\nprint('\\33[0m'+\"\")\nprint(\"Dimension = \", df.ndim)\nprint(\"Size = \", df.size)\nprint(\"Shape = \", df.shape)\nprint(\"Positive Class: \", df[df['Class']>0].shape)\nprint(\"Negative Class: \", df[df[\"Class\"]<1].shape)\norlen = len(df.index)\n\n#Check data type\ndf_dt = pd.DataFrame(df.dtypes, columns = ['data type'])\ndisplay(df_dt)\ndisplay(df.describe())\nprint(\"\")\n#df.info() #may lead to error due to diff. numpy version \n\n#Check for NaN value\nprint('\\33[91m'+\"Missing data in DateFrame\")\nprint('\\33[0m'+\"\")\nmissing_data = pd.concat([df.isnull().sum(),(((df.isnull().sum()/len(df.index))*100))], axis=1, keys=['Total', 'Percent %'])\ndisplay(missing_data[missing_data[\"Total\"]>0])\nprint(\"Number of variables contain Missing values = \", len(missing_data[missing_data[\"Total\"]>0].index))\nprint(\"Number of Rows contain Missing values = \", missing_data[\"Total\"].sum())\nprint(\"Percentage of Missing value = \", math.ceil(missing_data[\"Percent %\"].sum()*1000)/1000,\"%\")\nprint(\"\")\n\n#Handle Missing value -> Remove rows with missing value \n#                        Because it still contains 99.7% of data\nprint('\\33[91m'+\"Handling Missing values -> Remove rows that contains missing value \")\nprint('\\33[0m'+\"\")\ndf = df.dropna()\nprint(\"After handling the missing values, Dimension = \", df.ndim)\nprint(\"After handling the missing values, Size = \", df.size)\nmis = df.size\nprint(\"After handling the missing values, shape = \", df.shape)\nprint(f\"Data removed = {missing_data['Total'].sum()} during handling the missing values\")\nprint(f\"Data remaining = {orlen-missing_data['Total'].sum()} after handling the missing values\")\nprint(\"Positive Class: \", df[df['Class']>0].shape)\nprint(\"Negative Class: \", df[df[\"Class\"]<1].shape)\n\n#Check for Duplicated value\nprint(\"\")\nprint('\\33[91m'+'Duplicated value in DataFrame')\nprint('\\33[0m'+\"\")\ndf123 = df.copy()\ndf123.drop_duplicates(subset=None, inplace=True)\nDuplicated = len(df)-len(df123)\nprint(f'Duplicated values found: {Duplicated} with Percentage: {round((Duplicated/len(df)*100),4)}% in total')\nprint(\"\")\nprint('\\33[91m'+\"Handling Duplicated value -> Removal\")\nprint('\\33[0m'+\"\")\ndf = df.drop_duplicates()\nprint(\"After handling the duplicated value, Dimension = \", df.ndim)\nprint(\"After handling the duplicated value, Size = \", df.size)\nprint(\"After handling the duplicated value, shape = \", df.shape)\nprint(f\"Data removed = {(mis-df.size)//31} during handling the duplicated value\")\nprint(f\"Data remaining = {df.size//len(df.columns)} after handling the duplicated value\")\nprint(\"Positive Class: \", df[df['Class']>0].shape)\nprint(\"Negative Class: \", df[df[\"Class\"]<1].shape)\n\nprint(\"\")\nprint('\\33[91m'+\"Handling Outliers -> Keeping All Outliers\")\nprint('\\33[0m'+\"\")\nprint(\"Reason on keeping all outliers will be deliberate in the Summary part.\")\n#Q1 = df.quantile(0.25)\n#Q3 = df.quantile(0.75)\n#IQR = Q3 - Q1\n#cols = df.columns.values[:-1]\n#df = df[~((df[cols] < (Q1 - 2.5 * IQR)) |(df[cols] > (Q3 + 2.5 * IQR))).any(axis=1)]\n#print(\"After handling the outliers, Dimension = \", df.ndim)\n#print(\"After handling the outliers, Size = \", df.size)\n#print(\"After handling the outliers, shape = \", df.shape)\n#print(f\"Data removed = {(mis-df.size)//31} during handling the outliers\")\n#print(f\"Data remaining = {df.size//len(df.columns)} after handling the outliers\")\n#print(\"Positive Class: \", df[df['Class']>0].shape)\n#print(\"Negative Class: \", df[df[\"Class\"]<1].shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:55:55.877375Z","iopub.execute_input":"2022-05-08T07:55:55.87791Z","iopub.status.idle":"2022-05-08T07:56:01.799442Z","shell.execute_reply.started":"2022-05-08T07:55:55.877861Z","shell.execute_reply":"2022-05-08T07:56:01.798602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Module 2: Data Visualization\n","metadata":{}},{"cell_type":"code","source":"#1. Ratio of two Class\nfig=plt.figure(figsize=(10,10))\nprint('\\33[91m'+\"bar_label may cause error\")\nprint(\"If so, please update matplotlib to 3.4 or higher version\")\nax = sns.countplot(x = 'Class',data = df)\nax.bar_label(ax.containers[0])\nax.set_title(\"1. Number of Fraudulent Transactions vs. Non-Fraudulent Transactions\",color=\"r\", y=1.03)\nplt.xlabel('Class: \"0\" for Non-Fraudulent, \"1\" for Fraudulent')\nplt.show()\n#------------------------------------------------------------------------------------------------------------------------------\ndf_fraud_C = df[df['Class']>0].copy().drop([\"Class\"],axis=1)\ndf_normal_C = df[df[\"Class\"]<1].copy().drop([\"Class\"],axis=1)\n#------------------------------------------------------------------------------------------------------------------------------\n#2. Distribution of Non-Fraudulent Transactions\nfig=plt.figure(figsize=(20,20))\nfig.suptitle(\"2. Distribution of variables for Non-Fraudulent Transactions\", fontsize=16,color = \"r\")\nfor i,x in enumerate(df_normal_C.columns):\n    ax = plt.subplot(8,4,i+1)\n    sns.histplot(df_normal_C[x],bins=20,kde=True).set_title(\"Distribution of \"+x, fontweight =\"bold\")\nfig.tight_layout()  \nfig.subplots_adjust(top=0.93)\nplt.show()\n#------------------------------------------------------------------------------------------------------------------------------\n#3. Distribution of Fraudulent Transactions\nfig=plt.figure(figsize=(20,20))\nfig.suptitle(\"3. Distribution of variables for Fraudulent Transactions\", fontsize=16,color = \"r\")\nfor i,x in enumerate(df_fraud_C.columns):\n    ax = plt.subplot(8,4,i+1)\n    sns.histplot(df_fraud_C[x],bins=20,kde=True,color = \"darkorange\").set_title(\"Distribution of \"+x, fontweight =\"bold\")\nfig.tight_layout()  \nfig.subplots_adjust(top=0.93)\nplt.show()\n#------------------------------------------------------------------------------------------------------------------------------\n#4. Distribution of Financial Amounts Transactions in Different Class\nplt.figure(figsize=(10,8))\nsns.boxplot(data = df, x= 'Class', y='Amount')\nplt.ylim(-10,500)\nplt.title(\"Distribution of Financial Amounts Transacted in Different Class\",color = \"r\", y=1.03)\nplt.show()\n#print('Mean:')\n#mean = df.groupby('Class')['Amount'].mean()\n#mean_df = pd.DataFrame(data = mean)\n#display(mean_df)\n#print('Median:')\n#median = df.groupby('Class')['Amount'].median()\n#median_df = pd.DataFrame(data = median)\n#display(median_df)\n#------------------------------------------------------------------------------------------------------------------------------\n#5. Correlation Matrix of Fraudulent Transactions between Lable & Features\ncorr = df.corr()\ncorr = corr[['Class']]\nfig = plt.figure(figsize=(10,8))\nfig = sns.heatmap(corr, annot=True, linewidths=0.3)\nplt.title(\"Correlation between Class & Features\",color = \"r\", y=1.03)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:56:01.800818Z","iopub.execute_input":"2022-05-08T07:56:01.801019Z","iopub.status.idle":"2022-05-08T07:56:48.961824Z","shell.execute_reply.started":"2022-05-08T07:56:01.800994Z","shell.execute_reply":"2022-05-08T07:56:48.959192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Module 3: Dimension Reduction\n","metadata":{}},{"cell_type":"code","source":"#define function\ndef decide(df,df_inverse,y_data):\n    error_rate = np.sum((np.array(df)-np.array(df_inverse))**2,axis=1)\n    error_df = pd.DataFrame(error_rate,index=df.index)\n    decide_df = pd.concat([error_df,y_data],axis=1)\n    decide_df.columns = [\"error_rate\",\"correct_label\"]\n    #choose top 500 error rate\n    decide_df = decide_df.sort_values(by=\"error_rate\")\n    decide_df_500 = decide_df.tail(500)\n    precision = np.round(decide_df_500.error_rate[decide_df_500.correct_label==1].count()/500,4)\n    recall = np.round(decide_df_500.error_rate[decide_df_500.correct_label==1].count()/y_data.sum(),4)\n    F1 = np.round((2 * (precision * recall) / (precision + recall)),4)\n    print(\"Precision: \",precision)\n    print(\"Recall: \",recall)\n    print(\"F1-score: \",F1)\n#------------------------------------------------------------------------------------------------------------------------------\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n#------------------------------------------------------------------------------------------------------------------------------\n#Drop Label -> \"Class\"\ndf_C = df.copy().drop(['Class'],axis = 1)\n\n#Re-scaling Before PCA -> Use StandardScaler but not MinMaxScaler\nscaler = StandardScaler(copy=True)\ndf_C.loc[:,df_C.columns] = scaler.fit_transform(df_C[df_C.columns])\n\n#Use PCA to reduce dimension\n#1. Check For Number of Principal Components To Take\nprint('\\33[91m'+\"Cumulative Explained Variance with All 30 Components\")\nprint('\\33[0m'+\"\")\npca = PCA(n_components=30)\npca.fit(df_C)\ncum_var = []\nfor i in range(0,len(pca.explained_variance_ratio_)):\n    if i==0:\n        cum_var.append(round(pca.explained_variance_ratio_[i],4))\n    else:\n        cum_var.append(round(pca.explained_variance_ratio_[i]+cum_var[i-1],4))\nprint(cum_var)\nprint(\"\")\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title(\"Cumulative Explained Variance with All 30 Components\",color=\"r\")\nplt.show()\nprint(\"\")\n\n#2. Number of Principal Components: \"5% error\": 27\ncomponents = (next(x for x,ele in enumerate(cum_var) if ele > 0.95))+1\nprint('\\33[91m'+ f\"Number of Components Chosen: {components} for 95% of Explained Variance\")\npca = PCA(n_components=components)\nX_trained = pca.fit_transform(df_C)\ndumdf = pd.DataFrame(data = X_trained[:, :2])\ndumdf = pd.concat((dumdf,df[\"Class\"].copy()),join=\"inner\",axis=1)\ndumdf.columns = ['PCA1', 'PCA2','Class']\nsns.lmplot(x='PCA1', y='PCA2', hue='Class', data=dumdf, fit_reg=False)\nplt.title(\"PCA: PCA1 vs.PCA2\",color=\"r\")\nplt.show()\n#------------------------------------------------------------------------------------------------------------------------------\n#3. Interpretation\ndf_DimRed = pd.DataFrame(data = pca.inverse_transform(X_trained), index = df_C.index)\nprint(\"\")\nprint('\\33[91m'+ \"Interpretation\")\nprint('\\33[0m'+\"\")\nprint(\"PCA Type: Normal PCA\")\nprint(\"PCA Plot Used: 2D PCA\")\nprint(f\"Number of Components Chosen: {components} for 95% of Explained Variance\")\nprint(\"Result with PCA unsupervised learning: \")\ndecide(df_C,df_DimRed,df[\"Class\"].copy())","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:56:48.964096Z","iopub.execute_input":"2022-05-08T07:56:48.96436Z","iopub.status.idle":"2022-05-08T07:56:52.890915Z","shell.execute_reply.started":"2022-05-08T07:56:48.964329Z","shell.execute_reply":"2022-05-08T07:56:52.890119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Module 4: Classification\n","metadata":{}},{"cell_type":"code","source":"#define function\ndef info_model(y_test, y_pred):\n    #Info. of the model\n    acc_knn = round(accuracy_score(y_test, y_pred)*100, 4)\n    print('\\33[0m'+'Accuracy of the Model Classifier: ', acc_knn)\n    print(\"\")\n    print(classification_report(y_test, y_pred))\n    cm = confusion_matrix(y_test, y_pred)\n    cm_df=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n    sns.heatmap(cm_df, annot = True)\n    plt.title('Confusion Matrix',color='r')\n    plt.show()\n    print('True Positives: ',cm_df.iloc[1][1])\n    print('True Negatives: ',cm_df.iloc[0][0])\n    print('False Positives (Type I error): ',cm_df.iloc[0][1])\n    print('False Negatives ( Type II error): ',cm_df.iloc[1][0])\n    #PR Curve\n    average_precision = average_precision_score(y_test, y_pred)\n    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n    plt.plot(recall, precision)\n    plt.title(f'Precision-Recall Curve: Average Precision = {round(average_precision,2)}',color='r')\n    plt.ylabel('Precision')\n    plt.xlabel('Recall')\n    plt.show()\n    #Roc Curve\n    fpr, tpr, thresholds = roc_curve(y_test,y_pred)\n    areaUnderROC = auc(fpr, tpr)\n    plt.plot(fpr, tpr, color='r', lw=2, label= f'ROC curve (area = {round(areaUnderROC,4)})')\n    plt.plot([0, 1], [0, 1], color='k', lw=3, linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic Curve',color='r')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n#------------------------------------------------------------------------------------------------------------------------------\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n#------------------------------------------------------------------------------------------------------------------------------\n#Balanced the \"Class\" Data\ndum_df_C = df_C.copy()\ndum_df_C[\"Class\"] = df['Class'].copy()\ndf_fraud = dum_df_C[dum_df_C['Class']==1]\ndf_normal = dum_df_C[dum_df_C['Class']==0]\nfraud_len = len(df_fraud)\ndf_normal_bal = df_normal.sample(n=fraud_len,random_state=40)\nsample_df = pd.concat([df_fraud,df_normal_bal.set_index(df_fraud.index)])\nsample_df = sample_df.sample(frac=1, random_state=20)\nsample_df = sample_df.reset_index(drop=True)\n\n#Re-scale again to prevent any changes occur\nscaler_bal = StandardScaler()\nX = sample_df.copy().drop(['Class'],axis=1)\ny_train = sample_df['Class'].copy()\nX_train = scaler_bal.fit_transform(X)\n\n#load test case -> X_test & y_test\ntest_df = pd.read_csv(\"../input/creditcard-test-train/creditcard_test.csv\")\nX_test = test_df.copy().drop(['Class'],axis=1)\ny_test = test_df['Class'].copy()\nX_test = StandardScaler().fit_transform(X_test)\n#------------------------------------------------------------------------------------------------------------------------------\n#1. K Nearest Neighbor \nprint('\\33[91m'+\"1. K Nearest Neighbor Model\")\nprint('\\33[0m'+\"\")\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Hyperparameter Tuning w/ 5-fold cross-validation\nparam_grid = {'n_neighbors' : list(range(1,51))}\ngs = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy')\ng_res = gs.fit(X_train, y_train)\nprint(f'Best score (KNN Model): {g_res.best_score_} for {g_res.best_params_}')\nplt.plot(range(1,51),g_res.cv_results_.get('mean_test_score'))\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Cross-Validated Accuracy')\nplt.title('Accuracy for different value of K for KNN model',color='r')\nplt.show()\nprint(\"\")\n\n#Create model\nprint('\\33[91m'+\"Model Evaluation\")\nprint(\"\")\nk_value = g_res.best_params_.get('n_neighbors')\nmodel = KNeighborsClassifier(n_neighbors=k_value)\nmodel.fit(X_train, y_train)\n\n#Test model\ny_pred = model.predict(X_test)\n\ninfo_model(y_test, y_pred)\n#------------------------------------------------------------------------------------------------------------------------------\nprint('------------------------------------------------------------------------------------------------------------------------------')\n#2. Random Forest \nprint('\\33[91m'+\"2. Random Forest Model\")\nprint('\\33[0m'+\"\")\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Hyperparameter Tuning w/ 5-fold cross-validation\nparam_grid = {'n_estimators' : list(range(1,101))}\ngs = GridSearchCV(RandomForestClassifier(random_state=123), param_grid, cv=5, scoring='accuracy')\ng_res = gs.fit(X_train, y_train)\nprint(f'Best score (RF Model): {g_res.best_score_} for {g_res.best_params_}')\nplt.plot(range(1,101),g_res.cv_results_.get('mean_test_score'))\nplt.xlabel('Value of E for RF')\nplt.ylabel('Cross-Validated Accuracy')\nplt.title('Accuracy for different value of E for RF model',color='r')\nplt.show()\nprint(\"\")\n\n#Create model\nprint('\\33[91m'+\"Model Evaluation\")\nprint(\"\")\nk_value = g_res.best_params_.get('n_estimators')\nmodel = RandomForestClassifier(n_estimators=k_value)\nmodel.fit(X_train, y_train)\n\n#Test model\ny_pred = model.predict(X_test)\n\ninfo_model(y_test, y_pred)\n#------------------------------------------------------------------------------------------------------------------------------\nprint('------------------------------------------------------------------------------------------------------------------------------')\n#3. XGBoost\nprint('\\33[91m'+\"3. XGBoost Model\")\nprint('\\33[0m'+'Remark: XGBoost required to install manually. Installation guide has been provided in the code quoted by \"###\"')\nprint('\\33[0m'+\"\")\n### Installation -> XGBoost\n###import sys###\n###!{sys.executable} -m pip install xgboost###\nfrom xgboost import XGBClassifier\n\n# Hyperparameter Tuning w/ 5-fold cross-validation\nparam_grid = {'n_estimators' : list(range(1,101))}\ngs = GridSearchCV(XGBClassifier(eval_metric='mlogloss',use_label_encoder =False), param_grid, cv=5, scoring='accuracy')\ng_res = gs.fit(X_train, y_train)\nprint(f'Best score (xgb Model): {g_res.best_score_} for {g_res.best_params_}')\nplt.plot(range(1,101),g_res.cv_results_.get('mean_test_score'))\nplt.xlabel('Value of E for xgb')\nplt.ylabel('Cross-Validated Accuracy')\nplt.title('Accuracy for different value of E for xgb model',color='r')\nplt.show()\nprint(\"\")\n\n#Create model\nprint('\\33[91m'+\"Model Evaluation\")\nprint(\"\")\nk_value = g_res.best_params_.get('n_estimators')\nmodel = XGBClassifier(n_estimators=k_value, eval_metric='mlogloss',use_label_encoder =False)\nmodel.fit(X_train, y_train)\n\n#Test model\ny_pred = model.predict(X_test)\n\ninfo_model(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T07:56:52.892674Z","iopub.execute_input":"2022-05-08T07:56:52.89289Z","iopub.status.idle":"2022-05-08T08:00:22.062523Z","shell.execute_reply.started":"2022-05-08T07:56:52.892864Z","shell.execute_reply":"2022-05-08T08:00:22.061704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Module 5: Summary\n","metadata":{}},{"cell_type":"markdown","source":"**Data Exploration** <br>\nThe dataset used in this project contains 31 columns: ' Time,' 'Amount,' 'Class,' and 'V1' to 'V28' with around 290,000 data. We firstly had checked for null values and duplicated values in the dataset. The result indicated there are 798 missing values distributed under the 'V22' and 'V23' columns and 1076 duplicated values. We had filtered out the whole row instead of adopting the imputation method. It is believed that the dropped data would not significantly impact the result since there are around 99.3% remaining data. We also had reviewed the outliers in the dataset. However, we had chosen to keep all outliers rather than remove them. The reason for not releasing the outliers is that it may affect the accuracy of the model \nas the model may mislabel for the extreme cases in further analysis, which could be vital in discovering the fraudulent transactions. <br> \nAfter the data preprocessing, 282,783 observations remained with 427 data for the positive class and 282,356 data for the negative class. <br> <br>\nWe have visualized five figures on the dataset. The first one is a count plot on the number of fraudulent transactions and non-fraudulent transactions. The result had indicated that the dataset is highly imbalanced, with 282,356 data on non-fraudulent transactions but only 427 data on fraudulent transactions. We also had plotted the distribution of the variables in the two classes. It is discovered that the distribution of the following columns holds a similar distribution curve: 'V3,' 'V8,' 'V13,' 'V15,' 'V19,' 'V21' and 'V24.' However, due to confidentiality issues, we cannot interpret any outcome. There is an intersecting finding on the distribution of monetary amounts transacted in a different class. The average quantity transacted in the fraud group (\\\\$123.12) is higher than in the non-fraud group ($88.50). However, we obtained the opposite result when calculating the median, which was \\\\$11.4 for the fraud group and \\\\$22.0 for the non-fraud group. It may be because the criminals thought a small number of transactions could be hard to discover by the cardholder. Furthermore, we had examined the correlation between the  'Class' and Features. It is possible to see those Features only having a weak correlation to the 'Class'.","metadata":{}},{"cell_type":"markdown","source":"**Model Evaluation** <br>\n###### Dimension Reduction \nTo achieve dimension reduction, we have applied Principal component analysis. We had calculated the cumulative explained variance with all 30 components and had chosen 27 components for 95% explained variance. <br><br>\nCumulative Explained Variance : <br>\n[0.0657, 0.1222, 0.1567, 0.191, 0.2251, 0.2591, 0.293, 0.3269, 0.3607, 0.3945, 0.4282, 0.4618, 0.4954, 0.5289, 0.5624, 0.5959, 0.6293, 0.6627, 0.6961, 0.7294, 0.7627, 0.796, 0.8292, 0.8624, 0.895, 0.9274, 0.9594, 0.988, 0.9986, 1.0] <br><br>\nResult: <br>\nPrecision:  0.676<br>\nRecall:  0.7916<br>\nF1-score:  0.7292<br>\n###### Classification\nWe had chosen three classification models: K-Nearest Neighbor model, Random Forest classification model, and XGBoost classification Model. To maximize the accuracy, we applied the Hyperparameter Tuning method with 5-fold cross-validation to choose the best estimators for the model. <br> <br>\nFor the K-Nearest Neighbor model, the estimators (n_neighbors) utilized to construct the model is 9, with the best score of 0.915 under the training process. After the testing process, the model accuracy is 96.7%, with the following result: <br> <br>\nTrue Positives:  47<br>\nTrue Negatives:  98<br>\nFalse Positives (Type I error):  2<br>\nFalse Negatives ( Type II error):  3<br>\nArea under the ROC curve: 0.96 <br> <br>\nFor the Random Forest classification model, the estimators (n_estimators) utilized to construct the model is 35, with the best score of 0.940 under the training process. After the testing process, the model accuracy is 94.0%, with the following result: <br><br>\nTrue Positives:  49<br>\nTrue Negatives:  92<br>\nFalse Positives (Type I error):  8<br>\nFalse Negatives ( Type II error):  1<br>\nArea under the ROC curve: 0.94 <br> <br>\nFor the XGBoost classification model, the estimators (n_estimators) utilized to construct the model is 61, with the best score of 0.939 under the training process. After the testing process, the model accuracy is 92.0%, with the following result: <br><br>\nTrue Positives:  49<br>\nTrue Negatives:  89<br>\nFalse Positives (Type I error):  11<br>\nFalse Negatives ( Type II error):  1<br>\nArea under the ROC curve: 0.935 <br> <br>","metadata":{}},{"cell_type":"markdown","source":"**Conclusions** <br>\nBased on the model evaluation, it is discovered that the K-Nearest Neighbor model had the best accuracy with 96.7%. However, it should be noticed that the model accuracy under our evaluation existing error as we had initiated the \"seed\" in our model for repeatability on the result, which leads the model could achieve a better result by using a better seed. Another point that should be noticed is the weighting of Type  I error and Type II error. For example, if the business cares more about detecting the fraud group, the XGBoost classification model should be adopted as this model perform better in catching the fraud transactions in our evaluation.","metadata":{}}]}