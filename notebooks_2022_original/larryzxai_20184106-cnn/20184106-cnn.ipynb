{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:08.488891Z","iopub.execute_input":"2022-05-07T06:08:08.491639Z","iopub.status.idle":"2022-05-07T06:08:08.52039Z","shell.execute_reply.started":"2022-05-07T06:08:08.489533Z","shell.execute_reply":"2022-05-07T06:08:08.519511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainData = pd.read_csv('/kaggle/input/ml2021-2022-2-cnn/train.csv')\ntestData = pd.read_csv('/kaggle/input/ml2021-2022-2-cnn/test.csv')\n\ntrain_x = np.array(trainData)[:,1:]\ntrain_y = np.array(trainData)[:,0]\ntest_x = np.array(testData)\nprint('train_x:',train_x.shape)\nprint('train_y:',train_y.shape)\nprint('test_x:',test_x.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:08.521708Z","iopub.execute_input":"2022-05-07T06:08:08.521953Z","iopub.status.idle":"2022-05-07T06:08:14.598801Z","shell.execute_reply.started":"2022-05-07T06:08:08.521924Z","shell.execute_reply":"2022-05-07T06:08:14.597918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 处理数据","metadata":{}},{"cell_type":"markdown","source":"## 2.1转为二维图像","metadata":{}},{"cell_type":"code","source":"H , W = int(train_x.shape[1]**0.5) , int(train_x.shape[1]**0.5)\ntrain_imgs = train_x.reshape(-1,H,W)\ntest_imgs = test_x.reshape(-1,H,W)\n\nprint('train_x:',train_imgs.shape)\nprint('test_x:',test_imgs.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:14.600363Z","iopub.execute_input":"2022-05-07T06:08:14.600667Z","iopub.status.idle":"2022-05-07T06:08:14.607834Z","shell.execute_reply.started":"2022-05-07T06:08:14.600624Z","shell.execute_reply":"2022-05-07T06:08:14.606996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2可视化图像","metadata":{}},{"cell_type":"code","source":"#展示数字\nfrom matplotlib import pyplot as plt \n\ndef showImage(image):\n    plt.gray()\n    plt.imshow(image)\n    plt.show()\n    \nshow_num = 5\nfor i in range(show_num):\n    showImage(train_imgs[i])\n    print('数字是:',train_y[i])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:14.609736Z","iopub.execute_input":"2022-05-07T06:08:14.610076Z","iopub.status.idle":"2022-05-07T06:08:15.576968Z","shell.execute_reply.started":"2022-05-07T06:08:14.610042Z","shell.execute_reply":"2022-05-07T06:08:15.576078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3归一化","metadata":{}},{"cell_type":"code","source":"def normalization(x):\n    eps = 1e-5\n    if x.ndim > 2:\n        mean = np.mean(x, axis=(0, 2, 3))[:, np.newaxis, np.newaxis]\n        var = np.var(x, axis=(0, 2, 3))[:, np.newaxis, np.newaxis]\n        x = (x - mean) / np.sqrt(var + eps)\n    else:\n        mean = np.mean(x, axis=1)[:, np.newaxis]\n        var = np.var(x, axis=1)[:, np.newaxis] + eps\n        x = (x - mean) / np.sqrt(var)\n\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.578021Z","iopub.execute_input":"2022-05-07T06:08:15.57823Z","iopub.status.idle":"2022-05-07T06:08:15.585308Z","shell.execute_reply.started":"2022-05-07T06:08:15.578204Z","shell.execute_reply":"2022-05-07T06:08:15.584496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 多个模块的实现","metadata":{}},{"cell_type":"markdown","source":"## 3.1参数结构体","metadata":{}},{"cell_type":"code","source":"class parameter():\n    def __init__(self, w):\n        self.data = w     # 权重\n        self.grad = None  # 传到下一层的梯度","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.586484Z","iopub.execute_input":"2022-05-07T06:08:15.586799Z","iopub.status.idle":"2022-05-07T06:08:15.596121Z","shell.execute_reply.started":"2022-05-07T06:08:15.586741Z","shell.execute_reply":"2022-05-07T06:08:15.595394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2卷积层","metadata":{}},{"cell_type":"markdown","source":"### 3.2.1卷积","metadata":{}},{"cell_type":"code","source":"class conv():\n    def __init__(self, filter_shape, stride=1, padding='SAME', bias=True, requires_grad=True):\n        \"\"\"\n        :param filter_shape:元组（O, C, K, K）\n        :param stride: 步长\n        :param padding: 填充方式:{\"SAME\", \"VALID\"}\n        :param bias:是否有偏置\n        :param requires_grad:是否计算梯度\n        \"\"\"\n        self.weight = parameter(np.random.randn(*filter_shape) * (2 / reduce(lambda x,y: x*y, filter_shape[1:])**0.5))  # #kaiming初始化\n        self.stride = stride\n        self.padding = padding\n        self.requires_grad = requires_grad\n        self.output_channel = filter_shape[0]   # 输出通道数\n        self.input_channel = filter_shape[1]    # 输入通道数\n        self.filter_size = filter_shape[2]  # 卷积核大小\n        if bias:\n            self.bias = parameter(np.random.randn(self.output_channel))\n        else:\n            self.bias = None\n\n\n    def forward(self, input):\n        \"\"\"\n        :param input:feature map维度：[N,C,H,W]\n        :return:卷积结果result：[N,O,output_H, output_W]\n        \"\"\"\n        # 不做边缘填充\n        if self.padding == \"VALID\":\n            self.x = input\n        # 填充\n        elif self.padding == \"SAME\":\n            p = self.filter_size // 2\n            self.x = np.pad(input, ((0,0), (0,0), (p,p), (p,p)), \"constant\")\n\n        \"\"\"\n        输入的宽高不能恰好的被卷积核的大小和选定的步长所整除时，有几个具体解决的策略：\n        1、直接抛出异常；\n        2、直接抛弃掉多余部分；\n        3、边缘填0，使其满足要求等等\n        \"\"\"\n        x_fit = (self.x.shape[2]-self.filter_size)%self.stride\n        y_fit = (self.x.shape[3]-self.filter_size)%self.stride\n\n        # if x_fit!=0 or y_fit!=0:\n        #     print(\"input tensor width\\height can\\'t fit stride\")\n        #     return\n\n        if (self.stride>1):\n            if x_fit != 0:\n                self.x = self.x[:,:,0:self.x.shape[2]-x_fit,:]\n            if y_fit != 0:\n                self.x = self.x[:,:,:,0:self.x.shape[3]-y_fit]\n\n\n        # 卷积运算实现\n        N, C, H, W = self.x.shape\n        output_H, output_W = (H-self.filter_size)//self.stride+1, (W-self.filter_size)//self.stride+1\n        result = np.zeros((N, self.output_channel, output_H, output_W))\n\n        for n in range(N):\n            for o in range(self.output_channel):\n                for i in range(0, H-self.filter_size+1, self.stride):\n                    for j in range(0, W-self.filter_size+1, self.stride):\n                        result[n,o,i,j] = np.sum(self.x[n, :, i:i+self.filter_size, j:j+self.filter_size]\n                                                 * self.weight.data[o,:,:,:])\\\n                                          + (self.bias.data[o] if self.bias else 0)\n\n        return result\n\n\n    def backward(self,eta, lr):\n        \"\"\"\n        :param eta:上一层返回的梯度[N,O,output_H, output_W]\n        :return:本层的梯度result\n        说明：对于某一层conv层进行求导时分为两个部分：1、本层梯度反向传播到上一层；2、本层内求导，分别对 W,b\n        \"\"\"\n\n        # 在实现卷积的反向传播中，有两点需要注意：1、当步长大于1时，上一层返回的梯度要行、列之间插0；\n        # 2、对于“VALID”填充方式，要在梯度周围添加self.filter_size-1圈零；对于“SAME”填充方式，要在梯度周围添加self.filter_size//2 圈零\n        if self.stride>1:\n            N, O, output_H, output_W = eta.shape[:]\n            inserted_H, inserted_W = output_H + (self.stride-1)*(output_H-1), output_W + (self.stride-1)*(output_W-1)\n            insert_eta = np.zeros((N, O, inserted_H, inserted_W))\n            insert_eta[:,:,::self.stride, ::self.stride] = eta[:]\n            eta = insert_eta\n\n        # 本层内求导，分别对 W,b\n        N, _, H, W = eta.shape\n        self.b_grad = eta.sum(axis=(0,2,3))\n        self.W_grad = np.zeros(self.weight.data.shape)      # 形状[O, C, K, K]\n        for i in range(self.filter_size):\n            for j in range(self.filter_size):\n                self.W_grad[:,:,i,j] = np.tensordot(eta, self.x[:,:,i:i+H,j:j+W], ([0,2,3], [0,2,3]))\n        # 权重更新\n        self.weight.data -= lr * self.W_grad / N\n        self.bias.data -= lr * self.b_grad / N\n\n        # 第二步边缘填充\n        if self.padding == \"VALID\":\n            p = self.filter_size - 1\n            pad_eta = np.lib.pad(eta, ((0,0),(0,0),(p,p),(p,p)), \"constant\", constant_values=0)\n            eta = pad_eta\n        if self.padding == \"SAME\":\n            p = self.filter_size // 2\n            pad_eta = np.lib.pad(eta, ((0,0),(0,0),(p,p),(p,p)), \"constant\", constant_values=0)\n            eta = pad_eta\n\n\n        # 本层梯度反向传播到上一层\n        weight_flip = np.flip(self.weight.data, (2,3))  # 卷积核旋转180度\n        weight_flip_swap = np.swapaxes(weight_flip, 0, 1)  # 交换输入、输出通道的顺序[C,O,H,W]\n        N, O, H, W = eta.shape\n        output_H, output_W = (H-self.filter_size)//self.stride+1, (W-self.filter_size)//self.stride+1\n        self.weight.grad = np.zeros((N, weight_flip_swap.shape[0], output_H, output_W))\n\n        for n in range(N):\n            for c in range(weight_flip_swap.shape[0]):\n                for i in range(0, H-self.filter_size+1, self.stride):\n                    for j in range(0, W-self.filter_size+1, self.stride):\n                        self.weight.grad[n,c,i,j] = np.sum(eta[n, :, i:i+self.filter_size, j:j+self.filter_size]\n                                                 * weight_flip_swap[c,:,:,:])\n\n        return self.weight.grad","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.597739Z","iopub.execute_input":"2022-05-07T06:08:15.598256Z","iopub.status.idle":"2022-05-07T06:08:15.633341Z","shell.execute_reply.started":"2022-05-07T06:08:15.598219Z","shell.execute_reply":"2022-05-07T06:08:15.63262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.2快速卷积","metadata":{}},{"cell_type":"code","source":"class conv_fast():\n    def __init__(self, filter_shape, stride=1, padding='SAME', bias=True, requires_grad=True):\n        \"\"\"\n        :param filter_shape:元组（O, C, K, K）\n        :param stride: 步长\n        :param padding: 填充方式:{\"SAME\", \"VALID\"}\n        :param bias:是否有偏置\n        :param requires_grad:是否计算梯度\n        \"\"\"\n        self.weight = parameter(np.random.randn(*filter_shape) * (2/reduce(lambda x,y:x*y, filter_shape[1:]))**0.5)  #kaiming初始化\n        self.stride = stride\n        self.padding = padding\n        self.requires_grad = requires_grad\n        self.output_channel = filter_shape[0]   # 输出通道数\n        self.input_channel = filter_shape[1]    # 输入通道数\n        self.filter_size = filter_shape[2]  # 卷积核大小\n        if bias:\n            self.bias = parameter(np.random.randn(self.output_channel))\n        else:\n            self.bias =None\n\n    def forward(self, input):\n        \"\"\"\n        :param input:feature map 形状：[N,C,H,W]\n        :return:\n        \"\"\"\n        # 第一步边缘填充\n        if self.padding == \"VALID\":\n            self.x = input\n        if self.padding == \"SAME\":\n            p = self.filter_size // 2\n            self.x = np.lib.pad(input, ((0,0),(0,0),(p,p),(p,p)), \"constant\")\n        # 第二步处理输入的宽高不能恰好的被卷积核的大小和选定的步长所整除\n        x_fit = (self.x.shape[2] - self.filter_size) % self.stride\n        y_fit = (self.x.shape[3] - self.filter_size) % self.stride\n\n        if self.stride > 1:\n            if x_fit != 0:\n                self.x = self.x[:, :, 0:self.x.shape[2] - x_fit, :]\n            if y_fit != 0:\n                self.x = self.x[:, :, :, 0:self.x.shape[3] - y_fit]\n\n        # 实现卷积\n        N, _, H, W = self.x.shape\n        O, C, K, K = self.weight.data.shape\n        weight_cols = self.weight.data.reshape(O, -1).T\n        x_cols = self.img2col(self.x, self.filter_size, self.filter_size, self.stride)\n        result = np.dot(x_cols, weight_cols) + self.bias.data\n        output_H, output_W = (H-self.filter_size)//self.stride + 1, (W-self.filter_size)//self.stride + 1\n        result = result.reshape((N, result.shape[0]//N, -1)).reshape((N, output_H, output_W, O))\n        return result.transpose((0, 3, 1, 2))\n\n\n    def backward(self, eta, lr):\n        \"\"\"\n        :param eta:上一层返回的梯度[N,O,output_H, output_W]\n        param lr:学习率\n        :return:\n        \"\"\"\n        # 在eta行行列列之间插入插入0后，计算W,b的梯度；然后进行padding， padding后计算返回上一层的梯度\n\n\n        # 第一步步长大于1要在行行和列列之间插0\n        if self.stride > 1:\n            N, O, output_H, output_W = eta.shape\n            inserted_H, inserted_W = output_H + (output_H-1)*(self.stride-1), output_W + (output_W-1)*(self.stride-1)\n            inserted_eta = np.zeros((N, O, inserted_H, inserted_W))\n            inserted_eta[:,:,::self.stride, ::self.stride] = eta\n            eta = inserted_eta\n\n        # 计算本层的W,b的梯度\n        N, _, output_H, output_W = eta.shape\n        self.b_grad = eta.sum(axis=(0,2,3))\n        self.W_grad = np.zeros(self.weight.data.shape)      # 形状[O, C, K, K]\n        for i in range(self.filter_size):\n            for j in range(self.filter_size):\n                self.W_grad[:,:,i,j] = np.tensordot(eta, self.x[:,:,i:i+output_H,j:j+output_W], ([0,2,3], [0,2,3]))\n        # 权重更新\n        self.weight.data -= lr * self.W_grad / N\n        self.bias.data -= lr * self.b_grad / N\n\n\n        # 第二步边缘填充\n        if self.padding == \"VALID\":\n            p = self.filter_size - 1\n            pad_eta = np.lib.pad(eta, ((0,0),(0,0),(p,p),(p,p)), \"constant\", constant_values=0)\n            eta = pad_eta\n        elif self.padding == \"SAME\":\n            p = self.filter_size // 2\n            pad_eta = np.lib.pad(eta, ((0, 0), (0, 0), (p, p), (p, p)), \"constant\", constant_values=0)\n            eta = pad_eta\n\n        # 计算传到上一层的梯度\n        _, C, _, _ = self.weight.data.shape\n        weight_flip = np.flip(self.weight.data, (2,3))  # 卷积核旋转180度\n        weight_flip_swap = np.swapaxes(weight_flip, 0, 1)  # 交换输入、输出通道的顺序[C,O,H,W]\n        weight_flip = weight_flip_swap.reshape(C, -1).T\n        x_cols = self.img2col(eta, self.filter_size, self.filter_size, self.stride)\n        result = np.dot(x_cols, weight_flip)\n        N, _, H, W = eta.shape\n        output_H, output_W = (H - self.filter_size) // self.stride + 1, (W - self.filter_size) // self.stride + 1\n        result = result.reshape((N, result.shape[0] // N, -1)).reshape((N, output_H, output_W, C))\n        self.weight.grad = result.transpose((0, 3, 1, 2))\n\n        return self.weight.grad\n\n\n    def img2col(self, x, filter_size_x, filter_size_y, stride):\n        \"\"\"\n        现代计算机运算中矩阵运算已经极为成熟（无论是速度还是内存），因此思路是将x中的每个卷积单位[N,C,K,K]展开为行向量，然后与组成二维矩阵\n        与展开的权重进行矩阵乘法。最后把结果reshape一下就可以了。\n        缺点：虽然提升了速度，但是增大的内存开销（因为x展开成的二维矩阵，存在大量重复元素）\n        :param x:输入的feature map形状：[N,C,H,W]\n        :param filter_size_x:卷积核的尺寸x\n        :param filter_size_y:卷积核的尺寸y\n        :param stride:卷积步长\n        :return:二维矩阵 形状：[(H-filter_size+1)/stride * (W-filter_size+1)/stride*N, C * filter_size_x * filter_size_y]\n        \"\"\"\n\n        N, C, H, W = x.shape\n        output_H, output_W = (H-filter_size_x)//stride + 1, (W-filter_size_y)//stride + 1\n        out_size = output_H * output_W\n        x_cols = np.zeros((out_size*N, filter_size_x*filter_size_y*C))\n        for i in range(0, H-filter_size_x+1, stride):\n            i_start = i * output_W\n            for j in range(0, W-filter_size_y+1, stride):\n                temp = x[:,:, i:i+filter_size_x, j:j+filter_size_y].reshape(N,-1)\n                x_cols[i_start+j::out_size, :] = temp\n        return x_cols\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.635124Z","iopub.execute_input":"2022-05-07T06:08:15.635617Z","iopub.status.idle":"2022-05-07T06:08:15.671913Z","shell.execute_reply.started":"2022-05-07T06:08:15.635586Z","shell.execute_reply":"2022-05-07T06:08:15.671054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3池化层","metadata":{}},{"cell_type":"markdown","source":"### 3.3.1 MaxPooling","metadata":{}},{"cell_type":"code","source":"class Maxpooling():\n    def __init__(self, kernel_size=(2, 2), stride=2, ):\n        \"\"\"\n        :param kernel_size:池化核的大小(kx,ky)\n        :param stride: 步长\n        这里有个默认的前提条件就是：kernel_size=stride\n        \"\"\"\n        self.ksize = kernel_size\n        self.stride = stride\n\n    def forward(self, input):\n        \"\"\"\n        :param input:feature map形状[N,C,H,W]\n        :return:maxpooling后的结果[N,C,H/ksize,W/ksize]\n        \"\"\"\n        N, C, H, W = input.shape\n        out = input.reshape(N, C, H//self.stride, self.stride, W//self.stride, self.stride)\n        out = out.max(axis=(3,5))\n        self.mask = out.repeat(self.ksize[0], axis=2).repeat(self.ksize[1], axis=3) != input\n        return out\n\n    def backward(self, eta):\n        \"\"\"\n        :param eta:上一层返回的梯度[N,O,H,W]\n        :return:\n        \"\"\"\n        result = eta.repeat(self.ksize[0], axis=2).repeat(self.ksize[1], axis=3)\n        result[self.mask] = 0\n        return result","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.673205Z","iopub.execute_input":"2022-05-07T06:08:15.67375Z","iopub.status.idle":"2022-05-07T06:08:15.690229Z","shell.execute_reply.started":"2022-05-07T06:08:15.673705Z","shell.execute_reply":"2022-05-07T06:08:15.689481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3.2AveragePooling","metadata":{}},{"cell_type":"code","source":"class Averagepooling():\n    def __init__(self, kernel_size=(2,2), stride=2):\n        self.ksize = kernel_size\n        self.stride = stride\n\n    def forward(self, input):\n        N, C, H, W = input.shape\n        out = input.reshape((N, C, H//self.ksize, self.ksize, W//self.ksize, self.ksize))\n        out = out.sum(axis=(3,5))\n        out = out / self.ksize**2\n        return out\n\n    def backward(self, eta):\n        result = eta.repeat(self.ksize, axis=2).repeat(self.ksize, axis=3)\n        return result","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.692968Z","iopub.execute_input":"2022-05-07T06:08:15.693575Z","iopub.status.idle":"2022-05-07T06:08:15.705675Z","shell.execute_reply.started":"2022-05-07T06:08:15.693537Z","shell.execute_reply":"2022-05-07T06:08:15.704764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 全连接","metadata":{}},{"cell_type":"code","source":"class fc():\n    def __init__(self, input_num, output_num, bias=True, requires_grad=True):\n        \"\"\"\n        :param input_num:输入神经元个数\n        :param output_num: 输出神经元的个数\n        \"\"\"\n        self.input_num = input_num          # 输入神经元个数\n        self.output_num = output_num        # 输出神经元个数\n        self.requires_grad = requires_grad\n        self.weight = parameter(np.random.randn(self.input_num, self.output_num) * (2/self.input_num**0.5))\n        if bias:\n            self.bias = parameter(np.random.randn(self.output_num))\n        else:\n            self.bias = None\n\n\n    def forward(self, input):\n        \"\"\"\n        :param input: 输入的feature map 形状：[N,C,H,W]或[N,C*H*W]\n        :return:\n        \"\"\"\n        self.input_shape = input.shape    # 记录输入数据的形状\n        if input.ndim > 2:\n            N, C, H, W = input.shape\n            self.x = input.reshape((N, -1))\n        elif input.ndim == 2:\n            self.x = input\n        else:\n            print(\"fc.forward的输入数据维度存在问题\")\n        result = np.dot(self.x, self.weight.data)\n        if self.bias is not None:\n            result = result + self.bias.data\n        return result\n\n\n    def backward(self, eta, lr):\n        \"\"\"\n        :param eta:由上一层传入的梯度 形状：[N,output_num]\n        :param lr:学习率\n        :return: self.weight.grad 回传到上一层的梯度\n        \"\"\"\n        N, _ = eta.shape\n        # 计算传到下一层的梯度\n        next_eta = np.dot(eta, self.weight.data.T)\n        self.weight.grad = np.reshape(next_eta, self.input_shape)\n\n        # 计算本层W,b的梯度\n        x = self.x.repeat(self.output_num, axis=0).reshape((N, self.output_num, -1))\n        self.W_grad = x * eta.reshape((N, -1, 1))\n        self.W_grad = np.sum(self.W_grad, axis=0) / N\n        self.b_grad = np.sum(eta, axis=0) / N\n\n\n        # 权重更新\n        self.weight.data -= lr * self.W_grad.T\n        self.bias.data -= lr * self.b_grad\n\n        return self.weight.grad","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.708416Z","iopub.execute_input":"2022-05-07T06:08:15.710921Z","iopub.status.idle":"2022-05-07T06:08:15.726511Z","shell.execute_reply.started":"2022-05-07T06:08:15.710881Z","shell.execute_reply":"2022-05-07T06:08:15.72572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.5激活函数","metadata":{}},{"cell_type":"markdown","source":"### 3.5.1 ReLu","metadata":{}},{"cell_type":"code","source":"class Relu():\n    def forward(self, x):\n        self.x = x\n        return np.maximum(self.x, 0)\n\n    def backward(self, eta):\n        eta[self.x <= 0] = 0\n        return eta","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.72771Z","iopub.execute_input":"2022-05-07T06:08:15.728069Z","iopub.status.idle":"2022-05-07T06:08:15.74341Z","shell.execute_reply.started":"2022-05-07T06:08:15.728035Z","shell.execute_reply":"2022-05-07T06:08:15.742631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.2 Sigmoid","metadata":{}},{"cell_type":"code","source":"class sigmoid():\n    def forward(self, x):\n        self.out = 1 / (1 + np.exp(-x))\n        return self.out\n\n    def backward(self, eta):\n        result = eta * (self.out * (1-self.out))\n        return result","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.744508Z","iopub.execute_input":"2022-05-07T06:08:15.745328Z","iopub.status.idle":"2022-05-07T06:08:15.751978Z","shell.execute_reply.started":"2022-05-07T06:08:15.745292Z","shell.execute_reply":"2022-05-07T06:08:15.751378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.3 Tanh","metadata":{}},{"cell_type":"code","source":"class tanh():\n    def forward(self, x):\n        temp1 = np.exp(x) - np.exp(-x)\n        temp2 = np.exp(x) + np.exp(-x)\n        self.out = temp1 / temp2\n        return self.out\n\n    def backward(self, eta):\n        return eta * (1 - np.square(self.out))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.753065Z","iopub.execute_input":"2022-05-07T06:08:15.753586Z","iopub.status.idle":"2022-05-07T06:08:15.766526Z","shell.execute_reply.started":"2022-05-07T06:08:15.753547Z","shell.execute_reply":"2022-05-07T06:08:15.765668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.6归一化层","metadata":{}},{"cell_type":"markdown","source":"### 3.6.1BatchNorm","metadata":{}},{"cell_type":"code","source":"# BN层有两种实现的方法：一种是原论文中提到的对每个pixel计算均值、方差，并且学习两个参数alpha、beta，但是这样的话参数量较大；\n# 所以一般实现采用第二种方法：每个通道计算均值、方差，并学习两个参数alpha、beta。\nclass BN():\n    def __init__(self, channel, moving_decay=0.9, is_train=True):\n        \"\"\"\n        :param input_shape:输入需要归一化的数据：[N,C,H,W]\n        :param is_train: 当前是否为训练状态\n        \"\"\"\n        self.alpha = parameter(np.ones((channel,1,1)))\n        self.beta = parameter(np.zeros((channel,1,1)))\n        self.is_train = is_train\n        self.eps = 1e-5 # 数据归一化时防止下溢\n\n        self.moving_mean = np.zeros((channel,1,1))\n        self.moving_var = np.zeros((channel,1,1))\n        self.moving_decay = moving_decay\n\n\n    def forward(self, x, is_train=True):\n        \"\"\"\n        :param x:输入的feature map：[N,C,H,W]\n        :return: batch_normalization的结果[N,C,H,W]\n        \"\"\"\n        self.is_train = is_train\n        N, C, H, W = x.shape\n        self.x = x\n        if N <= 4:\n            print(\"batch size较小，BN层不能准确估计数据集的均值和方差，不建议使用BN层\")\n            return x\n\n        if self.is_train:       # 判断是否为训练状态\n            self.mean = np.mean(x, axis=(0,2,3))[:,np.newaxis, np.newaxis]\n            self.var = np.var(x, axis=(0,2,3))[:,np.newaxis, np.newaxis]\n\n            # 计算滑动平均\n            if (np.sum(self.moving_mean)==0) and (np.sum(self.moving_var)==0):\n                self.moving_mean = self.mean\n                self.moving_var = self.var\n            else:\n                self.moving_mean = self.moving_mean * self.moving_decay + (1-self.moving_decay) * self.mean\n                self.moving_var = self.moving_var * self.moving_decay + (1-self.moving_decay) * self.var\n\n            self.y = (x - self.mean) / np.sqrt(self.var + self.eps)\n            return  self.alpha.data * self.y + self.beta.data\n        else:   # 如果为测试阶段\n            self.y = (x - self.moving_mean) / np.sqrt(self.moving_var + self.eps)\n            return  self.alpha.data * self.y + self.beta.data\n\n\n    def backward(self, eta, lr):\n        \"\"\"\n        :param eta:\n        :param lr:学习率\n        :return:\n        \"\"\"\n        # 计算alpha和beta的梯度\n        N, _, H, W = eta.shape\n        alpha_grad = np.sum(eta * self.y, axis=(0,2,3))\n        beta_grad = np.sum(eta, axis=(0,2,3))\n\n        # 返回到上一层的梯度\n        # 注：这里关于ymean_grad和yvar_grad的计算不确定，欢迎留言指正\n        yx_grad = (eta * self.alpha.data)\n        ymean_grad = (-1.0 / np.sqrt(self.var +self.eps)) * yx_grad\n        ymean_grad = np.sum(ymean_grad, axis=(2,3))[:,:,np.newaxis,np.newaxis] / (H*W)\n        yvar_grad = -0.5*yx_grad*(self.x - self.mean) / (self.var+self.eps)**(3.0/2)\n        yvar_grad = 2 * (self.x-self.mean) * np.sum(yvar_grad,axis=(2,3))[:,:,np.newaxis,np.newaxis] / (H*W)\n        result = yx_grad*(1 / np.sqrt(self.var +self.eps)) + ymean_grad + yvar_grad\n\n\n        self.alpha.data -= lr * alpha_grad[:,np.newaxis, np.newaxis] / N\n        self.beta.data -=  lr * beta_grad[:, np.newaxis, np.newaxis] / N\n\n        return result","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.767893Z","iopub.execute_input":"2022-05-07T06:08:15.768306Z","iopub.status.idle":"2022-05-07T06:08:15.789703Z","shell.execute_reply.started":"2022-05-07T06:08:15.768272Z","shell.execute_reply":"2022-05-07T06:08:15.788835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.7 Dropout","metadata":{}},{"cell_type":"code","source":"class Dropout():\n    def __init__(self, drop_rate=0.5, is_train=True):\n        \"\"\"\n        :param drop_rate: 随机丢弃神经元的概率\n        :param is_train: 当前是否为训练状态\n        \"\"\"\n        self.drop_rate = drop_rate\n        self.is_train = is_train\n        self.fix_value = 1 - drop_rate   # 修正期望，保证输出值的期望不变\n\n\n    def forward(self, x):\n        \"\"\"\n        :param x:[N, m] N为batch_size, m为神经元个数\n        :return:\n        \"\"\"\n        if self.is_train==False:    # 当前为测试状态\n            return x\n        else:             # 当前为训练状态\n            N, m = x.shape\n            self.save_mask = np.random.uniform(0, 1, m) > self.drop_rate   # save_mask中为保留的神经元\n            return (x * self.save_mask) / self.fix_value\n\n\n    def backward(self, eta):\n        if self.is_train==False:\n            return eta\n        else:\n            return eta * self.save_mask","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.790928Z","iopub.execute_input":"2022-05-07T06:08:15.791598Z","iopub.status.idle":"2022-05-07T06:08:15.803909Z","shell.execute_reply.started":"2022-05-07T06:08:15.791556Z","shell.execute_reply":"2022-05-07T06:08:15.803209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.8Loss","metadata":{}},{"cell_type":"code","source":"class softmax():\n    # def __init__(self, shape):\n    #     \"\"\"\n    #     :param shape:[N, m] 其中N为batch_size，m为要预测的类别\n    #     \"\"\"\n    #     self.out = np.zeros(shape)\n    #     self.eta = np.zeros(shape)\n\n    def calculate_loss(self, x, label):\n        \"\"\"\n        :param x: 上一层输出的向量：[N, m] 其中N表示batch，m表示输出节点个数\n        :param label:数据的真实标签：[N]\n        :return:\n        \"\"\"\n        N, _ = x.shape\n        self.label = np.zeros_like(x)\n        for i in range(self.label.shape[0]):\n            self.label[i, label[i]] = 1\n\n        self.x = np.exp(x - np.max(x, axis=1)[:, np.newaxis])   # 为了防止x中出现极值导致溢出，每个样本减去其中最大的值\n        sum_x = np.sum(self.x, axis=1)[:, np.newaxis]\n        self.prediction = self.x / sum_x\n\n        self.loss = -np.sum(np.log(self.prediction+1e-6) * self.label)  # 防止出现log(0)的情况\n        return self.loss / N\n\n    def prediction_func(self, x):\n        x = np.exp(x - np.max(x, axis=1)[:, np.newaxis])  # 为了防止x中出现极值导致溢出，每个样本减去其中最大的值\n        sum_x = np.sum(x, axis=1)[:, np.newaxis]\n        self.out = x / sum_x\n        return self.out\n\n\n    def gradient(self):\n        self.eta = self.prediction.copy() - self.label\n        return self.eta","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.805312Z","iopub.execute_input":"2022-05-07T06:08:15.805629Z","iopub.status.idle":"2022-05-07T06:08:15.817805Z","shell.execute_reply.started":"2022-05-07T06:08:15.805583Z","shell.execute_reply":"2022-05-07T06:08:15.816775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4 网络结构--LeNet","metadata":{"execution":{"iopub.status.busy":"2022-05-07T04:04:23.409775Z","iopub.execute_input":"2022-05-07T04:04:23.410405Z","iopub.status.idle":"2022-05-07T04:04:23.414723Z","shell.execute_reply.started":"2022-05-07T04:04:23.410363Z","shell.execute_reply":"2022-05-07T04:04:23.413837Z"}}},{"cell_type":"code","source":"class LeNet5():\n    def __init__(self):\n        self.conv1 = conv_fast((6, 1, 5, 5), stride=1, padding='SAME', bias=True, requires_grad=True)\n        self.pooling1 = Maxpooling(kernel_size=(2, 2), stride=2)\n        self.BN1 = BN(6, moving_decay=0.9, is_train=True)\n        self.relu1 = Relu()\n\n        self.conv2 = conv_fast((16, 6, 5, 5), stride=1, padding=\"VALID\", bias=True, requires_grad=True)\n        self.pooling2 = Maxpooling(kernel_size=(2, 2), stride=2)\n        self.BN2 = BN(16, moving_decay=0.9, is_train=True)\n        self.relu2 = Relu()\n\n        self.conv3 = conv_fast((120, 16, 5, 5), stride=1, padding=\"VALID\", bias=True, requires_grad=True)\n\n        self.fc4 = fc(120*1*1, 84, bias=True, requires_grad=True)\n        self.relu4 = Relu()\n        self.fc5 = fc(84, 10, bias=True, requires_grad=True)\n\n        self.softmax = softmax()\n\n    def forward(self, imgs, labels, is_train=True):\n        \"\"\"\n        :param imgs:输入的图片：[N,C,H,W]\n        :param labels:\n        :return:\n        \"\"\"\n        x = self.conv1.forward(imgs)\n        x = self.pooling1.forward(x)\n        x = self.BN1.forward(x, is_train)\n        x = self.relu1.forward(x)\n\n        x = self.conv2.forward(x)\n        x = self.pooling2.forward(x)\n        x = self.BN2.forward(x, is_train)\n        x = self.relu2.forward(x)\n\n        x = self.conv3.forward(x)\n\n        x = self.fc4.forward(x)\n        x = self.relu4.forward(x)\n        x = self.fc5.forward(x)\n        \n        prediction = self.softmax.prediction_func(x)\n        \n        if is_train == True:\n            loss = self.softmax.calculate_loss(x, labels)\n            return loss, prediction\n        else:\n            return prediction\n            \n\n\n    def backward(self, lr):\n        \"\"\"\n        :param lr:学习率\n        :return:\n        \"\"\"\n        eta = self.softmax.gradient()\n\n        eta = self.fc5.backward(eta, lr)\n        eta = self.relu4.backward(eta)\n        eta = self.fc4.backward(eta, lr)\n\n        eta = self.conv3.backward(eta, lr)\n\n        eta = self.relu2.backward(eta)  # 激活层没有参数，不需要学习\n        eta = self.BN2.backward(eta, lr)\n        eta = self.pooling2.backward(eta)     # 池化层没有参数，不需要学习\n        eta = self.conv2.backward(eta, lr)\n\n        eta = self.relu1.backward(eta)\n        eta = self.BN1.backward(eta, lr)\n        eta = self.pooling1.backward(eta)\n        eta = self.conv1.backward(eta, lr)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.819086Z","iopub.execute_input":"2022-05-07T06:08:15.819525Z","iopub.status.idle":"2022-05-07T06:08:15.838984Z","shell.execute_reply.started":"2022-05-07T06:08:15.819494Z","shell.execute_reply":"2022-05-07T06:08:15.838363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5训练\n\n**由于kaggle上cuda使用困难 这里只使用 epoch=3来减少运算时间**","metadata":{}},{"cell_type":"code","source":"import glob\nimport struct\nimport time\nimport matplotlib.pyplot as plt\nfrom functools import reduce\nimport os \n\n#超参数\nbatch_size = 64  # 训练时的batch size\nepoch = 3 #由于kaggle上cuda使用困难 这里只使用 epoch=3来减少运算时间\nlearning_rate = 1e-3\n\n#存储中间过程\nx = []  # 保存训练过程中x轴的数据（训练次数）用于画图\ny_loss = []  # 保存训练过程中y轴的数据（loss）用于画图\ny_acc = []\niterations_num = 0 # 记录训练的迭代次数\n\n#实例化网络\nnet = LeNet5()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.84023Z","iopub.execute_input":"2022-05-07T06:08:15.840713Z","iopub.status.idle":"2022-05-07T06:08:15.857465Z","shell.execute_reply.started":"2022-05-07T06:08:15.840681Z","shell.execute_reply":"2022-05-07T06:08:15.856605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for E in range(epoch):\n    batch_loss = 0\n    batch_acc = 0\n\n    epoch_loss = 0\n    epoch_acc = 0\n\n    for i in range(train_imgs.shape[0] // batch_size):\n        img = train_imgs[i*batch_size:(i+1)*batch_size].reshape(batch_size, 1, 28, 28)\n        img = normalization(img)\n        label = train_y[i*batch_size:(i+1)*batch_size]\n        loss, prediction = net.forward(img, label, is_train=True)   # 训练阶段\n\n        epoch_loss += loss\n        batch_loss += loss\n        for j in range(prediction.shape[0]):\n            if np.argmax(prediction[j]) == label[j]:\n                epoch_acc += 1\n                batch_acc += 1\n        net.backward(learning_rate)\n\n        if (i+1)%50== 0:\n            print(\"epoch:%2d , batch:%2d , avg_batch_acc:%.4f , avg_batch_loss:%.4f  \"\n                  % (E+1, i+1, batch_acc/(batch_size*50), batch_loss/(batch_size*50)))\n            iterations_num += 1\n            x.append(iterations_num)\n            y_loss.append(batch_loss/(batch_size*50))\n            y_acc.append(batch_acc/(batch_size*50))\n            batch_loss = 0\n            batch_acc = 0\n\n    print(\"##epoch:%d , avg_epoch_acc:%.4f , avg_epoch_loss:%.4f ##\"% (E+1, epoch_acc/train_imgs.shape[0], epoch_loss/train_imgs.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.858685Z","iopub.execute_input":"2022-05-07T06:08:15.859335Z","iopub.status.idle":"2022-05-07T06:08:15.870201Z","shell.execute_reply.started":"2022-05-07T06:08:15.859296Z","shell.execute_reply":"2022-05-07T06:08:15.869475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(1, 2, 1)\nplt.title('Train-Loss')  # 添加子标题\nplt.xlabel('iterations', fontsize=10)  # 添加轴标签\nplt.ylabel('Loss', fontsize=10)\nplt.plot(x, y_loss, 'r')\nplt.subplot(1, 2, 2)\nplt.title('Accuracy')  # 添加子标题\nplt.xlabel('iterations', fontsize=10)  # 添加轴标签\nplt.ylabel('acc', fontsize=10)\nplt.plot(x, y_acc, 'b')","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:15.871539Z","iopub.execute_input":"2022-05-07T06:08:15.871816Z","iopub.status.idle":"2022-05-07T06:08:16.169823Z","shell.execute_reply.started":"2022-05-07T06:08:15.871769Z","shell.execute_reply":"2022-05-07T06:08:16.169204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6 预测","metadata":{}},{"cell_type":"code","source":"img = test_imgs.reshape(test_imgs.shape[0], 1, 28, 28)\nimg = normalization(img)\nprediction = net.forward(img, labels=None, is_train=False)   # 训练阶段\npre_label = np.argmax(prediction ,axis=1)\nprint(pre_label)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:08:16.170962Z","iopub.execute_input":"2022-05-07T06:08:16.171333Z","iopub.status.idle":"2022-05-07T06:08:24.600206Z","shell.execute_reply.started":"2022-05-07T06:08:16.171284Z","shell.execute_reply":"2022-05-07T06:08:24.599164Z"},"trusted":true},"execution_count":null,"outputs":[]}]}