{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"def apply_nms(orig_prediction, iou_thresh):\n    \"\"\"\n    Applies non max supression and eliminates low score bounding boxes.\n\n      Args:\n        orig_prediction: the model output. A dictionary containing element scores and boxes.\n        iou_thresh: Intersection over Union threshold. Every bbox prediction with an IoU greater than this value\n                      gets deleted in NMS.\n\n      Returns:\n        final_prediction: Resulting prediction\n    \"\"\"\n\n    # torchvision returns the indices of the bboxes to keep\n    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n\n    # Keep indices from nms\n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n\n    return final_prediction","metadata":{"id":"oScD8M1bQYY7","execution":{"iopub.status.busy":"2022-04-18T17:15:51.992036Z","iopub.execute_input":"2022-04-18T17:15:51.992642Z","iopub.status.idle":"2022-04-18T17:15:52.001322Z","shell.execute_reply.started":"2022-04-18T17:15:51.992604Z","shell.execute_reply":"2022-04-18T17:15:52.000527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_low_score_bb(orig_prediction, score_thresh):\n    \"\"\"\n    Eliminates low score bounding boxes.\n\n    Args:\n        orig_prediction: the model output. A dictionary containing element scores and boxes.\n        score_thresh: Boxes with a lower confidence score than this value get deleted\n\n    Returns:\n        final_prediction: Resulting prediction\n    \"\"\"\n\n    # Remove low confidence scores according to given threshold\n    index_list_scores = []\n    scores = orig_prediction['scores'].detach().cpu().numpy()\n    for i in range(len(scores)):\n        if scores[i] > score_thresh:\n            index_list_scores.append(i)\n    keep = torch.tensor(index_list_scores)\n\n    # Keep indices from high score bb\n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n\n    return final_prediction","metadata":{"id":"v_XrugkEVrco","execution":{"iopub.status.busy":"2022-04-18T17:15:57.01653Z","iopub.execute_input":"2022-04-18T17:15:57.016888Z","iopub.status.idle":"2022-04-18T17:15:57.024504Z","shell.execute_reply.started":"2022-04-18T17:15:57.016853Z","shell.execute_reply":"2022-04-18T17:15:57.022503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    # Collate function for Dataloader\n    return tuple(zip(*batch))","metadata":{"id":"gnkQE_ceQYY7","execution":{"iopub.status.busy":"2022-04-18T17:15:57.382773Z","iopub.execute_input":"2022-04-18T17:15:57.383103Z","iopub.status.idle":"2022-04-18T17:15:57.387376Z","shell.execute_reply.started":"2022-04-18T17:15:57.383073Z","shell.execute_reply":"2022-04-18T17:15:57.386299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def IOU(box1, box2):\n    '''\n    Intersection over Union - IoU\n    *------------\n    |   (x2min,y2min)\n    |   *----------\n    |   | ######| |\n    ----|------* (x1max,y1max)\n        |         |\n        ----------\n\n    Args:\n        box1: [xmin,ymin,xmax,ymax]\n        box2: [xmin,ymin,xmax,ymax]\n\n    Returns:\n        iou -> value of intersection over union of the 2 boxes\n\n    '''\n\n    # Compute coordinates of intersection\n    xmin_inter = max(box1[0], box2[0])\n    ymin_inter = max(box1[1], box2[1])\n    xmax_inter = min(box1[2], box2[2])\n    ymax_inter = min(box1[3], box2[3])\n\n    # calculate area of intersection rectangle\n    inter_area = max(0, xmax_inter - xmin_inter + 1) * max(0, ymax_inter - ymin_inter + 1) # FIXME why plus one?\n \n    # calculate boxes areas\n    area1 = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n    area2 = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n \n    # compute IoU\n    iou = inter_area / float(area1 + area2 - inter_area)\n    assert iou >= 0\n    return iou\n","metadata":{"id":"bUXLC1TVWKEm","execution":{"iopub.status.busy":"2022-04-18T17:15:57.861849Z","iopub.execute_input":"2022-04-18T17:15:57.862234Z","iopub.status.idle":"2022-04-18T17:15:57.869867Z","shell.execute_reply.started":"2022-04-18T17:15:57.862194Z","shell.execute_reply":"2022-04-18T17:15:57.868714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_AP(ground_truth, predictions, iou_thresh=0.5, n_classes=4):\n    \"\"\"\n    Calculates Average Precision across all classes.\n\n    Args:\n        ground_truth: list with ground-truth objects. Needs to have the following format: [sequence, frame, obj, [xmin, ymin, xmax, ymax], label, score]\n        predictions: list with predictions objects. Needs to have the following format: [sequence, frame, obj, [xmin, ymin, xmax, ymax], label, score]\n        iou_thresh: IoU to which a prediction compared to a ground-truth is considered right.\n        n_classes: number of existent classes\n\n    Returns:\n        Average precision for the specified threshold.\n    \"\"\"\n    # Initialize lists\n    APs = []\n    class_gt = []\n    class_predictions = []\n\n    # AP is computed for each class\n    for c in range(n_classes):\n        # Find gt and predictions of the class\n        for gt in ground_truth:\n            if gt[4] == c:\n                class_gt.append(gt)\n        for predict in predictions:\n            if predict[4] == c:\n                class_predictions.append(predict)\n\n        # Create dict with array of zeros for bb in each image\n        gt_amount_bb = Counter([gt[1] for gt in class_gt])\n        for key, val in gt_amount_bb.items():\n            gt_amount_bb[key] = np.zeros(val)\n\n        # Sort class predictions by their score\n        class_predictions = sorted(class_predictions, key=lambda x: x[5], reverse=True)\n\n        # Create arrays for Positives (True and False)\n        TP = np.zeros(len(class_predictions))\n        FP = np.zeros(len(class_predictions))\n        # Number of true boxes\n        truth = len(class_gt)\n\n        # Initializing aux variables\n        epsilon = 1e-6\n\n        # Iterate over predictions in each image and compare with ground truth\n        for predict_idx, prediction in enumerate(class_predictions):\n            # Filter prediction image ground truths\n            image_gt = [obj for obj in class_gt if obj[1] == prediction[1]]\n\n            # Initializing aux variables\n            best_iou = -1\n            best_gt_iou_idx = -1\n\n            # Iterate through image ground truths and calculate IoUs\n            for gt_idx, gt in enumerate(image_gt):\n                iou = IOU(prediction[3], gt[3])\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_iou_idx = gt_idx\n\n            # If the best IoU is greater that thresh than an TP prediction has been found\n            if best_iou > iou_thresh and best_gt_iou_idx > -1:\n                # Check if gt box was already covered\n                if  gt_amount_bb[prediction[1]][best_gt_iou_idx] == 0:\n                    gt_amount_bb[prediction[1]][best_gt_iou_idx] = 1  # set as covered\n                    TP[predict_idx] = 1  # Count as true positive\n                else:\n                    FP[predict_idx] = 1\n            else:\n                FP[predict_idx] = 1\n\n        # Calculate recall and precision\n        TP_cumsum = np.cumsum(TP)\n        FP_cumsum = np.cumsum(FP)\n        recall = np.append([0], TP_cumsum / (truth + epsilon))\n        precision = np.append([1], np.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon)))\n\n        # Calculate the area precision/recall and add to list\n        APs.append(np.trapz(precision, recall))\n\n    return sum(APs)/len(APs)  # average of class precisions\n\n\ndef compute_mAP(ground_truth, predictions, n_classes):\n    \"\"\"\n    Calls AP computation for different levels of IoUs, [0.5:.05:0.95].\n\n    Args:\n        ground_truth: list with ground-truth objects. Needs to have the following format: [sequence, frame, obj, [xmin, ymin, xmax, ymax], label, score]\n        predictions: list with predictions objects. Needs to have the following format: [sequence, frame, obj, [xmin, ymin, xmax, ymax], label, score]\n        n_classes: number of existent classes.\n\n    Returns:\n        mAp and list with APs for each IoU threshold.\n    \"\"\"\n    # return mAP\n    APs = [compute_AP(ground_truth, predictions, iou_thresh, n_classes) for iou_thresh in np.arange(0.5, 1.0, 0.05)]\n    return np.mean(APs), APs","metadata":{"id":"xgqlcNe6WNJp","execution":{"iopub.status.busy":"2022-04-18T17:15:58.554231Z","iopub.execute_input":"2022-04-18T17:15:58.55455Z","iopub.status.idle":"2022-04-18T17:15:58.569062Z","shell.execute_reply.started":"2022-04-18T17:15:58.554521Z","shell.execute_reply":"2022-04-18T17:15:58.568104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, data_loader, device, sequences=1):\n    \"\"\"\n    Evaluates model mAP for IoU range of [0.5:.05:0.95].\n\n    Args:\n        model: -\n        data_loader: -\n        device: -\n        sequences: the number of sequences of images to pass, if any\n\n    Returns:\n        mAP and AP list for each IoU threshold in range [0.5:.05:0.95]\n    \"\"\"\n\n    # Set evaluation mode flag\n    model.eval()\n    # Create list with all object detection -> [set, frame, obj, [xmin,ymin,xmax,ymax], label, score]\n    ground_truth = []\n    predictions = []\n\n    # Gather all targets and outputs on test set\n    for image, targets in data_loader:\n        image = [img.to(device) for img in image]\n        outputs = model(image)\n        for idx in range(len(outputs)):\n            outputs[idx] = apply_nms(outputs[idx], iou_thresh=0.5)\n\n        # create list for targets and outputs to pass to compute_mAP()\n        # lists have the following structure:  [sequence, frame, obj_idx, [xmin, ymin, xmax, ymax], label, score]\n        for s in range(sequences):\n            obj_gt = 0\n            obj_target = 0\n            for out, target in zip(outputs, targets):\n\n                for i in range(len(target['boxes'])):\n                    ground_truth.append([s, target['image_id'].detach().cpu().numpy()[0], obj_target,\n                                         target['boxes'].detach().cpu().numpy()[i],\n                                         target['labels'].detach().cpu().numpy()[i], 1])\n                    obj_target += 1\n\n                for j in range(len(out['boxes'])):\n                    predictions.append([s, target['image_id'].detach().cpu().numpy()[0], obj_gt,\n                                        out['boxes'].detach().cpu().numpy()[j],\n                                        out['labels'].detach().cpu().numpy()[j],\n                                        out['scores'].detach().cpu().numpy()[j]])\n                    obj_gt += 1\n\n    mAP, AP = compute_mAP(ground_truth, predictions, n_classes=4)\n    print(\"mAP:{:.3f}\".format(mAP))\n    for ap_metric, iou in zip(AP, np.arange(0.5, 1, 0.05)):\n        print(\"\\tAP at IoU level [{:.2f}]: {:.3f}\".format(iou, ap_metric))\n\n    return mAP, AP","metadata":{"id":"PVsf3fh2V57d","execution":{"iopub.status.busy":"2022-04-18T17:15:59.464302Z","iopub.execute_input":"2022-04-18T17:15:59.464635Z","iopub.status.idle":"2022-04-18T17:15:59.476614Z","shell.execute_reply.started":"2022-04-18T17:15:59.464604Z","shell.execute_reply":"2022-04-18T17:15:59.475609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Data Pipeline","metadata":{"id":"uSUWBSCNQYZG"}},{"cell_type":"code","source":"# Create Data Pipeline\n\n# Training Data\ndataset_train = MyDataset(ann_directory,img_directory, mode = 'train')\nloader_train = DataLoader(dataset_train, batch_size=4, shuffle=True, collate_fn=collate_fn)\n# Validation Data\ndataset_validation = MyDataset(ann_directory,img_directory, mode = 'validation')\nloader_val = DataLoader(dataset_validation, batch_size=4, shuffle=True, collate_fn=collate_fn)\n# Test Data\ndataset_test = MyDataset(ann_directory,img_directory, mode = 'test')\nloader_test = DataLoader(dataset_test, batch_size=4, shuffle=True, collate_fn=collate_fn)","metadata":{"id":"FWVFhp0oQYZG","execution":{"iopub.status.busy":"2022-04-18T17:16:04.524112Z","iopub.execute_input":"2022-04-18T17:16:04.524561Z","iopub.status.idle":"2022-04-18T17:16:13.659895Z","shell.execute_reply.started":"2022-04-18T17:16:04.524516Z","shell.execute_reply":"2022-04-18T17:16:13.658968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test if dataset is working correctly. Print out ground truth bounding box of first image.","metadata":{"id":"OrjLY9XKdwZx"}},{"cell_type":"code","source":"from engine import train_one_epoch\n# Training\nfor epoch in range(epochs):\n    # train for one epoch, printing every 50 iterations\n    train_one_epoch(model, optimizer, loader_train, device, epoch, print_freq=20)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, loader_val, device=device)","metadata":{"id":"7u1hUFfG_tBk","outputId":"5f3450e1-44cf-4324-f7f2-fbbe4babdb2a","execution":{"iopub.status.busy":"2022-04-18T17:17:07.596924Z","iopub.execute_input":"2022-04-18T17:17:07.597303Z","iopub.status.idle":"2022-04-18T17:38:43.263877Z","shell.execute_reply.started":"2022-04-18T17:17:07.597263Z","shell.execute_reply":"2022-04-18T17:38:43.263102Z"},"trusted":true},"execution_count":null,"outputs":[]}]}