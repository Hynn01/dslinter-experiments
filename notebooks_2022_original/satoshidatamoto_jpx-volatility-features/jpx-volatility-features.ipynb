{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Volatility Estimation Features\n>- In this notebook there are serveral functions to estimate the volatility of an asset. some can be used as features to a model.\n>- Enjoy the short scripts to obtain them! \n>","metadata":{"_cell_guid":"8af163ff-915a-4fae-aefe-6447e64952e5","_uuid":"b328cc9e-a536-4347-beed-d033e9f5ac6a","papermill":{"duration":0.023167,"end_time":"2022-01-10T19:12:36.707243","exception":false,"start_time":"2022-01-10T19:12:36.684076","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"features\">Volatility Features</span>\n<hr>\n\nIn this notebook I will introduce six  methods for calculating volatility that can be used for feature engineering:\n\n| Volatility | Price Information |\n| ----------------------- | ---------------------- |\n| Realized | Close |\n| Parkinson | High, Low |\n| Garman-Klass | Open, High, Low, Close |\n| Roger-Satchell | Open, High, Low, Close |\n| Garman-Klass-Yang-Zhang | Open, High, Low, Close |\n| Yang-Zhang | Open, High, Low, Close |\n\nThe last five methods all use a continuous rate of return, resulting in an underestimation of the real volatility.\n\nRealized, Garman-Klass-Yang-Zhang, and Yang-Zhang all use the price information of the previous day and the current day, which can be regarded as day-to-day volatility. \n\n## 1. Realized Volatility: Close-Close\n\n$$\\sigma_{realized} = \\sqrt{ \\frac{N}{n-2} \\sum\\limits_{i=1} ^{n-1} (r_t-\\bar r)^2 }$$\n\n$r_t =\\log\\frac{C_t}{C_{t-1}}$: rate of return\n\n$ \\bar r =\\frac{1}{n} \\sum\\limits_{n}^{t=1}r_t$: average rate of return\n\n## 2. Parkinson Volatility: High-Low Volatility\n\n$$\\sigma_{parkinson} = \\sqrt{ {\\frac{1}{4*\\ln{2}} * \\frac{252}{n} * \\sum\\limits_{t=1}^{n} { \\ln{(\\frac{H_t}{L_t})}}^2}}$$\n\nThe general volatility only considers the closing price. Parkinson Volatility takes the highest price and the lowest price into consideration. \n\n## 3. Garman-Klass Volatility: OHLC volatility\n\nAssumes Brown motion with zero drift and no opening spike.\n\n$$\\sigma_{garman-klass} = \\sqrt{\\frac{N}{n} \\sum\\limits_{i=1}^{N} \\lbrack {\\frac{1}{2} * (\\log{ \\frac{H_i}{L_i}})^2 -(2*\\log2 -1) * (\\log\\frac{C_i}{O_i})^2\\rbrack}}$$\n\nCompared with Parkinson Volatility, it further considers the opening and closing prices, and incorporates more price information, \n\n## 4. Roger-Satchell Volatility: OHLC Volatility\n\nAssumes for non-zero drift, but assumed no opening spike.\n\n$$\\sigma_{roger-satchel} = \\sqrt{ \\frac{N}{n} \\sum\\limits_{i=1}^{n} \\lbrack \\log \\frac{H_i}{L_i} * \\log \\ frac{H_i}{O_i} + \\log \\frac{HL_i}{L_i} * \\log \\frac{L_i}{O_i} \\rbrack }$$\n\n## 5. Garman-Klass-Yang-Zhang Volatility: OHLC Volatility\n\nA modified version of Garman-Klass estimator that allows for opening spikes.\n\n$$\\sigma_{garkla-yangzh} = \\sqrt {\\frac{N}{n} \\sum\\limits_{i=1}^{n} \\lbrack (\\log \\frac{O_i}{C_{i-1 }})^2 + {\\frac{1}{2} * (\\log{\\frac{H_i}{L_i}})^2 -(2*\\log2 -1) * (\\log\\frac{C_i} {O_i})^2}\\rbrack }$$\n\nIn the cases where the return on assets is not zero, the volatility will be overestimated.\n\n## 6. Yang-Zhang Volatility: OHLC Volatility\n\n$$\\sigma_{yang-zhang} = \\sqrt {\\sigma_o^2 + k * \\sigma_c^2 + (1-k) * \\sigma_{rs}^2}$$\n\n$\\mu_o = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\log \\frac {O_i}{C_{i-1}}$\n\n$\\sigma_o^2 = \\frac{N}{n-1} \\sum\\limits_{i=1}^{n} (\\log \\frac {O_i}{C_{i-1}}-\\mu_o)^ 2$, Open-Close Volatility or Overnight Volatility\n\n$\\mu_c = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\log \\frac {C_i}{O_i}$, Close-Open Volatility\n\n$\\sigma_c^2 = \\frac{N}{n-1} \\sum\\limits_{i=1}^{n} (\\log \\frac {C_i}{O_i}-\\mu_c)^2$\n\n$\\sigma_{rs}^2 = \\sigma_{roger-satchel}^2$\n\n$k^* = \\frac {\\alpha} {1+ \\alpha + \\frac{n+1}{n-1}}, \\alpha$ \n\ncan also be interpreted as a weighted average of the Roger-Satchell estimator, the Close-Open Volatility and the Open-Close Volatility.\n\n## References\n\n1. [Volatility and its Measurements](https://www.eurexchange.com/blob/116048/47ca53f0178cec31caeecdf94cc18f6e/data/volatility_and_its_measurements.pdf.pdf)\n\n2. [Drift Independent Volatility Estimation Based on High, Low, Open and Close Price](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.628.4037&rep=rep1&type=pdf)\n\n3. [volatility function | R Documentation](https://www.rdocumentation.org/packages/TTR/versions/0.23-3/topics/volatility)\n\n4. [Parkinson volatility-Breaking Down Finance](http://breakingdownfinance.com/finance-topics/risk-management/parkinson-volatility/)\n\n5. [MEASURING HISTORICAL VOLATILITY](http://www.todaysgroep.nl/media/236846/measuring_historic_volatility.pdf)\n","metadata":{"papermill":{"duration":0.024363,"end_time":"2022-01-10T19:12:37.118162","exception":false,"start_time":"2022-01-10T19:12:37.093799","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport gc\nimport traceback\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=pd.core.common.SettingWithCopyWarning)\n    \nplt.style.use('bmh')\nplt.rcParams['figure.figsize'] = [14, 8]  # width, height","metadata":{"_cell_guid":"7b701afb-9ee3-4f9e-a219-3b6206a8283d","_kg_hide-input":true,"_uuid":"35744779-02d1-445c-a8d4-dcbf1e1bdba2","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.196909,"end_time":"2022-01-10T19:12:37.339671","exception":false,"start_time":"2022-01-10T19:12:37.142762","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:55:45.846927Z","iopub.execute_input":"2022-04-23T10:55:45.847486Z","iopub.status.idle":"2022-04-23T10:55:45.856088Z","shell.execute_reply.started":"2022-04-23T10:55:45.847437Z","shell.execute_reply":"2022-04-23T10:55:45.855163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype.name\n\n        if col_type not in ['object', 'category', 'datetime64[ns, UTC]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"_cell_guid":"b56ab916-dae6-4ff7-bb2d-52398dd52e1c","_kg_hide-input":true,"_uuid":"1146dd4e-6f2e-45c4-a2fd-7fee6e751e98","papermill":{"duration":0.043298,"end_time":"2022-01-10T19:12:37.408539","exception":false,"start_time":"2022-01-10T19:12:37.365241","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:55:45.90208Z","iopub.execute_input":"2022-04-23T10:55:45.902363Z","iopub.status.idle":"2022-04-23T10:55:45.918002Z","shell.execute_reply.started":"2022-04-23T10:55:45.902334Z","shell.execute_reply":"2022-04-23T10:55:45.917128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_list = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv\")\nprices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\nstock_list = stock_list.loc[stock_list['SecuritiesCode'].isin(prices['SecuritiesCode'].unique())]\nstock_name_dict = {stock_list['SecuritiesCode'].tolist()[idx]: stock_list['Name'].tolist()[idx] for idx in range(len(stock_list))}\n\ndef load_training_data(asset_id = None):\n    prices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\n    supplemental_prices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv\")\n    df_train = pd.concat([prices, supplemental_prices]) if INCSUPP else prices\n    df_train = pd.merge(df_train, stock_list[['SecuritiesCode', 'Name']], left_on = 'SecuritiesCode', right_on = 'SecuritiesCode', how = 'left')\n    df_train['date'] = pd.to_datetime(df_train['Date'])\n    df_train['year'] = df_train['date'].dt.year\n    if not INC2022: df_train = df_train.loc[df_train['year'] != 2022]\n    if not INC2021: df_train = df_train.loc[df_train['year'] != 2021]\n    if not INC2020: df_train = df_train.loc[df_train['year'] != 2020]\n    if not INC2019: df_train = df_train.loc[df_train['year'] != 2019]\n    if not INC2018: df_train = df_train.loc[df_train['year'] != 2018]\n    if not INC2017: df_train = df_train.loc[df_train['year'] != 2017]\n    # asset_id = 1301 # Remove before flight\n    if asset_id is not None: df_train = df_train.loc[df_train['SecuritiesCode'] == asset_id]\n    # df_train = df_train[:1000] # Remove before flight\n    return df_train","metadata":{"_cell_guid":"8da68356-63f7-49c7-aca2-b61019c278b6","_kg_hide-input":true,"_uuid":"aafb204a-c174-4898-af33-eede907e879b","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":25.331514,"end_time":"2022-01-10T19:13:02.765406","exception":false,"start_time":"2022-01-10T19:12:37.433892","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:55:45.943901Z","iopub.execute_input":"2022-04-23T10:55:45.944728Z","iopub.status.idle":"2022-04-23T10:55:50.798324Z","shell.execute_reply.started":"2022-04-23T10:55:45.944676Z","shell.execute_reply":"2022-04-23T10:55:50.797333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# WHICH YEARS TO INCLUDE? YES=1 NO=0\nINC2022 = 1\nINC2021 = 1\nINC2020 = 1\nINC2019 = 1\nINC2018 = 1\nINC2017 = 1\nINCSUPP = 1\n\ntrain = load_training_data().sort_values('date') #.set_index(\"date\")\nprint(\"Loaded all data!\")\n\ntrain_data = train.copy()\ntrain_data['date'] = pd.to_datetime(train_data['Date'])\ndf = train_data.loc[train_data['SecuritiesCode'] == 1301]\ndf = df[-500000:]","metadata":{"_kg_hide-input":true,"papermill":{"duration":13.614827,"end_time":"2022-01-10T19:13:16.405971","exception":false,"start_time":"2022-01-10T19:13:02.791144","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:55:50.800428Z","iopub.execute_input":"2022-04-23T10:55:50.80095Z","iopub.status.idle":"2022-04-23T10:55:57.996942Z","shell.execute_reply.started":"2022-04-23T10:55:50.800902Z","shell.execute_reply":"2022-04-23T10:55:57.996179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"features\">Feature Engineering 🔬</span>\n<hr>","metadata":{"_cell_guid":"22bb5d25-9c27-4be9-b9ac-0701b71058a5","_uuid":"137210f4-aa91-4525-9f53-5e91f4bf9ced","papermill":{"duration":0.025551,"end_time":"2022-01-10T19:13:16.457774","exception":false,"start_time":"2022-01-10T19:13:16.432223","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy\nimport pandas as pd\nfrom math import sqrt, log\n\ndef plot_feature(feat, df, feat_name):\n    try: plt.close()\n    except: pass   \n    df2 = df[-len(feat):].reset_index().set_index('date')\n    fig = plt.figure(figsize = (12, 6))\n    # fig, ax_left = plt.subplots(figsize = (12, 6))\n    ax_left = fig.add_subplot(111)\n    ax_left.set_facecolor('azure')    \n    ax_right = ax_left.twinx()\n    ax_left.plot(feat, color = 'crimson', label = feat_name)\n    ax_right.plot(df['Close'], color = 'darkgrey', label = \"Price\")\n    plt.legend()\n    plt.grid()\n    plt.xlabel('Time')\n    plt.title('3 month rolling %s vs close price' % (feat_name))\n    plt.show()","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.038229,"end_time":"2022-01-10T19:13:16.521941","exception":false,"start_time":"2022-01-10T19:13:16.483712","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:55:57.998593Z","iopub.execute_input":"2022-04-23T10:55:57.99884Z","iopub.status.idle":"2022-04-23T10:55:58.007181Z","shell.execute_reply.started":"2022-04-23T10:55:57.998793Z","shell.execute_reply":"2022-04-23T10:55:58.006374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"features\">Realized Volatility: Close-to-Close</span>\n<hr>\n\nClose-to-Close volatility is a classic and most commonly used volatility measure, sometimes referred to as historical volatility.\n\nVolatility is an indicator of the speed of a stock price change. A stock with high volatility is one where the price changes rapidly and with a bigger amplitude. The more volatile a stock is, the riskier it is.\n\nClose-to-close historical volatility calculated using only stock's closing prices. It is the simplest volatility estimator. But in many cases, it is not precise enough. Stock prices could jump considerably during a trading session, and return to the open value at the end. That means that a big amount of price information is not taken into account by close-to-close volatility.\n\nDespite its drawbacks, Close-to-Close volatility is still useful in cases where the instrument doesn't have intraday prices. For example, mutual funds calculate their net asset values daily or weekly, and thus their prices are not suitable for more sophisticated volatility estimators.\n\n[source](https://portfolioslab.com/tools/close-to-close-volatility)\n\n\n<br>\n\nDefined as:\n\n$$\\sigma_{realized} = \\sqrt{ \\frac{N}{n-2} \\sum\\limits_{i=1} ^{n-1} (r_t-\\bar r)^2 }$$\n\nWhere the **Rate of return**:\n$$r_t =\\log\\frac{C_t}{C_{t-1}}$$\n\nand the **Average rate of return**:\n$$ \\bar r =\\frac{1}{n} \\sum\\limits_{n}^{t=1}r_t$$","metadata":{"papermill":{"duration":0.025435,"end_time":"2022-01-10T19:13:16.573173","exception":false,"start_time":"2022-01-10T19:13:16.547738","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def realized(close, N=240):\n    rt = list(log(C_t / C_t_1) for C_t, C_t_1 in zip(close[1:], close[:-1]))\n    rt_mean = sum(rt) / len(rt)\n    return sqrt(sum((r_i - rt_mean) ** 2 for r_i in rt) * N / (len(rt) - 1))\n\nfeat = df['Close'].rolling(60).apply(realized).bfill()\nplot_feature(feat, df, 'realized volatility')","metadata":{"papermill":{"duration":77.592831,"end_time":"2022-01-10T19:14:34.191713","exception":false,"start_time":"2022-01-10T19:13:16.598882","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:55:58.009771Z","iopub.execute_input":"2022-04-23T10:55:58.010098Z","iopub.status.idle":"2022-04-23T10:55:58.479063Z","shell.execute_reply.started":"2022-04-23T10:55:58.010058Z","shell.execute_reply":"2022-04-23T10:55:58.478224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"features\">Parkinson Volatility</span>\n<hr>\n\nParkinson volatility is a volatility measure that uses the stock’s high and low price of the day.\n\nThe main difference between regular volatility and Parkinson volatility is that the latter uses high and low prices for a day, rather than only the closing price. That is useful as close to close prices could show little difference while large price movements could have happened during the day. Thus Parkinson's volatility is considered to be more precise and requires less data for calculation than the close-close volatility.\n\nOne drawback of this estimator is that it doesn't take into account price movements after market close. Hence it systematically undervalues volatility. \n\n[source](https://portfolioslab.com/tools/parkinson)\n\n<br>\n\n\nDefined as: \n\n$$\\sigma_{parkinson} = \\sqrt{ {\\frac{1}{4*\\ln{2}} * \\frac{252}{n} * \\sum\\limits_{t=1}^{n} { \\ln{(\\frac{H_t}{L_t})}}^2}}$$\n","metadata":{"papermill":{"duration":0.028493,"end_time":"2022-01-10T19:14:34.249081","exception":false,"start_time":"2022-01-10T19:14:34.220588","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def parkinson(high, low, N=240):\n    sum_hl = sum(log(H_t / L_t) ** 2 for H_t, L_t in zip(high, low))\n    return sqrt(sum_hl * N / (4 * len(high) *log(2)))\n\nfeat = df.rolling(60).apply(lambda x: parkinson(df.loc[x.index, 'High'], df.loc[x.index, 'Low'])).bfill()\nplot_feature(feat, df, 'parkinson volatility')","metadata":{"papermill":{"duration":2269.423424,"end_time":"2022-01-10T19:52:23.701405","exception":false,"start_time":"2022-01-10T19:14:34.277981","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:55:58.480268Z","iopub.execute_input":"2022-04-23T10:55:58.480479Z","iopub.status.idle":"2022-04-23T10:56:04.696575Z","shell.execute_reply.started":"2022-04-23T10:55:58.480454Z","shell.execute_reply":"2022-04-23T10:56:04.695957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"features\">Garman-Klass Volatility</span>\n<hr>\n\nGarman Klass is a volatility estimator that incorporates open, low, high, and close prices of a security.\n\nGarman-Klass volatility extends Parkinson's volatility by taking into account the opening and closing price. As markets are most active during the opening and closing of a trading session, it makes volatility estimation more accurate.\n\nGarman and Klass also assumed that the process of price change is a process of continuous diffusion (geometric Brownian motion). However, this assumption has several drawbacks. The method is not robust for opening jumps in price and trend movements.\n\nDespite its drawbacks, the Garman-Klass estimator is still more effective than the basic formula since it takes into account not only the price at the beginning and end of the time interval but also intraday price extremums.\n\n[source](https://portfolioslab.com/tools/garman-klass)\n\n\n<br>\n\nDefined as:\n\n$$\\sigma_{garman-klass} = \\sqrt{\\frac{N}{n} \\sum\\limits_{i=1}^{N} \\lbrack {\\frac{1}{2} * (\\log{ \\frac{H_i}{L_i}})^2 -(2*\\log2 -1) * (\\log\\frac{C_i}{O_i})^2\\rbrack}}$$","metadata":{"papermill":{"duration":0.031709,"end_time":"2022-01-10T19:52:23.764361","exception":false,"start_time":"2022-01-10T19:52:23.732652","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def garman_klass(open, high, low, close, N=240):\n    sum_hl = sum(log(H_t / L_t) ** 2 for H_t, L_t in zip(high, low)) / 2\n    sum_co = sum(log(C_t / O_t) ** 2 for C_t, O_t in zip(close, open)) * (2 * log(2) - 1)\n    return sqrt((sum_hl - sum_co) * N / len(close))\n\nfeat = df.rolling(60).apply(lambda x: garman_klass(df.loc[x.index, 'Open'], df.loc[x.index, 'High'], df.loc[x.index, 'Low'], df.loc[x.index, 'Close'])).bfill()\nplot_feature(feat, df, 'garman klass')","metadata":{"papermill":{"duration":4297.854921,"end_time":"2022-01-10T21:04:01.650814","exception":false,"start_time":"2022-01-10T19:52:23.795893","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:56:04.697945Z","iopub.execute_input":"2022-04-23T10:56:04.698487Z","iopub.status.idle":"2022-04-23T10:56:16.108407Z","shell.execute_reply.started":"2022-04-23T10:56:04.69844Z","shell.execute_reply":"2022-04-23T10:56:16.107474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"features\">Roger-Satchell Volatility</span>\n<hr>\n\nRogers-Satchell is an estimator for measuring the volatility of securities with an average return not equal to zero.\n\nUnlike Parkinson and Garman-Klass estimators, Rogers-Satchell incorporates drift term (mean return not equal to zero). As a result, it provides a better volatility estimation when the underlying is trending.\n\nThe main disadvantage of this method is that it does not take into account price movements between trading sessions. It means an underestimation of volatility since price jumps periodically occur in the market precisely at the moments between sessions.\n\n\n[source](https://portfolioslab.com/tools/rogers-satchell)\n\n<br>\n\n\n\n$$\\sigma_{roger-satchel} = \\sqrt{ \\frac{N}{n} \\sum\\limits_{i=1}^{n} \\lbrack \\log \\frac{H_i}{L_i} * \\log \\ frac{H_i}{O_i} + \\log \\frac{HL_i}{L_i} * \\log \\frac{L_i}{O_i} \\rbrack }$$","metadata":{"papermill":{"duration":0.038277,"end_time":"2022-01-10T21:04:01.724018","exception":false,"start_time":"2022-01-10T21:04:01.685741","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def roger_satchell(open, high, low, close, N=240):\n    sum_ohlc = sum(log(H_t / C_t) * log(H_t / O_t) + log(L_t / C_t) * log(L_t / O_t) for O_t, H_t, L_t, C_t in zip(open, high, low, close))\n    return sqrt(sum_ohlc * N / len(close))\n\nfeat = df.rolling(60).apply(lambda x: roger_satchell(df.loc[x.index, 'Open'], df.loc[x.index, 'High'], df.loc[x.index, 'Low'], df.loc[x.index, 'Close'])).bfill()\nplot_feature(feat, df, 'roger satchell')","metadata":{"papermill":{"duration":4327.079485,"end_time":"2022-01-10T22:16:08.838882","exception":false,"start_time":"2022-01-10T21:04:01.759397","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:56:16.109739Z","iopub.execute_input":"2022-04-23T10:56:16.110045Z","iopub.status.idle":"2022-04-23T10:56:27.579445Z","shell.execute_reply.started":"2022-04-23T10:56:16.109999Z","shell.execute_reply":"2022-04-23T10:56:27.578656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"features\">Yang-Zhang Volatility</span>\n<hr>\n\nYang Zhang is a historical volatility estimator that handles both opening jumps and the drift and has a minimum estimation error.\n\nWe can think of the Yang-Zhang volatility as the combination of the overnight (close-to-open volatility) and a weighted average of the Rogers-Satchell volatility and the day’s open-to-close volatility. It considered being 14 times more efficient than the close-to-close estimator.\n\n[source](https://portfolioslab.com/tools/yang-zhang)\n\n<br>\n\nDefined as: \n\n$$\\sigma_{yang-zhang} = \\sqrt {\\sigma_o^2 + k * \\sigma_c^2 + (1-k) * \\sigma_{rs}^2}$$\n\n$\\mu_o = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\log \\frac {O_i}{C_{i-1}}$\n\n$\\sigma_o^2 = \\frac{N}{n-1} \\sum\\limits_{i=1}^{n} (\\log \\frac {O_i}{C_{i-1}}-\\mu_o)^ 2$, Open-Close Volatility or Overnight Volatility\n\n$\\mu_c = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\log \\frac {C_i}{O_i}$, Close-Open Volatility\n\n$\\sigma_c^2 = \\frac{N}{n-1} \\sum\\limits_{i=1}^{n} (\\log \\frac {C_i}{O_i}-\\mu_c)^2$\n\n$\\sigma_{rs}^2 = \\sigma_{roger-satchel}^2$\n\n$k^* = \\frac {\\alpha} {1+ \\alpha + \\frac{n+1}{n-1}}, \\alpha$ \n\ncan also be interpreted as a weighted average of the Roger-Satchell estimator, the Close-Open Volatility and the Open-Close Volatility.","metadata":{"papermill":{"duration":0.038957,"end_time":"2022-01-10T22:16:08.915949","exception":false,"start_time":"2022-01-10T22:16:08.876992","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def yang_zhang(open, high, low, close, N=240):\n    oc = list(log(O_t / C_t_1) for O_t, C_t_1 in zip(open[1:], close[:-1]))\n    n = len(oc)\n    oc_mean = sum(oc) / n\n    oc_var = sum((oc_i - oc_mean) ** 2 for oc_i in oc) * N / (n - 1)   \n    co = list(log(C_t / O_t) for O_t, C_t in zip(open[1:], close[1:]))\n    co_mean = sum(co) / n\n    co_var = sum((co_i - co_mean) ** 2 for co_i in co) * N / (n - 1)    \n    rs_var = (roger_satchell(open[1:], high[1:], low[1:], close[1:])) ** 2    \n    k = 0.34 / (1.34 + (n +1) / (n - 1))    \n    return sqrt(oc_var + k * co_var + (1-k) * rs_var)\n\nfeat = df.rolling(60).apply(lambda x: yang_zhang(df.loc[x.index, 'Open'], df.loc[x.index, 'High'], df.loc[x.index, 'Low'], df.loc[x.index, 'Close'])).bfill()\nplot_feature(feat, df, 'yang zhang')","metadata":{"papermill":{"duration":6354.905137,"end_time":"2022-01-11T00:02:03.858793","exception":false,"start_time":"2022-01-10T22:16:08.953656","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:56:27.580521Z","iopub.execute_input":"2022-04-23T10:56:27.580744Z","iopub.status.idle":"2022-04-23T10:56:44.657232Z","shell.execute_reply.started":"2022-04-23T10:56:27.58071Z","shell.execute_reply":"2022-04-23T10:56:44.65635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"features\">Garman-Klass-Yang-Zhang Volatility: OHLC Volatility</span>\n<hr>\n\nA modified version of Garman-Klass estimator that allows for opening spikes.\n\n$$\\sigma_{garkla-yangzh} = \\sqrt {\\frac{N}{n} \\sum\\limits_{i=1}^{n} \\lbrack (\\log \\frac{O_i}{C_{i-1 }})^2 + {\\frac{1}{2} * (\\log{\\frac{H_i}{L_i}})^2 -(2*\\log2 -1) * (\\log\\frac{C_i} {O_i})^2}\\rbrack }$$\n\nIn the cases where the return on assets is not zero, the volatility will be overestimated.","metadata":{"papermill":{"duration":0.040429,"end_time":"2022-01-11T00:02:03.939756","exception":false,"start_time":"2022-01-11T00:02:03.899327","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def garkla_yangzh(open, high, low, close, N=240):\n    sum_oc_1 = sum(log(O_t / C_t_1) ** 2 for O_t, C_t_1 in zip(open[1:], close[:-1]))\n    sum_hl = sum(log(H_t / L_t) ** 2 for H_t, L_t in zip(high[1:], low[1:])) / 2\n    sum_co = sum(log(C_t / O_t) ** 2 for C_t, O_t in zip(close[1:], open[1:])) * (2 * log(2) - 1)\n    return sqrt((sum_oc_1 + sum_hl - sum_co) * N / (len(close) - 1))\n\nfeat = df.rolling(60).apply(lambda x: garkla_yangzh(df.loc[x.index, 'Open'], df.loc[x.index, 'High'], df.loc[x.index, 'Low'], df.loc[x.index, 'Close'])).bfill()\nplot_feature(feat, df, 'garkla yangzh')","metadata":{"papermill":{"duration":5691.86572,"end_time":"2022-01-11T01:36:55.846129","exception":false,"start_time":"2022-01-11T00:02:03.980409","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:56:44.658733Z","iopub.execute_input":"2022-04-23T10:56:44.659028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# More to come..","metadata":{"papermill":{"duration":0.044447,"end_time":"2022-01-11T01:36:55.93442","exception":false,"start_time":"2022-01-11T01:36:55.889973","status":"completed"},"tags":[],"pycharm":{"name":"#%% md\n"}}}]}