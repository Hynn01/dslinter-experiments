{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fast AWP with small overhead\n\nThe *Adversarial Weight Perturbation (AWP)* was used in the [first-place solution of the Feedback Prize](https://www.kaggle.com/competitions/feedback-prize-2021/discussion/313177) (@wht1996) and it is showen to be  [effective in this competition as well](https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/discussion/315707) (@hengck23). The perturbation, however, requires two gradient computations per optimization step, and therefore takes twice more computation time (correct me if I am wrong). This implementation removes the overhead by approximating the gradient with the running mean stored in the Adam optimizer. \n\nSince the gradient given to the optimizer is perturbed by\nAWP, the AWP in this notebook uses \"wrong\" gradients, and may not work as intended. Still, the factor of two in time is large and seems to have some good effects; maybe AWP works with inaccurate gradients. Let me know in the comment if you see this AWP underperforms compared to the original. My PyTorch coding is pretty random and comments are welcome on that aspect, too, e.g., detach and copy are not appropriate. Maybe my code doing something completely random.\n\nThe removal of additional gradient calculation also allows gradient accumulation in the usual way. The overhead in computation time is little, but overhead in GPU RAM exists for a copy of model weights (0.5 - 1 GB).\n\n\n## Parameters\n\nTwo parameters control the amount of perturbations in units of fractional changes in the weights; 0.01 means 1% perturbation in weights (model parameters).\n\n```\nadv_lr or γ: fractional change in weight along the direction of gradient\nadv_eps or ε: the change in weight is limited to this fraction\n```\n\nTwo parameters are similar to learning rate and max norm in gradient clipping, but in units of fraction of weights.\n\nThe AWP gradients are evaluated at perturbed location:\n\n$$ w \\mapsto w + \\delta w = w + \\gamma \\frac{\\nabla}{\\lVert{\\nabla} \\rVert} \\lVert w \\rVert, $$\n\nbut change in each component is limitted to,\n\n$$ |\\delta w_i| \\le \\epsilon |w_i|. $$\n\nwhere `∇ = param.grad` is the gradient for the weight `w = param.data`.\n\n\nYou only need to see `class AWP` and use it in your training loop; you do not need to read other parts of my code.\n\n\n\n\n## Reference\n\nThis AWP modifies the 1st-place code in the Feedback Prize solution by @wht1996:\n\nhttps://www.kaggle.com/code/wht1996/feedback-nn-train \n\n\nThe original paper is,\n\nWu, Xia, and  Wang (2020), Adversarial Weight Perturbation Helps Robust Generalization\nhttps://arxiv.org/abs/2004.05884\n\nAuthors' implementation: https://github.com/csdongxian/AWP\n\nThe model and many codes for training are borrowed from the Nakama baseline >ω</ Thanks!:\n\nhttps://www.kaggle.com/code/yasufuminakama/nbme-deberta-base-baseline-train/notebook\n\nAnd many thanks to hengck23 for sharing the usefulness of AWP and the working parameters:\n\nhttps://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/discussion/315707","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pickle\nimport os\nimport ast\nimport time\nimport yaml\nimport random\nimport argparse\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import StratifiedGroupKFold\n\nimport transformers\nfrom transformers import AutoTokenizer, AutoConfig, AutoModel\nfrom transformers import get_cosine_schedule_with_warmup\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ntransformers.logging.set_verbosity_error()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    \ndevice = torch.device('cuda')","metadata":{"execution":{"iopub.status.busy":"2022-04-06T00:30:21.495651Z","iopub.execute_input":"2022-04-06T00:30:21.49634Z","iopub.status.idle":"2022-04-06T00:30:21.507508Z","shell.execute_reply.started":"2022-04-06T00:30:21.4963Z","shell.execute_reply":"2022-04-06T00:30:21.506793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Class AWP\n\nModified from https://www.kaggle.com/code/wht1996/feedback-nn-train (wht1996)","metadata":{}},{"cell_type":"code","source":"class AWP:\n    def __init__(self, model, optimizer, *, adv_param='weight',\n                 adv_lr=0.001, adv_eps=0.001):\n        self.model = model\n        self.optimizer = optimizer\n        self.adv_param = adv_param\n        self.adv_lr = adv_lr\n        self.adv_eps = adv_eps\n        self.backup = {}\n\n    def perturb(self, input_ids, attention_mask, y, criterion):\n        \"\"\"\n        Perturb model parameters for AWP gradient\n        Call before loss and loss.backward()\n        \"\"\"\n        self._save()  # save model parameters\n        self._attack_step()  # perturb weights\n\n    def _attack_step(self):\n        e = 1e-6\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and param.grad is not None and self.adv_param in name:\n                grad = self.optimizer.state[param]['exp_avg']\n                norm_grad = torch.norm(grad)\n                norm_data = torch.norm(param.detach())\n\n                if norm_grad != 0 and not torch.isnan(norm_grad):\n                    # Set lower and upper limit in change\n                    limit_eps = self.adv_eps * param.detach().abs()\n                    param_min = param.data - limit_eps\n                    param_max = param.data + limit_eps\n\n                    # Perturb along gradient\n                    # w += (adv_lr * |w| / |grad|) * grad\n                    param.data.add_(grad, alpha=(self.adv_lr * (norm_data + e) / (norm_grad + e)))\n\n                    # Apply the limit to the change\n                    param.data.clamp_(param_min, param_max)\n\n    def _save(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and param.grad is not None and self.adv_param in name:\n                if name not in self.backup:\n                    self.backup[name] = param.clone().detach()\n                else:\n                    self.backup[name].copy_(param.data)\n\n    def restore(self):\n        \"\"\"\n        Restore model parameter to correct position; AWP do not perturbe weights, it perturb gradients\n        Call after loss.backward(), before optimizer.step()\n        \"\"\"\n        for name, param in self.model.named_parameters():\n            if name in self.backup:\n                param.data.copy_(self.backup[name])","metadata":{"execution":{"iopub.status.busy":"2022-04-06T00:30:21.520855Z","iopub.execute_input":"2022-04-06T00:30:21.521324Z","iopub.status.idle":"2022-04-06T00:30:21.534938Z","shell.execute_reply.started":"2022-04-06T00:30:21.521294Z","shell.execute_reply":"2022-04-06T00:30:21.534183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Changes in AWP","metadata":{}},{"cell_type":"markdown","source":"- grad is using Adam exponentially averaged gradient;\n  * this saves computing gradient twice and **speeds up by factor of 2**;\n  * but this grad is pertubed by AWP and not the true gradient.\n- adv_step is removed; since grads are not computed, there is no reason to do multiple steps.\n- since adv_step=1, weight eps ranges do not have to be saved,\n  * which reduce GPU RAM from 3 extra copies of wegiths to 1 copy.\n\n\n## Original code by wht1996\n\n```python\nclass AWP:\n    def __init__(\n        self,\n        model,\n        optimizer,\n        adv_param=\"weight\",\n        adv_lr=1,\n        adv_eps=0.2,\n        start_epoch=0,\n        adv_step=1,\n        scaler=None\n    ):\n        self.model = model\n        self.optimizer = optimizer\n        self.adv_param = adv_param\n        self.adv_lr = adv_lr\n        self.adv_eps = adv_eps\n        self.start_epoch = start_epoch\n        self.adv_step = adv_step\n        self.backup = {}\n        self.backup_eps = {}\n        self.scaler = scaler\n  \n    def attack_backward(self, x, y, attention_mask,epoch):\n        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n            return None\n\n        self._save() \n        for i in range(self.adv_step):\n            self._attack_step() \n            with torch.cuda.amp.autocast():\n                adv_loss, tr_logits = self.model(input_ids=x, attention_mask=attention_mask, labels=y)\n                adv_loss = adv_loss.mean()\n            self.optimizer.zero_grad()\n            self.scaler.scale(adv_loss).backward()\n            \n        self._restore()\n\n    def _attack_step(self):\n        e = 1e-6\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and param.grad is not None and self.adv_param in name:\n                norm1 = torch.norm(param.grad)\n                norm2 = torch.norm(param.data.detach())\n                if norm1 != 0 and not torch.isnan(norm1):\n                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n                    param.data.add_(r_at)\n                    param.data = torch.min(\n                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n                    )\n                # param.data.clamp_(*self.backup_eps[name])\n\n    def _save(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and param.grad is not None and self.adv_param in name:\n                if name not in self.backup:\n                    self.backup[name] = param.data.clone()\n                    grad_eps = self.adv_eps * param.abs().detach()\n                    self.backup_eps[name] = (\n                        self.backup[name] - grad_eps,\n                        self.backup[name] + grad_eps,\n                    )\n\n    def _restore(self,):\n        for name, param in self.model.named_parameters():\n            if name in self.backup:\n                param.data = self.backup[name]\n        self.backup = {}\n        self.backup_eps = {}\n```","metadata":{}},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"def nakama_fix_annotations(train):\n    # Fix incorrect annotations (from Nakama baseline)\n    train.loc[338, 'annotation'] = \"['father heart attack']\"\n    train.loc[338, 'location'] = \"['764 783']\"\n\n    train.loc[621, 'annotation'] = \"['for the last 2-3 months']\"\n    train.loc[621, 'location'] = \"['77 100']\"\n\n    train.loc[655, 'annotation'] = \"['no heat intolerance'], ['no cold intolerance']\"\n    train.loc[655, 'location'] = \"['285 292;301 312', '285 287;296 312']\"\n\n    train.loc[1262, 'annotation'] = \"['mother thyroid problem']\"\n    train.loc[1262, 'location'] = \"['551 557;565 580']\"\n\n    train.loc[1265, 'annotation'] = \"[\\\"felt like he was going to 'pass out'\\\"]\"\n    train.loc[1265, 'location'] = \"['131 135;181 212']\"\n\n    train.loc[1396, 'annotation'] = \"['stool , with no blood']\"\n    train.loc[1396, 'location'] = \"['259 280']\"\n\n    train.loc[1591, 'annotation'] = \"['diarrhoe non blooody']\"\n    train.loc[1591, 'location'] = \"['176 184;201 212']\"\n\n    train.loc[1615, 'annotation'] = \"['diarrhea for last 2-3 days']\"\n    train.loc[1615, 'location'] = \"['249 257;271 288']\"\n\n    train.loc[1664, 'annotation'] = \"['no vaginal discharge']\"\n    train.loc[1664, 'location'] = \"['822 824;907 924']\"\n\n    train.loc[1714, 'annotation'] = \"['started about 8-10 hours ago']\"\n    train.loc[1714, 'location'] = \"['101 129']\"\n\n    train.loc[1929, 'annotation'] = \"['no blood in the stool']\"\n    train.loc[1929, 'location'] = \"['531 539;549 561']\"\n\n    train.loc[2134, 'annotation'] = \"['last sexually active 9 months ago']\"\n    train.loc[2134, 'location'] = \"['540 560;581 593']\"\n\n    train.loc[2191, 'annotation'] = \"['right lower quadrant pain']\"\n    train.loc[2191, 'location'] = \"['32 57']\"\n\n    train.loc[2553, 'annotation'] = \"['diarrhoea no blood']\"\n    train.loc[2553, 'location'] = \"['308 317;376 384']\"\n\n    train.loc[3124, 'annotation'] = \"['sweating']\"\n    train.loc[3124, 'location'] = \"['549 557']\"\n\n    train.loc[3858, 'annotation'] = \"['previously as regular', 'previously eveyr 28-29 days', 'previously lasting 5 days'], 'previously regular flow']\"\n    train.loc[3858, 'location'] = \"['102 123', '102 112;125 141', '102 112;143 157', '102 112;159 171']\"\n\n    train.loc[4373, 'annotation'] = \"['for 2 months']\"\n    train.loc[4373, 'location'] = \"['33 45']\"\n\n    train.loc[4763, 'annotation'] = \"['35 year old']\"\n    train.loc[4763, 'location'] = \"['5 16']\"\n\n    train.loc[4782, 'annotation'] = \"['darker brown stools']\"\n    train.loc[4782, 'location'] = \"['175 194']\"\n\n    train.loc[4908, 'annotation'] = \"['uncle with peptic ulcer']\"\n    train.loc[4908, 'location'] = \"['700 723']\"\n\n    train.loc[6016, 'annotation'] = \"['difficulty falling asleep']\"\n    train.loc[6016, 'location'] = \"['225 250']\"\n\n    train.loc[6192, 'annotation'] = \"['helps to take care of aging mother and in-laws']\"\n    train.loc[6192, 'location'] = \"['197 218;236 260']\"\n\n    train.loc[6380, 'annotation'] = \"['No hair changes', 'No skin changes', 'No GI changes', 'No palpitations', 'No excessive sweating']\"\n    train.loc[6380, 'location'] = \"['480 482;507 519', '480 482;499 503;512 519', '480 482;521 531', '480 482;533 545', '480 482;564 582']\"\n\n    train.loc[6562, 'annotation'] = \"['stressed due to taking care of her mother', 'stressed due to taking care of husbands parents']\"\n    train.loc[6562, 'location'] = \"['290 320;327 337', '290 320;342 358']\"\n\n    train.loc[6862, 'annotation'] = \"['stressor taking care of many sick family members']\"\n    train.loc[6862, 'location'] = \"['288 296;324 363']\"\n\n    train.loc[7022, 'annotation'] = \"['heart started racing and felt numbness for the 1st time in her finger tips']\"\n    train.loc[7022, 'location'] = \"['108 182']\"\n\n    train.loc[7422, 'annotation'] = \"['first started 5 yrs']\"\n    train.loc[7422, 'location'] = \"['102 121']\"\n\n    train.loc[8876, 'annotation'] = \"['No shortness of breath']\"\n    train.loc[8876, 'location'] = \"['481 483;533 552']\"\n\n    train.loc[9027, 'annotation'] = \"['recent URI', 'nasal stuffines, rhinorrhea, for 3-4 days']\"\n    train.loc[9027, 'location'] = \"['92 102', '123 164']\"\n\n    train.loc[9938, 'annotation'] = \"['irregularity with her cycles', 'heavier bleeding', 'changes her pad every couple hours']\"\n    train.loc[9938, 'location'] = \"['89 117', '122 138', '368 402']\"\n\n    train.loc[9973, 'annotation'] = \"['gaining 10-15 lbs']\"\n    train.loc[9973, 'location'] = \"['344 361']\"\n\n    train.loc[10513, 'annotation'] = \"['weight gain', 'gain of 10-16lbs']\"\n    train.loc[10513, 'location'] = \"['600 611', '607 623']\"\n\n    train.loc[11551, 'annotation'] = \"['seeing her son knows are not real']\"\n    train.loc[11551, 'location'] = \"['386 400;443 461']\"\n\n    train.loc[11677, 'annotation'] = \"['saw him once in the kitchen after he died']\"\n    train.loc[11677, 'location'] = \"['160 201']\"\n\n    train.loc[12124, 'annotation'] = \"['tried Ambien but it didnt work']\"\n    train.loc[12124, 'location'] = \"['325 337;349 366']\"\n\n    train.loc[12279, 'annotation'] = \"['heard what she described as a party later than evening these things did not actually happen']\"\n    train.loc[12279, 'location'] = \"['405 459;488 524']\"\n\n    train.loc[12289, 'annotation'] = \"['experienced seeing her son at the kitchen table these things did not actually happen']\"\n    train.loc[12289, 'location'] = \"['353 400;488 524']\"\n\n    train.loc[13238, 'annotation'] = \"['SCRACHY THROAT', 'RUNNY NOSE']\"\n    train.loc[13238, 'location'] = \"['293 307', '321 331']\"\n\n    train.loc[13297, 'annotation'] = \"['without improvement when taking tylenol', 'without improvement when taking ibuprofen']\"\n    train.loc[13297, 'location'] = \"['182 221', '182 213;225 234']\"\n\n    train.loc[13299, 'annotation'] = \"['yesterday', 'yesterday']\"\n    train.loc[13299, 'location'] = \"['79 88', '409 418']\"\n\n    train.loc[13845, 'annotation'] = \"['headache global', 'headache throughout her head']\"\n    train.loc[13845, 'location'] = \"['86 94;230 236', '86 94;237 256']\"\n\n    train.loc[14083, 'annotation'] = \"['headache generalized in her head']\"\n    train.loc[14083, 'location'] = \"['56 64;156 179']\"\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-06T00:30:21.614212Z","iopub.execute_input":"2022-04-06T00:30:21.614621Z","iopub.status.idle":"2022-04-06T00:30:21.642465Z","shell.execute_reply.started":"2022-04-06T00:30:21.614584Z","shell.execute_reply":"2022-04-06T00:30:21.641491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _replace_feature_text(text):\n    text = text.replace('I-year', '1-year')\n    text = text.replace('-OR-', ' or ')\n    text = text.replace('-', ' ')\n\n    return text\n\ndef get_segments(locations):\n    \"\"\"\n    Parse location list to sorted list of segments\n\n    Args:\n      location (str): ['85 99', '126 138', '126 131;143 151']\n\n    Returns: list[tuple]\n      List of (begin, end)\n    \"\"\"\n    segs = []\n\n    assert isinstance(locations, str)\n\n    locations = ast.literal_eval(locations)  # str -> list[str]\n    for loc in locations:\n        for span in loc.split(';'):\n            segs.append(tuple(map(int, span.split(' '))))\n\n    segs.sort(key=lambda pair: pair[0])\n\n    return segs\n\n\ndef create_label(segments, offset_mapping):\n    n = len(offset_mapping)\n    y = np.zeros(n, dtype=np.float32)\n\n    if not segments:\n        return y\n\n    iseg = 0\n    seg = segments[iseg]\n    nseg = len(segments)\n\n    k = 0\n    while k < n:\n        begin, end = offset_mapping[k]\n        if end <= seg[0]:  # token is left of seg\n            k += 1\n            continue\n        elif begin < seg[1]:  # token overlaps with seg\n            y[k] = 1\n            k += 1\n        else:  # begin passed seg: seg[1] <= begin\n            iseg += 1\n            if iseg < nseg:\n                seg = segments[iseg]\n            else:\n                break  # All segments processed\n\n    return y\n\n\ndef exclude_feature_label(input_ids, y, sep):\n    \"\"\"\n    Set y = -1 for feature_text; compute loss only with note texts excluding feature texts\n\n    Result:\n      y modified to -1 after first [SEP]\n    \"\"\"\n    n = len(input_ids)\n    assert n >= 2 and input_ids[-1] == sep\n    y[-1] = -1\n\n    for k in range(n - 2, -1, -1):\n        if input_ids[k] == sep:\n            break\n        else:\n            y[k] = -1\n\n\ndef create_character_indices(segments):\n    \"\"\"\n    Character indices\n\n    Args:\n      segments (list[tuple]): List of (begin, end)\n    \"\"\"\n    s = set()\n\n    for begin, end in segments:\n        s.update(range(begin, end))\n\n    return sorted(list(s))\n\n\ndef create_data(train, tokenizer, *, max_length=1024, pbar=False):\n    \"\"\"\n    Create input_ids and label array y\n\n    Args:\n      train (pd.DataFrame)\n      tokenizer (str or tokenizer): path to tokenizer dir if str\n\n    Returns: list[dict]\n      input_ids (np.array[int])\n      n (int): number of tokens\n      y (np.array[float32]): binary annotation\n    \"\"\"\n    sep = tokenizer.sep_token_id\n    nsep = 2\n\n    annotated = 'location' in train.columns\n\n    data = []\n    for i, r in tqdm(train.iterrows(), disable=(not pbar), total=len(train)):\n        text = r.pn_history\n        feature_text = r.feature_text\n\n        o = tokenizer(text, feature_text,\n                      add_special_tokens=True, max_length=max_length,\n                      truncation=True,\n                      return_offsets_mapping=True)\n\n        # Input ids\n        input_ids = o['input_ids']\n        n = len(input_ids)\n\n        input_ids = np.array(o['input_ids'], dtype=np.int32)\n        assert np.sum(input_ids == sep) == nsep  # Two tokens seperated by [SEP], <s/><s/> for roberta\n\n        # Attention mask\n        attention_mask = np.array(o['attention_mask'])\n        assert np.all(attention_mask == 1)\n\n        # Label\n        if annotated:\n            segs = get_segments(r['location'])\n            y = create_label(segs, o['offset_mapping'])\n\n            exclude_feature_label(input_ids, y, sep)\n\n            # Character label\n            label = create_character_indices(segs)\n        else:\n            y = None\n            label = None\n\n        d = {'id': r['id'],\n             'input_ids': input_ids,\n             'text': text,\n             'label': label,\n             'y': y,\n             'n': n,\n             'offset_mapping': o['offset_mapping']}\n        data.append(d)\n\n    return np.array(data)\n\n\nclass Dataset(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset(data)\n      data (np.array or list-like): input_ids and y\n    \"\"\"\n    def __init__(self, data, *, max_length=512):\n        self.data = data\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, i):\n        d = self.data[i]\n        n = min(d['n'], self.max_length)\n\n        input_ids = np.zeros(self.max_length, dtype=int)\n        input_ids[:n] = d['input_ids']\n\n        attention_mask = np.zeros(self.max_length, dtype=int)\n        attention_mask[:n] = 1\n\n        y = np.full(self.max_length, -1, dtype=np.float32)\n        y[:n] = d['y']\n\n        return {'input_ids': input_ids,\n                'attention_mask': attention_mask,\n                'y': y, 'n': n}","metadata":{"execution":{"iopub.status.busy":"2022-04-06T00:30:21.644873Z","iopub.execute_input":"2022-04-06T00:30:21.645562Z","iopub.status.idle":"2022-04-06T00:30:21.672632Z","shell.execute_reply.started":"2022-04-06T00:30:21.645515Z","shell.execute_reply":"2022-04-06T00:30:21.671882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, model_dir, *, dropout=0.2, pretrained=True):\n        super().__init__()\n\n        # Transformer\n        config = AutoConfig.from_pretrained(model_dir, add_pooling_layer=False)\n        if pretrained:\n            self.transformer = AutoModel.from_pretrained(model_dir, config=config)\n        else:\n            self.transformer = AutoModel.from_config(config)\n\n        self.fc_dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(config.hidden_size, 1)\n\n        self._init_weights(self.fc, config)\n\n    def _init_weights(self, module, config):\n        module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n        if module.bias is not None:\n            module.bias.data.zero_()\n\n    def forward(self, input_ids, attention_mask):\n        out = self.transformer(input_ids, attention_mask)\n        x = out['last_hidden_state']  # batch_size x max_length (512) x 768\n\n        x = self.fc_dropout(x)\n        x = self.fc(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-04-06T00:30:21.674377Z","iopub.execute_input":"2022-04-06T00:30:21.675047Z","iopub.status.idle":"2022-04-06T00:30:21.685973Z","shell.execute_reply.started":"2022-04-06T00:30:21.675004Z","shell.execute_reply":"2022-04-06T00:30:21.685251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and evaluate","metadata":{}},{"cell_type":"code","source":"def create_prediction(y_pred, d, *, th=0.5):\n    \"\"\"\n    Create character-level prediction\n\n    Args:\n      pred: pred['y_pred'] token-level prediction\n      d: d['offset_mapping']\n    \"\"\"\n    text = d['text']\n    offset_mapping = d['offset_mapping']\n\n    # Map token-level prob to character-level prob\n    y_prob = np.zeros(len(text))  # character-wise probabilities\n    i = 0\n    for p, (begin, end) in zip(y_pred, offset_mapping):\n        if i > 0 and begin == 0 and end == 0:\n            break  # This is end of patient note [sep]\n\n        y_prob[i:end] = p    # Set space before begin with p too (deberta style)\n        i = end\n\n    i_begin = i_last = None\n    li = []\n    for i, (x, p) in enumerate(zip(text, y_prob)):\n        if p >= th:\n            if i_begin is None and x != ' ':  # Do not include first space in span\n                i_begin = i_last = i\n                li.append(i)\n            elif i_begin is not None:         # Positive token is continuing\n                assert i_last + 1 == i\n                i_last = i\n                li.append(i)\n        else:\n            i_begin = i_last = None           # Negative\n\n    d = {'text': text,\n         'y_prob': y_prob,\n         'indices': li}\n\n    return d\n\n\ndef compute_score(preds, data):\n    \"\"\"\n    TP, FN, FP are collected globally and\n    f1 score is computed at the end\n    \"\"\"\n    assert len(preds) == len(data)\n\n    tp = 0\n    denom = 0\n    for pred, d in zip(preds, data):\n        c = create_prediction(pred['y_pred'], d)\n        x = set(d['label'])\n        y = set(c['indices'])\n        tp += len(x.intersection(y))\n        denom += len(x) + len(y)\n\n    return 2 * tp / denom\n\n\ndef evaluate(model, loader, criterion):\n    tb = time.time()\n    was_training = model.training\n    model.eval()\n\n    n_sum = 0\n    loss_sum = 0.0\n    preds = []\n    for d in loader:\n        input_ids = d['input_ids'].to(device)\n        attention_mask = d['attention_mask'].to(device)\n        y = d['y'].to(device)\n        n = y.size(0)\n\n        with torch.no_grad():\n            y_pred = model(input_ids, attention_mask)\n\n        loss = criterion(y_pred.view(-1), y.view(-1))\n        loss = torch.masked_select(loss, y.view(-1) != -1).mean()\n\n        n_sum += n\n        loss_sum += n * loss.item()\n\n        y_pred = y_pred.sigmoid().cpu().numpy()\n        y = d['y'].numpy()\n\n        for k, m in enumerate(d['n']):\n            preds.append({'y_pred': y_pred[k, :m].copy(),\n                          'y': y[k, :m].copy()})\n\n        del loss, y_pred, input_ids, attention_mask, y\n\n    model.train(was_training)\n\n    val = {'time': time.time() - tb,\n           'loss': loss_sum / n_sum,\n           'preds': preds}\n\n    return val\n\n\ndef get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_parameters = [\n        {'params': [p for n, p in model.transformer.named_parameters() if not any(nd in n for nd in no_decay)],\n         'lr': encoder_lr, 'weight_decay': weight_decay},\n        {'params': [p for n, p in model.transformer.named_parameters() if any(nd in n for nd in no_decay)],\n         'lr': encoder_lr, 'weight_decay': 0.0},\n        {'params': [p for n, p in model.named_parameters() if 'transformer' not in n],\n         'lr': decoder_lr, 'weight_decay': 0.0}\n    ]\n\n    return optimizer_parameters","metadata":{"execution":{"iopub.status.busy":"2022-04-06T00:30:21.688645Z","iopub.execute_input":"2022-04-06T00:30:21.689241Z","iopub.status.idle":"2022-04-06T00:30:21.720654Z","shell.execute_reply.started":"2022-04-06T00:30:21.6892Z","shell.execute_reply":"2022-04-06T00:30:21.719905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"debug = False\ntb_train = time.time()\n\n# Output directory\nname = 'deberta_base_awp'\nodir = name\n\nif not os.path.exists(odir):\n    os.mkdir(odir)\n\n# Data\ndi = '/kaggle/input/nbme-score-clinical-patient-notes/'\ntrain = pd.read_csv(di + 'train.csv')    \nfeatures = pd.read_csv(di + 'features.csv')\npatient_notes = pd.read_csv(di + 'patient_notes.csv')\n\nfeatures['feature_text'] = features['feature_text'].apply(_replace_feature_text)\n\n# Attach text `pn_history` to train annotations\ntrain = train.merge(features, on=['feature_num', 'case_num'], how='left')\ntrain = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n\nnakama_fix_annotations(train)\n\n# Tokenizer\ntransformer_path = 'microsoft/deberta-base'\ntransformer_name = transformer_path.split('/')[-1]   # deberta-base\n\ntokenizer = AutoTokenizer.from_pretrained(transformer_path)\ntokenizer.save_pretrained(odir + '/tokenizer')\n\nconfig = AutoConfig.from_pretrained(transformer_path, add_pooling_layer=False)\nconfig.save_pretrained(odir)\n\n# Tokenize data\ndata = create_data(train, tokenizer)\n\nseed_everything(42)\n\n# Kfold\nnfold = 5\nfolds = [0, 1, 2, 3, 4]\n\nkfold = StratifiedGroupKFold(nfold, shuffle=True, random_state=42)\ngroups = train['pn_num'].values\ncases = train['case_num'].values\n\n# Parameters\nepochs = 5\nbatch_size = 6\nbatch_size_val = 16\n\napex = True\nmax_grad_norm = 1000\noptimizer_batch_size = 6  # batch size for optimization step\ngradient_accumulation_steps = max(optimizer_batch_size // batch_size, 1)\nprint('gradient accumulation', gradient_accumulation_steps)\n\neval_per_epoch = 4\n\n# Training and evaluation\nfor ifold, (idx_train, idx_val) in enumerate(kfold.split(data, cases, groups=groups)):\n    if ifold not in folds:\n        continue\n\n    tb = time.time()\n    print('Fold %d' % ifold)\n    print('Epoch      Loss       lr  time')\n\n    # Train - validation split\n    data_train = data[idx_train]\n    data_val = data[idx_val]\n\n    if debug:\n        data_train = data_train[:64]\n        data_val = data_val[:16]\n        epochs = 2\n        folds = [0]\n\n    loader_train = DataLoader(Dataset(data_train),\n                              batch_size=batch_size, shuffle=True, drop_last=True)\n    loader_val = DataLoader(Dataset(data_val), batch_size=batch_size_val)\n\n    # Model\n    model = Model(transformer_path)\n    model.train()\n    model.to(device)\n\n    # Optimizer\n    lr = 2e-5\n    eps = 1e-6\n    betas = (0.9, 0.999)\n    weight_decay = 0.01\n    optimizer_parameters = get_optimizer_params(model,\n                                                encoder_lr=lr,\n                                                decoder_lr=lr,\n                                                weight_decay=weight_decay)\n\n    optimizer = AdamW(optimizer_parameters, lr=lr, eps=eps, betas=betas)\n\n    # One `step` is one optimizer step, `gradient_accumulation` mini batches\n    steps_per_epoch = len(loader_train) // gradient_accumulation_steps\n    eval_steps = [int(i * steps_per_epoch / eval_per_epoch) for i in range(1, eval_per_epoch)] + \\\n                 [steps_per_epoch]\n\n    # Scheduler\n    num_train_steps = steps_per_epoch * epochs\n    num_warmup_steps = 0\n    num_cycles = 0.5\n    scheduler = get_cosine_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=num_warmup_steps,\n                                                num_training_steps=num_train_steps,\n                                                num_cycles=num_cycles)\n\n    criterion = nn.BCEWithLogitsLoss(reduction='none')\n    best_score = 0\n    best_epoch = None\n\n    print('Enable AWP')\n    awp = AWP(model, optimizer, adv_lr=0.001, adv_eps=0.001)\n    awp_start = 1.0\n\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n\n        scaler = torch.cuda.amp.GradScaler(enabled=apex)\n\n        step = 0\n        n_sum = 0\n        loss_sum = 0\n        for ibatch, d in enumerate(loader_train):\n            input_ids = d['input_ids'].to(device)\n            attention_mask = d['attention_mask'].to(device)\n            y = d['y'].to(device)\n            n = y.size(0)\n\n            if epoch >= awp_start:\n                awp.perturb(input_ids, attention_mask, y, criterion)\n\n            with torch.cuda.amp.autocast(enabled=apex):\n                y_pred = model(input_ids, attention_mask)\n\n            loss = criterion(y_pred.view(-1), y.view(-1))\n            loss = torch.masked_select(loss, y.view(-1) != -1).mean()\n            loss_sum += n * loss.item()\n            n_sum += n\n\n            if gradient_accumulation_steps > 1:\n                loss = loss / gradient_accumulation_steps\n\n            scaler.scale(loss).backward()\n            awp.restore()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n\n            if (ibatch + 1) % gradient_accumulation_steps == 0:\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                if scheduler is not None:\n                    scheduler.step()\n                step += 1\n\n            del loss, y_pred, input_ids, attention_mask, y\n\n            ep = epoch + step / steps_per_epoch\n\n            # Validation\n            if step in eval_steps and (ibatch + 1) % gradient_accumulation_steps == 0:\n                val = evaluate(model, loader_val, criterion)\n                score = compute_score(val['preds'], data_val)\n\n                ep = epoch + step / steps_per_epoch\n                loss_train = loss_sum / n_sum\n                lr1 = optimizer.param_groups[0]['lr']\n                dt = (time.time() - tb) / 60\n\n                print('Epoch %.2f %.6f %.6f | %.4f %.2e %6.2f %.2f min' %\n                      (ep, loss_train, val['loss'], score,\n                       lr1, dt, val['time'] / 60))\n\n                n_sum = loss_sum = 0\n\n                if step == steps_per_epoch:\n                    break  # drop incomplete gradient accumulation\n\n        # Save model\n        if score >= best_score:\n            best_score = score\n            best_epoch = epoch + 1\n\n            model_filename = 'model%d_best.pytorch' % ifold\n            torch.save(model.state_dict(), model_filename)\n            #print(model_filename, 'written')\n\n    # Save final model\n    if best_epoch == epochs:\n        print('Last model is same as best')\n    #else:\n        #model_filename = '%s/model%d.pytorch' % (odir, ifold)\n        #model.to('cpu')\n        #model.eval()\n        #torch.save(model.state_dict(), model_filename)\n        #print(model_filename, 'written')\n\n    del model\n    del awp\n\ndt = (time.time() - tb_train) / 3600\nprint('Train done %.2f hr' % dt)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T00:30:21.722338Z","iopub.execute_input":"2022-04-06T00:30:21.722833Z","iopub.status.idle":"2022-04-06T00:31:04.385152Z","shell.execute_reply.started":"2022-04-06T00:30:21.722786Z","shell.execute_reply":"2022-04-06T00:31:04.384366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! ls\n! ls deberta_base_awp","metadata":{"execution":{"iopub.status.busy":"2022-04-06T00:31:04.395182Z","iopub.execute_input":"2022-04-06T00:31:04.397353Z","iopub.status.idle":"2022-04-06T00:31:05.836649Z","shell.execute_reply.started":"2022-04-06T00:31:04.397307Z","shell.execute_reply":"2022-04-06T00:31:05.835676Z"},"trusted":true},"execution_count":null,"outputs":[]}]}