{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Finding the Best Features for Predicting Fuel Consumption of vehicles**\n\n## **Table of Contents**\n\n  * [Data Set Information: Fuel Consumption Data Set](#Information)<br></br>\n  * [Load the Fuel Consumption dataset](#Dataset)<br></br>\n  * [Data Exploration](#EDA)<br></br>\n  * [Data Preprocessing](#Preprocessing)<br></br>\n  * [Building Linear Regression Model](#Building)<br></br>\n  * [Model Development](#Model)<br></br>\n  * [Model Enhancement](#Enhancement)\n    \n\n\n### **Import Libraries & Primary modules**","metadata":{"execution":{"iopub.status.busy":"2022-05-03T16:43:24.749492Z","iopub.execute_input":"2022-05-03T16:43:24.750524Z","iopub.status.idle":"2022-05-03T16:43:24.78187Z","shell.execute_reply.started":"2022-05-03T16:43:24.750383Z","shell.execute_reply":"2022-05-03T16:43:24.78021Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:51:46.022693Z","iopub.execute_input":"2022-05-07T05:51:46.02301Z","iopub.status.idle":"2022-05-07T05:51:47.370498Z","shell.execute_reply.started":"2022-05-07T05:51:46.022929Z","shell.execute_reply":"2022-05-07T05:51:47.369121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Dataset Information** <a name=\"DatasetInformation\"></a>\nDatasets provide model-specific fuel consumption ratings and estimated carbon dioxide emissions for new light-duty vehicles for retail sale in Canada.\n\n**Model:**\n\n* 4WD/4X4 = Four-wheel drive\n* AWD = All-wheel drive\n* FFV = Flexible-fuel vehicle\n* SWB = Short wheelbase\n* LWB = Long wheelbase\n* EWB = Extended wheelbase\n\n**Transmission:**\n* A = automatic\n* AM = automated manual\n* AS = automatic with select shift\n* AV = continuously variable\n* M = manual\n* 3 – 10 = Number of gears\n\n**Fuel type:**\n* X = regular gasoline\n* Z = premium gasoline\n* D = diesel\n* E = ethanol (E85)\n* N = natural gas\n\n**Fuel consumption:** City and highway fuel consumption ratings are shown in litres per 100 kilometres (L/100 km) - the combined rating (55% city, 45% hwy) is shown in L/100 km and in miles per imperial gallon (mpg)\n\n**CO2 emissions:** the tailpipe emissions of carbon dioxide (in grams per kilometre) for combined city and highway driving\n\n**CO2 rating:** the tailpipe emissions of carbon dioxide rated on a scale from 1 (worst) to 10 (best)\n\n**Smog rating:** the tailpipe emissions of smog-forming pollutants rated on a scale from 1 (worst) to 10 (best)","metadata":{}},{"cell_type":"markdown","source":"## **Load the Fuel Consumption dataset**<a name = \"Dataset\"></a>","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/2022-fuel-consumption-ratings/MY2022 Fuel Consumption Ratings.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:51:47.372214Z","iopub.execute_input":"2022-05-07T05:51:47.372416Z","iopub.status.idle":"2022-05-07T05:51:47.422979Z","shell.execute_reply.started":"2022-05-07T05:51:47.37239Z","shell.execute_reply":"2022-05-07T05:51:47.422138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data Exploration**<a name=\"EDA\"></a>","metadata":{}},{"cell_type":"code","source":"viz = df[['Cylinders','Engine Size(L)','CO2 Rating', 'Smog Rating','CO2 Emissions(g/km)','Fuel Consumption(Comb (L/100 km))']]\nviz.hist(color = 'Blue', figsize = (10, 10))\nplt.tight_layout();\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:51:47.424172Z","iopub.execute_input":"2022-05-07T05:51:47.424478Z","iopub.status.idle":"2022-05-07T05:51:48.239264Z","shell.execute_reply.started":"2022-05-07T05:51:47.42445Z","shell.execute_reply":"2022-05-07T05:51:48.238087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n#set font of all elements to size 15\nplt.rc('font', size=12)\nscatter_matrix(df[['Cylinders','Engine Size(L)','CO2 Rating', 'Smog Rating','CO2 Emissions(g/km)','Fuel Consumption(Comb (L/100 km))']], figsize=(20, 20))\nplt.tight_layout;\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:51:48.241431Z","iopub.execute_input":"2022-05-07T05:51:48.242516Z","iopub.status.idle":"2022-05-07T05:51:51.013798Z","shell.execute_reply.started":"2022-05-07T05:51:48.242471Z","shell.execute_reply":"2022-05-07T05:51:51.012806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.PairGrid(df[['Cylinders','Engine Size(L)','CO2 Rating', 'Smog Rating','CO2 Emissions(g/km)','Fuel Consumption(Comb (L/100 km))']], height=4, aspect=0.9)\ng.map_upper(sns.regplot,scatter_kws={\"color\": \"red\"}, line_kws={\"color\": \"blue\"})\ng.map_lower(sns.kdeplot, cmap=\"plasma\", shade=True, thresh=0.05)\ng.map_diag(plt.hist,color = 'cyan')\nplt.tight_layout;\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:51:51.015125Z","iopub.execute_input":"2022-05-07T05:51:51.016374Z","iopub.status.idle":"2022-05-07T05:52:19.663031Z","shell.execute_reply.started":"2022-05-07T05:51:51.016305Z","shell.execute_reply":"2022-05-07T05:52:19.662054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = df.drop(['Model Year'], axis = 1).corr()\ncorr_top = corr[abs(corr)>=.8]\nplt.figure(figsize=(12,8))\nsns.heatmap(corr_top, cmap=\"cividis_r\", annot = True);","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:19.664928Z","iopub.execute_input":"2022-05-07T05:52:19.665875Z","iopub.status.idle":"2022-05-07T05:52:20.190007Z","shell.execute_reply.started":"2022-05-07T05:52:19.665839Z","shell.execute_reply":"2022-05-07T05:52:20.188605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **P-value:**\n\nIn statistical analysis P-value is used to know the significance of the correlation estimate.The P-value is the probability value that the correlation between these two variables is statistically significant. Normally, we choose a significance level of 0.05, which means that we are 95% confident that the correlation between the variables is significant.\nBy convention, when the\n* p-value is $<$ 0.001: we say there is strong evidence that the correlation is significant.\n* p-value is $<$ 0.05: there is moderate evidence that the correlation is significant.\n* p-value is $<$ 0.1: there is weak evidence that the correlation is significant.\n* p-value is $>$ 0.1: there is no evidence that the correlation is significant.\n\nThis information is obtained using \"stats\" module in the \"scipy\" library.","metadata":{}},{"cell_type":"code","source":"from scipy import stats\npearson_coef, p_value = stats.pearsonr(df['CO2 Emissions(g/km)'], df['Fuel Consumption(Comb (L/100 km))'])\nprint(\"The Pearson Correlation Coefficient of CO2 Emissions(g/km) vs Fuel Consumption(Comb (L/100 km)) is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['CO2 Rating'], df['Fuel Consumption(Comb (L/100 km))'])\nprint(\"The Pearson Correlation Coefficient of CO2 Rating vs Fuel Consumption(Comb (L/100 km)) is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['Smog Rating'], df['Fuel Consumption(Comb (L/100 km))'])\nprint(\"The Pearson Correlation Coefficient of Smog Rating vs Fuel Consumption(Comb (L/100 km)) is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['Cylinders'], df['Fuel Consumption(Comb (L/100 km))'])\nprint(\"The Pearson Correlation Coefficient of Cylinders vs  Combined City & Highway Fuel Consumption is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['Engine Size(L)'], df['Fuel Consumption(Comb (L/100 km))'])\nprint(\"The Pearson Correlation Coefficient Engine Size vs Combined City & Highway Fuel Consumption is\", pearson_coef, \" with a P-value of P =\", p_value)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:20.19178Z","iopub.execute_input":"2022-05-07T05:52:20.192011Z","iopub.status.idle":"2022-05-07T05:52:20.212675Z","shell.execute_reply.started":"2022-05-07T05:52:20.191975Z","shell.execute_reply":"2022-05-07T05:52:20.21085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **ANOVA: Analysis of Variance**\nThe Analysis of Variance (ANOVA) is a statistical method used to test whether there are significant differences between the means of two or more groups. ANOVA can be used to find the correlation between different groups of a categorical variable. \n\nANOVA returns two parameters:\n\n* F-test score: ANOVA assumes the means of all groups are the same, calculates how much the actual means deviate from the assumption, and reports it as the F-test score. The F-test calculates the ratio of variation between groups means over the variation within each of the sample group means.  A larger score means there is a larger difference between the means.\n* P-value: P-value tells how statistically significant is our calculated score value.\nIf the emission variable is strongly correlated with the variable we are analyzing, expect ANOVA to return a sizeable F-test score and a small p-value.\nUse the function 'f_oneway' in the module 'stats' to obtain the F-test score and P-value.","metadata":{}},{"cell_type":"markdown","source":"#### **Fuel Consumption vs No. of Cylinders**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\n#set font of all elements to size 15\nplt.rc('font', size=15) \nsns.boxplot(x=\"Cylinders\", y=\"Fuel Consumption(Comb (L/100 km))\", data=df,palette=\"Blues\", showmeans=True, \n            meanprops={\"marker\":\"o\", \"markerfacecolor\":\"Red\", \"markeredgecolor\":\"black\",\"markersize\":\"10\"})\nplt.title (\"Fuel Consumption vs No. of Cylinders\");","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:20.214877Z","iopub.execute_input":"2022-05-07T05:52:20.215183Z","iopub.status.idle":"2022-05-07T05:52:20.485271Z","shell.execute_reply.started":"2022-05-07T05:52:20.215146Z","shell.execute_reply":"2022-05-07T05:52:20.48406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ANOVA\n# Use the function 'f_oneway' in the module 'stats' to obtain the F-test score and P-value.\nfrom scipy import stats\nstats.f_oneway(df['Fuel Consumption(Comb (L/100 km))'][df['Cylinders'] == 3],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Cylinders'] == 4],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Cylinders'] == 5],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Cylinders'] == 6],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Cylinders'] == 8],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Cylinders'] == 10],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Cylinders'] == 12],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Cylinders'] == 16])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:20.487481Z","iopub.execute_input":"2022-05-07T05:52:20.487917Z","iopub.status.idle":"2022-05-07T05:52:20.502857Z","shell.execute_reply.started":"2022-05-07T05:52:20.487878Z","shell.execute_reply":"2022-05-07T05:52:20.501837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Fuel Consumption vs Vehicle Class**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\n#set font of all elements to size 15\nplt.rc('font', size=15) \nsns.boxplot(x=\"Vehicle Class\", y=\"Fuel Consumption(Comb (L/100 km))\", data=df,palette=\"Greens\", showmeans=True, \n            meanprops={\"marker\":\"o\", \"markerfacecolor\":\"Red\", \"markeredgecolor\":\"black\",\"markersize\":\"10\"})\nplt.xticks(rotation=90)\nplt.title (\"Fuel Consumption vs Vehicle Class\");","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:20.503993Z","iopub.execute_input":"2022-05-07T05:52:20.504162Z","iopub.status.idle":"2022-05-07T05:52:20.892473Z","shell.execute_reply.started":"2022-05-07T05:52:20.50414Z","shell.execute_reply":"2022-05-07T05:52:20.891239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sorted = df[['Vehicle Class', 'Fuel Consumption(Comb (L/100 km))']].groupby(\"Vehicle Class\").mean()\nsorted = sorted.sort_values('Fuel Consumption(Comb (L/100 km))')\nsorted_index = sorted.index\nplt.figure(figsize=(15,10))\n#set font of all elements to size 15\nplt.rc('font', size=15) \nplt.xticks(rotation=90)\nsns.boxplot(x=\"Vehicle Class\", y=\"Fuel Consumption(Comb (L/100 km))\", data=df,showmeans=True, order=sorted_index, \n            meanprops={\"marker\":\"o\", \"markerfacecolor\":\"Red\", \"markeredgecolor\":\"black\",\"markersize\":\"10\"})\nplt.title (\"Fuel Consumption vs Vehicle Class\");","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:20.894924Z","iopub.execute_input":"2022-05-07T05:52:20.895122Z","iopub.status.idle":"2022-05-07T05:52:21.438073Z","shell.execute_reply.started":"2022-05-07T05:52:20.895098Z","shell.execute_reply":"2022-05-07T05:52:21.437112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ANOVA\n# Use the function 'f_oneway' in the module 'stats' to obtain the F-test score and P-value.\nstats.f_oneway(df['Fuel Consumption(Comb (L/100 km))'][df['Vehicle Class'] == 'SUV: Small'],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Vehicle Class'] == 'SUV: Standard'],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Vehicle Class'] == 'Mid-size'],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Vehicle Class'] == 'Pickup truck: Standard'],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Vehicle Class'] == 'Subcompact'],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Vehicle Class'] == 'Compact'],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Vehicle Class'] == 'Full-size'],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Vehicle Class'] == 'Two-seater'],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Vehicle Class'] == 'Minicompact'],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Vehicle Class'] == 'Pickup truck: Small'],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Vehicle Class'] == 'Station wagon: Small'],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Vehicle Class'] == 'Special purpose vehicle'],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Vehicle Class'] == 'Station wagon: Mid-size'],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Vehicle Class'] == 'Minivan'])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:21.439416Z","iopub.execute_input":"2022-05-07T05:52:21.43991Z","iopub.status.idle":"2022-05-07T05:52:21.458178Z","shell.execute_reply.started":"2022-05-07T05:52:21.43964Z","shell.execute_reply":"2022-05-07T05:52:21.456721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Fuel Consumption vs Fuel Type**","metadata":{}},{"cell_type":"code","source":"sorted = df[['Fuel Type', 'Fuel Consumption(Comb (L/100 km))']].groupby(\"Fuel Type\").mean()\nsorted = sorted.sort_values('Fuel Consumption(Comb (L/100 km))')\nsorted_index = sorted.index\n\nplt.figure(figsize=(15,10))\n#set font of all elements to size 15\nplt.rc('font', size=15) \nplt.xticks(rotation=0, size = 18)\nsns.boxplot(x=\"Fuel Type\", y=\"Fuel Consumption(Comb (L/100 km))\", data=df,showmeans=True, palette = 'hsv', order=sorted_index, \n            meanprops={\"marker\":\"o\", \"markerfacecolor\":\"Red\", \"markeredgecolor\":\"black\",\"markersize\":\"10\"})\nplt.title (\"Fuel Consumption vs Fuel Type\");","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:21.460692Z","iopub.execute_input":"2022-05-07T05:52:21.460927Z","iopub.status.idle":"2022-05-07T05:52:21.700053Z","shell.execute_reply.started":"2022-05-07T05:52:21.460899Z","shell.execute_reply":"2022-05-07T05:52:21.698757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ANOVA\n# Use the function 'f_oneway' in the module 'stats' to obtain the F-test score and P-value.\nstats.f_oneway(df['Fuel Consumption(Comb (L/100 km))'][df['Fuel Type'] == 'X'],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Fuel Type'] == 'D'],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Fuel Type'] == 'Z'],\n               df['Fuel Consumption(Comb (L/100 km))'][df['Fuel Type'] == 'E'])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:21.701761Z","iopub.execute_input":"2022-05-07T05:52:21.70199Z","iopub.status.idle":"2022-05-07T05:52:21.714141Z","shell.execute_reply.started":"2022-05-07T05:52:21.701961Z","shell.execute_reply":"2022-05-07T05:52:21.712632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Fuel Consumption vs Transmission**","metadata":{}},{"cell_type":"code","source":"sorted = df[['Transmission', 'Fuel Consumption(Comb (L/100 km))']].groupby(\"Transmission\").mean()\nsorted = sorted.sort_values('Fuel Consumption(Comb (L/100 km))')\nsorted_index = sorted.index\n\nplt.figure(figsize=(18,10))\n#set font of all elements to size 15\nplt.rc('font', size=15) \nplt.xticks(rotation=0, size = 16)\nsns.boxplot(x=\"Transmission\", y=\"Fuel Consumption(Comb (L/100 km))\", data=df,showmeans=True, palette = 'rainbow', order=sorted_index, \n            meanprops={\"marker\":\"o\", \"markerfacecolor\":\"Red\", \"markeredgecolor\":\"black\",\"markersize\":\"10\"})\nplt.title (\"Fuel Consumption vs Transmission\");","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:21.715704Z","iopub.execute_input":"2022-05-07T05:52:21.715947Z","iopub.status.idle":"2022-05-07T05:52:22.324256Z","shell.execute_reply.started":"2022-05-07T05:52:21.715918Z","shell.execute_reply":"2022-05-07T05:52:22.323028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Residual Plot**\n\nA good way to visualize the variance of the data is to use a residual plot. A good way to visualize the variance of the data is to use a residual plot. \n\n**Residuals**\nThe difference between the observed value (y) and the predicted value (Yhat) is called the residual (e). When we look at a regression plot, the residual is the distance from the data point to the fitted regression line.\n\nPay attention to the spread of the residuals when looking at a residual plot:\n- If the points in a residual plot are randomly spread out around the x-axis, then a linear model is appropriate for the data. Randomly spread out residuals means that the variance is constant, and thus the linear model is a good fit for this data.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=((6,5)))\n#set font of all elements to size 15\nplt.rc('font', size=15) \nsns.residplot(x = 'Engine Size(L)', y = 'Fuel Consumption(Comb (L/100 km))', data = df, color = 'Red')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:22.325758Z","iopub.execute_input":"2022-05-07T05:52:22.325993Z","iopub.status.idle":"2022-05-07T05:52:22.496358Z","shell.execute_reply.started":"2022-05-07T05:52:22.325964Z","shell.execute_reply":"2022-05-07T05:52:22.494684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=((6,5)))\nsns.residplot(x = 'Cylinders', y = 'Fuel Consumption(Comb (L/100 km))', data = df, color = 'Red')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:22.498384Z","iopub.execute_input":"2022-05-07T05:52:22.498729Z","iopub.status.idle":"2022-05-07T05:52:22.658035Z","shell.execute_reply.started":"2022-05-07T05:52:22.49869Z","shell.execute_reply":"2022-05-07T05:52:22.657431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=((6,5)))\n#set font of all elements to size 15\nplt.rc('font', size=15) \nsns.residplot(x = 'CO2 Emissions(g/km)', y = 'Fuel Consumption(Comb (L/100 km))', data = df, color = 'Red')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:22.65927Z","iopub.execute_input":"2022-05-07T05:52:22.660014Z","iopub.status.idle":"2022-05-07T05:52:22.836267Z","shell.execute_reply.started":"2022-05-07T05:52:22.659973Z","shell.execute_reply":"2022-05-07T05:52:22.83571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=((6,5)))\n#set font of all elements to size 15\nplt.rc('font', size=15) \nsns.residplot(x = 'CO2 Rating', y = 'Fuel Consumption(Comb (L/100 km))', data = df, color = 'Red')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:22.837335Z","iopub.execute_input":"2022-05-07T05:52:22.838514Z","iopub.status.idle":"2022-05-07T05:52:22.99638Z","shell.execute_reply.started":"2022-05-07T05:52:22.838467Z","shell.execute_reply":"2022-05-07T05:52:22.995064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=((6,5)))\n#set font of all elements to size 15\nplt.rc('font', size=15) \nsns.residplot(x = 'Smog Rating', y = 'Fuel Consumption(Comb (L/100 km))', data = df, color = 'Red')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:22.99883Z","iopub.execute_input":"2022-05-07T05:52:22.999131Z","iopub.status.idle":"2022-05-07T05:52:23.131621Z","shell.execute_reply.started":"2022-05-07T05:52:22.999101Z","shell.execute_reply":"2022-05-07T05:52:23.131211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Residual plots show that linear model is better than non linear models for this data. ","metadata":{}},{"cell_type":"markdown","source":"## **Data Preprocessing**<a name = 'Preprocessing'></a>","metadata":{}},{"cell_type":"markdown","source":"### **Label Encoding**\nSome features in this dataset are categorical such as Vehicle Class,Transmission, and Fuel Type. Unfortunately, Sklearn models do not handle categorical variables. To convert these features to numerical values, LabelEncoder from sklearn.preprocessing can be used to convert categorical variable into dummy/indicator variables.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['Vehicle Class'] = le.fit_transform(df['Vehicle Class'])\ndf['Transmission'] = le.fit_transform(df['Transmission'])\ndf['Fuel Type'] = le.fit_transform(df['Fuel Type'])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:23.132563Z","iopub.execute_input":"2022-05-07T05:52:23.132947Z","iopub.status.idle":"2022-05-07T05:52:23.328049Z","shell.execute_reply.started":"2022-05-07T05:52:23.132908Z","shell.execute_reply":"2022-05-07T05:52:23.326546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Building Linear Regression Model**<a name ='Building'></a>\nWhen a dependent variable is predicted with more than one independent variable, the method is termed as multiple linear regression. \n\nScikit-learn uses plain Ordinary Least Squares method to find parameters like the intercept and coefficients of hyperplane. \n\n**Ordinary Least Squares (OLS)**\n\nOLS is a method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by minimizing the sum of the squares of the differences between the target dependent variable and those predicted by the linear function. In other words, it tries to minimizes the sum of squared errors (SSE) or mean squared error (MSE) between the target variable (y) and the predicted output ( y^ ) over all samples in the dataset.\n\nOLS can find the best parameters using of the following methods: - Solving the model parameters analytically using closed-form equations - Using an optimization algorithm (Gradient Descent, Stochastic Gradient Descent, Newton’s Method, etc.)\n\nA final linear model with the structure should be obtained:\n$$\n\\hat{y}=a + bx_1 + cx_2 + dx_3\n$$","metadata":{}},{"cell_type":"markdown","source":"### **Cost Function & Gradient descent to find the best $theta$ values for a predictive model**\nThe multivariable form of the hypothesis function accommodating these multiple features is as follows:\n<br>\n$\nh_\\theta (x) = \\theta_0 + \\theta_1  x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\cdots + \\theta_n x_n\n$\n<br></br>\n$\nParameters : \\theta_0, \\theta_1,  \\theta_2,  \\theta_3,   \\cdots  \\theta_n \n$\n<br></br>\nThe accuracy of the hypothesis function can be measured by using a cost function. This takes an average difference of all the results of the hypothesis with inputs from x's and the actual output y's. The objective of linear regression is to minimize the cost function.  Cost function is represented by the following equation.\n<br>\n$\nJ(\\theta_0, \\theta_1) = \\frac {1}{2m}{\\sum_{i=1} ^{m} (\\hat{y_i} -y_i)^2} = \\frac {1}{2m}{\\sum_{i=1} ^{m} (h_\\theta (x_i) -y_i)^2}\n$\n<br></br>\nTo break it apart, it is the mean of the squares of $ h_\\theta (x_{i}) - y_{i} $, or the difference between the predicted value and the actual value. This function is otherwise called the \"Squared error function\", or \"Mean squared error\". The mean is halved $(\\frac{1}{2})$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $(\\frac{1}{2})$ term.\n<br></br>\nThe idea here is that the model get to choose the parameters theta 0, theta 1 so that h(x),meaning the value to be predicted on input x, is at least close to the values y for the examples in the training set. The parameters  $\\theta_0, \\theta_1,  \\theta_2,  \\theta_3,   \\cdots  \\theta_n$  are the values to be  adjusted to minimize cost function.\n<br></br>\nOne way to do this is to use the batch gradient descent algorithm. In batch gradient descent, each iteration performs the update. The goal is to minimize $J(\\theta_0, \\theta_1)$ by simultaneously updating each $\\theta $ values after each gradient descent iterations. \n<br></br>\n***To summarize:***\n<br>\n* **Hypothesis:**\n<ul>\n    $\nh_\\theta (x) = \\theta_0 + \\theta_1  x $\n\n</ul>\n\n* **Parameters:**\n<ul>\n    $\n\\theta_0 + \\theta_1\n$\n    \n</ul>\n\n* **Cost Function:** \n<ul>\n$\nJ(\\theta_0, \\theta_1) = \\frac {1}{2m}{\\sum_{i=1} ^{m} (\\hat{y_i} -y_i)^2} = \\frac {1}{2m}{\\sum_{i=1} ^{m} (h_\\theta (x_i) -y_i)^2}\n$\n\n</ul>\n  \n* **Goal:** \n<ul>\nminimize $ J(\\theta_0, \\theta_1)$\n</ul>\n","metadata":{}},{"cell_type":"markdown","source":"#### **Parameter Learning**\n\nGradient Descent: a general algorithm used all over the place in ML\n\n* Gradient $\\frac{d}{d\\theta_j} J(\\theta_j)$ represents that when θ is moved + or − a unit, how much J will + or − correspondingly\n* Since the goal is to decrease J, θ is updated  with partial gradient (partial unit of θ change) controlled by learning rate $(\\alpha)$.\n* Update all parameters at the same time (simultaneous updation) as given by the following equation , so that the cost function keeps same for all parameters in single round.\n<br>\n$\\theta_1:=\\theta_1-\\alpha \\frac{d}{d\\theta_1} J(\\theta_1)$<br></br>\n$\\theta_2:=\\theta_2-\\alpha \\frac{d}{d\\theta_2} J(\\theta_2)$\n<br>\n.\n<br>\n.\n<br>\n,\n<br>\n$\\theta_j:=\\theta_j-\\alpha \\frac{d}{d\\theta_j} J(\\theta_j)$  (simoultaneously update $\\theta_j$ for all values of j.\n\nWith each step of gradient descent, the parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost .\n\n\nRegardless of the slope's sign for $ (\\frac{d}{d\\theta_j} J(\\theta_j))$ eventually converges to its minimum value. When the slope is negative, the value of $\\theta_j$ increases and when it is positive, the value of $\\theta_j$ decreases.\n\n* If slope is +ve : θj = θj – (+ve value). Hence value of θj decreases.\n\n* If slope is -ve : θj = θj – (-ve value). Hence value of θj increases.\n\n**Gradient Descent of Linear Regression**\n\n* It is always a convex function without local optima\n\n* Batch Gradient Descent: consider all training examples when doing gradient descent\n\nParameter learning by gradient descent to minimize cost function on features generated using some random values are illustrated below:","metadata":{}},{"cell_type":"markdown","source":"### **Illustration of Gradient Descent of Simple Linear Regression using random values**","metadata":{}},{"cell_type":"code","source":"np.random.seed(15)\nx = np.arange(20)\n##You can adjust the slope and intercept to verify the changes in the graph\n#y = 5*(x) + 2\ny =  5*x + np.random.normal(0, 4, 20)\ny_noise = 2 * np.random.normal(size=x.size)\ny = y + y_noise\nplt.figure(figsize=(6,5))\nplt.plot(x, y, linestyle=\"\", marker=\"^\", markersize = 8, color=\"C3\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xlim(0, 20)\nplt.ylim(0, 110);","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:23.329739Z","iopub.execute_input":"2022-05-07T05:52:23.330038Z","iopub.status.idle":"2022-05-07T05:52:23.481474Z","shell.execute_reply.started":"2022-05-07T05:52:23.330001Z","shell.execute_reply":"2022-05-07T05:52:23.480532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_function(theta, x, y):\n    # h(x[i]) = theta * x[i]\n    return (1/2* len(x)) * sum([(theta* x[i] - y[i])**2 for i in range(len(x))])\n\nthetas = np.arange(0, 10, 0.5)\nlosses = loss_function(thetas, x, y)\nlosses","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:23.482578Z","iopub.execute_input":"2022-05-07T05:52:23.482806Z","iopub.status.idle":"2022-05-07T05:52:23.49199Z","shell.execute_reply.started":"2022-05-07T05:52:23.482778Z","shell.execute_reply":"2022-05-07T05:52:23.490927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(5,4), dpi = 200)\nplt.plot(thetas, losses, linestyle=\"-\", marker=\"x\", color=\"C4\")\nplt.plot(thetas[15], losses[15], linestyle=\"\", marker=\"o\", color=\"C0\", markersize = 10)\n\nslope = (losses[16]-losses[14])/(thetas[16]-thetas[14])\nxrange = np.linspace(-1, 1, 10)\nplt.plot(xrange+thetas[15], slope*(xrange)+losses[15], 'C3--', linewidth = 3)\nplt.xlabel(\"theta\")\nplt.ylabel(\"loss\")\nplt.yticks(rotation=0, size = 10)\nplt.xlim()\nplt.ylim();","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:23.493023Z","iopub.execute_input":"2022-05-07T05:52:23.493229Z","iopub.status.idle":"2022-05-07T05:52:23.716274Z","shell.execute_reply.started":"2022-05-07T05:52:23.493201Z","shell.execute_reply":"2022-05-07T05:52:23.715393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Illustration of Gradient Descent of Simple Linear Regression using Engine size feature and Fuel Consumption target**","metadata":{}},{"cell_type":"code","source":"X = df[['Engine Size(L)']].values\ny = df[['Fuel Consumption(Comb (L/100 km))']].values\nX[0:5]","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:23.717259Z","iopub.execute_input":"2022-05-07T05:52:23.717447Z","iopub.status.idle":"2022-05-07T05:52:23.730108Z","shell.execute_reply.started":"2022-05-07T05:52:23.717422Z","shell.execute_reply":"2022-05-07T05:52:23.728328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualization of Gradient descent**","metadata":{}},{"cell_type":"code","source":"thetas = np.arange(-15, 17, 0.5)\nlosses = loss_function(thetas, x, y)\nfig = plt.figure(figsize=(5,4), dpi = 200)\nplt.plot(thetas, losses, linestyle=\"-\", marker=\"x\", color=\"C4\")\nplt.plot(thetas[15], losses[15], linestyle=\"\", marker=\"o\", color=\"C0\", markersize = 10)\n\nslope = (losses[20]-losses[14])/(thetas[20]-thetas[14])\nxrange = np.linspace(-1, 1, 10)\nplt.plot(xrange+thetas[15], slope*(xrange)+losses[15], 'C3--', linewidth = 3)\nplt.xlabel(\"theta\")\nplt.ylabel(\"loss\")\nplt.xticks(rotation=0, size = 10)\nplt.yticks(rotation=0, size = 10)\nplt.xlim()\nplt.ylim();","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:23.735614Z","iopub.execute_input":"2022-05-07T05:52:23.735928Z","iopub.status.idle":"2022-05-07T05:52:23.940558Z","shell.execute_reply.started":"2022-05-07T05:52:23.735901Z","shell.execute_reply":"2022-05-07T05:52:23.939703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Parameter Learning**\n**Steps involved in Parameter Learning**","metadata":{}},{"cell_type":"markdown","source":"Adding another dimension to the data by adding 1 to the column 1, (ie, $x_{0}^{(i)}$=1) to accommodate the  intercept term $\\theta_0$ so that $\\theta_0 * x_{0}^{(i)}$ = $\\theta_0$.\n\nIn other words, for convenience reasons, just assume $x_{0}^{(i)} =1 \\text{ for } (i\\in { 1,\\dots, m } ).$ This makes matrix operations with theta and x easier. Hence making the two vectors $\\theta$ and $x^{(i)}$ match each other element-wise (that is, have the same number of elements: n+1).]","metadata":{}},{"cell_type":"code","source":"x = np.insert(X, 0, 1, axis=1)\ntheta = np.zeros((x.shape[1], 1))\nprint(x.shape)\nprint(theta.shape)\nx[0:5]","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:23.941875Z","iopub.execute_input":"2022-05-07T05:52:23.942113Z","iopub.status.idle":"2022-05-07T05:52:23.951862Z","shell.execute_reply.started":"2022-05-07T05:52:23.942082Z","shell.execute_reply":"2022-05-07T05:52:23.950724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The hypothesis $h_\\theta (x)$ is obtained by multiplying the 'x' matrix and the 'theta' vector. An m x n matrix of 'x' multiplied by an n x 1 vector of 'theta' results in an m x 1 $h_\\theta (x)$ vector, the size of which matches with the size of y. The difference between hypothesis $h_\\theta (x)$ vector and y vector gives the error vector. ","metadata":{}},{"cell_type":"code","source":"m = len(y)\nh = x.dot(theta)\nerror_vector = h - y\nerror_vector.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:23.953342Z","iopub.execute_input":"2022-05-07T05:52:23.953542Z","iopub.status.idle":"2022-05-07T05:52:23.970018Z","shell.execute_reply.started":"2022-05-07T05:52:23.953517Z","shell.execute_reply":"2022-05-07T05:52:23.968969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The \"gradient\" is obtained by multiplying X and the \"error_vector\", scaled by alpha (learning rate) and 1/m. Since X is (m x n), and the \"error_vector\" is (m x 1), and the result should be of the same size as theta (which is (n x 1), X needs to be transposed  before it can be multiplied by the \"error_vector\".\nAn n x m matrix of 'transpose of x' multiplied by an m x 1 vector of 'error' results in an n x 1 vector, that matches with the size of theta.","metadata":{}},{"cell_type":"code","source":"alpha = 0.0005\ngradient = alpha*(1 / m) * x.T.dot(error_vector) \ngradient.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:23.971466Z","iopub.execute_input":"2022-05-07T05:52:23.971775Z","iopub.status.idle":"2022-05-07T05:52:23.988203Z","shell.execute_reply.started":"2022-05-07T05:52:23.971734Z","shell.execute_reply":"2022-05-07T05:52:23.9872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Subtract this \"gradient\" obtained from the original value of theta. ","metadata":{}},{"cell_type":"code","source":"theta = theta - gradient\ntheta","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:23.989522Z","iopub.execute_input":"2022-05-07T05:52:23.989768Z","iopub.status.idle":"2022-05-07T05:52:24.007755Z","shell.execute_reply.started":"2022-05-07T05:52:23.989738Z","shell.execute_reply":"2022-05-07T05:52:24.007095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define gradientDescent function**\n\nCombine all of the above steps together to  define gradientDescent function","metadata":{}},{"cell_type":"code","source":"def gradientDescent(x, y, alpha, iters):\n    x = np.insert(X, 0, 1, axis=1)\n    theta = np.zeros((x.shape[1], 1))\n    for i in range(iters):\n        m = len(y)\n        error = x.dot(theta)-y \n        gradient = alpha*(1 / m) * x.T.dot(error) \n        theta = theta - gradient\n    return theta\niters = 150\nalpha = 0.0005\ntheta = gradientDescent(x, y,alpha, iters)\ntheta","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:24.008984Z","iopub.execute_input":"2022-05-07T05:52:24.009332Z","iopub.status.idle":"2022-05-07T05:52:24.029273Z","shell.execute_reply.started":"2022-05-07T05:52:24.0093Z","shell.execute_reply":"2022-05-07T05:52:24.027761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The final equation obtained with the parameters obtained above is: \n<br>\n$\ny = 0.55144973 + 1.85484014 * x_1\n$\n\n$ y = 0.55144973 + 1.85484014 * Engine Size(L) $","metadata":{}},{"cell_type":"markdown","source":"#### **Visualization of Gradient Descent in 3D plot of Simple Linear Regression using random values**","metadata":{}},{"cell_type":"code","source":"np.random.seed(10)\nx = np.arange(10)\ny =  6 + 2*x + np.random.normal(0, 5, 10)\ndef loss_function(theta1, theta2, x, y):\n    # h(x[i]) = theta1 + theta2 * x[i]\n    return (1/2* len(x)) * sum([(theta1 + theta2* x[i] - y[i])**2 for i in range(len(x))])\n\nthetas1 = np.arange(-30, 30, 0.1)\nthetas2 = np.arange(-5, 10, 0.1)\n\nfig = plt.figure(figsize=(7,6),dpi=200)\nax = plt.axes(projection='3d')\nX, Y = np.meshgrid(thetas1, thetas2)\nZ = loss_function(X, Y, x, y)\nax.contour3D(X, Y, Z, 200, cmap = 'hsv')\nax.set_xlabel('theta1')\nax.set_ylabel('theta2')\nax.set_zlabel('\\n' + 'loss', linespacing=4)\nplt.xlim(-30, 30)\nplt.ylim(-5, 10)\nax.set_zlim(-1, 120000)\nax.view_init(60, 20)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:24.030785Z","iopub.execute_input":"2022-05-07T05:52:24.031044Z","iopub.status.idle":"2022-05-07T05:52:25.129472Z","shell.execute_reply.started":"2022-05-07T05:52:24.031007Z","shell.execute_reply":"2022-05-07T05:52:25.128234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Visualization of Gradient descent of Simple Linear Regression of Fuel Consumption vs Engine Size in 3D plot**","metadata":{}},{"cell_type":"code","source":"x = df['Engine Size(L)'].values\ny = df['Fuel Consumption(Comb (L/100 km))'].values\nthetas1 = np.arange(-30, 50, 0.1)\nthetas2 = np.arange(-15, 10, 0.1)\n\nfig = plt.figure(figsize=(7,6),dpi=200)\nax = plt.axes(projection='3d')\nX, Y = np.meshgrid(thetas1, thetas2)\nZ = loss_function(X, Y, x, y)\nax.contour3D(X, Y, Z, 200, cmap = 'plasma')\nax.set_xlabel('theta1')\nax.set_ylabel('theta2')\nax.set_zlabel('\\n' + 'loss', linespacing=4)\nplt.xlim(-30, 50)\nplt.ylim(-15, 10)\nax.set_zlim()\nax.view_init(60, 10)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:25.130569Z","iopub.execute_input":"2022-05-07T05:52:25.132293Z","iopub.status.idle":"2022-05-07T05:52:27.452184Z","shell.execute_reply.started":"2022-05-07T05:52:25.132265Z","shell.execute_reply":"2022-05-07T05:52:27.451685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Cost function and Parameter Learning with Gradient Descent of Multiple Linear Regression using Engine size and Smog Rating as features and Fuel Consumption as target**","metadata":{}},{"cell_type":"code","source":"X = df[['Engine Size(L)', 'Smog Rating']].values\ny = df[['Fuel Consumption(Comb (L/100 km))']].values\nX1 = np.insert(X, 0, 1, axis=1)\nX1.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:27.45311Z","iopub.execute_input":"2022-05-07T05:52:27.453392Z","iopub.status.idle":"2022-05-07T05:52:27.462712Z","shell.execute_reply.started":"2022-05-07T05:52:27.453347Z","shell.execute_reply":"2022-05-07T05:52:27.461794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iters = 150\nalpha = 0.0005\ntheta = gradientDescent(X1, y,alpha, iters)\ntheta","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:27.463836Z","iopub.execute_input":"2022-05-07T05:52:27.464284Z","iopub.status.idle":"2022-05-07T05:52:27.481841Z","shell.execute_reply.started":"2022-05-07T05:52:27.46425Z","shell.execute_reply":"2022-05-07T05:52:27.480441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The final equation obtained with the parameters obtained above is: \n<br>\n$\ny = 0.30376001 + 1.17398293 * x_1 + 1.19879281 * x_2\n$\n\n$ y = 0.30376001 + 1.17398293 * Engine Size(L) + 1.19879281 * Smog Rating $","metadata":{}},{"cell_type":"markdown","source":"## **Model Development: Multiple Linear Regression**<a name=\"Model\"></a>\nIn this project, FUELCONSUMPTION_COMB is predicted  using 'Vehicle Class', 'Engine Size(L)', 'Cylinders', 'Transmission', 'Fuel Type','CO2 Emissions(g/km)', 'CO2 Rating', 'Smog Rating' of vehicles. ","metadata":{}},{"cell_type":"code","source":"X = df[['Vehicle Class', 'Engine Size(L)', 'Cylinders', 'Transmission', 'Fuel Type','CO2 Emissions(g/km)', 'CO2 Rating', 'Smog Rating']]\ny = df[['Fuel Consumption(Comb (L/100 km))']]\nX.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:27.48331Z","iopub.execute_input":"2022-05-07T05:52:27.48441Z","iopub.status.idle":"2022-05-07T05:52:27.506171Z","shell.execute_reply.started":"2022-05-07T05:52:27.484339Z","shell.execute_reply":"2022-05-07T05:52:27.505558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Train/Test Split**\nOut of Sample Accuracy is the percentage of correct predictions that the model makes on data that that the model has NOT been trained on. Doing a train and test on the same dataset will most likely have low out-of-sample accuracy, due to the likelihood of being over-fit. It is important that the model developed have a high, out-of-sample accuracy, because the purpose of any model, of course, is to make correct predictions on unknown data. \nOne way to improve out-of-sample accuracy is to use an evaluation approach called Train/Test Split. Train/Test Split involves splitting the dataset into training and testing sets respectively, which are mutually exclusive to train with the training set and test with the testing set. This will provide a more accurate evaluation on out-of-sample accuracy because the testing dataset is not part of the dataset that have been used to train the data. It is more realistic for real world problems.\n\nThis means that the outcome of each data point in this dataset is known, making it great to test with! And since this data has not been used to train the model, the model has no knowledge of the outcome of these data points. So, in essence, it is truly an out-of-sample testing.\n\nSplit the entire dataset into 80% for training, and the 20% for testing using train_test_split from sklearn model_selection","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:27.507136Z","iopub.execute_input":"2022-05-07T05:52:27.507905Z","iopub.status.idle":"2022-05-07T05:52:27.595938Z","shell.execute_reply.started":"2022-05-07T05:52:27.507845Z","shell.execute_reply":"2022-05-07T05:52:27.594876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import linear_model\nmlm = linear_model.LinearRegression()\nmlm.fit (X_train, y_train)\n#The value of the intercept (a)\nprint('The value of the intercept of mutiple linear regression model is: ', mlm.intercept_)\n# The coefficients\nprint ('The Coefficients of mutiple linear regression model is: ', mlm.coef_)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:27.597256Z","iopub.execute_input":"2022-05-07T05:52:27.597512Z","iopub.status.idle":"2022-05-07T05:52:27.712154Z","shell.execute_reply.started":"2022-05-07T05:52:27.597489Z","shell.execute_reply":"2022-05-07T05:52:27.711051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plugging in the actual values to obtain the linear function.\n\n$$\nFuel Consumption(Comb (L/100 km)) = 0.06231241 + 0.00066058 x  Vehicle Class + 0.13549093 x Engine Size(L) -0.01479244 x Cylinders + 0.00789365 x Transmission -0.03712664 x Fuel Type + 0.0413805 x CO2 Emissions(g/km) -0.0418067 x CO2 Rating +  0.03372079 x Smog Rating )\n$$","metadata":{}},{"cell_type":"markdown","source":"## **Model Evaluation**<a name=\"Evaluation\"></a>\nModel evaluation is done by comparing the predicted values with the actual values and  the difference is used to calculate the accuracy of a regression model. \n\nThere are different methods of model evaluation metrics as follows: \n<ul>\n    <li> Mean absolute error: It is the mean of the absolute value of the errors.</li>\n    <li> Mean Squared Error (MSE): Mean Squared Error (MSE) is the mean of the squared error. It’s more popular than Mean absolute error because the focus is geared more towards large errors. This is due to the squared term exponentially increasing larger errors in comparison to smaller ones.</li>\n    <li> Root Mean Squared Error (RMSE): This is the square root of the Mean Square Error. </li>\n    <li> Residual: The difference between the observed value (y) and the predicted value (Yhat) is called the residual (e). \n    <li> Total sum of squares (tss): Sum of squares of difference between the mean value and the actual value.\n    <li> Residual sum of squares (rss): Sum of residuals squared\n    <li> R-squared is not error, but is a popular metric for accuracy of your model. It represents how close the data are to the fitted regression line. The higher the R-squared, the better the model fits your data. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).</li>\n    \n    R-squared: 1 - (rss/tss)\n</ul>\n<ul>\n    <li> Explained Variance Score = 1 - [Variance(Ypredicted - Yactual) / Variance(Yactual)] </li>\n    __explained variance regression score:__  \nIf $\\hat{y}$ is the estimated target output, y the corresponding (correct) target output, and Var is Variance, the square of the standard deviation, then the explained variance is estimated as follow:\n\n$\\texttt{explainedVariance}(y, \\hat{y}) = 1 - \\frac{Var\\{ y - \\hat{y}\\}}{Var\\{y\\}}$  \nThe best possible score is 1.0, lower values are worse.\n</ul>","metadata":{}},{"cell_type":"code","source":"Yhat_mlm = mlm.predict(X_test)\nk = X_test.shape[1]\nn = len(X_test)\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score, r2_score\nprint('Mean Absolute Error(MAE) of Multiple Linear regression:', metrics.mean_absolute_error(y_test, Yhat_mlm))\nprint('Mean Squared Error(MSE) of Multiple Linear regression:', metrics.mean_squared_error(y_test, Yhat_mlm))\nprint('Root Mean Squared Error (RMSE) of Multiple Linear regression:', np.sqrt(metrics.mean_squared_error(y_test, Yhat_mlm)))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score of Multiple Linear regression: %.2f' % mlm.score(X_train, y_train))\nprint('Explained Variance Score (EVS) of Multiple Linear regression:',explained_variance_score(y_test, Yhat_mlm))\n#Residual sum of squares (rss)\nprint(\"Residual sum of squares of Multiple Linear regression: %.2f\" % np.mean((Yhat_mlm - y_test) ** 2))\nprint('R2 of Multiple Linear regression:',metrics.r2_score(y_test, Yhat_mlm))\nprint('R2 rounded of Multiple Linear regression:',(metrics.r2_score(y_test, Yhat_mlm)).round(2))\nr2 = r2_score(y_test, Yhat_mlm)\nr2_rounded = r2_score(y_test, Yhat_mlm).round(2)\nadjusted_r2 = (1- (1-r2)*(n-1)/(n-k-1)).round(3)\nprint('Adjusted_r2 of Multiple Linear regression: ', (1- (1-r2)*(n-1)/(n-k-1)).round(3))\naccuracy = mlm.score(X_test, y_test)\nprint(\"Accuracy of Multiple Linear regression: {}\".format(accuracy))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:27.714215Z","iopub.execute_input":"2022-05-07T05:52:27.714518Z","iopub.status.idle":"2022-05-07T05:52:27.756468Z","shell.execute_reply.started":"2022-05-07T05:52:27.714489Z","shell.execute_reply":"2022-05-07T05:52:27.755266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Model Enhancement**<a name = \"Enhancement\"></a>","metadata":{}},{"cell_type":"markdown","source":"#### **Summary statistics of Train set**","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as sm\nX2 = sm.add_constant(X_train)\nest = sm.OLS(y_train, X2)\nest2 = est.fit()\nprint(est2.summary())","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:27.757511Z","iopub.execute_input":"2022-05-07T05:52:27.7578Z","iopub.status.idle":"2022-05-07T05:52:28.704106Z","shell.execute_reply.started":"2022-05-07T05:52:27.757771Z","shell.execute_reply":"2022-05-07T05:52:28.702501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Backward Elimination: Removing the least significant variable from the model**\n\nFrom the statistics summary above, it seems the Engine size is not a statistically significant features of CO2 emission prediction. Backward Elimination is irrelevant in Python, because the Scikit-Learn library automatically takes care of selecting the statistically significant features when training the model to make accurate predictions. Eventhough that's the case trying for it just to make sure to see any improvement in accuracy. ","metadata":{}},{"cell_type":"code","source":"# Removing Vehicle Class Cylinders,CO2 Rating, and  Fuel Type  with very high p values of 0.924, 0.695, 0.498 and 0.358 respectively\n\nX = df[['Engine Size(L)', 'Transmission', 'CO2 Emissions(g/km)', 'Smog Rating']]\ny = df[['Fuel Consumption(Comb (L/100 km))']]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nfrom sklearn import linear_model\nmlm = linear_model.LinearRegression()\nmlm.fit (X_train, y_train)\n\nimport statsmodels.api as sm\nX2 = sm.add_constant(X_train)\nest = sm.OLS(y_train, X2)\nest2 = est.fit()\nprint(est2.summary())","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:28.705446Z","iopub.execute_input":"2022-05-07T05:52:28.705749Z","iopub.status.idle":"2022-05-07T05:52:28.732204Z","shell.execute_reply.started":"2022-05-07T05:52:28.705717Z","shell.execute_reply":"2022-05-07T05:52:28.730143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Feature selection using Feature Importance**\nFeature importance can be used to improve a predictive model. Feature importance refers to techniques to assign the importance score to input features based on how useful they are at predicting a target variable. Based on these scores those features with (lowest scores) can be deleted and those features with highest scores can be retained for predicting the output of the model. ","metadata":{}},{"cell_type":"markdown","source":"#### **Feature importance with Random samples**","metadata":{}},{"cell_type":"code","source":"# test regression dataset\nfrom sklearn.datasets import make_regression\n# define dataset\nX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n# summarize the dataset\nprint(X.shape, y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:28.734094Z","iopub.execute_input":"2022-05-07T05:52:28.734391Z","iopub.status.idle":"2022-05-07T05:52:28.84561Z","shell.execute_reply.started":"2022-05-07T05:52:28.73436Z","shell.execute_reply":"2022-05-07T05:52:28.844928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# linear regression feature importance\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom matplotlib import pyplot\n# define dataset\nX, y = make_regression(n_samples=10000, n_features=10, n_informative=5, random_state=1)\n# define the model\nmodel = LinearRegression()\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.coef_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\npyplot.bar([x for x in range(len(importance))], importance)\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:28.846598Z","iopub.execute_input":"2022-05-07T05:52:28.84695Z","iopub.status.idle":"2022-05-07T05:52:29.007464Z","shell.execute_reply.started":"2022-05-07T05:52:28.846919Z","shell.execute_reply":"2022-05-07T05:52:29.006694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Feature importance with the Fuel Consumption dataset**","metadata":{}},{"cell_type":"code","source":"# linear regression feature importance\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom matplotlib import pyplot\n# define dataset\nX = df[['Vehicle Class', 'Engine Size(L)', 'Cylinders', 'Transmission', 'Fuel Type','CO2 Emissions(g/km)', 'CO2 Rating', 'Smog Rating']]\ny = df['Fuel Consumption(Comb (L/100 km))']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\nFeatures = ['Vehicle Class', 'Engine Size(L)', 'Cylinders', 'Transmission', 'Fuel Type','CO2 Emissions(g/km)', 'CO2 Rating', 'Smog Rating']\n\n# define the model\nmodel = LinearRegression()\n# fit the model\nmodel.fit(X_train, y_train)\n# get importance\nimportance = model.coef_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\npyplot.bar([x for x in range(len(importance))], importance)\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:29.008717Z","iopub.execute_input":"2022-05-07T05:52:29.009028Z","iopub.status.idle":"2022-05-07T05:52:29.150268Z","shell.execute_reply.started":"2022-05-07T05:52:29.008999Z","shell.execute_reply":"2022-05-07T05:52:29.149677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10,5))\nplot = sns.barplot(x=Features, y=importance, palette = 'hsv')\nax.set_title('Multiple Linear Regression Coefficients as Feature Importance Scores')\nplot.set_xticklabels(plot.get_xticklabels(),rotation='vertical')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:29.151333Z","iopub.execute_input":"2022-05-07T05:52:29.152032Z","iopub.status.idle":"2022-05-07T05:52:29.36044Z","shell.execute_reply.started":"2022-05-07T05:52:29.151995Z","shell.execute_reply":"2022-05-07T05:52:29.359681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sorting feature_importance values in descending order\nFeatureImportance = pd.DataFrame(importance, columns = [\"feature_importances\"])\nFeatureImportance['Features'] = Features\nFeatureImportance = FeatureImportance[['Features','feature_importances']]\nFeatureImportance","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:29.36164Z","iopub.execute_input":"2022-05-07T05:52:29.362019Z","iopub.status.idle":"2022-05-07T05:52:29.373574Z","shell.execute_reply.started":"2022-05-07T05:52:29.361986Z","shell.execute_reply":"2022-05-07T05:52:29.373012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FeatureImportance = FeatureImportance.sort_values(by = 'feature_importances', ascending = False)\nf, ax = plt.subplots(figsize=(10,5))\nplot = sns.barplot(x='Features', y='feature_importances', data = FeatureImportance, palette = 'hsv')\nax.set_title('Multiple Linear Regression Coefficients as Feature Importance Scores')\nplot.set_xticklabels(plot.get_xticklabels(),rotation='vertical')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:29.374743Z","iopub.execute_input":"2022-05-07T05:52:29.375157Z","iopub.status.idle":"2022-05-07T05:52:29.557598Z","shell.execute_reply.started":"2022-05-07T05:52:29.375125Z","shell.execute_reply":"2022-05-07T05:52:29.556999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The value of the intercept (a)\nprint('The value of the intercept of mutiple linear regression model is: ', mlm.intercept_)\n# The coefficients\nprint ('The Coefficients of mutiple linear regression model is: ', mlm.coef_)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:29.558671Z","iopub.execute_input":"2022-05-07T05:52:29.559004Z","iopub.status.idle":"2022-05-07T05:52:29.565756Z","shell.execute_reply.started":"2022-05-07T05:52:29.558974Z","shell.execute_reply":"2022-05-07T05:52:29.564306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The value of the intercept $(\\theta_0)$ of mutiple linear regression model is:  -0.43037685\n\nThe Coefficients $(\\theta_1, \\theta_2, \\theta_3, \\theta_4)$ of mutiple linear regression model for the four features namely,  Engine Size(L), Transmission, CO2 Emissions(g/km), Smog Rating $ (x_1, x_2, x_3 and x_4)$ are 0.11680868, 0.00633435, 0.04216356, 0.03478887\n\n\nPlugging in the actual values to obtain the linear function.\n\n$$\nFuel Consumption(Comb (L/100 km)) = -0.43037685 + 0.11680868 x  Engine Size(L) + 0.00633435 x Transmission + 0.04216356 x CO2 Emissions(g/km) + 0.03478887 x Smog Rating)\n$$","metadata":{}},{"cell_type":"code","source":"# Appying features with highest scores obtained from feature importance and and also confirmed throgh backward elimination\n\nX = df[['Engine Size(L)', 'Transmission', 'CO2 Emissions(g/km)', 'Smog Rating']]\ny = df[['Fuel Consumption(Comb (L/100 km))']]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nfrom sklearn import linear_model\nmlm = linear_model.LinearRegression()\nmlm.fit (X_train, y_train)\n\n\nYhat_mlm = mlm.predict(X_test)\nk = X_test.shape[1]\nn = len(X_test)\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score, r2_score\nprint('Mean Absolute Error(MAE) of Multiple Linear regression:', metrics.mean_absolute_error(y_test, Yhat_mlm))\nprint('Mean Squared Error(MSE) of Multiple Linear regression:', metrics.mean_squared_error(y_test, Yhat_mlm))\nprint('Root Mean Squared Error (RMSE) of Multiple Linear regression:', np.sqrt(metrics.mean_squared_error(y_test, Yhat_mlm)))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score of Multiple Linear regression: %.2f' % mlm.score(X_train, y_train))\nprint('Explained Variance Score (EVS) of Multiple Linear regression:',explained_variance_score(y_test, Yhat_mlm))\n#Residual sum of squares (rss)\nprint(\"Residual sum of squares of Multiple Linear regression: %.2f\" % np.mean((Yhat_mlm - y_test) ** 2))\nprint('R2 of Multiple Linear regression:',metrics.r2_score(y_test, Yhat_mlm))\nprint('R2 rounded of Multiple Linear regression:',(metrics.r2_score(y_test, Yhat_mlm)).round(2))\nr2 = r2_score(y_test, Yhat_mlm)\nr2_rounded = r2_score(y_test, Yhat_mlm).round(2)\nadjusted_r2 = (1- (1-r2)*(n-1)/(n-k-1)).round(3)\nprint('Adjusted_r2 of Multiple Linear regression: ', (1- (1-r2)*(n-1)/(n-k-1)).round(3))\naccuracy = mlm.score(X_test, y_test)\nprint(\"Accuracy of Multiple Linear regression: {}\".format(accuracy))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:29.567614Z","iopub.execute_input":"2022-05-07T05:52:29.567899Z","iopub.status.idle":"2022-05-07T05:52:29.618659Z","shell.execute_reply.started":"2022-05-07T05:52:29.567865Z","shell.execute_reply":"2022-05-07T05:52:29.617466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final predictions\nplt.scatter(y_test, Yhat_mlm, label=\"Predicted vs Actual\")\n\n# Perfect predictions\nplt.xlabel('Fuel Consumption(Comb (L/100 km))')\nplt.ylabel('Fuel Consumption(Comb (L/100 km))')\nplt.plot(y_test,y_test,'r', label=\"Perfect Prediction\")\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2022-05-07T05:52:29.620086Z","iopub.execute_input":"2022-05-07T05:52:29.62032Z","iopub.status.idle":"2022-05-07T05:52:29.831865Z","shell.execute_reply.started":"2022-05-07T05:52:29.620289Z","shell.execute_reply":"2022-05-07T05:52:29.830385Z"},"trusted":true},"execution_count":null,"outputs":[]}]}