{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n# [UW-Madison GI Tract Image Segmentation][1]\n\n- The goal of this competition is to create a model to automatically segment stomach and intestines on MRI scans.\n\n---\n\n#### **The aim of this notebook is to**\n- **1. Conduct Exploratory Data Analysis (EDA).**\n- **2. Build the U-Net model with ResNet18 (pretrained on ImageNet) as encoder, and decoder from scratch.**\n- **3. Build the Attention U-Net model with ResNet18 (pretrained on ImageNet) as encoder.**\n- **4. Train the model with focal loss function for the unbalanced multi label semantic segmentation problem.**\n\n---\n\n#### **Please note**\n- **We need Internet access to run this code. Thus, this notebook doesn't fulfill the conditions for submitting (Internet access disabled).**\n\n---\n\n#### **If you find this notebook useful, please do give me an upvote. It helps me keep up my motivation.**\n#### **Also, I would appreciate it if you find any mistakes and help me correct them.**\n\n---\n\n[1]: https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/overview","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>TABLE OF CONTENTS</center></h1>\n\n<ul class=\"list-group\" style=\"list-style-type:none;\">\n    <li><a href=\"#0\" class=\"list-group-item list-group-item-action\">0. Settings</a></li>\n    <li><a href=\"#1\" class=\"list-group-item list-group-item-action\">1. Data Loading</a>\n        <ul class=\"list-group\" style=\"list-style-type:none;\">\n            <li><a href=\"#1.1\" class=\"list-group-item list-group-item-action\">1.1 Feature Engineering </a></li>\n        </ul>\n    </li>\n    <li><a href=\"#2\" class=\"list-group-item list-group-item-action\">2. Exploratory Data Analysis</a></li>\n    <li><a href=\"#3\" class=\"list-group-item list-group-item-action\">3. Dataset & DataLoader</a></li>\n    <li><a href=\"#4\" class=\"list-group-item list-group-item-action\">4. Model Building</a>\n        <ul class=\"list-group\" style=\"list-style-type:none;\">\n            <li><a href=\"#4.1\" class=\"list-group-item list-group-item-action\">4.1 U-Net </a></li>\n            <li><a href=\"#4.2\" class=\"list-group-item list-group-item-action\">4.2 Attention U-Net </a></li>\n        </ul>\n    </li>\n    <li><a href=\"#5\" class=\"list-group-item list-group-item-action\">5. Training</a></li>\n    <li><a href=\"#6\" class=\"list-group-item list-group-item-action\">6. Prediction</a></li>\n</ul>\n","metadata":{}},{"cell_type":"markdown","source":"<a id =\"0\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>0. Settings</center></h1>","metadata":{}},{"cell_type":"code","source":"## Import dependencies \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.offline as offline\n\nimport os\nimport pathlib\nimport gc\nimport sys\nimport re\nimport math \nimport random\nimport time \nimport datetime as dt\nfrom tqdm import tqdm \nfrom pprint import pprint\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\nimport torchvision.transforms.functional as TF\nfrom torchvision.models import resnet18\n!pip install torchinfo -q --user\nfrom torchinfo import summary\n\nfrom PIL import Image\n\nprint('import done!')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:16:53.28096Z","iopub.execute_input":"2022-04-29T14:16:53.281829Z","iopub.status.idle":"2022-04-29T14:17:08.463275Z","shell.execute_reply.started":"2022-04-29T14:16:53.281706Z","shell.execute_reply":"2022-04-29T14:17:08.462376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For reproducible results    \ndef seed_all(s):\n    random.seed(s)\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed(s)\n    os.environ['PYTHONHASHSEED'] = str(s) \n    print('Seeds setted!')\n    \nglobal_seed = 42\nseed_all(global_seed)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:17:08.466371Z","iopub.execute_input":"2022-04-29T14:17:08.466994Z","iopub.status.idle":"2022-04-29T14:17:08.478186Z","shell.execute_reply.started":"2022-04-29T14:17:08.466961Z","shell.execute_reply":"2022-04-29T14:17:08.477482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"1\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>1. Data Loading</center></h1>","metadata":{}},{"cell_type":"markdown","source":"---\n### [Files Descriptions](https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/data)\n\n- **train.csv** - IDs and masks for all training objects.\n\n- **train** - A folder of case/day folders, each containing slice images for a particular case on a given day.\n\n- **test** - The test set is entirely unseen. It is roughly 50 cases, with a varying number of days and slices, as seen in the training set.\n\n- **sample_submission.csv** - A sample submission file in the correct format.\n\n---\n### [Field Descriptions](https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/data)\n\n- **train.csv**\n - `id` - unique identifier for object\n - `class` - the predicted class for the object\n - `EncodedPixels` - RLE-encoded pixels for the identified object\n \n--- \n\n### [Submission File](https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/overview/evaluation)\nIn order to reduce the submission file size, our metric uses run-length encoding on the pixel values.  Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n\nNote that, at the time of encoding, the mask should be binary, meaning the masks for all objects in an image are joined into a single large mask. A value of 0 should indicate pixels that are not masked, and a value of 1 will indicate pixels that are masked.\n\nThe competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.","metadata":{}},{"cell_type":"code","source":"## Data Loading\ndata_config = {'train_csv_path': '../input/uw-madison-gi-tract-image-segmentation/train.csv',\n               'train_folder_path': '../input/uw-madison-gi-tract-image-segmentation/train',\n               'test_folder_path': '../input/uw-madison-gi-tract-image-segmentation/test',\n               'sample_submission_path': '../input/uw-madison-gi-tract-image-segmentation/sample_submission.csv',\n              }\n\ntrain_df = pd.read_csv(data_config['train_csv_path'])\nsubmission_df = pd.read_csv(data_config['sample_submission_path'])\n\nprint(f'train_length: {len(train_df)}')\nprint(f'submission_length: {len(submission_df)}')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:17:08.481017Z","iopub.execute_input":"2022-04-29T14:17:08.482632Z","iopub.status.idle":"2022-04-29T14:17:08.955389Z","shell.execute_reply.started":"2022-04-29T14:17:08.481495Z","shell.execute_reply":"2022-04-29T14:17:08.954002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Null Value Check\nprint('train_df.info()'); print(train_df.info(), '\\n')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:17:08.957543Z","iopub.execute_input":"2022-04-29T14:17:08.957795Z","iopub.status.idle":"2022-04-29T14:17:09.00995Z","shell.execute_reply.started":"2022-04-29T14:17:08.957759Z","shell.execute_reply":"2022-04-29T14:17:09.008894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:17:09.011235Z","iopub.execute_input":"2022-04-29T14:17:09.011461Z","iopub.status.idle":"2022-04-29T14:17:09.02667Z","shell.execute_reply.started":"2022-04-29T14:17:09.011429Z","shell.execute_reply":"2022-04-29T14:17:09.025899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"1.1\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>1.1 Feature Engineering</center></h2>","metadata":{}},{"cell_type":"code","source":"## Separate 'id' columns' texts, and create new id columns.\n## This code takes about 2 minutets to execute.\n\ndef create_id_list(text, p_train = pathlib.Path(data_config['train_folder_path'])):\n    t = text.split('_')\n    \n    case_id = t[0][4:]\n    day_id = t[1][3:]\n    slice_id = t[3]\n    \n    case_folder = t[0]\n    day_folder = ('_').join([t[0], t[1]])\n    slice_file = ('_').join([t[2], t[3]])\n    \n    p_folder = p_train / case_folder / day_folder / 'scans'\n    file_name = [p.name for p in p_folder.iterdir() if p.name[6:10] == slice_id]\n    id_list = [case_id, day_id, slice_id, case_folder, day_folder, slice_file]\n    id_list.extend(file_name)    \n    return id_list\n\ndef create_new_ids(dataframe, new_ids = ['case_id', 'day_id', 'slice_id', 'case_folder', 'day_folder', 'slice_file', 'file_name']):\n    dataframe['id_list'] = dataframe['id'].map(create_id_list)   \n    for i, item in enumerate(new_ids):\n        dataframe[item] = dataframe['id_list'].map(lambda x: x[i])\n    dataframe = dataframe.drop(['id_list'], axis=1)\n    return dataframe\n\ntrain_df = create_new_ids(train_df)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:17:09.027947Z","iopub.execute_input":"2022-04-29T14:17:09.028257Z","iopub.status.idle":"2022-04-29T14:18:46.398469Z","shell.execute_reply.started":"2022-04-29T14:17:09.028221Z","shell.execute_reply":"2022-04-29T14:18:46.397733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create detection column (1: non NaN segmentation, 0: NaN segmentation).\ntrain_df['detection'] = train_df['segmentation'].notna() * 1\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:18:46.399934Z","iopub.execute_input":"2022-04-29T14:18:46.400184Z","iopub.status.idle":"2022-04-29T14:18:46.42646Z","shell.execute_reply.started":"2022-04-29T14:18:46.400152Z","shell.execute_reply":"2022-04-29T14:18:46.425715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_img_n = int(len(train_df) / 3)\nprint('The number of imgs: ', total_img_n)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:18:46.427618Z","iopub.execute_input":"2022-04-29T14:18:46.427926Z","iopub.status.idle":"2022-04-29T14:18:46.434801Z","shell.execute_reply.started":"2022-04-29T14:18:46.427891Z","shell.execute_reply":"2022-04-29T14:18:46.43381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Calculate segmentation areas and img size.\ndef cal_pos_area(segmentation):\n    pos_area = 0\n    if type(segmentation) is str:\n        seg_list = segmentation.split(' ')\n        for i in range(len(seg_list)//2):\n            pos_area += int(seg_list[i*2 + 1])\n    return pos_area\n\ndef cal_total_area(file_name):\n    img_h = int(file_name[11:14])\n    img_w = int(file_name[15:18])\n    total_area = img_h * img_w\n    return total_area\n\ntrain_df['pos_area'] = train_df['segmentation'].map(cal_pos_area)\ntrain_df['total_area'] = train_df['file_name'].map(cal_total_area)\ntrain_df['pos_area_percentage'] = train_df['pos_area'] / train_df['total_area'] * 100\n\n## Check\ntrain_df[1920:1930]","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:18:46.436515Z","iopub.execute_input":"2022-04-29T14:18:46.436777Z","iopub.status.idle":"2022-04-29T14:18:47.451403Z","shell.execute_reply.started":"2022-04-29T14:18:46.436743Z","shell.execute_reply":"2022-04-29T14:18:47.450711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Split the samples based on the 'class'.\ntrain_lb_df = train_df[train_df['class']=='large_bowel'].reset_index(drop=True)\ntrain_sb_df = train_df[train_df['class']=='small_bowel'].reset_index(drop=True)\ntrain_st_df = train_df[train_df['class']=='stomach'].reset_index(drop=True)\n\n## Calculate each segmentation pixels' ratio to the total img pixels.\nlb_area_ratio = train_lb_df['pos_area'].sum() / train_lb_df['total_area'].sum()\nsb_area_ratio = train_sb_df['pos_area'].sum() / train_sb_df['total_area'].sum()\nst_area_ratio = train_st_df['pos_area'].sum() / train_st_df['total_area'].sum()\nbg_area_ratio = 1 - (lb_area_ratio + sb_area_ratio + st_area_ratio)\n\nprint(lb_area_ratio, sb_area_ratio, st_area_ratio, bg_area_ratio)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:18:47.454588Z","iopub.execute_input":"2022-04-29T14:18:47.454797Z","iopub.status.idle":"2022-04-29T14:18:47.583266Z","shell.execute_reply.started":"2022-04-29T14:18:47.454772Z","shell.execute_reply":"2022-04-29T14:18:47.582509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Split the samples which have non-null values in 'segmentation' as positive ones.\ntrain_positive_df = train_df.dropna(subset=['segmentation']).reset_index(drop=True)\ntrain_negative_df = train_df[train_df['segmentation'].isna()].reset_index(drop=True)\n\npos_lb_df = train_positive_df[train_positive_df['class']=='large_bowel'].reset_index(drop=True)\npos_sb_df = train_positive_df[train_positive_df['class']=='small_bowel'].reset_index(drop=True)\npos_st_df = train_positive_df[train_positive_df['class']=='stomach'].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:18:47.584452Z","iopub.execute_input":"2022-04-29T14:18:47.584712Z","iopub.status.idle":"2022-04-29T14:18:47.697736Z","shell.execute_reply.started":"2022-04-29T14:18:47.584676Z","shell.execute_reply":"2022-04-29T14:18:47.696954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"2\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>2. Exploratory Data Analysis</center></h1>","metadata":{}},{"cell_type":"code","source":"## Plot the bar graph of the detection percentages (per total number of images) of each classes.\nclass_group = train_df.groupby(['class'])['detection'].mean() * 100\n\nfig = px.bar(class_group)\nfig.update_layout(title = \"<span style='font-size:36px;>Detection Percentages (per total number of images) of Each Classes</span>\", \n                  yaxis_title = 'detection percentage')  ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:18:47.699047Z","iopub.execute_input":"2022-04-29T14:18:47.699303Z","iopub.status.idle":"2022-04-29T14:18:48.407978Z","shell.execute_reply.started":"2022-04-29T14:18:47.69927Z","shell.execute_reply":"2022-04-29T14:18:48.407265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plot the histogram of the detection percentage of large_bowel class in each case_ids\nlb_detection_mean = train_lb_df.groupby(['case_id'])['detection'].mean() * 100\nfig = px.histogram(lb_detection_mean, nbins=25, marginal='box')\nfig.update_layout(title = \"<span style='font-size:36px;>Detection Percentage of 'large_bowel' in Each Case_ids</span>\",\n                  xaxis_title = 'detection percentage')  ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:18:48.409464Z","iopub.execute_input":"2022-04-29T14:18:48.409745Z","iopub.status.idle":"2022-04-29T14:18:48.536862Z","shell.execute_reply.started":"2022-04-29T14:18:48.409708Z","shell.execute_reply":"2022-04-29T14:18:48.53605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plot the histogram of the detection percentage of small_bowel class in each case_ids\nsb_detection_mean = train_sb_df.groupby(['case_id'])['detection'].mean() * 100\nfig = px.histogram(sb_detection_mean, nbins=25, marginal='box')\nfig.update_layout(title = \"<span style='font-size:36px;>Detection Percentage of 'small_bowel' in Each Case_ids</span>\", \n                  xaxis_title = 'detection percentage')  ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:18:48.538322Z","iopub.execute_input":"2022-04-29T14:18:48.538595Z","iopub.status.idle":"2022-04-29T14:18:48.627712Z","shell.execute_reply.started":"2022-04-29T14:18:48.53855Z","shell.execute_reply":"2022-04-29T14:18:48.627061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plot the histogram of the detection percentage of stomach class in each case_ids\nst_detection_mean = train_st_df.groupby(['case_id'])['detection'].mean() * 100\nfig = px.histogram(st_detection_mean, nbins=25, marginal='box')\nfig.update_layout(title = \"<span style='font-size:36px;>Histogram of Detection Percentage of 'stomach' in Each Case_ids</span>\", \n                  xaxis_title = 'detection percentage')  ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:18:48.628813Z","iopub.execute_input":"2022-04-29T14:18:48.629084Z","iopub.status.idle":"2022-04-29T14:18:48.717805Z","shell.execute_reply.started":"2022-04-29T14:18:48.629049Z","shell.execute_reply":"2022-04-29T14:18:48.717003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Compare the above three histograms in one figure.\nfig = go.Figure()\nlb_detection_mean = train_lb_df.groupby(['case_id'])['detection'].mean() * 100\nfig.add_trace(go.Histogram(x=lb_detection_mean.values, nbinsx=25, \n                           opacity=0.5, name='large_bowel',\n                           histnorm='probability'))\n\nsb_detection_mean = train_sb_df.groupby(['case_id'])['detection'].mean() * 100\nfig.add_trace(go.Histogram(x=sb_detection_mean.values, nbinsx=25, \n                           opacity=0.5, name='small_bowel',\n                           histnorm='probability'))\n\nst_detection_mean = train_st_df.groupby(['case_id'])['detection'].mean() * 100\nfig.add_trace(go.Histogram(x=st_detection_mean.values, nbinsx=25, \n                           opacity=0.5, name='stomach',\n                           histnorm='probability'))\nfig.update_layout(barmode='overlay', \n                  title = \"<span style='font-size:36px;>Comparison of Detection Percentages in Each Case_ids</span>\",\n                  xaxis_title = 'detection percentage',\n                  yaxis_title = 'n_cases / n_total_cases')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:18:48.718979Z","iopub.execute_input":"2022-04-29T14:18:48.719286Z","iopub.status.idle":"2022-04-29T14:18:48.76225Z","shell.execute_reply.started":"2022-04-29T14:18:48.719248Z","shell.execute_reply":"2022-04-29T14:18:48.761627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plot the bar graph of the detection area percentages of three classes.\nclass_group = train_positive_df.groupby(['class'])['pos_area_percentage']\n\nfig = px.bar(class_group.mean(), error_y=class_group.std())\nfig.update_layout(title = \"<span style='font-size:36px;>Mean Area Percentages (for the area of an image) of Eacg Classes</span>\", \n                  yaxis_title = 'detection area percentage') ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:18:48.763292Z","iopub.execute_input":"2022-04-29T14:18:48.763694Z","iopub.status.idle":"2022-04-29T14:18:48.833941Z","shell.execute_reply.started":"2022-04-29T14:18:48.763655Z","shell.execute_reply":"2022-04-29T14:18:48.833304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plot the histogram of the detection area percentages of three classes.\nfig = go.Figure()\nfig.add_trace(go.Histogram(x=pos_lb_df['pos_area_percentage'], nbinsx=100, \n                           opacity=0.5, name='large_bowel',\n                           histnorm='probability'))\nfig.add_trace(go.Histogram(x=pos_sb_df['pos_area_percentage'], nbinsx=100, \n                           opacity=0.5, name='small_bowel',\n                           histnorm='probability'))\nfig.add_trace(go.Histogram(x=pos_st_df['pos_area_percentage'], nbinsx=100, \n                           opacity=0.5, name='stomach',\n                           histnorm='probability'))\nfig.update_layout(barmode='overlay', \n                  title = \"<span style='font-size:36px;>Comparison of Detection Area Percentage </span>\",\n                  xaxis_title = 'detection area percentage',\n                  yaxis_title = 'n_images / n_total_detected_imgs')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:18:48.834932Z","iopub.execute_input":"2022-04-29T14:18:48.835153Z","iopub.status.idle":"2022-04-29T14:18:48.865559Z","shell.execute_reply.started":"2022-04-29T14:18:48.835119Z","shell.execute_reply":"2022-04-29T14:18:48.864963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>3. Dataset & DataLoader</center></h1>","metadata":{}},{"cell_type":"code","source":"## Train - Valid - Test split\n## I split the train, valid, test data based on the case_id (imgs that have the same case_id are assigned in the same set).\n\ntrain_ratio = 0.85\nvalid_ratio = 0.10\ntest_ratio = 0.05\n\ncase_ids = train_df['case_id'].unique()\nidxs = np.random.permutation(range(len(case_ids)))\ncut_1 = int(train_ratio * len(idxs))\ncut_2 = int((train_ratio + valid_ratio) * len(idxs))\n\ntrain_case_ids = case_ids[idxs[:cut_1]]\nvalid_case_ids = case_ids[idxs[cut_1:cut_2]]\ntest_case_ids = case_ids[idxs[cut_2:]]\n\ntrain = train_df.query('case_id in @train_case_ids')\nvalid = train_df.query('case_id in @valid_case_ids')\ntest = train_df.query('case_id in @test_case_ids')\n\nprint(len(train), len(valid), len(test), len(train_df))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:18:48.866467Z","iopub.execute_input":"2022-04-29T14:18:48.866674Z","iopub.status.idle":"2022-04-29T14:18:48.961775Z","shell.execute_reply.started":"2022-04-29T14:18:48.866648Z","shell.execute_reply":"2022-04-29T14:18:48.961005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_case_folders = train['case_folder'].unique()\ntrain_files = []\nfor case_folder in train_case_folders:\n    p_train = pathlib.Path(data_config['train_folder_path'])\n    p_folder = p_train / case_folder\n    tmp_files = list(p_folder.glob('**/scans/*.png'))\n    train_files.extend(tmp_files)\n    \nvalid_case_folders = valid['case_folder'].unique()\nvalid_files = []\nfor case_folder in valid_case_folders:\n    p_train = pathlib.Path(data_config['train_folder_path'])\n    p_folder = p_train / case_folder\n    tmp_files = list(p_folder.glob('**/scans/*.png'))\n    valid_files.extend(tmp_files)\n    \ntest_case_folders = test['case_folder'].unique()\ntest_files = []\nfor case_folder in test_case_folders:\n    p_train = pathlib.Path(data_config['train_folder_path'])\n    p_folder = p_train / case_folder\n    tmp_files = list(p_folder.glob('**/scans/*.png'))\n    test_files.extend(tmp_files)\n    \nprint(len(train_files), len(valid_files), len(test_files))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:18:48.963158Z","iopub.execute_input":"2022-04-29T14:18:48.963589Z","iopub.status.idle":"2022-04-29T14:18:50.458868Z","shell.execute_reply.started":"2022-04-29T14:18:48.96354Z","shell.execute_reply":"2022-04-29T14:18:50.458122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Building Dataset and DataLoader\nclass UWMadison2022Dataset(torch.utils.data.Dataset):\n    def __init__(self, files, dataframe=None, input_shape=256,):\n        self.files = files\n        self.df = dataframe\n        self.input_shape = input_shape\n        self.transforms = transforms.Compose([\n            transforms.CenterCrop(self.input_shape),\n            transforms.Normalize(mean=[(0.485+0.456+0.406)/3], std=[(0.229+0.224+0.225)/3]),\n        ])\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, idx):\n        p_file = self.files[idx]\n        #img = torchvision.io.read_image(p_file)\n        img = np.array(Image.open(p_file))\n        img_shape = torch.tensor(img.shape)\n        img = transforms.functional.to_tensor(img) / 255.\n        img = self.transforms(img)\n        #img = torch.cat([img, img, img], dim=0)\n        \n        if self.df is not None:\n            f_name = str(p_file).split('/')\n            case_day_id = f_name[5]\n            slice_id = f_name[7][:10]\n            f_id = '_'.join([case_day_id, slice_id])\n            labels_df = self.df.query('id == @f_id')\n            \n            label = torch.zeros([img_shape[0]*img_shape[1]])\n            for i, organ in enumerate(['large_bowel', 'small_bowel', 'stomach']):\n                segmentation = labels_df[labels_df['class'] == organ]['segmentation'].item()\n                if type(segmentation) is str:\n                    segmentation = segmentation.split(' ')\n                    for j in range(len(segmentation)//2):\n                        start_idx = int(segmentation[j*2])\n                        span = int(segmentation[j*2 + 1])\n                        label[start_idx:(start_idx+span)] = (i+1)\n            label = torch.reshape(label, (img_shape[0], img_shape[1]))\n            label = transforms.CenterCrop(self.input_shape)(label)\n            label = torch.nn.functional.one_hot(label.to(torch.int64), num_classes=4)\n            label = label.permute(2, 0, 1)\n            return img, label, img_shape\n        \n        else: return img, img_shape\n        \ntrain_ds = UWMadison2022Dataset(train_files, train, input_shape=256)\nvalid_ds = UWMadison2022Dataset(valid_files, valid, input_shape=256)\ntest_ds = UWMadison2022Dataset(test_files, test, input_shape=256)\n\nBATCH_SIZE = 32\n\n## Checking dataset and dataloder  \nprint('------ train_dl ------')\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\ntmp = train_dl.__iter__()\nx, y, shape = tmp.next()\nprint(f\"x : {x.shape}\")\nprint(f\"labels: {y.shape}\")\nprint(f\"img_shapes: {shape.shape}\")\nprint(f\"n_samples: {len(train_ds)}\")\nprint(f\"n_batches: {len(tmp)}\")\nprint()\n\nprint('------ valid_dl ------')\nvalid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\ntmp = valid_dl.__iter__()\nx, y, shape = tmp.next()\nprint(f\"x : {x.shape}\")\nprint(f\"labels: {y.shape}\")\nprint(f\"img_shapes: {shape.shape}\")\nprint(f\"n_samples: {len(valid_ds)}\")\nprint(f\"n_batches: {len(tmp)}\")\nprint()\n\nprint('------ test_dl ------')\ntest_dl = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\ntmp = test_dl.__iter__()\nx, y, shape = tmp.next()\nprint(f\"x : {x.shape}\")\nprint(f\"labels: {y.shape}\")\nprint(f\"img_shapes: {shape.shape}\")\nprint(f\"n_samples: {len(test_ds)}\")\nprint(f\"n_batches: {len(tmp)}\")\nprint()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:18:50.46032Z","iopub.execute_input":"2022-04-29T14:18:50.46079Z","iopub.status.idle":"2022-04-29T14:18:52.018408Z","shell.execute_reply.started":"2022-04-29T14:18:50.460751Z","shell.execute_reply":"2022-04-29T14:18:52.017536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"4\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>4. Model Building</center></h1>","metadata":{}},{"cell_type":"markdown","source":"<a id =\"4.1\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>4.1 U-Net</center></h2>","metadata":{}},{"cell_type":"markdown","source":"- **The U-Net consists of encoder - decoder network architecture.**\n- **We use ResNet18 (pretrained on Imagenet) for the encoder, and build decoder from scratch.**\n- **We have to make skip connections from encoder to decoder.**\n\n<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" width=\"500\"/>","metadata":{}},{"cell_type":"code","source":"resnet = resnet18(pretrained=True)\nbatch_size = 16\n\nsummary(\n    resnet,\n    input_size=(batch_size, 3, 256, 256),\n    col_names=[\"output_size\", \"num_params\"],\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-29T14:18:52.019917Z","iopub.execute_input":"2022-04-29T14:18:52.020197Z","iopub.status.idle":"2022-04-29T14:19:02.157385Z","shell.execute_reply.started":"2022-04-29T14:18:52.020157Z","shell.execute_reply":"2022-04-29T14:19:02.156701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I used the feature outputs of ResNet18 at \n- **ReLU: 1-3 [batch_size, 64, 128, 128]**\n- **Sequential: 1-5 [batch_size, 64, 64, 64]**\n- **Sequential: 1-6 [batch_size, 128, 32, 32]**\n- **Sequential: 1-7 [batch_size, 256, 16, 16]**\n\n for the skip connections. And, outputs at \n\n- **Sequential: 1-8 [16, 512, 8, 8]**\n\n is for the bottleneck features (first inputs for the decoder).","metadata":{}},{"cell_type":"code","source":"## The Extractor of intermediate features of ResNet (encoder) for the skip connections to the decoder.\nclass FeatureExtractor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.resnet = resnet18(pretrained=True)\n        # Change first conv layer to accept single-channel (grayscale) input\n        self.resnet.conv1.weight = torch.nn.Parameter(self.resnet.conv1.weight.sum(dim=1).unsqueeze(1)) \n        \n    def forward(self, x):\n        skip_connections = []\n        for i in range(8):\n            x = list(self.resnet.children())[i](x)\n            if i in [2, 4, 5, 6, 7]:\n                skip_connections.append(x)\n        encoder_outputs = skip_connections.pop(-1)\n        skip_connections = skip_connections[::-1]\n        \n        return encoder_outputs, skip_connections","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:19:02.158863Z","iopub.execute_input":"2022-04-29T14:19:02.159125Z","iopub.status.idle":"2022-04-29T14:19:02.166277Z","shell.execute_reply.started":"2022-04-29T14:19:02.159091Z","shell.execute_reply":"2022-04-29T14:19:02.165429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## The modules used for building the decoder architecture.\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.dconv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n        \n    def forward(self, x):\n        return  self.dconv(x)\n    \n    \nclass UnetUpSample(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.convt = nn.ConvTranspose2d(\n            in_channels, out_channels, kernel_size=2, stride=2)\n        self.norm1 = nn.BatchNorm2d(out_channels)\n        self.act1 = nn.ReLU(inplace=True)\n        self.dconv = DoubleConv(out_channels*2, out_channels)\n         \n    def forward(self, layer_input, skip_input):\n        u = self.convt(layer_input)\n        u = self.norm1(u)\n        u = self.act1(u)\n        u = torch.cat((u, skip_input), dim=1)\n        u = self.dconv(u)\n        return u","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:19:02.167635Z","iopub.execute_input":"2022-04-29T14:19:02.167886Z","iopub.status.idle":"2022-04-29T14:19:02.196543Z","shell.execute_reply.started":"2022-04-29T14:19:02.167851Z","shell.execute_reply":"2022-04-29T14:19:02.195883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## U-Net architechture.\nclass UW2022Unet(nn.Module):\n    def __init__(self, out_channels):\n        super().__init__()\n        self.encoder = FeatureExtractor()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.upsample1 = UnetUpSample(512, 256)\n        self.upsample2 = UnetUpSample(256, 128)\n        self.upsample3 = UnetUpSample(128, 64)\n        self.upsample4 = UnetUpSample(64, 64)\n        \n        self.final_convt = nn.ConvTranspose2d(\n            64, 64, kernel_size=2, stride=2)\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        x1, skip_connections = self.encoder(x)\n        x2 = self.upsample1(x1, skip_connections[0])\n        x3 = self.upsample2(x2, skip_connections[1])\n        x4 = self.upsample3(x3, skip_connections[2])\n        x5 = self.upsample4(x4, skip_connections[3])\n        x6 = self.final_convt(x5)\n        \n        return self.final_conv(x6)\n    \nmodel = UW2022Unet(out_channels=4)\n\nsummary(\n    model,\n    input_size=(batch_size, 1, 256, 256),\n    col_names=[\"output_size\", \"num_params\"],\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-29T14:19:02.199503Z","iopub.execute_input":"2022-04-29T14:19:02.199893Z","iopub.status.idle":"2022-04-29T14:19:02.531261Z","shell.execute_reply.started":"2022-04-29T14:19:02.199856Z","shell.execute_reply":"2022-04-29T14:19:02.530425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"4.2\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>4.2 Attention U-Net</center></h2>","metadata":{}},{"cell_type":"markdown","source":"- **The [Attention U-Net](https://arxiv.org/abs/1804.03999) has attention architecture (the figure below) in the skip connections between encoder and decoder.**\n\n<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/y/y_kurashina/20190429/20190429231337.jpg\" width=\"500\"/>\n<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/y/y_kurashina/20190429/20190429233922.jpg\" width=\"500\"/>","metadata":{}},{"cell_type":"code","source":"## The Modules used for building Attention architecture.\nclass GateSignal(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.gate_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        ) \n        \n    def forward(self, x):\n        return self.gate_conv(x)\n        \nclass AttentionBlock(nn.Module):\n    def __init__(self, gate_channels, x_channels, inter_channels):\n        super().__init__()\n        self.theta = nn.Conv2d(x_channels, inter_channels, kernel_size=2, stride=2)\n        self.phi = nn.Conv2d(gate_channels, inter_channels, kernel_size=1, stride=1)\n        self.act1 = nn.ReLU(inplace=True)\n        self.psi = nn.Conv2d(inter_channels, 1, kernel_size=1, stride=1)\n        self.act2 = nn.Sigmoid()\n        self.upsample = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2)\n        self.final_conv = nn.Conv2d(inter_channels, x_channels, kernel_size=1, stride=1, bias=False)\n        self.norm = nn.BatchNorm2d(x_channels)\n        \n    def forward(self, x, g):\n        theta_x = self.theta(x)\n        phi_g = self.phi(g)\n        xg = torch.add(theta_x, phi_g)\n        xg = self.act1(xg)\n        score = self.psi(xg)\n        score = self.act2(score)\n        score = self.upsample(score)\n        score = score.expand(x.shape)        \n        att_x = torch.mul(score, x)\n        att_x = self.final_conv(att_x) \n        att_x = self.norm(att_x)\n        return att_x","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:19:02.532745Z","iopub.execute_input":"2022-04-29T14:19:02.533017Z","iopub.status.idle":"2022-04-29T14:19:02.546192Z","shell.execute_reply.started":"2022-04-29T14:19:02.532981Z","shell.execute_reply":"2022-04-29T14:19:02.545278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Attention U-Net architechture.\nclass UW2022AttentionUnet(nn.Module):\n    def __init__(self, out_channels):\n        super().__init__()\n        self.encoder = FeatureExtractor()\n        \n        self.upsample1 = UnetUpSample(512, 256)\n        self.upsample2 = UnetUpSample(256, 128)\n        self.upsample3 = UnetUpSample(128, 64)\n        self.upsample4 = UnetUpSample(64, 64)\n        \n        self.gate_signal1 = GateSignal(512, 256)\n        self.gate_signal2 = GateSignal(256, 128)\n        self.gate_signal3 = GateSignal(128, 64)\n        self.gate_signal4 = GateSignal(64, 64)\n        \n        self.attention1 = AttentionBlock(256, 256, 256)\n        self.attention2 = AttentionBlock(128, 128, 128)\n        self.attention3 = AttentionBlock(64, 64, 64)\n        self.attention4 = AttentionBlock(64, 64, 64)\n        \n        self.final_convt = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        x1, skip_connections = self.encoder(x)\n        \n        g1 = self.gate_signal1(x1)\n        attention_skip1 = self.attention1(skip_connections[0], g1)\n        x2 = self.upsample1(x1, attention_skip1)\n        \n        g2 = self.gate_signal2(x2)\n        attention_skip2 = self.attention2(skip_connections[1], g2)\n        x3 = self.upsample2(x2, attention_skip2)\n        \n        g3 = self.gate_signal3(x3)\n        attention_skip3 = self.attention3(skip_connections[2], g3)\n        x4 = self.upsample3(x3, attention_skip3)\n        \n        g4 = self.gate_signal4(x4)\n        attention_skip4 = self.attention4(skip_connections[3], g4)\n        x5 = self.upsample4(x4, attention_skip4)\n\n        x6 = self.final_convt(x5)       \n        return self.final_conv(x6)\n\nmodel = UW2022AttentionUnet(out_channels=4)\n\nsummary(\n    model,\n    input_size=(batch_size, 1, 256, 256),\n    col_names=[\"output_size\", \"num_params\"],\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-29T14:19:02.548595Z","iopub.execute_input":"2022-04-29T14:19:02.549147Z","iopub.status.idle":"2022-04-29T14:19:02.911304Z","shell.execute_reply.started":"2022-04-29T14:19:02.54911Z","shell.execute_reply":"2022-04-29T14:19:02.91063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"5\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>5. Training</center></h1>","metadata":{}},{"cell_type":"markdown","source":"Because this competition's label data (mask) is unbalanced (positive areas are smaller than negative areas), I used **\"Focal Loss\"** instead of normal cross entropy loss for the training loss function. It is said that focal loss function is good for multiclass classification where some classes are difficult to classify, and others are easy. **\"gamma\"** is a parameter of focal loss function that decides how emphasise the minor labels (the bigger gamma is, the more emphasis on minors we put).","metadata":{}},{"cell_type":"code","source":"## Focal Loss Function\nclass SegmentationFocalLoss(nn.Module):\n    def __init__(self, gamma=2, weight=None):\n        super().__init__()\n        self.gamma = gamma\n        if torch.cuda.is_available():\n            self.loss = torch.nn.CrossEntropyLoss(weight=weight).cuda()\n        else:\n            self.loss = nn.CrossEntropyLoss(weight=weight)\n\n    def forward(self, pred, target):\n        ce_loss = self.loss(pred, target)\n        #ce_loss = torch.nn.functional.cross_entropy(pred, target, reduce=False)\n        pt = torch.exp(-ce_loss)\n        focal_loss = (1. - pt) ** self.gamma * ce_loss\n        return torch.mean(focal_loss)\n\n##Setting the weight parameter of CrossEntropyLoss.\nlb_weight = 1 / lb_area_ratio\nsb_weight = 1 / sb_area_ratio\nst_weight = 1 / st_area_ratio\nbg_weight = 1 / bg_area_ratio\ntotal_weight = lb_weight + sb_weight + st_weight + bg_weight\n\nlb_weight = lb_weight / total_weight * 5\nsb_weight = sb_weight / total_weight * 5 \nst_weight = st_weight / total_weight * 5\nbg_weight = bg_weight / total_weight * 5\nweight = torch.tensor([bg_weight, lb_weight, sb_weight, st_weight], dtype=torch.float)\nprint(f'bg:{bg_weight}, lb:{lb_weight}, sb:{sb_weight}, st{st_weight}')\n\nloss_fn = SegmentationFocalLoss(gamma=3, weight=weight)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:19:02.914993Z","iopub.execute_input":"2022-04-29T14:19:02.915523Z","iopub.status.idle":"2022-04-29T14:19:02.934465Z","shell.execute_reply.started":"2022-04-29T14:19:02.915493Z","shell.execute_reply":"2022-04-29T14:19:02.933728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LEARNING_RATE = 1e-4\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:19:02.935909Z","iopub.execute_input":"2022-04-29T14:19:02.936155Z","iopub.status.idle":"2022-04-29T14:19:02.941538Z","shell.execute_reply.started":"2022-04-29T14:19:02.936122Z","shell.execute_reply":"2022-04-29T14:19:02.940563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For the model training loop.\nif torch.cuda.is_available():\n    DEVICE = 'cuda'\nelse: DEVICE = 'cpu'\n\ndef train_fn(loader, model, optimizer, loss_fn, device=DEVICE):\n    model.train()\n    train_loss = 0.\n    loop = tqdm(loader)\n    \n    for batch_idx, (data, targets, img_size) in enumerate(loop):\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n        \n        predictions = model(data)\n        targets = torch.argmax(targets, dim=1)\n        loss = loss_fn(predictions, targets)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        loop.set_postfix(loss=loss.item())\n        train_loss += loss.detach().cpu().numpy() * BATCH_SIZE\n        \n    train_loss = train_loss / (BATCH_SIZE * len(train_dl))\n    return train_loss\n\n## For the model validation loop.\ndef valid_fn(loader, model, loss_fn, device=DEVICE):\n    model.eval()\n    valid_loss = 0.\n    loop = tqdm(loader)\n    \n    with torch.no_grad():\n        for batch_idx, (data, targets, img_size) in enumerate(loop):\n            data = data.to(device=device)\n            targets = targets.to(device=device)\n            \n            predictions = model(data)\n            targets = torch.argmax(targets, dim=1)\n            loss = loss_fn(predictions, targets)\n            valid_loss += loss * BATCH_SIZE\n            \n            loop.set_postfix(loss=loss.item())\n            \n        valid_loss = valid_loss / (BATCH_SIZE * len(valid_dl))\n    return valid_loss","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:19:02.942993Z","iopub.execute_input":"2022-04-29T14:19:02.943581Z","iopub.status.idle":"2022-04-29T14:19:02.95623Z","shell.execute_reply.started":"2022-04-29T14:19:02.943546Z","shell.execute_reply":"2022-04-29T14:19:02.955429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For the train & validation loop.\nNUM_EPOCHS = 1\n\nmodel.to(device=DEVICE)\n\nbest_loss = 100\nfor epoch in range(NUM_EPOCHS):\n    print('-------------')\n    print('Epoch {}/{}'.format(epoch+1, NUM_EPOCHS))\n    print('-------------')\n    \n    train_loss = train_fn(train_dl, model, optimizer, loss_fn, DEVICE)\n    valid_loss = valid_fn(valid_dl, model, loss_fn, DEVICE)\n    \n    if valid_loss < best_loss:\n        checkpoint = {\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }\n        torch.save(checkpoint, \"./checkpoint.pth\")\n        print('best model saved!')\n        best_loss = valid_loss\n    \n    print(f'Train Loss: {train_loss},  Valid Loss: {valid_loss}')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:19:02.957533Z","iopub.execute_input":"2022-04-29T14:19:02.958385Z","iopub.status.idle":"2022-04-29T14:33:47.572107Z","shell.execute_reply.started":"2022-04-29T14:19:02.958348Z","shell.execute_reply":"2022-04-29T14:33:47.571308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"6\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>6. Prediction</center></h1>","metadata":{}},{"cell_type":"code","source":"checkpoint = torch.load(\"./checkpoint.pth\")\nmodel.load_state_dict(checkpoint[\"model\"])\noptimizer.load_state_dict(checkpoint[\"optimizer\"])\n        \nmodel.eval()\npredictions = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_dl):\n        x = batch[0].to(DEVICE)\n        test_pred = model(x)\n        test_pred = torch.argmax(test_pred, dim=1)\n        test_pred = torch.nn.functional.one_hot(test_pred, num_classes=4)\n        test_pred = torch.permute(test_pred, dims=[0, 3, 1, 2])\n        test_pred = test_pred[:, 1:, ...] ## We don't need background predictions.\n        test_pred = test_pred.detach().cpu().numpy()\n        predictions.append(test_pred)\n    \npredictions = np.concatenate(predictions, axis=0)\npredictions = predictions.reshape([-1, 256, 256])\nprint(predictions.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:33:47.573534Z","iopub.execute_input":"2022-04-29T14:33:47.573974Z","iopub.status.idle":"2022-04-29T14:34:17.361913Z","shell.execute_reply.started":"2022-04-29T14:33:47.573936Z","shell.execute_reply":"2022-04-29T14:34:17.360375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_encode(img):\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    encodes = ' '.join(str(x) for x in runs)\n    if encodes == '':\n        encodes = np.nan\n    return encodes\n\npredictions_rle = []\n\nfor pred in predictions:\n    pred_rle = rle_encode(pred)\n    predictions_rle.append(pred_rle)\n    \npredictions_rle = np.concatenate([predictions_rle], axis=0)\ntest['prediction'] = predictions_rle\n\ntest.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:35:25.517671Z","iopub.execute_input":"2022-04-29T14:35:25.518383Z","iopub.status.idle":"2022-04-29T14:35:27.229117Z","shell.execute_reply.started":"2022-04-29T14:35:25.518342Z","shell.execute_reply":"2022-04-29T14:35:27.228287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}