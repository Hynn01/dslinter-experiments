{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Humour Detection using NLP</h1>\n<img src=\"https://knowledgeone.ca/wp-content/uploads/2020/06/humour-3-questions.jpg\" style=\"width:100%;margin:auto;\">","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"<h2>Problem Statement</h2>\n<p>Detect if a text is humorous or not from a short text input</p>","metadata":{}},{"cell_type":"markdown","source":"<h2>Getting Started</h2>","metadata":{}},{"cell_type":"code","source":"!pip install autocorrect\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom autocorrect import Speller\nimport unidecode\nfrom wordcloud import WordCloud, STOPWORDS\nfrom textblob import TextBlob\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:12:05.720183Z","iopub.execute_input":"2022-05-08T05:12:05.720933Z","iopub.status.idle":"2022-05-08T05:12:16.929123Z","shell.execute_reply.started":"2022-05-08T05:12:05.720873Z","shell.execute_reply":"2022-05-08T05:12:16.927872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Data Loading</h2>","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"../input/200k-short-texts-for-humor-detection/dataset.csv\")\n\ndata","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:12:16.931723Z","iopub.execute_input":"2022-05-08T05:12:16.932096Z","iopub.status.idle":"2022-05-08T05:12:17.323852Z","shell.execute_reply.started":"2022-05-08T05:12:16.932047Z","shell.execute_reply":"2022-05-08T05:12:17.322518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Data Engineering</h2>\n<ul>\n    <li>Case conversion</li>\n    <li>Removing special characters</li>\n    <li>Removing shorthands</li>\n    <li>Removing stopwords</li>\n    <li>Removing links</li>\n    <li>Removing accents</li>\n    <li>Normalize spaces</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"def case_convert():\n    data.text = [i.lower() for i in data.text.values]\n\ndef remove_specials():\n    data.text =  [re.sub(r\"[^a-zA-Z]\",\" \",text) for text in data.text.values]\n\ndef remove_shorthands():\n    CONTRACTION_MAP = {\n    \"ain't\": \"is not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"i'd\": \"i would\",\n    \"i'd've\": \"i would have\",\n    \"i'll\": \"i will\",\n    \"i'll've\": \"i will have\",\n    \"i'm\": \"i am\",\n    \"i've\": \"i have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so as\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\",\n    \"you'll've\": \"you will have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\"\n    }\n    texts = []\n    for text in data.text.values:\n        string = \"\"\n        for word in text.split(\" \"):\n            if word.strip() in list(CONTRACTION_MAP.keys()):\n                string = string + \" \" + CONTRACTION_MAP[word]\n            else:\n                string = string + \" \" + word\n        texts.append(string.strip())\n    data.text = texts\n\ndef remove_stopwords():\n    texts = []\n    stopwords_list = stopwords.words('english')\n    for item in data.text.values:\n        string = \"\"\n        for word in item.split(\" \"):\n            if word.strip() in stopwords_list:\n                continue\n            else:\n                string = string + \" \" + word\n        texts.append(string)\n                \ndef remove_links():\n    texts = []\n    for text in data.text.values:\n        remove_https = re.sub(r'http\\S+', '', text)\n        remove_com = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n        texts.append(remove_com)\n    data.text = texts\n\ndef remove_accents():\n    data.text = [unidecode.unidecode(text) for text in data.text.values]\n\ndef normalize_spaces():\n    data.text = [re.sub(r\"\\s+\",\" \",text) for text in data.text.values]\n\ncase_convert()\nremove_links()\nremove_shorthands()\nremove_accents()\nremove_specials()\nremove_stopwords()\nnormalize_spaces()\nprint(data)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:12:17.325632Z","iopub.execute_input":"2022-05-08T05:12:17.325923Z","iopub.status.idle":"2022-05-08T05:12:34.582305Z","shell.execute_reply.started":"2022-05-08T05:12:17.325891Z","shell.execute_reply":"2022-05-08T05:12:34.581478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.humor.replace(True,1,inplace=True)\ndata.humor.replace(False,0,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:12:34.584831Z","iopub.execute_input":"2022-05-08T05:12:34.585896Z","iopub.status.idle":"2022-05-08T05:12:34.740642Z","shell.execute_reply.started":"2022-05-08T05:12:34.585853Z","shell.execute_reply":"2022-05-08T05:12:34.739551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>EDA</h2>\n","metadata":{}},{"cell_type":"markdown","source":"<h3>Class Balancing</h3>","metadata":{}},{"cell_type":"code","source":"un, count = np.unique(data.humor, return_counts=True)\nplt.bar([\"Humor\" if i == 1 else \"Non-Humor\" for i in un], count)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Count\")\nplt.title(\"Class Balance\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:12:34.742166Z","iopub.execute_input":"2022-05-08T05:12:34.74252Z","iopub.status.idle":"2022-05-08T05:12:34.953454Z","shell.execute_reply.started":"2022-05-08T05:12:34.742472Z","shell.execute_reply":"2022-05-08T05:12:34.952719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Inference : </b> Well-balanced classes, hence no action required.","metadata":{}},{"cell_type":"markdown","source":"<h3>Word Cloud for humorous texts</h3>","metadata":{}},{"cell_type":"code","source":"string = \"\"\nfor i in data[data.humor == 1].text.values:\n    string = string + \" \" + i.strip()\n    \nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = set(STOPWORDS),\n                min_font_size = 10).generate(string)\n \n# plot the WordCloud image                      \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()\ndel string","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:12:34.954805Z","iopub.execute_input":"2022-05-08T05:12:34.955562Z","iopub.status.idle":"2022-05-08T05:14:17.095095Z","shell.execute_reply.started":"2022-05-08T05:12:34.95552Z","shell.execute_reply":"2022-05-08T05:14:17.093987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Word cloud for Non-Humorous texts</h3>","metadata":{}},{"cell_type":"code","source":"string = \"\"\nfor i in data[data.humor == 0].text.values:\n    string = string + \" \" + i.strip()\n    \nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = set(STOPWORDS),\n                min_font_size = 10).generate(string)\n \n# plot the WordCloud image                      \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()\ndel string","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:14:17.096669Z","iopub.execute_input":"2022-05-08T05:14:17.097192Z","iopub.status.idle":"2022-05-08T05:15:00.584813Z","shell.execute_reply.started":"2022-05-08T05:14:17.097139Z","shell.execute_reply":"2022-05-08T05:15:00.584023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Sentiment distribution for Humorous and Non-Humorous texts</h3>","metadata":{}},{"cell_type":"code","source":"sentiment = []\nfor text in data.text.values:\n    polarity = TextBlob(text).sentiment.polarity\n    if polarity < 0:\n        sentiment.append(\"Negative\")\n    elif polarity == 0:\n        sentiment.append(\"Neutral\")\n    else:\n        sentiment.append(\"Positive\")\ndata[\"sentiment\"] = sentiment\ndel sentiment","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:15:00.586053Z","iopub.execute_input":"2022-05-08T05:15:00.586459Z","iopub.status.idle":"2022-05-08T05:15:44.830547Z","shell.execute_reply.started":"2022-05-08T05:15:00.586418Z","shell.execute_reply":"2022-05-08T05:15:44.829465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(13,4))\nplt.subplot(1,2,1)\nun, count = np.unique(data[data.humor == 1].sentiment,return_counts=True)\nplt.bar(un, count)\nplt.xlabel(\"Sentiment\")\nplt.ylabel(\"Count\")\nplt.title(\"Humorous Texts\")\nplt.subplot(1,2,2)\nun, count = np.unique(data[data.humor == 0].sentiment,return_counts=True)\nplt.bar(un, count)\nplt.xlabel(\"Sentiment\")\nplt.ylabel(\"Count\")\nplt.title(\"Non-Humorous Texts\")\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:15:44.832116Z","iopub.execute_input":"2022-05-08T05:15:44.832529Z","iopub.status.idle":"2022-05-08T05:15:45.362727Z","shell.execute_reply.started":"2022-05-08T05:15:44.832482Z","shell.execute_reply":"2022-05-08T05:15:45.361818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Inference : </b><b>Humorous</b> texts have more <b>Negative</b> sentiment and less <b>Positive </b> sentiment in comparison with <b>Non-Humorous</b> texts mostly due to few jokes being offensive and containing adultery or swear words.","metadata":{}},{"cell_type":"code","source":"del data['sentiment']","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:15:45.365Z","iopub.execute_input":"2022-05-08T05:15:45.365236Z","iopub.status.idle":"2022-05-08T05:15:45.370285Z","shell.execute_reply.started":"2022-05-08T05:15:45.365208Z","shell.execute_reply":"2022-05-08T05:15:45.369259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Modelling</h2>","metadata":{}},{"cell_type":"markdown","source":"<b>Data preparation</b>","metadata":{}},{"cell_type":"code","source":"vec = TfidfVectorizer(max_features=3000)\n\nX_train, X_test, Y_train, Y_test = train_test_split(vec.fit_transform(data.text.values).toarray(), \n                                                    data.humor.values.reshape(-1,1), \n                                                    test_size=0.2, \n                                                    shuffle=True, \n                                                    random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:15:45.37194Z","iopub.execute_input":"2022-05-08T05:15:45.372278Z","iopub.status.idle":"2022-05-08T05:15:56.659472Z","shell.execute_reply.started":"2022-05-08T05:15:45.372234Z","shell.execute_reply":"2022-05-08T05:15:56.658583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Model Training</b>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nmodel = RandomForestClassifier(n_estimators=10,random_state=42)\nmodel.fit(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:15:56.661017Z","iopub.execute_input":"2022-05-08T05:15:56.662037Z","iopub.status.idle":"2022-05-08T05:17:26.411022Z","shell.execute_reply.started":"2022-05-08T05:15:56.661988Z","shell.execute_reply":"2022-05-08T05:17:26.409821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Metrics</b>","metadata":{}},{"cell_type":"code","source":"print(\"Train Accuracy  : {:.2f} %\".format(accuracy_score(model.predict(X_train), Y_train)*100))\nprint(\"Test Accuracy   : {:.2f} %\".format(accuracy_score(model.predict(X_test), Y_test)*100))\nprint(\"Precision       : {:.2f} %\".format(precision_score(model.predict(X_test), Y_test)*100))\nprint(\"Recall          : {:.2f} %\".format(recall_score(model.predict(X_test), Y_test)*100))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:17:26.413182Z","iopub.execute_input":"2022-05-08T05:17:26.413519Z","iopub.status.idle":"2022-05-08T05:17:32.269824Z","shell.execute_reply.started":"2022-05-08T05:17:26.413475Z","shell.execute_reply":"2022-05-08T05:17:32.268893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Confusion Matrix</b>","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ncm = confusion_matrix(Y_test, model.predict(X_test))\ndisp = ConfusionMatrixDisplay(cm, display_labels=[\"Non-Humor\",\"Humor\"])\ndisp.plot()\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:17:32.271063Z","iopub.execute_input":"2022-05-08T05:17:32.271286Z","iopub.status.idle":"2022-05-08T05:17:33.345706Z","shell.execute_reply.started":"2022-05-08T05:17:32.271259Z","shell.execute_reply":"2022-05-08T05:17:33.344713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Conclusion</b> : The model performs fairly well with 89% accuracy.","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"margin:auto;text-align:center;background-color:rgb(232, 230, 223);border-radius : 5px;padding-top : 25px;padding-bottom : 25px; width : 80%;font-size : 25px;\">Thank you for reading! Upvote and share my notebook if you liked it</h1>","metadata":{}}]}