{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n!pip -q install transformers \nfrom transformers import AutoTokenizer, AutoModel , AutoConfig\n!pip -q  install datasets \nfrom datasets import * ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-07T17:44:40.071641Z","iopub.execute_input":"2022-05-07T17:44:40.072116Z","iopub.status.idle":"2022-05-07T17:45:18.45706Z","shell.execute_reply.started":"2022-05-07T17:44:40.07201Z","shell.execute_reply":"2022-05-07T17:45:18.455536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# when we are dealing with multilingual datasets they might have multiple configs. \nfrom datasets import get_dataset_config_names\nxtreme_subsets = get_dataset_config_names(\"xtreme\")\nprint(f\"Xtreme is having different {len(xtreme_subsets)} configurations\")","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:45:18.459609Z","iopub.execute_input":"2022-05-07T17:45:18.459895Z","iopub.status.idle":"2022-05-07T17:45:19.228847Z","shell.execute_reply.started":"2022-05-07T17:45:18.459857Z","shell.execute_reply":"2022-05-07T17:45:19.22788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filtering the dataset based on what we want to load German corpus we are passing de code to load_dataset fucntion \n[x for x in xtreme_subsets if x.startswith(\"PAN\")][:5]","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:46:13.731479Z","iopub.execute_input":"2022-05-07T17:46:13.732128Z","iopub.status.idle":"2022-05-07T17:46:13.7375Z","shell.execute_reply.started":"2022-05-07T17:46:13.732087Z","shell.execute_reply":"2022-05-07T17:46:13.736919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the de dataset\nload_dataset(\"xtreme\", name =\"PAN-X.de\")","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:47:13.131114Z","iopub.execute_input":"2022-05-07T17:47:13.131382Z","iopub.status.idle":"2022-05-07T17:47:30.031554Z","shell.execute_reply.started":"2022-05-07T17:47:13.131355Z","shell.execute_reply":"2022-05-07T17:47:30.03053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# since we have to perform NER on german french italian and english downloading those dataset to \nfrom collections import defaultdict\nfrom datasets import DatasetDict\n\nlangs = [\"de\", \"fr\" , \"it\", \"en\"]\nfracs = [0.629, 0.228 , 0.084, 0.059]\n\npanx_ch = defaultdict(DatasetDict)\n\nfor lang, frac in zip(langs, fracs):\n    # Load monolingual corpus\n    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n    # Shuffle and downsample each split according to spoken proportion\n    for split in ds:\n            panx_ch[lang][split] = (\n            ds[split]\n            .shuffle(seed=0)\n            .select(range(int(frac * ds[split].num_rows))))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:50:42.808196Z","iopub.execute_input":"2022-05-07T17:50:42.808533Z","iopub.status.idle":"2022-05-07T17:51:10.667776Z","shell.execute_reply.started":"2022-05-07T17:50:42.808493Z","shell.execute_reply":"2022-05-07T17:51:10.666891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking out for how many rows does each dataset have  with ds.num_rows\n\npd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs},\nindex=[\"Number of training examples\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:52:06.554982Z","iopub.execute_input":"2022-05-07T17:52:06.55572Z","iopub.status.idle":"2022-05-07T17:52:06.569153Z","shell.execute_reply.started":"2022-05-07T17:52:06.555668Z","shell.execute_reply":"2022-05-07T17:52:06.567741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"element = panx_ch[\"de\"][\"train\"][0]\nfor key, value in element.items():\n    print(f\"{key}: {value}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:52:28.094246Z","iopub.execute_input":"2022-05-07T17:52:28.094494Z","iopub.status.idle":"2022-05-07T17:52:28.103695Z","shell.execute_reply.started":"2022-05-07T17:52:28.094469Z","shell.execute_reply":"2022-05-07T17:52:28.102923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key, value in panx_ch[\"de\"][\"train\"].features.items():\n    print(f\"{key}: {value}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:53:00.749899Z","iopub.execute_input":"2022-05-07T17:53:00.750396Z","iopub.status.idle":"2022-05-07T17:53:00.757392Z","shell.execute_reply.started":"2022-05-07T17:53:00.75036Z","shell.execute_reply":"2022-05-07T17:53:00.756373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\nprint(tags)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:53:24.67113Z","iopub.execute_input":"2022-05-07T17:53:24.671427Z","iopub.status.idle":"2022-05-07T17:53:24.677908Z","shell.execute_reply.started":"2022-05-07T17:53:24.671397Z","shell.execute_reply":"2022-05-07T17:53:24.676518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_tag_names(batch):\n    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:53:50.510772Z","iopub.execute_input":"2022-05-07T17:53:50.511071Z","iopub.status.idle":"2022-05-07T17:53:50.516845Z","shell.execute_reply.started":"2022-05-07T17:53:50.511038Z","shell.execute_reply":"2022-05-07T17:53:50.51561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"panx_de = panx_ch[\"de\"].map(create_tag_names)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:53:59.412245Z","iopub.execute_input":"2022-05-07T17:53:59.412555Z","iopub.status.idle":"2022-05-07T17:54:05.055029Z","shell.execute_reply.started":"2022-05-07T17:53:59.412522Z","shell.execute_reply":"2022-05-07T17:54:05.053931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"de_example = panx_de[\"train\"][0]\npd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\n['Tokens', 'Tags'])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:54:13.051505Z","iopub.execute_input":"2022-05-07T17:54:13.051862Z","iopub.status.idle":"2022-05-07T17:54:13.072959Z","shell.execute_reply.started":"2022-05-07T17:54:13.051821Z","shell.execute_reply":"2022-05-07T17:54:13.072343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the distribution\nfrom collections import Counter\nsplit2freqs = defaultdict(Counter)\nfor split, dataset in panx_de.items():\n    for row in dataset[\"ner_tags_str\"]:\n        for tag in row:\n            if tag.startswith(\"B\"):\n                tag_type = tag.split(\"-\")[1]\n                split2freqs[split][tag_type] += 1\npd.DataFrame.from_dict(split2freqs, orient=\"index\")","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:55:53.070677Z","iopub.execute_input":"2022-05-07T17:55:53.070999Z","iopub.status.idle":"2022-05-07T17:55:53.384081Z","shell.execute_reply.started":"2022-05-07T17:55:53.070967Z","shell.execute_reply":"2022-05-07T17:55:53.382986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WORK UNDER PROGRESS ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}