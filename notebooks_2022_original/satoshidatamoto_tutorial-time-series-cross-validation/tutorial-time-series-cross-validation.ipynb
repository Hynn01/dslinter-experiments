{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":">### Let's Talk Time Series Validation\n>Time Series Forecasting can be overwhelming. Especially if you are just getting startet. There are many different types of Time Series tasks each differ by the number of input or output sequences, the number of steps to predict, whether the input and/or the output sequence length is static or changing, and so on. In this notebook, we will experiment with **different types of Time Series Cross Validation Strategies** in order to become familiar with them and understand which works best for what case.\n>","metadata":{"_cell_guid":"8af163ff-915a-4fae-aefe-6447e64952e5","_uuid":"b328cc9e-a536-4347-beed-d033e9f5ac6a","papermill":{"duration":0.052517,"end_time":"2022-01-12T08:04:29.606073","exception":false,"start_time":"2022-01-12T08:04:29.553556","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"____\n\n# Time Series Forecasting\n\nTime Series Forecasting can be overwhelming. Especially if you are just getting startet. There are many different types of Time Series tasks each differ by the number of input or output sequences, the number of steps to predict, whether the input and/or the output sequence length is static or changing, and so on. In this notebook, we will experiment with **different types of Time Series Cross Validation Strategies** in order to become familiar with them and understand which works best for what case. \n\nAs written before, Time Series problems can be of different variations, so in order to get a deeper understanding we should explore each. \n\n<br>\n<font color='#EC7063'>\n    \n* **Variation I:** Number of Input / Output sequences\n    * (1.0) Single input and single output with input = output (Univariate with `in` = `out`)\n    * (1.1) Single input and single output with input $\\neq$ output (Multivariate with `in` $\\neq$  `out`)\n    * (1.2) Multiple inputs and multiple outputs with inputs = outputs (Multivariate with `in[N]` = `out[N]`)\n    * (1.3) Multiple inputs and multiple outputs with inputs $\\neq$ outputs (Multivariate with `in[N]` $\\neq$  `out[N]`)  \n\n</font>\n\n\n<br>\n<font color='#5499C7'>\n\n* **Variation II:** Length of Output sequences\n    * (2.0) Single step output sequence     \n    * (2.1) Multistep: Predict all steps at once (Single-shot) \n    * (2.2) Multistep: Predict single step at a time and feedback to model to predict for multiple steps (Autoregressive) \n\n</font>\n\n\n<br>\n<font color='#45B39D'>\n\n* **Variation III** Type of input sequence\n    * (3.0) Static length input sequence (Sliding Window) \n    * (3.1) Variable length input sequence (Expanding Window) \n  \n</font>\n<br>\n\nEvery different Time Series task have different combination of the above properties. For example, we could have:\n\n    \n<br>\n<center>One sequence for which we are trying to <font color='#EC7063'>predict its values</font> <font color='#5499C7'>for the next 5 timesteps</font> <font color='#45B39D'>based on the previous 10 timesteps</font></center>\n<br>\n\nThis would make this task: \n\n<br>\n<center><b><font color='#EC7063'>Univariate</font> <font color='#5499C7'>multistep</font> time series forecasting with a <font color='#45B39D'>sliding window</font> (<font color='#45B39D'>3.0</font>, <font color='#5499C7'>2.1</font>, <font color='#EC7063'>1.0</font>)</b></center>\n<br>\n\nOr we might need to \n\n<br>\n<center><b>Predict the amount of <font color='#5499C7'>snow for the next day</font> based on <font color='#45B39D'>all available past data</font> of <font color='#EC7063'>temperature and rain</font></b></center>\n<br>\n\n[Take a moment to try and guess the problem types yourself..]\n\n<br>\n\n<center>\nThis is a <b><font color='#EC7063'>Multivariate</font> <font color='#45B39D'>single step</font> time series forecasting with an <font color='#45B39D'>expanding window</font> (<font color='#5499C7'>2.0</font>, <font color='#45B39D'>3.1</font>, <font color='#EC7063'>1.3</font>).</b>\n</center>\n\n<br>\n\n\n<hr> \n\n**Credits:** Some sections of this notebook (Including this intro) are heavily insipred by the great notebook made by Leonie: [Time Series Forecasting: Building Intuition](https://www.kaggle.com/iamleonie/time-series-forecasting-building-intuition). If you find this notebook useful, Please go upvote the original! \n\n<hr> ","metadata":{"papermill":{"duration":0.056133,"end_time":"2022-01-12T08:04:30.048828","exception":false,"start_time":"2022-01-12T08:04:29.992695","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom numpy.random import seed \nfrom datetime import datetime, date \nfrom IPython.display import display_html\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 14})\nSEED = 42\nos.environ['PYTHONHASHSEED']=str(SEED)\nseed(SEED)\nnp.random.seed(SEED)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":1.138913,"end_time":"2022-01-12T08:04:31.237917","exception":false,"start_time":"2022-01-12T08:04:30.099004","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:32.191269Z","iopub.execute_input":"2022-04-23T12:43:32.192492Z","iopub.status.idle":"2022-04-23T12:43:32.197856Z","shell.execute_reply.started":"2022-04-23T12:43:32.192437Z","shell.execute_reply":"2022-04-23T12:43:32.197107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time-Series Toy Problems\nIn order for us to get a better understanding, we will use a fictional time series so we can see the different types of time series problems in practice. There will be three features. `feature_1` is following a sine wave, `feature_2` is a linear function, and `feature_3` is a modulo function. The `feature_4` column is the result of a combination of the feature columns. The time series consists of 100 timesteps.","metadata":{"papermill":{"duration":0.049807,"end_time":"2022-01-12T08:04:31.338165","exception":false,"start_time":"2022-01-12T08:04:31.288358","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def split_sequences(features, targets, n_steps_in, n_steps_out, n_sliding_steps, window_type):\n    X, y = list(), list()\n    for i in range(0, len(features), n_sliding_steps):\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out\n        if out_end_ix > len(features): break\n        if window_type == 'sliding': seq_x, seq_y = features[i:end_ix, :], targets[end_ix:out_end_ix, :]\n        else: seq_x, seq_y = features[0:end_ix, :], targets[end_ix:out_end_ix, :]\n        X.append(seq_x)\n        y.append(seq_y)\n    return np.array(X), np.array(y)\ndef plot_time_series_problem(X, y):\n    fig, ax = plt.subplots(nrows=X.shape[0], ncols=1, figsize=(15, 2.5*X.shape[0]))\n    for i in range(X.shape[0]):\n        sns.lineplot(x=ts_df['timestamp'].values, y=ts_df['feature_1'].values, ax=ax[i], color='lightgrey', marker='o')\n        if i < X.shape[0]-1:\n            sns.lineplot(x=X[i][ :, 0], y=X[i][ :, 1].astype(float), ax=ax[i], color='cornflowerblue', label='train', marker='o')\n            sns.lineplot(x=y[i][ :, 0], y=y[i][ :, 1].astype(float), ax=ax[i], color='orange', label='val', marker='o')\n            ax[i].set_title(f\"Training Sample {i}\")\n        else:\n            sns.lineplot(x=X[i][ :, 0], y=X[i][ :, 1].astype(float), ax=ax[i], color='mediumseagreen', label='in', marker='o')\n            sns.lineplot(x=y[i][ :, 0], y=y[i][ :, 1].astype(float), ax=ax[i], color='coral', label='pred', marker='o')\n            ax[i].set_title(f\"Testing\")    \n        ax[i].set_xlim([date(2021, 1, 1), date(2021, 4, 11)])\n    plt.tight_layout()\n    plt.show()  \n\ndef plot_history(history):\n    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n    plt.figure(1)\n    for l in loss_list: plt.plot(epochs, history.history[l], 'cornflowerblue', label='Training loss')\n    for l in val_loss_list: plt.plot(epochs, history.history[l], 'orange', label='Validation loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\ntime = np.arange(0, 1100, 10)\nts_df = pd.DataFrame({'feature_1' : 15*np.sin(0.021*time+30)+13, 'feature_2' : (0.05*time)+20, 'feature_3' : (( time + 100 ) % 280)*0.1 + 50, })\nts_df['feature_4'] =  0.1 * ts_df.feature_1 * (ts_df.feature_2 + 5) - ts_df.feature_3.shift(10) + 100\nts_df = ts_df[10:110]\nts_df['timestamp'] = pd.date_range('2021-01-01', periods=100, freq='D')\nts_df.set_index('timestamp', inplace=True)\nts_df.reset_index(drop=False, inplace=True)\ndisplay(ts_df.head())","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.208041,"end_time":"2022-01-12T08:04:31.596369","exception":false,"start_time":"2022-01-12T08:04:31.388328","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:32.214672Z","iopub.execute_input":"2022-04-23T12:43:32.215599Z","iopub.status.idle":"2022-04-23T12:43:32.246344Z","shell.execute_reply.started":"2022-04-23T12:43:32.215553Z","shell.execute_reply":"2022-04-23T12:43:32.245113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualizing the time series**","metadata":{"papermill":{"duration":0.05032,"end_time":"2022-01-12T08:04:31.697528","exception":false,"start_time":"2022-01-12T08:04:31.647208","status":"completed"},"tags":[]}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=4, ncols=1, figsize=(15, 10))\nsns.lineplot(x=ts_df.timestamp, y=ts_df.feature_1, ax=ax[0], color='cornflowerblue', marker='o')\nsns.lineplot(x=ts_df.timestamp, y=ts_df.feature_2, ax=ax[1], color='cornflowerblue', marker='o')\nsns.lineplot(x=ts_df.timestamp, y=ts_df.feature_3, ax=ax[2], color='cornflowerblue', marker='o')\nsns.lineplot(x=ts_df.timestamp, y=ts_df.feature_4, ax=ax[3], color='cornflowerblue', marker='o')\nfor i in range(4): ax[i].set_xlim([date(2021, 1, 1), date(2021, 4, 10)])\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.851107,"end_time":"2022-01-12T08:04:32.599841","exception":false,"start_time":"2022-01-12T08:04:31.748734","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:32.248007Z","iopub.execute_input":"2022-04-23T12:43:32.24838Z","iopub.status.idle":"2022-04-23T12:43:32.986912Z","shell.execute_reply.started":"2022-04-23T12:43:32.248343Z","shell.execute_reply":"2022-04-23T12:43:32.986428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following function will help us build our lunch menu. It will take your input and/or output sequences and return you the training and testing data according to your order. You can specify the length of the input sequence, the length of the output sequence, and the step size of your window.","metadata":{"papermill":{"duration":0.052892,"end_time":"2022-01-12T08:04:32.706472","exception":false,"start_time":"2022-01-12T08:04:32.65358","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Variation I: <font color='#EC7063'>Number of Input/Output Sequences</font>\nLet's look at the <font color='#EC7063'>number of input/output sequences</font>. To make things easier to understand, we will only <font color='#EC7063'>variate the number of input/output sequences </font>, an we will <font color='#45B39D'>keep the length of the output sequences</font> and the <font color='#45B39D'>type of input sequences</font> same for the following. \n\nWe will use a <font color='#5499C7'>multistep output sequence of 5 steps</font> and the <font color='#45B39D'>sliding window setting with a stepsize of 20 for this.</font> ","metadata":{"papermill":{"duration":0.052967,"end_time":"2022-01-12T08:04:32.812547","exception":false,"start_time":"2022-01-12T08:04:32.75958","status":"completed"},"tags":[]}},{"cell_type":"code","source":"### Help functions ###\ndef display_input_and_output_df(input_cols, output_cols):\n    ts_df_styler1 = ts_df[input_cols].head().style.set_table_attributes(\"style='display:inline'\").set_caption('Input (X)')\n    ts_df_styler2 = ts_df[output_cols].head().style.set_table_attributes(\"style='display:inline'\").set_caption('Output (y)')\n    display_html(ts_df_styler1._repr_html_()+ts_df_styler2._repr_html_(), raw=True)","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.060734,"end_time":"2022-01-12T08:04:32.92639","exception":false,"start_time":"2022-01-12T08:04:32.865656","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:32.987961Z","iopub.execute_input":"2022-04-23T12:43:32.98816Z","iopub.status.idle":"2022-04-23T12:43:32.993639Z","shell.execute_reply.started":"2022-04-23T12:43:32.988135Z","shell.execute_reply":"2022-04-23T12:43:32.992121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Univariate: <font color='#EC7063'>Single input</font> and <font color='#EC7063'>single output</font> when input series == output series","metadata":{"papermill":{"duration":0.054885,"end_time":"2022-01-12T08:04:33.034444","exception":false,"start_time":"2022-01-12T08:04:32.979559","status":"completed"},"tags":[]}},{"cell_type":"code","source":"input_cols = ['timestamp', 'feature_1']\noutput_cols = input_cols","metadata":{"papermill":{"duration":0.061995,"end_time":"2022-01-12T08:04:33.151006","exception":false,"start_time":"2022-01-12T08:04:33.089011","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:32.995704Z","iopub.execute_input":"2022-04-23T12:43:32.996454Z","iopub.status.idle":"2022-04-23T12:43:33.009902Z","shell.execute_reply.started":"2022-04-23T12:43:32.996415Z","shell.execute_reply":"2022-04-23T12:43:33.009193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_input_and_output_df(input_cols, output_cols)\n\nX, y = split_sequences(ts_df[input_cols].values, \n                       ts_df[output_cols].values, \n                       n_steps_in = 15, \n                       n_steps_out = 5, \n                       n_sliding_steps = 20, \n                       window_type='sliding')\n\nfig, ax = plt.subplots(nrows=X.shape[0], ncols=1, figsize=(15, 2.5*X.shape[0]))\nfig.suptitle('Univariate: Single input and single output with input = output')\nfor i in range(X.shape[0]):\n    sns.lineplot(x=ts_df['timestamp'].values, y=ts_df['feature_1'].values, ax=ax[i], color='lightgrey', marker='o')\n\n    if i < X.shape[0]-1:\n        sns.lineplot(x=X[i][ :, 0], y=X[i][ :, 1].astype(float), ax=ax[i], color='cornflowerblue', label='input', marker='o')\n        sns.lineplot(x=y[i][ :, 0], y=y[i][ :, 1].astype(float), ax=ax[i], color='orange', label='output', marker='o')\n        ax[i].set_title(f\"Training Sample {i}\")\n    else:\n        sns.lineplot(x=X[i][ :, 0], y=X[i][ :, 1].astype(float), ax=ax[i], color='mediumseagreen', label='input', marker='o')\n        sns.lineplot(x=y[i][ :, 0], y=y[i][ :, 1].astype(float), ax=ax[i], color='coral', label='predict', marker='o')\n        ax[i].set_title(f\"Testing\")    \n    ax[i].set_xlim([date(2021, 1, 1), date(2021, 4, 11)])\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show() ","metadata":{"_kg_hide-input":true,"papermill":{"duration":1.370814,"end_time":"2022-01-12T08:04:34.575153","exception":false,"start_time":"2022-01-12T08:04:33.204339","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:33.011244Z","iopub.execute_input":"2022-04-23T12:43:33.011757Z","iopub.status.idle":"2022-04-23T12:43:34.367373Z","shell.execute_reply.started":"2022-04-23T12:43:33.011728Z","shell.execute_reply":"2022-04-23T12:43:34.366105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multivariate: <font color='#EC7063'>Multiple inputs</font> and <font color='#EC7063'>multiple outputs</font> with inputs series = outputs series","metadata":{"papermill":{"duration":0.057491,"end_time":"2022-01-12T08:04:34.690926","exception":false,"start_time":"2022-01-12T08:04:34.633435","status":"completed"},"tags":[]}},{"cell_type":"code","source":"input_cols = ['timestamp', 'feature_1', 'feature_2']\noutput_cols = input_cols","metadata":{"papermill":{"duration":0.065873,"end_time":"2022-01-12T08:04:34.814619","exception":false,"start_time":"2022-01-12T08:04:34.748746","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:34.368979Z","iopub.execute_input":"2022-04-23T12:43:34.369223Z","iopub.status.idle":"2022-04-23T12:43:34.374395Z","shell.execute_reply.started":"2022-04-23T12:43:34.369193Z","shell.execute_reply":"2022-04-23T12:43:34.373417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_input_and_output_df(input_cols, output_cols)\n\nX, y = split_sequences(ts_df[input_cols].values, \n                       ts_df[output_cols].values, \n                       n_steps_in = 15, \n                       n_steps_out = 5, \n                       n_sliding_steps = 20, \n                       window_type='sliding')\n\nfig, ax = plt.subplots(nrows=X.shape[0], ncols=2, figsize=(15, 2.5*X.shape[0]))\nfig.suptitle(r'Multivariate: Multiple inputs and multiple outputs with input = output')\n\nfor i in range(X.shape[0]):\n    sns.lineplot(x=ts_df['timestamp'].values, y=ts_df['feature_1'].values, ax=ax[i, 0], color='lightgrey', marker='o')\n    sns.lineplot(x=ts_df['timestamp'].values, y=ts_df['feature_2'].values, ax=ax[i, 1], color='lightgrey', marker='o')\n\n    if i < X.shape[0]-1:\n        sns.lineplot(x=X[i][ :, 0], y=X[i][ :, 1].astype(float), ax=ax[i, 0], color='cornflowerblue', marker='o')\n        sns.lineplot(x=y[i][ :, 0], y=y[i][ :, 1].astype(float), ax=ax[i, 0], color='orange', marker='o')\n        \n        sns.lineplot(x=X[i][ :, 0], y=X[i][ :, 2].astype(float), ax=ax[i, 1], color='cornflowerblue', marker='o')\n        sns.lineplot(x=y[i][ :, 0], y=y[i][ :, 2].astype(float), ax=ax[i, 1], color='orange', marker='o')\n        \n        ax[i, 0].set_title(f\"Training Sample {i}\")\n        ax[i, 1].set_title(f\"Training Sample {i}\")\n\n    else:\n        sns.lineplot(x=X[i][ :, 0], y=X[i][ :, 1].astype(float), ax=ax[i, 0], color='mediumseagreen', marker='o')\n        sns.lineplot(x=y[i][ :, 0], y=y[i][ :, 1].astype(float), ax=ax[i, 0], color='coral', marker='o')\n        \n        sns.lineplot(x=X[i][ :, 0], y=X[i][ :, 2].astype(float), ax=ax[i, 1], color='mediumseagreen', marker='o')\n        sns.lineplot(x=y[i][ :, 0], y=y[i][ :, 2].astype(float), ax=ax[i, 1], color='coral', marker='o')\n        ax[i, 0].set_title(f\"Testing Sample\")    \n        ax[i, 1].set_title(f\"Testing Sample (Prediction)\")    \n\n    ax[i, 0].set_xlim([date(2021, 1, 1), date(2021, 4, 11)])\n    ax[i, 1].set_xlim([date(2021, 1, 1), date(2021, 4, 11)])\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show() ","metadata":{"_kg_hide-input":true,"papermill":{"duration":2.150105,"end_time":"2022-01-12T08:04:37.022377","exception":false,"start_time":"2022-01-12T08:04:34.872272","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:34.375994Z","iopub.execute_input":"2022-04-23T12:43:34.376242Z","iopub.status.idle":"2022-04-23T12:43:36.667336Z","shell.execute_reply.started":"2022-04-23T12:43:34.376213Z","shell.execute_reply":"2022-04-23T12:43:36.666745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multivariate: <font color='#EC7063'>Single input</font> and <font color='#EC7063'>single output</font> when input series $\\neq$ output series","metadata":{"papermill":{"duration":0.062212,"end_time":"2022-01-12T08:04:37.14778","exception":false,"start_time":"2022-01-12T08:04:37.085568","status":"completed"},"tags":[]}},{"cell_type":"code","source":"input_cols = ['timestamp', 'feature_1']\noutput_cols = ['timestamp', 'feature_4']","metadata":{"papermill":{"duration":0.07009,"end_time":"2022-01-12T08:04:37.280645","exception":false,"start_time":"2022-01-12T08:04:37.210555","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:36.66856Z","iopub.execute_input":"2022-04-23T12:43:36.668924Z","iopub.status.idle":"2022-04-23T12:43:36.673028Z","shell.execute_reply.started":"2022-04-23T12:43:36.668895Z","shell.execute_reply":"2022-04-23T12:43:36.672107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_input_and_output_df(input_cols, output_cols)\n\nX, y = split_sequences(ts_df[input_cols].values, \n                       ts_df[output_cols].values, \n                       n_steps_in = 15, \n                       n_steps_out = 5, \n                       n_sliding_steps = 20, \n                       window_type='sliding')\n\nfig, ax = plt.subplots(nrows=X.shape[0], ncols=2, figsize=(15, 2.5*X.shape[0]))\nfig.suptitle(r'Multivariate: Single input and single output with input $\\neq$ output')\n\nfor i in range(X.shape[0]):\n    sns.lineplot(x=ts_df['timestamp'].values, y=ts_df['feature_1'].values, ax=ax[i, 0], color='lightgrey', marker='o')\n    sns.lineplot(x=ts_df['timestamp'].values, y=ts_df['feature_4'].values, ax=ax[i, 1], color='lightgrey', marker='o')\n\n    if i < X.shape[0]-1:\n        sns.lineplot(x=X[i][ :, 0], y=X[i][ :, 1].astype(float), ax=ax[i, 0], color='cornflowerblue', marker='o')\n        sns.lineplot(x=y[i][ :, 0], y=y[i][ :, 1].astype(float), ax=ax[i, 1], color='orange', marker='o')\n        ax[i, 0].set_title(f\"Input Sequence of Training Sample {i}\")\n        ax[i, 1].set_title(f\"Output Sequence of Training Sample {i}\")\n\n    else:\n        sns.lineplot(x=X[i][ :, 0], y=X[i][ :, 1].astype(float), ax=ax[i, 0], color='mediumseagreen', marker='o')\n        sns.lineplot(x=y[i][ :, 0], y=y[i][ :, 1].astype(float), ax=ax[i, 1], color='coral', marker='o')\n        ax[i, 0].set_title(f\"Input Sequence of Testing Sample\")    \n        ax[i, 1].set_title(f\"Output Sequence of Testing Sample (Prediction)\")    \n\n    ax[i, 0].set_xlim([date(2021, 1, 1), date(2021, 4, 11)])\n    ax[i, 1].set_xlim([date(2021, 1, 1), date(2021, 4, 11)])\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show() ","metadata":{"_kg_hide-input":true,"papermill":{"duration":2.273196,"end_time":"2022-01-12T08:04:39.617561","exception":false,"start_time":"2022-01-12T08:04:37.344365","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:36.674203Z","iopub.execute_input":"2022-04-23T12:43:36.674619Z","iopub.status.idle":"2022-04-23T12:43:39.152199Z","shell.execute_reply.started":"2022-04-23T12:43:36.674592Z","shell.execute_reply":"2022-04-23T12:43:39.151619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multivariate: <font color='#EC7063'>Multiple inputs</font> and <font color='#EC7063'>multiple outputs</font> when inputs series â‰  outputs series","metadata":{"papermill":{"duration":0.071413,"end_time":"2022-01-12T08:04:39.758828","exception":false,"start_time":"2022-01-12T08:04:39.687415","status":"completed"},"tags":[]}},{"cell_type":"code","source":"input_cols = ['timestamp', 'feature_1', 'feature_2']\noutput_cols = ['timestamp', 'feature_3', 'feature_4']","metadata":{"papermill":{"duration":0.07871,"end_time":"2022-01-12T08:04:39.906866","exception":false,"start_time":"2022-01-12T08:04:39.828156","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:39.154857Z","iopub.execute_input":"2022-04-23T12:43:39.155199Z","iopub.status.idle":"2022-04-23T12:43:39.15999Z","shell.execute_reply.started":"2022-04-23T12:43:39.155171Z","shell.execute_reply":"2022-04-23T12:43:39.158395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_input_and_output_df(input_cols, output_cols)\n\nX, y = split_sequences(ts_df[input_cols].values, \n                       ts_df[output_cols].values, \n                       n_steps_in = 15, \n                       n_steps_out = 5, \n                       n_sliding_steps = 20, \n                       window_type='sliding')\n\nfig, ax = plt.subplots(nrows=X.shape[0]*2, ncols=2, figsize=(15, 2.5*X.shape[0]*2))\nfig.suptitle(r'Multivariate: Single input and single output with input $\\neq$ output')\n\nfor i in range(X.shape[0]):\n    sns.lineplot(x=ts_df['timestamp'].values, y=ts_df['feature_1'].values, ax=ax[(i*2), 0], color='lightgrey', marker='o')\n    sns.lineplot(x=ts_df['timestamp'].values, y=ts_df['feature_2'].values, ax=ax[(i*2)+1, 0], color='lightgrey', marker='o')\n\n    sns.lineplot(x=ts_df['timestamp'].values, y=ts_df['feature_3'].values, ax=ax[(i*2), 1], color='lightgrey', marker='o')\n    sns.lineplot(x=ts_df['timestamp'].values, y=ts_df['feature_4'].values, ax=ax[(i*2)+1, 1], color='lightgrey', marker='o')\n\n    if i < X.shape[0]-1:\n        sns.lineplot(x=X[i][ :, 0], y=X[i][ :, 1].astype(float), ax=ax[(i*2), 0], color='cornflowerblue', marker='o')\n        sns.lineplot(x=X[i][ :, 0], y=X[i][ :, 2].astype(float), ax=ax[(i*2)+1, 0], color='cornflowerblue', marker='o')\n        sns.lineplot(x=y[i][ :, 0], y=y[i][ :, 1].astype(float), ax=ax[(i*2), 1], color='orange', marker='o')\n        sns.lineplot(x=y[i][ :, 0], y=y[i][ :, 2].astype(float), ax=ax[(i*2)+1, 1], color='orange', marker='o')\n        ax[(i*2), 0].set_title(f\"Input Sequence 1 of Training Sample {i}\")\n        ax[(i*2), 1].set_title(f\"Output Sequence 1 of Training Sample {i}\")\n\n        ax[(i*2)+1, 0].set_title(f\"Input Sequence 2 of Training Sample {i}\")\n        ax[(i*2)+1, 1].set_title(f\"Output Sequence 2 of Training Sample {i}\")\n\n    else:\n        #sns.lineplot(x=X[i][ :, 0], y=X[i][ :, 1].astype(float), ax=ax[(i*2), 0], color='mediumseagreen', marker='o')\n        #sns.lineplot(x=y[i][ :, 0], y=y[i][ :, 1].astype(float), ax=ax[(i*2)+1, 1], color='coral', marker='o')\n        \n        sns.lineplot(x=X[i][ :, 0], y=X[i][ :, 1].astype(float), ax=ax[(i*2), 0], color='mediumseagreen', marker='o')\n        sns.lineplot(x=X[i][ :, 0], y=X[i][ :, 2].astype(float), ax=ax[(i*2)+1, 0], color='mediumseagreen', marker='o')\n        sns.lineplot(x=y[i][ :, 0], y=y[i][ :, 1].astype(float), ax=ax[(i*2), 1], color='coral', marker='o')\n        sns.lineplot(x=y[i][ :, 0], y=y[i][ :, 2].astype(float), ax=ax[(i*2)+1, 1], color='coral', marker='o')\n        \n        ax[(i*2), 0].set_title(f\"Input Sequence 1 of Testing Sample {i}\")\n        ax[(i*2), 1].set_title(f\"Output Sequence 1 of Testing Sample {i} (Prediction)\")\n\n        ax[(i*2)+1, 0].set_title(f\"Input Sequence 2 of Testing Sample {i}\")\n        ax[(i*2)+1, 1].set_title(f\"Output Sequence 2 of Testing Sample {i} (Prediction)\")\n\n    ax[i, 0].set_xlim([date(2021, 1, 1), date(2021, 4, 11)])\n    ax[i, 1].set_xlim([date(2021, 1, 1), date(2021, 4, 11)])\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show() ","metadata":{"_kg_hide-input":true,"papermill":{"duration":4.193197,"end_time":"2022-01-12T08:04:44.170005","exception":false,"start_time":"2022-01-12T08:04:39.976808","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:39.161537Z","iopub.execute_input":"2022-04-23T12:43:39.161776Z","iopub.status.idle":"2022-04-23T12:43:42.383049Z","shell.execute_reply.started":"2022-04-23T12:43:39.161743Z","shell.execute_reply":"2022-04-23T12:43:42.38253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Variation II: <font color='#5499C7'>Length of Output Sequences</font>\nLet's look at the length of output sequences (Variation II). We will will keep the <font color='#EC7063'>number of input/output sequences</font> and the <font color='#45B39D'>type of input sequences</font> same for the following. \n\nWe will use a <font color='#EC7063'>univariate</font> problem with the <font color='#45B39D'>sliding window</font>  setting with a stepsize of 20 for this. \n\n### <font color='#5499C7'>Single Step</font> Output Sequence ","metadata":{"papermill":{"duration":0.077284,"end_time":"2022-01-12T08:04:44.324851","exception":false,"start_time":"2022-01-12T08:04:44.247567","status":"completed"},"tags":[]}},{"cell_type":"code","source":"input_cols = ['timestamp', 'feature_1']\noutput_cols = input_cols\n\nX, y = split_sequences(ts_df[input_cols].values, \n                       ts_df[output_cols].values, \n                       n_steps_in = 19, \n                       n_steps_out = 1, \n                       n_sliding_steps = 20, \n                       window_type='sliding')\n\nplot_time_series_problem(X, y)","metadata":{"_kg_hide-input":true,"papermill":{"duration":1.337901,"end_time":"2022-01-12T08:04:45.739823","exception":false,"start_time":"2022-01-12T08:04:44.401922","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:42.383956Z","iopub.execute_input":"2022-04-23T12:43:42.384666Z","iopub.status.idle":"2022-04-23T12:43:43.590228Z","shell.execute_reply.started":"2022-04-23T12:43:42.384642Z","shell.execute_reply":"2022-04-23T12:43:43.589284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color='#5499C7'>Multistep</font> Output Sequence ","metadata":{"papermill":{"duration":0.080144,"end_time":"2022-01-12T08:04:45.900183","exception":false,"start_time":"2022-01-12T08:04:45.820039","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X, y = split_sequences(ts_df[input_cols].values, \n                       ts_df[output_cols].values, \n                       n_steps_in = 15, \n                       n_steps_out = 5, \n                       n_sliding_steps = 20, \n                       window_type='sliding')\n\nplot_time_series_problem(X, y)","metadata":{"_kg_hide-input":true,"papermill":{"duration":1.391212,"end_time":"2022-01-12T08:04:47.37329","exception":false,"start_time":"2022-01-12T08:04:45.982078","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:43.592313Z","iopub.execute_input":"2022-04-23T12:43:43.5927Z","iopub.status.idle":"2022-04-23T12:43:44.588561Z","shell.execute_reply.started":"2022-04-23T12:43:43.592668Z","shell.execute_reply":"2022-04-23T12:43:44.5878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Variation III: <font color='#45B39D'>Type of Input Sequences</font>\n\nNow we will explore at the type of input sequences (<font color='#45B39D'>Variation III</font>). We will will keep the <font color='#EC7063'>number of input/output sequences</font> and <font color='#5499C7'>length of the output sequences</font> same for the following. \n\nWe will use a <font color='#EC7063'>univariate</font> problem with a <font color='#5499C7'>multistep</font> output sequence of 5 steps for this. \n\n\n### Input Type: <font color='#45B39D'>Sliding Window</font>","metadata":{"papermill":{"duration":0.08378,"end_time":"2022-01-12T08:04:47.541959","exception":false,"start_time":"2022-01-12T08:04:47.458179","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X, y = split_sequences(ts_df[input_cols].values, \n                       ts_df[output_cols].values, \n                       n_steps_in = 15, \n                       n_steps_out = 5, \n                       n_sliding_steps = 20, \n                       window_type='sliding')\n\nplot_time_series_problem(X, y)","metadata":{"_kg_hide-input":true,"papermill":{"duration":1.36283,"end_time":"2022-01-12T08:04:48.988404","exception":false,"start_time":"2022-01-12T08:04:47.625574","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:44.589489Z","iopub.execute_input":"2022-04-23T12:43:44.59122Z","iopub.status.idle":"2022-04-23T12:43:45.610096Z","shell.execute_reply.started":"2022-04-23T12:43:44.591132Z","shell.execute_reply":"2022-04-23T12:43:45.609166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Input Type: <font color='#45B39D'>Expanding Window</font>","metadata":{"papermill":{"duration":0.088348,"end_time":"2022-01-12T08:04:49.171936","exception":false,"start_time":"2022-01-12T08:04:49.083588","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X, y = split_sequences(ts_df[input_cols].values, \n                       ts_df[output_cols].values, \n                       n_steps_in = 15, \n                       n_steps_out = 5, \n                       n_sliding_steps = 20, \n                       window_type='expanding')\n\nplot_time_series_problem(X, y)","metadata":{"_kg_hide-input":true,"papermill":{"duration":1.393331,"end_time":"2022-01-12T08:04:50.654415","exception":false,"start_time":"2022-01-12T08:04:49.261084","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:45.611021Z","iopub.execute_input":"2022-04-23T12:43:45.611228Z","iopub.status.idle":"2022-04-23T12:43:46.631913Z","shell.execute_reply.started":"2022-04-23T12:43:45.611206Z","shell.execute_reply":"2022-04-23T12:43:46.63146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_____\n\n# Time Series Grouped Cross Validation\n### How to validate a model on chronologically ordered data which also contains groups?\n\nIt is of highly importance to be able to 'locally' estimate an indication of our model's performance. \nAn estimate that is independent of the (time expensive and limited) submission API. This allows for much better tuning of hyper-parameters or other aspects of the model's training process.\n\nOn the next chaper we will a couple of techniques that can be used to do this. For every step we will see that there is a problem with using it for this particular competition. Fortunately the last cells provides a solution! \n\n\n>**TL;DR: If you are not interested in an introduction in test and validation techniques, then skip to the bottom.** \n\nFirst up: train and test subsets.","metadata":{"papermill":{"duration":0.094723,"end_time":"2022-01-12T08:04:50.844107","exception":false,"start_time":"2022-01-12T08:04:50.749384","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"-----\n### Credit:\n- Based on the great notebook: https://www.kaggle.com/jorijnsmit/found-the-holy-grail-grouptimeseriessplit\n-----","metadata":{"execution":{"iopub.execute_input":"2021-11-04T09:31:44.045023Z","iopub.status.busy":"2021-11-04T09:31:44.044577Z","iopub.status.idle":"2021-11-04T09:31:44.050321Z","shell.execute_reply":"2021-11-04T09:31:44.049581Z","shell.execute_reply.started":"2021-11-04T09:31:44.044989Z"},"papermill":{"duration":0.094342,"end_time":"2022-01-12T08:04:51.035846","exception":false,"start_time":"2022-01-12T08:04:50.941504","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.093097,"end_time":"2022-01-12T08:04:51.229799","exception":false,"start_time":"2022-01-12T08:04:51.136702","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and Test Subsets\n\nThe first obvious step is to set apart some data which the model never gets to see. After the model has been trained, we use that unseen data to verify our model's predicitions. Scikit-learn's [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) makes the process of splitting datasets easy for us. Let's load our training data and set aside a test set:","metadata":{"papermill":{"duration":0.094087,"end_time":"2022-01-12T08:04:51.418528","exception":false,"start_time":"2022-01-12T08:04:51.324441","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport datatable as dt","metadata":{"_kg_hide-output":true,"papermill":{"duration":0.190102,"end_time":"2022-01-12T08:04:51.704958","exception":false,"start_time":"2022-01-12T08:04:51.514856","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:46.632712Z","iopub.execute_input":"2022-04-23T12:43:46.633129Z","iopub.status.idle":"2022-04-23T12:43:46.636741Z","shell.execute_reply.started":"2022-04-23T12:43:46.633106Z","shell.execute_reply":"2022-04-23T12:43:46.635646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtrain = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\ndtrain.head()\n","metadata":{"_kg_hide-output":true,"papermill":{"duration":22.502054,"end_time":"2022-01-12T08:05:14.302175","exception":false,"start_time":"2022-01-12T08:04:51.800121","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:46.637967Z","iopub.execute_input":"2022-04-23T12:43:46.638174Z","iopub.status.idle":"2022-04-23T12:43:49.351025Z","shell.execute_reply.started":"2022-04-23T12:43:46.638148Z","shell.execute_reply":"2022-04-23T12:43:49.350323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dlabels = dtrain[['Target']]\ndtrain = dtrain.drop(columns = 'Target')\ndtrain['date'] = pd.to_datetime(dtrain['Date']).dt.date\n\nprint(dtrain.columns)\nprint(dlabels.columns)","metadata":{"papermill":{"duration":10.186225,"end_time":"2022-01-12T08:05:24.797785","exception":false,"start_time":"2022-01-12T08:05:14.61156","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:49.351853Z","iopub.execute_input":"2022-04-23T12:43:49.352013Z","iopub.status.idle":"2022-04-23T12:43:50.16604Z","shell.execute_reply.started":"2022-04-23T12:43:49.351991Z","shell.execute_reply":"2022-04-23T12:43:50.16477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df,do_categoricals=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            if do_categoricals==True:\n                df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))   \n    return df\n\ndtrain = reduce_mem_usage(dtrain)","metadata":{"_kg_hide-input":true,"papermill":{"duration":3.6441,"end_time":"2022-01-12T08:05:28.53893","exception":false,"start_time":"2022-01-12T08:05:24.89483","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:50.167129Z","iopub.execute_input":"2022-04-23T12:43:50.167307Z","iopub.status.idle":"2022-04-23T12:43:50.461145Z","shell.execute_reply.started":"2022-04-23T12:43:50.167283Z","shell.execute_reply":"2022-04-23T12:43:50.459941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split \n\nx_train, x_test, y_train, y_test = train_test_split(\n    dtrain,\n    dlabels,\n    test_size=.25,\n    random_state=1,\n    shuffle=False\n)\n\nprint(x_train.shape, x_test.shape)\nprint(x_train.index)","metadata":{"papermill":{"duration":2.154825,"end_time":"2022-01-12T08:05:30.789448","exception":false,"start_time":"2022-01-12T08:05:28.634623","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:50.462701Z","iopub.execute_input":"2022-04-23T12:43:50.462959Z","iopub.status.idle":"2022-04-23T12:43:50.923603Z","shell.execute_reply.started":"2022-04-23T12:43:50.462922Z","shell.execute_reply":"2022-04-23T12:43:50.922642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use of `shuffle=False` is key here; since otherwise we would lose all chronological order.\n\nHowever, this approach is problematic because by constantly verifying on the same data, we also slowly start to overfit on the test set (\"leakage\"). Splitting the test set again into a validation set could solve this: the model's hyper-parameters are tuned and verified on the validation set and once that is completely finished we test it (only once!) on the test set. The problem now becomes that either the test and validation sets become too small to be useful or that so much data is used to validate and test that nog enough data remains to train on.","metadata":{"papermill":{"duration":0.094631,"end_time":"2022-01-12T08:05:30.979374","exception":false,"start_time":"2022-01-12T08:05:30.884743","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Cross-validation\n\nIn cross-validation (CV), multiple validation sets are derived from the training set. Every *fold* a new part of the training set is used as the vaildation set, and the data previously used for validation now becomes part of the training set again:\n\n![img](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n\n(Source: [scikit-learn's User Guide, Ch. 3.1](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-evaluating-estimator-performance).)","metadata":{"papermill":{"duration":0.094823,"end_time":"2022-01-12T08:05:31.170173","exception":false,"start_time":"2022-01-12T08:05:31.07535","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nfor train_idx, test_idx in KFold().split(x_train):\n    #print(train, test)\n    print(x_train.loc[train_idx, 'date'].index)\n    print(x_train.loc[test_idx, 'date'].index)\n    break","metadata":{"papermill":{"duration":3.908906,"end_time":"2022-01-12T08:05:35.175258","exception":false,"start_time":"2022-01-12T08:05:31.266352","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:50.924719Z","iopub.execute_input":"2022-04-23T12:43:50.924954Z","iopub.status.idle":"2022-04-23T12:43:51.112389Z","shell.execute_reply.started":"2022-04-23T12:43:50.924922Z","shell.execute_reply":"2022-04-23T12:43:51.111223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TimeSeriesSplit\n\nNote however, that in the first split shown above we are validating our *chronological* data on the past. We are training on trades starting from `358574` but testing on trades starting from `0`. In other words, our model has been trained using information which wasn't yet available at the time of the validation set. This is clear leakage; we are predicting the past with knowledge from the future. But our aim is to predict data in the future! This problem has already been addressed by scikit-learn in the form of [TimeSeriesSplit](https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split):\n![](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0101.png)\nBut what is the problem this time? `TimeSeriesSplit` does not respect the groups available in the data. Although not clearly visible in this plot, we can imagine that a group can partially fall in the training set and partially in the test set.","metadata":{"papermill":{"duration":0.095139,"end_time":"2022-01-12T08:05:35.366788","exception":false,"start_time":"2022-01-12T08:05:35.271649","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\n\nfor train_idx, test_idx in TimeSeriesSplit().split(x_train):\n    print(x_train.loc[train_idx, 'date'].unique())\n    print(x_train.loc[test_idx, 'date'].unique())\n    break","metadata":{"papermill":{"duration":1.294996,"end_time":"2022-01-12T08:05:36.757775","exception":false,"start_time":"2022-01-12T08:05:35.462779","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:51.113559Z","iopub.execute_input":"2022-04-23T12:43:51.113759Z","iopub.status.idle":"2022-04-23T12:43:51.196264Z","shell.execute_reply.started":"2022-04-23T12:43:51.113735Z","shell.execute_reply":"2022-04-23T12:43:51.195304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Already in the first split we can see data from day `44` present in both the training and test set. That would mean that we are training on half of the trades of a certain day, just to validate their performance on the other half of the trades of that day. What we of course want is to train on all trades of a particular day, and to validate them on the day that follows! Otherwise again leaking will occur.","metadata":{"papermill":{"duration":0.101591,"end_time":"2022-01-12T08:05:36.956442","exception":false,"start_time":"2022-01-12T08:05:36.854851","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### GroupKFold\n\n![](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0051.png)","metadata":{"papermill":{"duration":0.108903,"end_time":"2022-01-12T08:05:37.173168","exception":false,"start_time":"2022-01-12T08:05:37.064265","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\n\nfor train_idx, test_idx in GroupKFold().split(x_train, groups=x_train['date']):\n    print(x_train.loc[train_idx, 'date'].unique())\n    print(x_train.loc[test_idx, 'date'].unique())\n    break","metadata":{"papermill":{"duration":15.988932,"end_time":"2022-01-12T08:05:53.258929","exception":false,"start_time":"2022-01-12T08:05:37.269997","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:51.197371Z","iopub.execute_input":"2022-04-23T12:43:51.19789Z","iopub.status.idle":"2022-04-23T12:43:52.184131Z","shell.execute_reply.started":"2022-04-23T12:43:51.197864Z","shell.execute_reply":"2022-04-23T12:43:52.183117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The [GroupKFold](https://scikit-learn.org/stable/modules/cross_validation.html#group-k-fold) iterator does respect groupings: no group will ever be part of two folds. Unfortunately, it is also clear that it mixes up the order completely and thus loses the temporal dimension again. What we need is a a crossover between `GroupKFold` and `TimeSeriesSplit`: `GroupTimesSeriesSplit`.","metadata":{"papermill":{"duration":0.097376,"end_time":"2022-01-12T08:05:53.455134","exception":false,"start_time":"2022-01-12T08:05:53.357758","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### GroupTimesSeriesSplit\n\nOK, so this iterator does not exist yet in scikit-learn. However, a request for it has been documented on GitHub over a year ago ([Feature request: Group aware Time-based cross validation #14257](https://github.com/scikit-learn/scikit-learn/issues/14257)) and is almost ready for release. Thanks to open source we can take a sneak peek already! \n\n**Do note that this is not fully reviewed yet!!!** This might be the final code that it will make it into `sklearn`'s version `0.24` as a major feature, but there's also a chance of bugs still being present.\n\nI did not write *any* of this but it did take me a good day of research and trying to write it myself. All credits go to [@getgaurav2](https://github.com/getgaurav2/).\n\nHere are some more attempts at grouped cross-validation I encountered in my research:\n- https://stackoverflow.com/questions/51963713/cross-validation-for-grouped-time-series-panel-data\n- https://datascience.stackexchange.com/questions/77684/time-series-grouped-cross-validation\n- https://nander.cc/writing-custom-cross-validation-methods-grid-search","metadata":{"papermill":{"duration":0.098923,"end_time":"2022-01-12T08:05:53.653079","exception":false,"start_time":"2022-01-12T08:05:53.554156","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\nclass GroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_size : int, default=None\n        Maximum size for a single training set.\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\\\n                           'b', 'b', 'b', 'b', 'b',\\\n                           'c', 'c', 'c', 'c',\\\n                           'd', 'd', 'd'])\n    >>> gtss = GroupTimeSeriesSplit(n_splits=3)\n    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):\n    ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n    ...     print(\"TRAIN GROUP:\", groups[train_idx],\\\n                  \"TEST GROUP:\", groups[test_idx])\n    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\\\n    TEST GROUP: ['b' 'b' 'b' 'b' 'b']\n    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\\\n    TEST GROUP: ['c' 'c' 'c' 'c']\n    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\\\n    TEST: [15, 16, 17]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\\\n    TEST GROUP: ['d' 'd' 'd']\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_size=None\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_size = max_train_size\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n        group_test_size = n_groups // n_folds\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n            for train_group_idx in unique_groups[:group_test_start]:\n                train_array_tmp = group_dict[train_group_idx]\n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n            train_end = train_array.size\n            if self.max_train_size and self.max_train_size < train_end:\n                train_array = train_array[train_end -\n                                          self.max_train_size:train_end]\n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n            yield [int(i) for i in train_array], [int(i) for i in test_array]","metadata":{"papermill":{"duration":0.115333,"end_time":"2022-01-12T08:05:53.867181","exception":false,"start_time":"2022-01-12T08:05:53.751848","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:52.185561Z","iopub.execute_input":"2022-04-23T12:43:52.185821Z","iopub.status.idle":"2022-04-23T12:43:52.204408Z","shell.execute_reply.started":"2022-04-23T12:43:52.185788Z","shell.execute_reply":"2022-04-23T12:43:52.202885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfor idx, (train_idx, test_idx) in enumerate(GroupTimeSeriesSplit().split(x_train, groups=x_train['date'])):\n    print('-' * 80)\n    print('Fold: ', idx)\n    print(x_train.loc[train_idx, 'date'].unique())\n    print(x_train.loc[test_idx, 'date'].unique())\n    print('-' * 80)\n\"\"\"","metadata":{"papermill":{"duration":0.10543,"end_time":"2022-01-12T08:05:54.072681","exception":false,"start_time":"2022-01-12T08:05:53.967251","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:52.206296Z","iopub.execute_input":"2022-04-23T12:43:52.20661Z","iopub.status.idle":"2022-04-23T12:43:52.221473Z","shell.execute_reply.started":"2022-04-23T12:43:52.206577Z","shell.execute_reply":"2022-04-23T12:43:52.220992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# The Best Time Series Cross Validation\n\n> \"There are many different ways one can do cross-validation, and **it is the most critical step when building a good machine learning model** which is generalizable when it comes to unseen data.\"\n-- **Approaching (Almost) Any Machine Learning Problem**, by Abhishek Thakur\n\nCV is the **first** step, but very few notebooks are talking about this. Here we look at \"purged rolling time series CV\" and actually apply it in hyperparameter tuning for a basic estimator. This notebook owes a debt of gratitude to the notebook [\"Found the Holy Grail GroupTimeSeriesSplit\"](https://www.kaggle.com/jorijnsmit/found-the-holy-grail-grouptimeseriessplit). That notebook is excellent and this solution is an extention of the quoted pending sklearn estimator. I modify that estimator to make it more suitable for the task at hand in this competition. The changes are\n\n- you can specify a **gap** between each train and validation split. This is important because even though the **group** aspect keeps whole days together, we suspect that the anonymized features have some kind of lag or window calculations in them (which would be standard for financial features). By introducing a gap, we mitigate the risk that we leak information from train into validation\n- we can specify the size of the train and validation splits in terms of **number of days**. The ability to specify a validation set size is new and the the ability to specify days, as opposed to samples, is new.\n\nThe code for `PurgedTimeSeriesSplit` is below. I've hiden it becaused it is really meant to act as an imported class. If you want to see the code and copy for your work, click on the \"Code\" box.","metadata":{"papermill":{"duration":0.098664,"end_time":"2022-01-12T08:05:54.269922","exception":false,"start_time":"2022-01-12T08:05:54.171258","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.utils.validation import _deprecate_positional_args\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n\n# modified code for group gaps; source\n# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\nclass PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Allows for a gap in groups to avoid potentially leaking info from\n    train into test if the model has windowed or lag features.\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 verbose=False\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n        self.verbose = verbose\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n\n        group_test_size = min(n_groups // n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n                \n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n\n            train_end = train_array.size\n \n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n\n            test_array  = test_array[group_gap:]\n            \n            \n            if self.verbose > 0:\n                    pass\n                    \n            yield [int(i) for i in train_array], [int(i) for i in test_array]","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","papermill":{"duration":0.117033,"end_time":"2022-01-12T08:05:54.485232","exception":false,"start_time":"2022-01-12T08:05:54.368199","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:52.222457Z","iopub.execute_input":"2022-04-23T12:43:52.223016Z","iopub.status.idle":"2022-04-23T12:43:52.248533Z","shell.execute_reply.started":"2022-04-23T12:43:52.222985Z","shell.execute_reply":"2022-04-23T12:43:52.247573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To show the general idea, we generate some simple grouped data. Imagine we have a dataset of 2,000 samples which below to 20 groups.","metadata":{"papermill":{"duration":0.098443,"end_time":"2022-01-12T08:05:54.683764","exception":false,"start_time":"2022-01-12T08:05:54.585321","status":"completed"},"tags":[]}},{"cell_type":"code","source":"n_samples = 2000\nn_groups = 20\nassert n_samples % n_groups == 0\n\nidx = np.linspace(0, n_samples-1, num=n_samples)\nX_train = np.random.random(size=(n_samples, 5))\ny_train = np.random.choice([0, 1], n_samples)\ngroups = np.repeat(np.linspace(0, n_groups-1, num=n_groups), n_samples/n_groups)","metadata":{"papermill":{"duration":0.110068,"end_time":"2022-01-12T08:05:54.893412","exception":false,"start_time":"2022-01-12T08:05:54.783344","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:52.249646Z","iopub.execute_input":"2022-04-23T12:43:52.250261Z","iopub.status.idle":"2022-04-23T12:43:52.275978Z","shell.execute_reply.started":"2022-04-23T12:43:52.250229Z","shell.execute_reply":"2022-04-23T12:43:52.275179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# this is code slightly modified from the sklearn docs here:\n# https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py\ndef plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n    \n    cmap_cv = plt.cm.coolwarm\n\n    jet = plt.cm.get_cmap('jet', 256)\n    seq = np.linspace(0, 1, 256)\n    _ = np.random.shuffle(seq)   # inplace\n    cmap_data = ListedColormap(jet(seq))\n\n    # Generate the training/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n        # Fill in indices with the training/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n                   vmin=-.2, vmax=1.2)\n\n    # Plot the data classes and groups at the end\n    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n               c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n\n    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n               c=group, marker='_', lw=lw, cmap=cmap_data)\n\n    # Formatting\n    yticklabels = list(range(n_splits)) + ['target', 'day']\n    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n           xlabel='Sample index', ylabel=\"CV iteration\",\n           ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n    return ax","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.113845,"end_time":"2022-01-12T08:05:55.106419","exception":false,"start_time":"2022-01-12T08:05:54.992574","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:52.281158Z","iopub.execute_input":"2022-04-23T12:43:52.281403Z","iopub.status.idle":"2022-04-23T12:43:52.295652Z","shell.execute_reply.started":"2022-04-23T12:43:52.281379Z","shell.execute_reply":"2022-04-23T12:43:52.295008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's again imagine we want to do\n- a rolling time series split\n- where we have a gap of 2 days between train and validation sets\n- and we make the maximum size of each train set to be 7 days\n\nHere we specify the number of splits, the maximum number of groups in each train set, and the maximum number of groups in each valdiation set (sklearn has this convention where they call it the \"test\" set; I preserve that in the variable names, but prefer to call it the validation set).","metadata":{"papermill":{"duration":0.098398,"end_time":"2022-01-12T08:05:55.303596","exception":false,"start_time":"2022-01-12T08:05:55.205198","status":"completed"},"tags":[]}},{"cell_type":"code","source":"fig, ax = plt.subplots()\n\ncv = PurgedGroupTimeSeriesSplit(\n    n_splits=5,\n    max_train_group_size=7,\n    group_gap=2,\n    max_test_group_size=3\n)\n\nplot_cv_indices(cv, X_train, y_train, groups, ax, 5, lw=20);","metadata":{"papermill":{"duration":0.439858,"end_time":"2022-01-12T08:05:55.84163","exception":false,"start_time":"2022-01-12T08:05:55.401772","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T12:43:52.297025Z","iopub.execute_input":"2022-04-23T12:43:52.297281Z","iopub.status.idle":"2022-04-23T12:43:52.592125Z","shell.execute_reply.started":"2022-04-23T12:43:52.29725Z","shell.execute_reply":"2022-04-23T12:43:52.591638Z"},"trusted":true},"execution_count":null,"outputs":[]}]}