{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objs as go\n\nimport plotly\nplotly.offline.init_notebook_mode(connected=True)\n\n#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-06T21:33:04.237678Z","iopub.execute_input":"2022-05-06T21:33:04.238474Z","iopub.status.idle":"2022-05-06T21:33:06.376829Z","shell.execute_reply.started":"2022-05-06T21:33:04.238343Z","shell.execute_reply":"2022-05-06T21:33:06.37615Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center style=\"font-family:verdana;\"><h1 style=\"font-size:200%; padding: 10px; background: #8FBC8F;\"><b style=\"color:black;\">BLEU Score(Bilingual Evaluation Understudy)</b></h1></center>\n\n\"BLEU (Bilingual Evaluation Understudy) is a measurement of the differences between an automatic translation and one or more human-created reference translations of the same source sentence.\"\n\nScoring process\n\n\"The BLEU algorithm compares consecutive phrases of the automatic translation with the consecutive phrases it finds in the reference translation, and counts the number of matches, in a weighted fashion. These matches are position independent. A higher match degree indicates a higher degree of similarity with the reference translation, and higher score. Intelligibility and grammatical correctness are not taken into account.\"\n\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/translator/custom-translator/what-is-bleu-score","metadata":{}},{"cell_type":"markdown","source":"![](https://www.journaldev.com/wp-content/uploads/2020/12/bleu_score.png)journaldev.com","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/machine-translation-mbr-with-neural-metrics/mbr_neural_metrics/de-en/newstest2021/human_eval/mqm_mbr_deen.tsv', sep='\\t', error_bad_lines=False,warn_bad_lines=False)\ndf.tail()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:04:14.523307Z","iopub.execute_input":"2022-05-06T22:04:14.524038Z","iopub.status.idle":"2022-05-06T22:04:14.57675Z","shell.execute_reply.started":"2022-05-06T22:04:14.523999Z","shell.execute_reply":"2022-05-06T22:04:14.575942Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Sentence BLEU Score\n\n\"NLTK provides the sentence_bleu() function for evaluating a candidate sentence against one or more reference sentences.\"\n\n\"The reference sentences must be provided as a list of sentences where each reference is a list of tokens. The candidate sentence is provided as a list of tokens. For example:\"\n\nhttps://machinelearningmastery.com/calculate-bleu-score-for-text-python/","metadata":{}},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['the', 'new', 'season', 'starts'], ['new' 'season', 'starts']]\ncandidate = ['the', 'new', 'season', 'starts']\nscore = sentence_bleu(reference, candidate)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:37:38.651434Z","iopub.execute_input":"2022-05-06T21:37:38.652054Z","iopub.status.idle":"2022-05-06T21:37:39.429015Z","shell.execute_reply.started":"2022-05-06T21:37:38.652015Z","shell.execute_reply":"2022-05-06T21:37:39.428099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Comparing German with English","metadata":{}},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['Die', 'Corona', 'krise', 'hat', 'verhindert'], ['Corona', 'krise', 'hat', 'verhindert']]\ncandidate = ['The', 'Corona', 'crisis', 'has', 'prevented']\nscore = sentence_bleu(reference, candidate)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:48:16.657767Z","iopub.execute_input":"2022-05-06T21:48:16.658634Z","iopub.status.idle":"2022-05-06T21:48:16.663925Z","shell.execute_reply.started":"2022-05-06T21:48:16.658564Z","shell.execute_reply":"2022-05-06T21:48:16.663035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#German with German","metadata":{}},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['Mit', 'viel', 'Verzögerung', 'startet'], ['Mit','Verzögerung', 'startet']]\ncandidate = ['Mit ', 'fast', 'einem', 'halben','Jahr' 'Verzögerung', 'startet']\nscore = sentence_bleu(reference, candidate)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T21:55:55.256507Z","iopub.execute_input":"2022-05-06T21:55:55.256822Z","iopub.status.idle":"2022-05-06T21:55:55.262802Z","shell.execute_reply.started":"2022-05-06T21:55:55.256785Z","shell.execute_reply":"2022-05-06T21:55:55.261878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Corpus BLEU Score\n\n\"NLTK also provides a function called corpus_bleu() for calculating the BLEU score for multiple sentences such as a paragraph or a document.\"\n\n\"The references must be specified as a list of documents where each document is a list of references and each alternative reference is a list of tokens, e.g. a list of lists of lists of tokens. The candidate documents must be specified as a list where each document is a list of tokens, e.g. a list of lists of tokens.\"\n\n\"This is a little confusing; here is an example of two references for one document.\"\n\nhttps://machinelearningmastery.com/calculate-bleu-score-for-text-python/","metadata":{}},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\n# two references for one document\nfrom nltk.translate.bleu_score import corpus_bleu\nreferences = [[['the', 'main', 'focus', 'was', 'to', 'get'], ['main', 'focus', 'was', 'get']]]\ncandidates = [['the', 'main', 'focus', 'was', 'to', 'get']]\nscore = corpus_bleu(references, candidates)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:02:29.737449Z","iopub.execute_input":"2022-05-06T22:02:29.737859Z","iopub.status.idle":"2022-05-06T22:02:29.743238Z","shell.execute_reply.started":"2022-05-06T22:02:29.737823Z","shell.execute_reply":"2022-05-06T22:02:29.742673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Bilingual Evaluation Understudy Score\n\nBy Jason Brownlee Last Updated on December 19, 2019 (A Gentle Introduction to Calculating the BLEU Score for Text in Python)\n\n\"The Bilingual Evaluation Understudy Score, or BLEU for short, is a metric for evaluating a generated sentence to a reference sentence.\"\n\n\"A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.\"\n\n\"The score was developed for evaluating the predictions made by automatic machine translation systems. It is not perfect, but does offer 5 compelling benefits:\"\n\nIt is quick and inexpensive to calculate.\n\nIt is easy to understand.\n\nIt is language independent.\n\nIt correlates highly with human evaluation.\n\nIt has been widely adopted.\n\nThe BLEU score was proposed by Kishore Papineni, et al. in their 2002 paper “BLEU: a Method for Automatic Evaluation of Machine Translation“.\n\n\"The approach works by counting matching n-grams in the candidate translation to n-grams in the reference text, where 1-gram or unigram would be each token and a bigram comparison would be each word pair. The comparison is made regardless of word order.\"\n\nhttps://machinelearningmastery.com/calculate-bleu-score-for-text-python/","metadata":{}},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\n# two references for one document\nfrom nltk.translate.bleu_score import corpus_bleu\nreferences = [[['Sie', 'sind', 'so', 'klein'], ['Sie', 'sind', 'klein']]]\ncandidates = [['Sie', 'sind', 'so', 'winzig']]\nscore = corpus_bleu(references, candidates)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:08:38.238018Z","iopub.execute_input":"2022-05-06T22:08:38.238319Z","iopub.status.idle":"2022-05-06T22:08:38.243781Z","shell.execute_reply.started":"2022-05-06T22:08:38.238275Z","shell.execute_reply":"2022-05-06T22:08:38.243212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Individual N-Gram Scores\n\n\"An individual N-gram score is the evaluation of just matching grams of a specific order, such as single words (1-gram) or word pairs (2-gram or bigram).\"\n\n\"The weights are specified as a tuple where each index refers to the gram order. To calculate the BLEU score only for 1-gram matches, you can specify a weight of 1 for 1-gram and 0 for 2, 3 and 4 (1, 0, 0, 0). For example:\"\n\nhttps://machinelearningmastery.com/calculate-bleu-score-for-text-python/","metadata":{}},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\n# 1-gram individual BLEU\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['Ihre', 'Haut', 'ist', 'manchmal', 'so', 'zart']]\ncandidate = ['Ihre', 'Haut', 'ist', 'zart']\nscore = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:12:50.889857Z","iopub.execute_input":"2022-05-06T22:12:50.890135Z","iopub.status.idle":"2022-05-06T22:12:50.895705Z","shell.execute_reply.started":"2022-05-06T22:12:50.890102Z","shell.execute_reply":"2022-05-06T22:12:50.895051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Decoding and Diversity in Machine Translation\n\nAuthors: Nicholas Roberts, Davis Liang, Graham Neubig, Zachary C. Lipton\n\n\"Neural Machine Translation (NMT) systems are typically evaluated using automated metrics that assess the agreement between generated translations and ground truth candidates. To improve systems with respect to these metrics, NLP researchers employ a variety of heuristic techniques, including searching for the conditional mode (vs. sampling) and incorporating various training heuristics (e.g., label smoothing).\"\n\n\"While search strategies significantly improve BLEU score, they yield deterministic outputs that lack the diversity of human translations. Moreover, search can amplify socially problematic biases in the data, as has been observed in machine translation of gender pronouns. This makes human-level BLEU a misleading benchmark; modern MT systems cannot approach human-level BLEU while simultaneously maintaining\nhuman-level translation diversity.\"\n\n\"In this paper, the authors characterized distributional differences between generated and real translations, examining the cost in diversity paid for the BLEU scores enjoyed by NMT. Moreover, their study implicated search as a salient source of known bias when translating gender pronouns.\"\n\nhttps://arxiv.org/pdf/2011.13477.pdf","metadata":{}},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\n# n-gram individual BLEU\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['Ihre', 'Haut', 'ist', 'manchmal', 'so', 'zart']]\ncandidate = ['Ihre', 'Haut', 'ist', 'zart']\nprint('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\nprint('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\nprint('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\nprint('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:15:28.774014Z","iopub.execute_input":"2022-05-06T22:15:28.774281Z","iopub.status.idle":"2022-05-06T22:15:28.782369Z","shell.execute_reply.started":"2022-05-06T22:15:28.774253Z","shell.execute_reply":"2022-05-06T22:15:28.781681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Cumulative N-Gram Scores\n\n\"Cumulative scores refer to the calculation of individual n-gram scores at all orders from 1 to n and weighting them by calculating the weighted geometric mean.\"\n\n\"By default, the sentence_bleu() and corpus_bleu() scores calculate the cumulative 4-gram BLEU score, also called BLEU-4.\"\n\n\"The weights for the BLEU-4 are 1/4 (25%) or 0.25 for each of the 1-gram, 2-gram, 3-gram and 4-gram scores. For example:\"\n\nhttps://machinelearningmastery.com/calculate-bleu-score-for-text-python/","metadata":{}},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\n# 4-gram cumulative BLEU\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['Und', 'sie', 'sind', 'so', 'winzig']]\ncandidate = ['Sie', 'sind','winzig']\nscore = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:19:20.273121Z","iopub.execute_input":"2022-05-06T22:19:20.273384Z","iopub.status.idle":"2022-05-06T22:19:20.27938Z","shell.execute_reply.started":"2022-05-06T22:19:20.273357Z","shell.execute_reply":"2022-05-06T22:19:20.278448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"The cumulative and individual 1-gram BLEU use the same weights, e.g. (1, 0, 0, 0). The 2-gram weights assign a 50% to each of 1-gram and 2-gram and the 3-gram weights are 33% for each of the 1, 2 and 3-gram scores.\"\n\n\"Let’s make this concrete by calculating the cumulative scores for BLEU-1, BLEU-2, BLEU-3 and BLEU-4:\"\n\n#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/","metadata":{}},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\n# cumulative BLEU scores\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['Und', 'sie', 'sind', 'so', 'winzig']]\ncandidate = ['Sie', 'sind','winzig']\nprint('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\nprint('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\nprint('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\nprint('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:33:15.042536Z","iopub.execute_input":"2022-05-06T22:33:15.043238Z","iopub.status.idle":"2022-05-06T22:33:15.050773Z","shell.execute_reply.started":"2022-05-06T22:33:15.043197Z","shell.execute_reply":"2022-05-06T22:33:15.050143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#A perfect score. A perfect match","metadata":{}},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\n# prefect match\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['Sie', 'sind', 'so', 'klein', 'und', 'so', 'früh', 'geboren']]\ncandidate = ['Sie', 'sind', 'so', 'klein', 'und', 'so', 'früh', 'geboren']\nscore = sentence_bleu(reference, candidate)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:37:15.3982Z","iopub.execute_input":"2022-05-06T22:37:15.398461Z","iopub.status.idle":"2022-05-06T22:37:15.405035Z","shell.execute_reply.started":"2022-05-06T22:37:15.398434Z","shell.execute_reply":"2022-05-06T22:37:15.404126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Let’s change one word","metadata":{}},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\n# one word different\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['Sie', 'sind', 'so', 'klein', 'und', 'so', 'früh', 'geboren']]\ncandidate = ['Sie', 'sind', 'so', 'winzig', 'und', 'so', 'früh', 'geboren']\nscore = sentence_bleu(reference, candidate)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:39:50.601265Z","iopub.execute_input":"2022-05-06T22:39:50.601545Z","iopub.status.idle":"2022-05-06T22:39:50.607515Z","shell.execute_reply.started":"2022-05-06T22:39:50.601515Z","shell.execute_reply":"2022-05-06T22:39:50.606689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Changing two words","metadata":{}},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\n# two words different\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['Sie', 'sind', 'so', 'klein', 'und', 'so', 'früh', 'geboren']]\ncandidate = ['Sie', 'sind', 'so', 'winzig', 'und', 'so', 'spät', 'geboren']\nscore = sentence_bleu(reference, candidate)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:44:38.906975Z","iopub.execute_input":"2022-05-06T22:44:38.907251Z","iopub.status.idle":"2022-05-06T22:44:38.913064Z","shell.execute_reply.started":"2022-05-06T22:44:38.907221Z","shell.execute_reply":"2022-05-06T22:44:38.912391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#All words are different in the candidate","metadata":{}},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\n# all words different\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['Sie', 'sind', 'so', 'klein', 'und', 'so', 'früh', 'geboren']]\ncandidate = ['Ihre', 'Haut', 'ist', 'manchmal', 'zart']\nscore = sentence_bleu(reference, candidate)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:48:21.789557Z","iopub.execute_input":"2022-05-06T22:48:21.789971Z","iopub.status.idle":"2022-05-06T22:48:21.795342Z","shell.execute_reply.started":"2022-05-06T22:48:21.789941Z","shell.execute_reply":"2022-05-06T22:48:21.794602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#A candidate that has fewer words than the reference","metadata":{}},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\n# shorter candidate\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['Sie', 'sind', 'so', 'klein', 'und', 'so', 'früh', 'geboren']]\ncandidate = ['Ihre', 'Haut', 'ist', 'manchmal', 'so', 'zart']\nscore = sentence_bleu(reference, candidate)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:51:40.649943Z","iopub.execute_input":"2022-05-06T22:51:40.650193Z","iopub.status.idle":"2022-05-06T22:51:40.657022Z","shell.execute_reply.started":"2022-05-06T22:51:40.650168Z","shell.execute_reply":"2022-05-06T22:51:40.656411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Making the candidate two words longer than the reference","metadata":{}},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\n# longer candidate\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['Sie', 'sind', 'klein', 'und', 'früh', 'geboren']]\ncandidate = ['Sie', 'sind', 'so', 'klein', 'und', 'so', 'früh', 'geboren']\nscore = sentence_bleu(reference, candidate)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:54:01.396265Z","iopub.execute_input":"2022-05-06T22:54:01.396528Z","iopub.status.idle":"2022-05-06T22:54:01.402043Z","shell.execute_reply.started":"2022-05-06T22:54:01.396501Z","shell.execute_reply":"2022-05-06T22:54:01.401247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\n# very short\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['Sie', 'sind', 'so', 'klein', 'und', 'so', 'früh', 'geboren']]\ncandidate = ['Sie', 'sind', 'klein']\nscore = sentence_bleu(reference, candidate)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:57:09.784554Z","iopub.execute_input":"2022-05-06T22:57:09.785075Z","iopub.status.idle":"2022-05-06T22:57:09.790375Z","shell.execute_reply.started":"2022-05-06T22:57:09.785041Z","shell.execute_reply":"2022-05-06T22:57:09.789638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Calculating the Bilingual Evaluation Understudy (BLEU) score\n\nCloistered Monkey 2021-02-11 19:51 \n\n\"The author will be implementing a popular metric for evaluating the quality of machine-translated text: the BLEU score proposed by Kishore Papineni, et al. In their 2002 paper \"BLEU: a Method for Automatic Evaluation of Machine Translation\", the BLEU score works by comparing \"candidate\" text to one or more \"reference\" translations. The result is better the closer the score is to 1. \"\n\nhttps://necromuralist.github.io/Neurotic-Networking/posts/nlp/bleu-score/","metadata":{}},{"cell_type":"code","source":"#Code by https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n\n# very very short\nfrom nltk.translate.bleu_score import sentence_bleu\nreference = [['Sie', 'sind', 'so', 'klein', 'und', 'so', 'früh', 'geboren']]\ncandidate = ['Sie']\nscore = sentence_bleu(reference, candidate)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:58:15.232197Z","iopub.execute_input":"2022-05-06T22:58:15.232463Z","iopub.status.idle":"2022-05-06T22:58:15.239226Z","shell.execute_reply.started":"2022-05-06T22:58:15.232429Z","shell.execute_reply":"2022-05-06T22:58:15.238378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Acknowledgement:\n\nA Gentle Introduction to Calculating the BLEU Score for Text in Python - By Jason Brownlee on November 20, 2017 \n\n https://machinelearningmastery.com/calculate-bleu-score-for-text-python/  ","metadata":{"execution":{"iopub.status.busy":"2022-05-06T22:59:07.400957Z","iopub.execute_input":"2022-05-06T22:59:07.401721Z","iopub.status.idle":"2022-05-06T22:59:07.406591Z","shell.execute_reply.started":"2022-05-06T22:59:07.401688Z","shell.execute_reply":"2022-05-06T22:59:07.406049Z"}}}]}