{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Cars Prices with Multiple Linear Regression and RFE"},{"metadata":{},"cell_type":"markdown","source":"## Problem Description\nA Chinese automobile company **Teclov_chinese** aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts. They have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. Essentially, the company wants to know:\n\n• Which variables are significant in predicting the price of a car\n\n• How well those variables describe the price of a car Based on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the American market.\n\n## Business Goal\n\nYou are required to model the price of cars with the available independent\nvariables. It will be used by the management to understand how exactly\nthe prices vary with the independent variables. They can accordingly\nmanipulate the design of the cars, the business strategy etc. to meet certain\nprice levels. Further, the model will be a good way for the management to\nunderstand the pricing dynamics of a new market.\nData Preparation\n\n• There is a variable named CarName which is comprised of two parts:\n- the first word is the name of 'car company' and the second is the 'car\nmodel'. For example, chevrolet impala has 'chevrolet' as the car\ncompany name and 'impala' as the car model name. You need to\nconsider only company name as the independent variable for model\nbuilding."},{"metadata":{},"cell_type":"markdown","source":"   **1. Cleaning Data**\n   \n   **2. Exploratory of Data**\n   \n   **3. Data preparation**\n   \n   **4. Model Construction**\n   \n   **5. Prediction & Evaluation**\n   \n   **6. RFE**\n   \n   **7. Final Summary**\n   \n   **8. Next step ?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n#importing the libraries\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (20.0, 10.0)\nimport seaborn as sns\n\n#pour afficher tous les colonnes d'un tableau\npd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/CarPrice_Assignment.csv')\nprint(\"Dimension of our data set is: \")\nprint(data.shape)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**** Dataset is clean and no substitution of Null values is required ****\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Cleaning Data\n"},{"metadata":{},"cell_type":"markdown","source":"**1. Separate the CarName variable to two columns: CompanyName and CarModel**"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Chaque élément du colonne CarName sera diviser en deux String, et on va garder seulement le premier\nCompanyName = data['CarName'].apply(lambda x : x.split(' ')[0])\n\n#Insérer la nouvelle variable comme colonne dans notre dataset\ndata.insert(3,\"CompanyName\",CompanyName)\n\n#Supprimer la colonne CarModel\ndata.drop(['CarName'],axis=1,inplace=True)\n\n#Supprimer la colonne CarID, car elle n'a aucune effet sur notre dataset\ndata.drop(['car_ID'],axis=1,inplace=True)\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Examine categorical variables and correct them if spelling errors are found**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_variable_type(element) :\n    \"\"\"\n     Vérifier que les colonnes sont de variable continue ou catégorique.\n     L'hypothèse est que si:\n                  nombre unique <20 alors on suppose c'est catégorique\n                  nombre unique> = 20 et dtype = [int64 ou float64] alors on suppose c'est continu\n     \"\"\"\n    if element==0:\n        return \"Not Known\"\n    elif element < 20 and element!=0 :\n        return \"Categorical\"\n    elif element >= 20 and element!=0 :\n        return \"Contineous\"\n    \ndef predict_variable_type(metadata_matrix):\n    metadata_matrix[\"Variable_Type\"] = metadata_matrix[\"Valeurs_Uniques_Count\"].apply(get_variable_type).astype(str)\n    metadata_matrix[\"frequency\"] = metadata_matrix[\"Null_Count\"] - metadata_matrix[\"Null_Count\"]\n    metadata_matrix[\"frequency\"].astype(int)\n    return metadata_matrix \n\ndef get_meta_data(dataframe) :\n    \"\"\"\n     Méthode pour obtenir des métadonnées sur n'importe quel dataset transmis\n    \"\"\"\n    metadata_matrix = pd.DataFrame({\n                    'Datatype' : dataframe.dtypes.astype(str), # types de données de colonnes\n                    'Non_Null_Count': dataframe.count(axis = 0).astype(int), # nombre total d'éléments dans les colonnes\n                    'Null_Count': dataframe.isnull().sum().astype(int), # total des valeurs nulles dans les colonnes\n                    'Null_Percentage': dataframe.isnull().sum()/len(dataframe) * 100, # pourcentage de valeurs nulles\n                    'Valeurs_Uniques_Count': dataframe.nunique().astype(int) # nombre de valeurs uniques\n                     })\n    \n    metadata_matrix = predict_variable_type(metadata_matrix)\n    return metadata_matrix\n\ndef list_potential_categorical_type(dataframe,data) :\n    print(\"*********colonnes de type de données catégoriques potentielles*********\")\n    metadata_matrix_categorical = dataframe[dataframe[\"Variable_Type\"] == \"Categorical\"]\n    \n    length = len(metadata_matrix_categorical)\n    if length == 0 :\n        print(\"Aucune colonne catégorique dans un jeu de données donné.\")  \n    else :    \n        metadata_matrix_categorical = metadata_matrix_categorical.filter([\"Datatype\",\"Valeurs_Uniques_Count\"])\n        metadata_matrix_categorical.sort_values([\"Valeurs_Uniques_Count\"], axis=0,ascending=False, inplace=True)\n        col_to_check = metadata_matrix_categorical.index.tolist()\n        name_list = []\n        values_list = []\n        \n        for name in col_to_check :\n            name_list.append(name)\n            values_list.append(data[name].unique())\n        \n        temp = pd.DataFrame({\"index\":name_list,\"Valeurs_Uniques\":values_list})\n        metadata_matrix_categorical = metadata_matrix_categorical.reset_index()\n        metadata_matrix_categorical = pd.merge(metadata_matrix_categorical,temp,how='inner',on='index')\n        display(metadata_matrix_categorical.set_index(\"index\"))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"metadata = get_meta_data(data)\n\n#List potential columns of categorical variables\nlist_potential_categorical_type(metadata,data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But there is another variable that is not considered categorical. It's the column \"CompanyName\".\nLet's see are there any repetitive values?"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.CompanyName.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In reviewing the above data, we found that few company names were identical but misspelled, such as:"},{"metadata":{},"cell_type":"markdown","source":"1. 'maxda' Et 'mazda' ================> mazda\n2. 'porsche' Et 'porcshce' ===========> porsche\n3. 'toyota' Et 'toyouta' =============> toyota\n4. 'vokswagen' Et 'volkswagen','vw' ==> volkswagen\n5. 'Nissan' Et 'nissan' ==============> 'nissan'"},{"metadata":{},"cell_type":"markdown","source":"So we have to adjust things by replacing the values with one identical variable:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data = data.replace(to_replace =\"maxda\", value =\"mazda\") \ndata = data.replace(to_replace =\"porcshce\", value =\"porsche\") \ndata = data.replace(to_replace =\"toyouta\", value =\"toyota\") \ndata = data.replace(to_replace =\"vokswagen\", value =\"volkswagen\") \ndata = data.replace(to_replace =\"vw\", value =\"volkswagen\")\ndata = data.replace(to_replace =\"Nissan\", value =\"nissan\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.CompanyName.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory of Data"},{"metadata":{},"cell_type":"markdown","source":"Since the independent variable (i.e Price) is continuous numerical variable, and there is many dependat variables, we we will use Multiple linear regression"},{"metadata":{},"cell_type":"markdown","source":"**A. Key assumptions of multiple regression:**"},{"metadata":{},"cell_type":"markdown","source":"To perform **multiple linear regression**, the following assumptions must be met:\n\n** --- Before model construction: --- **\n\n1. ** Linear relationship: ** The dependent variable Y (i.e Price) has a linear relationship with the independent variables X, and to verify this, one must ensure that the XY dispersion graph is linear.\n\n\n2. ** No multi-collinearity: ** Multiple regression assumes that independent variables X are not strongly correlated with each other. This assumption is tested using ** Variance Inflation Factor (VIF) ** or using ** Correlation Matrix **.\n\n** --- After: Residual analysis of the model --- **\n\n\n3. ** Normality of Error Distribution **\n\n\n4. ** Independence of errors **\n\n\n5. ** Homo-scedasticity **"},{"metadata":{},"cell_type":"markdown","source":"# Dependent variable visualization: Price"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.title('Car Price Spread')\nsns.boxplot(y=data.price)\nplt.show()\nprint(data.price.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the box-mustache, the price field has an average **around 13K** and a **median around 10k** with the most expensive **car values at 45k** and the **cheapest cars at 5k**.\n\nSince we have mean > median, then our distribution is positively asymmetric, as we can see in the following histogram:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Car Price Distribution Plot')\nsns.distplot(data.price)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n\nWhich means that most of the prices offered by this company are low.\n\nAs seen below, we have **75%** prices are around **16k**, or **25%** between **17k and 45k**."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(data.price.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization of independent variables:"},{"metadata":{},"cell_type":"markdown","source":"# ******** Numerical ********"},{"metadata":{},"cell_type":"markdown","source":"**I. Check the linear relationship between the dependent variable \"Price\" and the numerical independent variables**"},{"metadata":{},"cell_type":"markdown","source":"Draw XY scatter plot, and check are they linear or not?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\nfrom scipy.stats.stats import pearsonr\n\ndef pairplot(x_axis,y_axis) :\n    sns.pairplot(data,x_vars=x_axis,y_vars=y_axis,height=4,aspect=1,kind=\"scatter\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Determiner la variable indépendante\ny_vars=['price']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Price VS Wheelbase - curbweight - boreratio**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_vars=['wheelbase','curbweight','boreratio']\npairplot(x_vars,y_vars)\nprint(\"At first glance, the 3 variables are positively correlated but spread at higher values.\")\n\np1=data['wheelbase']\np2=data['curbweight']\np3=data['boreratio']\n\npearson_coeff, p_value = pearsonr(p1,data['price'])\nprint('\\nWe can make sure of this by looking at the Coefficient of Correlation')\nprint('\\nCoefficient of Correlation between Price and wheelbase:',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p2,data['price'])\nprint('Correlation coefficient between Price and curbweight:',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p3,data['price'])\nprint('Correlation coefficient between Price and boreratio: ',pearson_coeff*100,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Price VS carlength - carwidth - carheight**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_vars=['carlength','carwidth', 'carheight']\npairplot(x_vars,y_vars)\nprint(\"Carlength and Carwidth are more correlated than carheight which is more spread out but positive.\")\n\np1=data['carlength']\np2=data['carwidth']\np3=data['carheight']\n\npearson_coeff, p_value = pearsonr(p1,data['price'])\nprint('\\nWe can make sure of this by looking at the Coefficient of Correlation')\nprint('\\nCorrelation coefficient between Price and carlength:',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p2,data['price'])\nprint('Correlation coefficient between Price and carwidth: ',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p3,data['price'])\nprint('Correlation coefficient between Price and carheight: ',pearson_coeff*100,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Price VS enginesize - horsepower - stroke**"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"x_vars=['enginesize','horsepower','stroke']\npairplot(x_vars,y_vars)\nprint(\"Enginesize and Horsepower are positively correlated, but Stroke is more spread out (may not be related).\")\n\np1=data['enginesize']\np2=data['horsepower']\np3=data['stroke']\n\npearson_coeff, p_value = pearsonr(p1,data['price'])\nprint('\\nWe can make sure of this by looking at the Coefficient of Correlation')\nprint('\\nCorrelation coefficient between Price and enginesize: ',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p2,data['price'])\nprint('Correlation coefficient between Price and horsepower: ',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p3,data['price'])\nprint('Correlation coefficient between Price and stroke: ',pearson_coeff*100,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Price VS compressionratio - peakrpm - symboling**"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"x_vars=['compressionratio','peakrpm',\"symboling\"]\npairplot(x_vars,y_vars)\nprint(\"Compressionratio, Peakrpm and symboling are not correlated.\")\n\np1=data['compressionratio']\np2=data['peakrpm']\np3=data['symboling']\n\npearson_coeff, p_value = pearsonr(p1,data['price'])\nprint('\\nWe can make sure of this by looking at the Coefficient of Correlation')\nprint('\\nCorrelation coefficient between Price and compressionratio: ',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p2,data['price'])\nprint('Correlation coefficient between Price and peakrpm: ',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p3,data['price'])\nprint('Correlation coefficient between Price and symboling: ',pearson_coeff*100,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Price VS citympg - highwaympg**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_vars=['citympg', 'highwaympg']\npairplot(x_vars,y_vars)\nprint('Citympg & Highwaympg are negatively correlated.\\nThe more prices get lower, the higher the distances get, which means that the cheapest cars have better mileage than expensive cars.')\n\np1=data['citympg']\np2=data['highwaympg']\n\npearson_coeff, p_value = pearsonr(p1,data['price'])\nprint('\\nWe can make sure of this by looking at the Coefficient of Correlation')\nprint('\\nCorrelation coefficient between Price and citympg: ',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p2,data['price'])\nprint('Correlation coefficient between Price and highwaympg: ',pearson_coeff*100,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n\n(+) positively correlated variables with Price: ** wheelbase, carlenght, carwidth, curbweight, enginesize, boreratio, horesepower **\n\n(-) negatively correlated variables with Price: ** citympg, highwaympg **\n\nThese variables should be kept for a better model, and the other variables should be ignored as they are not correlated with Price"},{"metadata":{},"cell_type":"markdown","source":"**II. Checking the multicollinearity between the correlated independent variables above and Price**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def heatmap(x,y,dataframe):\n    sns.heatmap(dataframe.corr(),cmap=\"OrRd\",annot=True)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heatmap(20,12,data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"   **a. Examination of the correlation between the variables specific to the dimensions of a car i.e. weight, height etc**"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"dimension_col_list = ['wheelbase', 'carlength', 'carwidth','curbweight']\n\nheatmap(10,10,data.filter(dimension_col_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Wheelbase** , **carlength**, **carwidth** et **curbweight** [ 0.80 - 0.88 ] are very correlated and we have to keep only one between them."},{"metadata":{},"cell_type":"markdown","source":"**b. Examination of the correlation between the variables specific to the performance of a car**"},{"metadata":{"trusted":true},"cell_type":"code","source":"performance_col_list = ['enginesize','boreratio','horsepower']\nheatmap(10,10,data.filter(performance_col_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Horsepower** and **enginesize** are highly correlated and we need to keep only one."},{"metadata":{},"cell_type":"markdown","source":"**c. Examining the correlation between citympg and highwaympg**"},{"metadata":{"trusted":true},"cell_type":"code","source":"performance_col_list = ['citympg','highwaympg']\nheatmap(10,10,data.filter(performance_col_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**citympg** and **highwaympg** are highly correlated and we need to keep one of them."},{"metadata":{},"cell_type":"markdown","source":"# ******** Categorical ********"},{"metadata":{},"cell_type":"markdown","source":"**Price VS CompanyName**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,9))\n\nplt.xticks(rotation = 90)\norder = data['CompanyName'].value_counts(ascending=False).index\nsns.countplot(x='CompanyName', data=data, order=order)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the above histogram, **Toyota** seems to be very popular, followed by **Nissan** and **Mazda**."},{"metadata":{},"cell_type":"markdown","source":"**Price VS fueltype**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 12))\n\nplt.subplot(2,3,1)\nsns.boxplot(x = 'fueltype', y = 'price', data = data)\n\nplt.subplot(2,3,2)\nplt.title('Fuel Type Histogram')\norder = data['fueltype'].value_counts(ascending=False).index\nsns.countplot(x='fueltype', data=data, order=order)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average price of a **diesel car** is higher than that of **gas cars**, which explains, according to the histogram, why the company sold more **gas cars** than **diesel cars**.\n\n**>>>** Note: Existence of Outliers for **Gas**"},{"metadata":{},"cell_type":"markdown","source":"**Price VS aspiration**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 12))\n\nplt.subplot(2,3,1)\nsns.boxplot(x = 'aspiration', y = 'price', data = data)\n\nplt.subplot(2,3,2)\nplt.title('Aspiration Histogram')\norder = data['aspiration'].value_counts(ascending=False).index\nsns.countplot(x='aspiration', data=data, order=order)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average price of cars with **turbo** aspiration is higher than that of **standard** aspiration, which explains, according to the histogram, why the company sells cars with **standard** aspiration more than of cars with **turbo** aspiration.\n\n** >>> ** Note: Existence of Outliers for ** Turbo ** and ** std **"},{"metadata":{},"cell_type":"markdown","source":"**Price VS doornumber**"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 12))\n\nplt.subplot(2,3,1)\nsns.boxplot(x = 'doornumber', y = 'price', data = data)\n\nplt.subplot(2,3,2)\nplt.title('Door Number Histogram')\norder = data['doornumber'].value_counts(ascending=False).index\nsns.countplot(x='doornumber', data=data, order=order)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**doornumber** values are pretty close, which means the price is not affected by **doornumber**\n\n**>>>** Note: Existence of Outliers in **four** and **two**"},{"metadata":{},"cell_type":"markdown","source":"**Price VS enginelocation**"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 12))\n\nplt.subplot(2,3,1)\nsns.boxplot(x = 'enginelocation', y = 'price', data = data)\n\nplt.subplot(2,3,2)\nplt.title('Engine Location Histogram')\norder = data['enginelocation'].value_counts(ascending=False).index\nsns.countplot(x='enginelocation', data=data, order=order)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear that **rear cars** are very expensive, which is why the company sold more cars with **front** rear.\n\n** >>> ** Note: Existence of Outliers in ** front **"},{"metadata":{},"cell_type":"markdown","source":"**Price VS carbody**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(2,3,1)\nsns.boxplot(x='carbody',y='price',data = data)\n\nplt.subplot(2,3,2)\nplt.title('Car Body Histogram')\norder = data['carbody'].value_counts(ascending=False).index\nsns.countplot(x='carbody', data=data, order=order)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that ** sedan ** is the most favored.\n\n** hardtop ** has the highest average price.\n\n** >>> ** Note: Existence of Outliers for several values."},{"metadata":{},"cell_type":"markdown","source":"**Price VS fuelsystem**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(2,3,1)\nsns.boxplot(x='fuelsystem',y='price',data = data)\n\nplt.subplot(2,3,2)\nplt.title('Fuel System Histogram')\norder = data['fuelsystem'].value_counts(ascending=False).index\nsns.countplot(x='fuelsystem', data=data, order=order)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** mpfi ** is the most favored type of ** fuelsystem **, even though it has the highest average price.\n\n** >>> ** Note: Existence of Outliers for ** mpfi ** and ** 2bbl **"},{"metadata":{},"cell_type":"markdown","source":"**Price VS enginetype**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(2,3,1)\nsns.boxplot(x='enginetype',y='price',data = data)\n\nplt.subplot(2,3,2)\nplt.title('Engine Type Histogram')\norder = data['enginetype'].value_counts(ascending=False).index\nsns.countplot(x='enginetype', data=data, order=order)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** ohc ** is the most favored engine type.\n\n** >>> ** Note: Existence of Outliers for several values."},{"metadata":{},"cell_type":"markdown","source":"**Price VS cylindernumber**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(2,3,1)\nsns.boxplot(x='cylindernumber',y='price',data = data)\n\nplt.subplot(2,3,2)\nplt.title('Cylinder Number Histogram')\norder = data['cylindernumber'].value_counts(ascending=False).index\nsns.countplot(x='cylindernumber', data=data, order=order)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ** four-cylinder ** seems to be the most favored.\n\nWe can see that expensive cars have ** eight-cylinder **, and ** four-cylinder ** are the cheapest.\n\n** >>> ** Note: Existence of Outliers for ** four **"},{"metadata":{},"cell_type":"markdown","source":"**Price VS drivewheel**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(2,3,1)\nsns.boxplot(x = 'drivewheel', y = 'price', data = data)\n\nplt.subplot(2,3,2)\nplt.title('DriveWheel Histogram')\norder = data['drivewheel'].value_counts(ascending=False).index\nsns.countplot(x='drivewheel', data=data, order=order)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** FWD ** is the most favored, followed by ** RWD **, and ** 4WD ** is the least favored even though it is cheaper than ** RWD **.\n\n** >>> ** Note: Existence of Outliers for several values."},{"metadata":{},"cell_type":"markdown","source":"**Price VS symboling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(2,3,1)\nsns.boxplot(x=data.symboling, y=data.price)\n\n\nplt.subplot(2,3,2)\nplt.title('Symboling Histogram')\norder = data['symboling'].value_counts(ascending=False).index\nsns.countplot(x='symboling', data=data, order=order)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that ** symboling ** ** 0 and 1 ** are the most favored.\n\nCars with ** symboling -1 and -2 ** are the most expensive, which is logical because it means that the car is more secure.\n\n** >>> ** Note: Existence of Outliers for several values."},{"metadata":{},"cell_type":"markdown","source":"# Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata_matrix_dataframe = get_meta_data(data)\nlist_potential_categorical_type(metadata_matrix_dataframe,data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Remove the uncorrelated variables with Price, and choose only one variable among the variables correlated with it **"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['carheight' ,'stroke' ,'compressionratio' ,'peakrpm' ,'carlength' ,'carwidth' ,'curbweight' ,'enginesize' ,'highwaympg'], axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dummy Variables creation**"},{"metadata":{},"cell_type":"markdown","source":"We can see that the values of these variables are in strings.\n\nHowever, to fit a regression line, we would need numeric values instead of strings. Therefore, we must convert them to 1 and 0 using dummy variables.\n\nWe can do the following :\n\n** fueltype ** {\"** gas **\": 1, \"** diesel **\": 0}\n\n** suction ** {\"** std **\": 1, \"** turbo **\": 0}\n\n** doornumber ** {\"** two **\": 1, \"** oven **\": 0}\n\n** enginelocation ** {\"** front **\": 1, \"** rear **\": 0}"},{"metadata":{"trusted":true},"cell_type":"code","source":"def binary_dummy_replace(x) :\n     return x.map({\"gas\":1,\"diesel\":0,\n                   \"std\":1,\"turbo\":0,\n                   \"two\":1, \"four\":0,\n                   \"front\": 1, \"rear\": 0})\ndef dummies(x,df):  \n    temp = pd.get_dummies(df[x], prefix=x, drop_first = True)\n    \n    #l = temp.columns.values\n    #for nm in l:\n        #newt=x+\"_\"+nm\n        #temp.rename({nm: Replace_Name(x)+\"_\"+nm}, axis=1, inplace=True)\n        \n    #print(temp.columns.values)\n        \n    df = pd.concat([df, temp], axis = 1)\n    df.drop([x], axis = 1, inplace = True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = dummies('symboling',data)\ndata = dummies('CompanyName',data)\ndata = dummies('fueltype',data)\ndata = dummies('aspiration',data)\ndata = dummies('doornumber',data)\ndata = dummies('carbody',data)\ndata = dummies('drivewheel',data)\ndata = dummies('enginelocation',data)\ndata = dummies('enginetype',data)\ndata = dummies('cylindernumber',data)\ndata = dummies('fuelsystem',data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And here is our Dataset, all is numeric"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preparation of Train and Test data**"},{"metadata":{},"cell_type":"markdown","source":"We will divide our dataset into ** 67% ** for learning, and ** 33% ** for testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\ncars_train, cars_test= train_test_split(data, train_size=0.67, test_size=0.33, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Scaling: Train Set & Test Set**"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler,scale\n#we create an object of the class StandardScaler\nsc = StandardScaler() \n\ncol_to_scale = ['wheelbase','boreratio','horsepower','citympg','price',]\n\ncars_train[col_to_scale] = sc.fit_transform(cars_train[col_to_scale])\ncars_test[col_to_scale] = sc.fit_transform(cars_test[col_to_scale])\n\ncars_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# "},{"metadata":{},"cell_type":"markdown","source":"**Division into X_train and y_train sets for model construction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = cars_train.loc[:,cars_train.columns == 'price']\n\nX_train = cars_train.loc[:, cars_train.columns != 'price']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Division into X_test and y_test sets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = cars_test.loc[:,cars_test.columns == 'price']\n\nX_test = cars_test.loc[:, cars_test.columns != 'price']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction and Evaluation"},{"metadata":{},"cell_type":"markdown","source":"# 1. Prediction"},{"metadata":{},"cell_type":"markdown","source":"**Now let's use our model to make predictions.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions\nimport statsmodels.api as sm \n\nlm = sm.OLS(y_train,X_train).fit()\n\ny_pred=lm.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Evaluation"},{"metadata":{},"cell_type":"markdown","source":"# **Residual analysis of the model**"},{"metadata":{},"cell_type":"markdown","source":"Residuals (i.e. errors) are simply the difference between predictions and observations"},{"metadata":{"trusted":true},"cell_type":"code","source":"resid = y_test - y_pred.to_frame('price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2-A. Normality of the error distribution**"},{"metadata":{},"cell_type":"markdown","source":"At any time in our independent values X, the data points must be fairly close to the line, evenly distributed with only a few outliers."},{"metadata":{},"cell_type":"markdown","source":"A histogram of the residuals (errors) in our model can be used to check if they are normally distributed or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(9,6))\nsns.distplot(resid, bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis:** The error terms are distributed around zero, which means that the prediction of the model is not random."},{"metadata":{},"cell_type":"markdown","source":"Another method to check, is to draw a graph of the predictions against the residualss, and see if the points are evenly distributed or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,9))\nplt.scatter(y_pred, resid)\nplt.hlines(0,-2,4)\nplt.suptitle('Residuals vs Predictions', fontsize=16)\nplt.xlabel('Predictions')\nplt.ylabel('Residuals')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis:** We can see that the values are ** evenly distributed ** around 0, with only a few ** outliers **."},{"metadata":{},"cell_type":"markdown","source":"**Anderson-Darling Test** and **Q-Q Plot of residuals**"},{"metadata":{},"cell_type":"markdown","source":"1. **Q-Q Plot**:\n    \nAn arc-shaped deflection trace with respect to the diagonal implies that the residuals have an excessive asymmetry, meaning that the distribution is not symmetrical, with too many important residuals in one direction."},{"metadata":{},"cell_type":"markdown","source":"2. **Anderson-Darling Test**"},{"metadata":{},"cell_type":"markdown","source":"Remains the most used by experts, to check the standard of errors.\n\nIt tests:\n\n**H0:** Data follow normal distribution\n\n**H1:** Data does not follow normal distribution\n\nHow to check **H0**?\n\nA level of significance is chosen by us (5% in our case), which is associated with a critical value.\n\nIf the returned **A-D** statistic is greater than the critical value for the chosen significance level, then **H0** must be rejected."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\ndef normality_of_residuals_test(model):\n    '''\n    Function to establish the normal QQ graph of the residues and perform the Anderson-Darming statistical test to study the normality of the residuals.\n    \n    Arg:\n    * model - OLS models adapted from statsmodels\n    '''\n    sm.ProbPlot(lm.resid).qqplot(line='s');\n    plt.title('Q-Q plot');\n\n    ad = stats.anderson(lm.resid, dist='norm')\n    \n    print(f'----Anderson-Darling test ---- \\nstatistic: {ad.statistic:.4f}, critical value of 5%: {ad.critical_values[2]:.4f}')\n    \nnormality_of_residuals_test(lm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The trace does not have an arc shape.\n\nthe test accepts **H0** that the data follow the normal distribution (stats < critical value of 5%)"},{"metadata":{},"cell_type":"markdown","source":"**2-B. Independence of errors**"},{"metadata":{},"cell_type":"markdown","source":"This means that the residuals (i.e errors) should not be correlated."},{"metadata":{},"cell_type":"markdown","source":"1. **Plot of residuals against their order.**"},{"metadata":{},"cell_type":"markdown","source":"What must alert us is the existence of a tendency, which tells us that the errors are in fact dependent."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,9))\nplt.scatter(resid.index, resid.values)\nplt.hlines(0,0,200)\nplt.suptitle('Residuals by order', fontsize=16)\nplt.xlabel('Order')\nplt.ylabel('Residuals')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see, the points are scattered randomly. no tendancy to be found."},{"metadata":{},"cell_type":"markdown","source":"2. **Durbin_Watson** Test"},{"metadata":{},"cell_type":"markdown","source":"The test of **Durbin-Watson** tests the null hypothesis that the residuals are not dependent (autocorrelated) on each other.\n\nThe test returns a value **d** between 0 and 4.\n\nA value **d = 2**: No autocorrelation detected in the sample.\n\nA value **d <** **2**: indicate a positive autocorrelation.\n\nA **d >** **2** value: indicate a negative autocorrelation.\n\n\nIn general, values of **d ~ 2** indicate that there is no dependence (no autocorrelation) between the residuals."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.stattools import durbin_watson","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(durbin_watson(resid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. **ACF - Auto-Correlation Function plots**"},{"metadata":{},"cell_type":"markdown","source":"We want to see if the value of **ACF** is significant for each bar.\n\nBy calling the function, we indicate the level of significance that interests us (alpha = 0.05 in our case) and the critical zone is drawn on the graph (In blue).\n\nSignificant correlations lie outside this area.\n\n**>>> Note**: First bar is always at 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.tsa.api as smt\n\nacf = smt.graphics.plot_acf(resid, lags=40 , alpha=0.05)\nacf.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that all the bars are inside the blue zone, except one, which is acceptable since **Durbin_Watson** returned a value very close to 2."},{"metadata":{},"cell_type":"markdown","source":"**2-C. Homoscedasticity**"},{"metadata":{},"cell_type":"markdown","source":"This assumption indicates that the variance of the residuals must be similar for the values of the independent variables.\n\nWe can verify this by plotting the residuals against the predicted values.\n\nTo identify **homo-dedasticity** in the graph, the location of the points should be random and no trend should be visible, and the red regression line in the graph should be as flat as possible (not of an arch form).\n\n**Goldfeld-Quandt test:**\n\nHe tests:\n\n1. Null hypothesis **H0**: the error terms are homoscedastic\n2. Alternative Hypothesis **H1**: The error terms are heteroscedastic."},{"metadata":{},"cell_type":"markdown","source":"**Recall:**\n\nIf P-value <= 0.05 ==> We reject the null hypothesis **H0**\n\nIf P-Value> 0.05 ==> The null hypothesis **H0** is true"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format ='retina'\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\nsns.set_style('darkgrid')\nsns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n\ndef homoscedasticity_test(model):\n    '''\n    Fonction de test de l'homoscédasticité des résidus dans un modèle de régression linéaire.\n\n    Il compare les valeurs résiduelles aux valeurs prédites et exécute les tests de Goldfeld-Quandt.\n    \n    Args:\n    * model - fitted OLS model from statsmodels\n    '''\n    fitted_vals = model.predict()\n    resids = model.resid\n\n    #fit_reg=False\n    sns.regplot(x=fitted_vals, y=resids, lowess=True, line_kws={'color': 'red'})\n    plt.suptitle('Résidus vs Prédictions', fontsize=16)\n    plt.xlabel('Prédictions')\n    plt.ylabel('Résidus')\n\n    print('\\n----Goldfeld-Quandt test ----')\n    name = ['F statistic', 'p-value']\n    test = sms.het_goldfeldquandt(lm.resid, lm.model.exog)\n    print(lzip(name, test))\n    print('\\n----Residuals plots ----')\n\nhomoscedasticity_test(lm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis:**\n\nP-Value test **Goldfeld-Quandt** > 0.05, so we accept **H0** saying that the error terms are homoscedastic, which means that the residuals have a constant variance.\n\nAlso, in the plot, the points are scattered randomly, no tendancy to be found, and the line doesn't have the form of an arch"},{"metadata":{},"cell_type":"markdown","source":"all the hypotheses are verified"},{"metadata":{},"cell_type":"markdown","source":"**Evaluation of the test via the comparison of y_pred and y_test**"},{"metadata":{},"cell_type":"markdown","source":"What we are looking for is that the errors should be as close as possible to the line, wich is the case in the plot below"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(11,5))\nplt.scatter(y_test,y_pred)\nplt.xlabel('y_test', fontsize=18)\nplt.ylabel('y_pred', fontsize=16)\n\n#Regression Line function\nf = lambda x: x\n\n# x values of line to plot\nx = np.array(y_test)\n\n# plot fit\nplt.plot(x,f(x),lw=2.5, c=\"orange\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Summary - Part 1"},{"metadata":{},"cell_type":"markdown","source":"**Coefficient of determination R²:**"},{"metadata":{},"cell_type":"markdown","source":"**Recall:**\n\nIf **R² = 0**: the dependent variable Y can not be predicted from the independent variable **X**\n\nIf **R² = 1**: the dependent variable Y can be predicted from the independent variable **X**\n\nIf **0 <R² <1**: Indicates the percentage at which the dependent variable Y is predictable by **X**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score \nr2_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Is it good ?"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**59** variables used to have R²> 0.8, which is too much."},{"metadata":{},"cell_type":"markdown","source":"===================================================================================================\n\n**P-values** of some variables appear to be greater than 0.05, meaning they are statistically insignificant."},{"metadata":{},"cell_type":"markdown","source":"**==== The 2nd warning says: ===**\n    \n     The smallest eigenvalue is 2.4e-30. This may indicate that there are strong problems of multicollinearity or\n     that the design matrix is singular.\n    \nViolation of hypothesis of multicollinearity. There are correlated variables between them which should not exist"},{"metadata":{},"cell_type":"markdown","source":"**Can we do better?**"},{"metadata":{},"cell_type":"markdown","source":"Yes, by using **RFE**"},{"metadata":{},"cell_type":"markdown","source":"## RFE"},{"metadata":{},"cell_type":"markdown","source":"In the section of Exploring Data, we have identified the relevant independent variables that are correlated with the independent variable Price, and which are not correlated with each other (ie no multi-collinearity), and we said that we need to keep them, and igon the rest.\n\nTo do that, we will use a mixed approach to find the relevant features:\n\n     1.Identify these variables using RFE (Recursive Feature Elimination)\n\n     2.Manual approach to find the right fit"},{"metadata":{},"cell_type":"markdown","source":"The use of RFE gives us an automated way to select important attributes that can influence the dependent variable (i.e Price).\n\nWe will use a mixed approach here and initially, we will simply use the functionalities returned by RFE as a starting model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regression = LinearRegression()\nregression.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Call the RFE method to have important variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe = RFE(regression,10)\nrfe = rfe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Extract important variables**\n\nThe **RFE()** method returns a **\"support_\"** array of variables considered important:\n\nIf **support_** == **True**: Important variable\n\nIf **support_** == **False**: Not important variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"for z in range(len(X_train.columns)):\n    print(X_train.columns[z],'\\t\\t\\t',rfe.support_[z])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The columns selected by RFE**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col = X_train.columns[rfe.support_]\nfor x in col:\n    print(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using statsmodel package, for detailed statistics**"},{"metadata":{},"cell_type":"markdown","source":"Here is our Dataset for learning, after RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rfe = X_train[X_train.columns[rfe.support_]]\nX_train_rfe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm \n\ndef color_code_vif_values(val):\n    \"\"\"\n    Take a scalar and return a string with the property css 'color: red' for 10, black otherwise.\n    \"\"\"\n    if val > 10 : color = 'red' \n    elif val > 5 and val <= 10 : color = 'blue'\n    elif val > 0 and val <= 5 : color = 'darkgreen'\n    else : color = 'black'\n    return 'color: %s' % color\n\ndef drop_col(dataframe,col_to_drop) :\n    dataframe.drop([col_to_drop],axis=1,inplace=True)\n    return dataframe\n\ndef display_vif(x) :\n    #Calculer les VIFs pour le nouveau modèle\n    from statsmodels.stats.outliers_influence import variance_inflation_factor\n    vif = pd.DataFrame()\n    X = x\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.set_index(\"Features\")\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    df = pd.DataFrame(vif.VIF).style.applymap(color_code_vif_values)\n    display(df)\n    \nmodel_count = 0\n\ndef statsmodel_summary(y_var,x_var) :\n    global model_count\n    model_count = model_count + 1\n    text = \"*****MODEL - \" + str(model_count)\n    print(text)\n    \n    x_var_const = sm.add_constant(x_var) # adding constant\n    lm = sm.OLS(y_var,x_var_const).fit() # calculating the fit\n    print(lm.summary()) # print summary for analysis\n    display_vif(x_var_const.drop(['const'],axis=1))\n    return x_var_const , lm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## I. Check the multicollinearity between the variables selected by RFE"},{"metadata":{},"cell_type":"markdown","source":"To do that, we base ourselves on 2 things:\n\n1.**P-Value:**\n\n    P-Value <= **0.05** means that this particular independent variable greatly improves the fit of the model\n\n    P-value > **0.05** no improvement\n\n2.**VIF (Variance Inflation Factor):**\n\n    VIF > **5**, there is an indication that multicollinearity may be present, but not enough to worry.\n\n    VIF > **10**, there is certainly a multicollinearity among the variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = statsmodel_summary(y_train,X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"P-value analysis: P-value of **carbody_sedan** > **0.05**. We must delete it."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rfe = X_train_rfe.drop([\"carbody_sedan\"], axis = 1)\nX_train_rfe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = statsmodel_summary(y_train,X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"P-value analysis: P-value of **carbody_wagon** > **0.05**. We must delete it."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rfe = X_train_rfe.drop([\"carbody_wagon\"], axis = 1)\nX_train_rfe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = statsmodel_summary(y_train,X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"P-value analysis: All variables have p-value <0.05\n\nNext step: Delete **CompanyName_porsche** because VIF> 10"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rfe = X_train_rfe.drop([\"CompanyName_porsche\"], axis = 1)\nX_train_rfe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = statsmodel_summary(y_train,X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All P-Values are less than 0.05, and all VIF values are less than 5.\n\nTurning now to the prediction and evaluation of our model"},{"metadata":{},"cell_type":"markdown","source":"## Prediction and Evaluation"},{"metadata":{},"cell_type":"markdown","source":"## 1. Prédiction"},{"metadata":{},"cell_type":"markdown","source":"Creation of X_test_new by keeping only the relevant variables found by RFE."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Array containing names of important variables\nfinal_features = list(X_train_rfe.columns)\n\n#Filter the test dataset\nX_test_new = X_test.filter(final_features)\n\nX_test_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's use our model now to make predictions.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions\nlm = sm.OLS(y_train,X_train_rfe).fit()\n\ny_pred=lm.predict(X_test_new)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Evaluation"},{"metadata":{},"cell_type":"markdown","source":"**Residual analysis of the model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"resid = y_test - y_pred.to_frame('price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2-A. Independence of errors: **\n\n   **1.Graphe des résidus contre leur ordre.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,9))\nplt.scatter(resid.index, resid.values)\nplt.hlines(0,0,200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see, the points are scattered at random. no trend."},{"metadata":{},"cell_type":"markdown","source":"**2. Durbin_Watson Test**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(durbin_watson(resid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. ACF - Auto-Correlation Function plots**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.tsa.api as smt\n\nacf = smt.graphics.plot_acf(resid, lags=40 , alpha=0.05)\nacf.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that all the bars are inside the blue zone, except one, which is acceptable since Durbin_Watson has returned a value very close to 2.0"},{"metadata":{},"cell_type":"markdown","source":"**2-B. Homoscedasticity**"},{"metadata":{"trusted":true},"cell_type":"code","source":"homoscedasticity_test(lm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis:**\n\nP-Value of **Goldfeld-Quandt Test** > 0.05, so we accept **H0** saying that the error terms are homoscedastic, which means that the residuals have a constant variance.\n\nAlso, in the plot, the points are scattered randomly, no tendancy to be found, and the line doesn't have the form of an arc"},{"metadata":{},"cell_type":"markdown","source":"**2-C. Normality of the error distribution**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(9,6))\nsns.distplot(resid, bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis:** The error terms are a little bit asymmetric."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_pred, resid)\nplt.hlines(0,-2,4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis:** Data points are a bit more on one side than the other"},{"metadata":{},"cell_type":"markdown","source":"**Anderson-Darling Test and Q-Q Plot**"},{"metadata":{"trusted":true},"cell_type":"code","source":"normality_of_residuals_test(lm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The blue trace slightly take form of an arc, but not aggressively"},{"metadata":{},"cell_type":"markdown","source":"the test rejects H0 that the data follow a normal distribution (stats > critical value of 5%)"},{"metadata":{},"cell_type":"markdown","source":"**Evaluation of the test via the comparison of y_pred and y_test**"},{"metadata":{},"cell_type":"markdown","source":"What we are looking for is that the errors should be as close as possible to the line"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(11,5))\nplt.scatter(y_test,y_pred)\nplt.xlabel('y_test', fontsize=18)\nplt.ylabel('y_pred', fontsize=16)\n\n#Regression Line function\nf = lambda x: x\n\n# x values of line to plot\nx = np.array(y_test)\n\n# plot fit\nplt.plot(x,f(x),lw=2.5, c=\"orange\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final summary of the model"},{"metadata":{},"cell_type":"markdown","source":"**Coefficient of determination R².**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score \nr2_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**What has changed with RFE?**"},{"metadata":{},"cell_type":"markdown","source":"    1.R² decreased by 0.05 and remains above 0.8 which is good.\n\n    2.The value of the Durbin-watson test has increased, indicating more independence of errors.\n\n    3.The value of the Golden-Quandt test has increased sharply from 0.070 to 0.130, and the red line remains flat. (Much better homo-scedasticity)\n\n    4.No multicollinearity.\n\nBut, we see that the hypothesis of normality of errors is violated.\n\nActual data rarely includes normally distributed errors, and it may not be possible to adapt your data to a model whose errors do not violate the normality assumption.\n\nIt is usually best to focus on violations of other assumptions and / or on the influence of some Outliers (who may in any case be responsible for violations of normality).\n\nSo, if I have the choice between, violating the hypothesis of multicollinearity or the hypothesis of normality of the errors. I will choose normality because it is the least sensitive hypothesis among the others."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the P-Values are under **0.05**. Also, the Previous Alert that we had about existence of Multi-colinearity is not there any more"},{"metadata":{},"cell_type":"markdown","source":"**Reduction of dimension**\n\n7 variables against 59 for **R²> 0.8** !!! It's much better"},{"metadata":{},"cell_type":"markdown","source":"**Is it good ?**\n\nStill not 100% perfect, because of the existence of Outliers that strongly affect our model"},{"metadata":{},"cell_type":"markdown","source":"**Next step ?**\n\nDetect and treat Outliers (either delete them, or fix them if there are errors) and see if there are improvements.\n\nCheck my Other Kernel Regarding this Step"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}