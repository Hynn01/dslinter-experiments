{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget -c https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n!tar -vxf cifar-10-python.tar.gz\n!rm cifar-10-python.tar.gz","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-06T19:26:19.618297Z","iopub.execute_input":"2022-05-06T19:26:19.619195Z","iopub.status.idle":"2022-05-06T19:26:30.442798Z","shell.execute_reply.started":"2022-05-06T19:26:19.619053Z","shell.execute_reply":"2022-05-06T19:26:30.441367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchviz","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:26:30.45127Z","iopub.execute_input":"2022-05-06T19:26:30.454125Z","iopub.status.idle":"2022-05-06T19:26:44.264735Z","shell.execute_reply.started":"2022-05-06T19:26:30.454075Z","shell.execute_reply":"2022-05-06T19:26:44.2636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport pickle\nimport time\nimport skimage.io\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport PIL.Image\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler, RandomSampler, SequentialSampler\nimport torchvision.models as models\nimport albumentations\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom tqdm import tqdm_notebook as tqdm\nimport os\nimport torchvision\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport gc\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_classification\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom torchviz import make_dot\n\n\ndevice = torch.device('cuda')\n\n### Utility functions:\n\ndef unpickle(file):\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo,encoding='latin1')\n    return dict\n\n\ndef scorer(models,x_test,y_true):\n    for model in models:\n        y_pred = model.predict(x_test)\n        print(accuracy_score(y_true, y_pred))\n        \n\nfrom prettytable import PrettyTable\n\ndef count_parameters(model):\n    #reference and credits : https://stackoverflow.com/a/62508086/9017542\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if not parameter.requires_grad: continue\n        params = parameter.numel()\n        table.add_row([name, params])\n        total_params+=params\n    print(table)\n    print(f\"Total Trainable Params: {total_params}\")\n    return total_params\n        \n","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:26:44.266615Z","iopub.execute_input":"2022-05-06T19:26:44.266992Z","iopub.status.idle":"2022-05-06T19:26:48.859934Z","shell.execute_reply.started":"2022-05-06T19:26:44.266943Z","shell.execute_reply":"2022-05-06T19:26:48.858878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT_PATH='./' \nbatch1 = unpickle(ROOT_PATH+\"cifar-10-batches-py/data_batch_1\")\nbatch2 = unpickle(ROOT_PATH+\"cifar-10-batches-py/data_batch_2\")\nbatch3 = unpickle(ROOT_PATH+\"cifar-10-batches-py/data_batch_3\")\nbatch4 = unpickle(ROOT_PATH+\"cifar-10-batches-py/data_batch_4\")\nbatch5 = unpickle(ROOT_PATH+\"cifar-10-batches-py/data_batch_5\")\ntest_batch = unpickle(ROOT_PATH+\"cifar-10-batches-py/test_batch\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:26:48.862577Z","iopub.execute_input":"2022-05-06T19:26:48.86281Z","iopub.status.idle":"2022-05-06T19:26:49.104096Z","shell.execute_reply.started":"2022-05-06T19:26:48.86278Z","shell.execute_reply":"2022-05-06T19:26:49.103126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualising Images","metadata":{}},{"cell_type":"code","source":"class_mapping = {0: 'airplane',\n1: 'automobile',\n2: 'bird',\n3: 'cat',\n4: 'deer',\n5: 'dog',\n6: 'frog' ,\n7: 'horse',\n8: 'ship',\n9: 'truck'}\n\ndef visualize(batch):\n    from pylab import rcParams\n    rcParams['figure.figsize'] = 20,10\n    for i in range(2):\n        f, axarr = plt.subplots(1,5)\n        for p in range(5):\n            idx = np.random.randint(0, len(batch['data']))\n            img = batch['data'][idx]\n            label = batch['labels'][idx]\n            name = batch['filenames'][idx]\n            name = name.split('_')[0]\n            axarr[p].imshow(np.fliplr(np.rot90(np.transpose(img.flatten().reshape(3,32,32)), k=-1)))\n            axarr[p].set_title(class_mapping[label]+' ('+str(label)+')')\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-06T20:36:55.054784Z","iopub.execute_input":"2022-05-06T20:36:55.058561Z","iopub.status.idle":"2022-05-06T20:36:55.07608Z","shell.execute_reply.started":"2022-05-06T20:36:55.058508Z","shell.execute_reply":"2022-05-06T20:36:55.074724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize(batch1)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T20:36:58.280035Z","iopub.execute_input":"2022-05-06T20:36:58.280704Z","iopub.status.idle":"2022-05-06T20:36:59.823299Z","shell.execute_reply.started":"2022-05-06T20:36:58.280667Z","shell.execute_reply":"2022-05-06T20:36:59.822343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize(batch2)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:26:50.67601Z","iopub.execute_input":"2022-05-06T19:26:50.676537Z","iopub.status.idle":"2022-05-06T19:26:52.374358Z","shell.execute_reply.started":"2022-05-06T19:26:50.676489Z","shell.execute_reply":"2022-05-06T19:26:52.373404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize(batch3)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:26:52.375945Z","iopub.execute_input":"2022-05-06T19:26:52.376459Z","iopub.status.idle":"2022-05-06T19:26:54.640527Z","shell.execute_reply.started":"2022-05-06T19:26:52.376412Z","shell.execute_reply":"2022-05-06T19:26:54.639624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize(batch4)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:26:54.645363Z","iopub.execute_input":"2022-05-06T19:26:54.647794Z","iopub.status.idle":"2022-05-06T19:26:56.301404Z","shell.execute_reply.started":"2022-05-06T19:26:54.647747Z","shell.execute_reply":"2022-05-06T19:26:56.30047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize(batch5)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:26:56.305601Z","iopub.execute_input":"2022-05-06T19:26:56.305832Z","iopub.status.idle":"2022-05-06T19:26:58.068485Z","shell.execute_reply.started":"2022-05-06T19:26:56.305803Z","shell.execute_reply":"2022-05-06T19:26:58.067504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Numpy DataSet","metadata":{}},{"cell_type":"code","source":"def load_data0(btch):\n    labels = btch['labels']\n    imgs = btch['data'].reshape((-1, 32, 32, 3))\n    \n    res = []\n    for ii in range(imgs.shape[0]):\n        img = imgs[ii].copy()\n        img = np.fliplr(np.rot90(np.transpose(img.flatten().reshape(3,32,32)), k=-1))\n        res.append(img.flatten())\n    imgs = np.stack(res)\n    return labels, imgs\n\n\ndef load_data():\n    x_train_l = []\n    y_train_l = []\n    for ibatch in [batch1, batch2, batch3, batch4, batch5]:\n        labels, imgs = load_data0(ibatch)\n        x_train_l.append(imgs)\n        y_train_l.extend(labels)\n    x_train = np.vstack(x_train_l)\n    y_train = np.vstack(y_train_l)\n    \n    x_test_l = []\n    y_test_l = []\n    labels, imgs = load_data0(test_batch)\n    x_test_l.append(imgs)\n    y_test_l.extend(labels)\n    x_test = np.vstack(x_test_l)\n    y_test = np.vstack(y_test_l)\n    return (x_train, y_train), (x_test, y_test)\n\n(x_train, y_train), (x_test, y_test) = load_data()\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:26:58.070038Z","iopub.execute_input":"2022-05-06T19:26:58.070934Z","iopub.status.idle":"2022-05-06T19:27:01.027396Z","shell.execute_reply.started":"2022-05-06T19:26:58.070877Z","shell.execute_reply":"2022-05-06T19:27:01.0262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = y_train.ravel()\ny_test = y_test.ravel()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:27:01.029153Z","iopub.execute_input":"2022-05-06T19:27:01.029443Z","iopub.status.idle":"2022-05-06T19:27:01.036017Z","shell.execute_reply.started":"2022-05-06T19:27:01.029401Z","shell.execute_reply":"2022-05-06T19:27:01.034698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Logistic Regression, Decision Trees, Random Forests. ","metadata":{}},{"cell_type":"code","source":"print('Building Logistic Regression')\n\nlg = make_pipeline(StandardScaler(), LogisticRegression(max_iter = 250, n_jobs = -1, random_state=0))\nlg = lg.fit(x_train, y_train)\ngc.collect()\n\n\nprint('Building Decision Trees')\ndt = make_pipeline(StandardScaler(),DecisionTreeClassifier(random_state=42))\ndt = dt.fit(x_train,y_train)\n\n\nprint('Building Random Forests')\nrf = make_pipeline(StandardScaler(),RandomForestClassifier(max_depth=2, random_state=0))\nrf = rf.fit(x_train,y_train)\n\n\nscorer([lg,dt,rf],x_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:38:52.641145Z","iopub.execute_input":"2022-05-06T19:38:52.641745Z","iopub.status.idle":"2022-05-06T19:38:52.778014Z","shell.execute_reply.started":"2022-05-06T19:38:52.641699Z","shell.execute_reply":"2022-05-06T19:38:52.776847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural Networks\n","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\ntransform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) ","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:38:52.77972Z","iopub.execute_input":"2022-05-06T19:38:52.780366Z","iopub.status.idle":"2022-05-06T19:38:52.790709Z","shell.execute_reply.started":"2022-05-06T19:38:52.780303Z","shell.execute_reply":"2022-05-06T19:38:52.789478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.relu = nn.ReLU(inplace=False)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x1 = self.relu(x)\n        x = x+x1 #resnet like arch\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:38:52.792782Z","iopub.execute_input":"2022-05-06T19:38:52.793555Z","iopub.status.idle":"2022-05-06T19:38:52.806075Z","shell.execute_reply.started":"2022-05-06T19:38:52.793509Z","shell.execute_reply":"2022-05-06T19:38:52.805007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Vanilla_Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(32*32*3, 1024)\n        self.fc2 = nn.Linear(1024, 256)\n        self.fc3 = nn.Linear(256, 10)\n\n    def forward(self, x):\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:38:52.808278Z","iopub.execute_input":"2022-05-06T19:38:52.808657Z","iopub.status.idle":"2022-05-06T19:38:52.822713Z","shell.execute_reply.started":"2022-05-06T19:38:52.808613Z","shell.execute_reply":"2022-05-06T19:38:52.821671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class resnet_cifar10(nn.Module):\n    def __init__(self, out_dim):\n        super(resnet_cifar10, self).__init__()\n        self.model = models.resnet18(pretrained=True)\n        self.myfc = nn.Linear(self.model.fc.in_features, out_dim)\n        self.model.fc = nn.Identity()\n\n    def extract(self, x):\n        return self.model(x)\n\n    def forward(self, x):\n        x = self.extract(x)\n        x = self.myfc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:38:52.824644Z","iopub.execute_input":"2022-05-06T19:38:52.825079Z","iopub.status.idle":"2022-05-06T19:38:52.835398Z","shell.execute_reply.started":"2022-05-06T19:38:52.825034Z","shell.execute_reply":"2022-05-06T19:38:52.834246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(loader, optimizer):\n    total = 0\n    correct = 0\n    model.train()\n    train_loss = []\n    bar = tqdm(loader)\n    for (data, target) in bar:\n        \n        data, target = data.to(device), target.to(device)\n        loss_func = criterion\n        optimizer.zero_grad()\n        logits = model(data)\n        _,pred = torch.max(logits,1)\n        \n        # Accuaracy code\n        total += target.size(0)\n        correct += (pred == target).sum().item()\n        acc = 100 * (correct / total)\n        \n        loss = loss_func(logits, target)\n        loss.backward()\n        optimizer.step()\n\n        loss_np = loss.detach().cpu().numpy()\n        train_loss.append(loss_np)\n        #smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n        bar.set_description('loss: %.5f, Accuracy: %.5f' % (loss_np, acc))\n    return train_loss,acc\n\n\ndef val_epoch(loader, get_output=False):\n    total = 0\n    correct = 0\n    model.eval()\n    val_loss = []\n    LOGITS = []\n    PREDS = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for (data, target) in tqdm(loader):\n            data, target = data.to(device), target.to(device)\n            logits = model(data)\n            \n            loss = criterion(logits, target)\n            #print(logits)\n            _, pred = torch.max(logits, 1)\n            LOGITS.append(logits)\n            PREDS.append(pred)\n            TARGETS.append(target)\n            total += target.size(0)\n            correct += (pred == target).sum().item()\n            \n            val_loss.append(loss.detach().cpu().numpy())\n        val_loss = np.mean(val_loss)\n\n    LOGITS = torch.cat(LOGITS).cpu().numpy()\n    PREDS = torch.cat(PREDS).cpu().numpy()\n    TARGETS = torch.cat(TARGETS).cpu().numpy()\n    \n    #print(correct,total)\n    acc = 100 * (correct / total)\n        \n\n    if get_output:\n        return LOGITS\n    else:\n        return val_loss, acc","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:38:52.837013Z","iopub.execute_input":"2022-05-06T19:38:52.837589Z","iopub.status.idle":"2022-05-06T19:38:52.856818Z","shell.execute_reply.started":"2022-05-06T19:38:52.83754Z","shell.execute_reply":"2022-05-06T19:38:52.85538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\n\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:38:52.858459Z","iopub.execute_input":"2022-05-06T19:38:52.859651Z","iopub.status.idle":"2022-05-06T19:39:04.516344Z","shell.execute_reply.started":"2022-05-06T19:38:52.859577Z","shell.execute_reply":"2022-05-06T19:39:04.515414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\nnum_workers = 2\nn_epochs = 10\n\n\ntrain_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n    \nvalid_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:39:04.517815Z","iopub.execute_input":"2022-05-06T19:39:04.51814Z","iopub.status.idle":"2022-05-06T19:39:04.527703Z","shell.execute_reply.started":"2022-05-06T19:39:04.518098Z","shell.execute_reply":"2022-05-06T19:39:04.526547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run(model,mtype,tl,vl):\n\n\n    #dataset_train = CIFARDataset(data , transform=transform)\n    #dataset_valid = CIFARDataset(test , transform=transform)\n  \n\n    #model = Net()\n    model = model.to(device)\n\n    optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9, weight_decay=5e-4)\n    #optim.Adam(model.parameters(), lr=0.01)\n    \n\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n\n    #print(len(dataset_train), len(dataset_valid))\n    \n    train_list = []\n    train_acc_list = []\n    val_list = []\n    val_acc_list = []\n    \n    acc_max = 0\n    for epoch in range(1, n_epochs+1):\n        print(time.ctime(), 'Epoch:', epoch)\n\n        train_loss,t_acc = train_epoch(tl, optimizer)\n        train_list.append(np.mean(train_loss))\n        train_acc_list.append(t_acc)\n        val_loss, acc = val_epoch(vl)\n        val_list.append(np.mean(val_loss))\n        val_acc_list.append(acc)\n        scheduler.step()\n        \n        #print)\n\n        content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}, val loss: {np.mean(val_loss):.5f}, acc: {(acc):.5f}'\n        print(content)\n        with open(f'log_basic_cnn.txt', 'a') as appender:\n            appender.write(content + '\\n')\n\n        if acc > acc_max:\n            print('score2 ({:.6f} --> {:.6f}).  Saving model ...'.format(acc_max, acc))\n            torch.save(model.state_dict(), f'model.pth')\n            acc_max = acc\n    \n    \n    x_ticks = [1,2,3,4,5,6,7,8,9,10]\n    x_labels = [1,2,3,4,5,6,7,8,9,10] \n    \n    plt.figure(figsize=(10, 8))\n    plt.plot(train_list)\n    plt.plot(val_list)\n    plt.title(f'Training Loss vs Validation Loss for {mtype}')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.xticks(ticks=x_ticks, labels=x_labels)\n    plt.legend(['train', 'val'], loc='upper right')\n    plt.show()\n    \n    plt.figure(figsize=(10, 8))\n    plt.plot(train_acc_list)\n    plt.plot(val_acc_list)\n    plt.title(f'Training Accuracy vs Validation Accuracy for {mtype}')\n    plt.ylabel('Accuracy')\n    plt.xlabel('epoch')\n    plt.xticks(ticks=x_ticks, labels=x_labels)\n    plt.legend(['train', 'val'], loc='upper right')\n    plt.show()\n    \n    torch.save(model.state_dict(), os.path.join(f'basic_cnn_final.pth'))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:39:04.529516Z","iopub.execute_input":"2022-05-06T19:39:04.530236Z","iopub.status.idle":"2022-05-06T19:39:04.550047Z","shell.execute_reply.started":"2022-05-06T19:39:04.530187Z","shell.execute_reply":"2022-05-06T19:39:04.549099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Net()\nrun(model,'Convolutional Neural Network',train_loader,valid_loader)\nbatch = next(iter(train_loader))\nyhat = model(batch[0].to(device))\nmake_dot(yhat, params=dict(list(model.named_parameters()))).render(\"cnn\")\ncount_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:39:04.552256Z","iopub.execute_input":"2022-05-06T19:39:04.553077Z","iopub.status.idle":"2022-05-06T19:42:18.46492Z","shell.execute_reply.started":"2022-05-06T19:39:04.553031Z","shell.execute_reply":"2022-05-06T19:42:18.463733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Vanilla_Net()\nrun(model,'Neural Network',train_loader,valid_loader)\nbatch = next(iter(train_loader))\nyhat = model(batch[0].to(device))\nmake_dot(yhat, params=dict(list(model.named_parameters()))).render(\"nn\")\ncount_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:42:18.467261Z","iopub.execute_input":"2022-05-06T19:42:18.467994Z","iopub.status.idle":"2022-05-06T19:45:18.418143Z","shell.execute_reply.started":"2022-05-06T19:42:18.467941Z","shell.execute_reply":"2022-05-06T19:45:18.416534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = resnet_cifar10(out_dim = 10)\nrun(model,'Resnet 18',train_loader,valid_loader)\nbatch = next(iter(train_loader))\nyhat = model(batch[0].to(device))\nmake_dot(yhat, params=dict(list(model.named_parameters()))).render(\"resnet\")\ncount_parameters(model) ","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:45:18.421157Z","iopub.execute_input":"2022-05-06T19:45:18.421887Z","iopub.status.idle":"2022-05-06T19:49:27.647485Z","shell.execute_reply.started":"2022-05-06T19:45:18.421812Z","shell.execute_reply":"2022-05-06T19:49:27.64628Z"},"trusted":true},"execution_count":null,"outputs":[]}]}