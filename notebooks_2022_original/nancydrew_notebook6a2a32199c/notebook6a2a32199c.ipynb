{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-07T09:55:15.968556Z","iopub.execute_input":"2022-05-07T09:55:15.969127Z","iopub.status.idle":"2022-05-07T09:55:15.982374Z","shell.execute_reply.started":"2022-05-07T09:55:15.969081Z","shell.execute_reply":"2022-05-07T09:55:15.981537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing the dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:55:16.429999Z","iopub.execute_input":"2022-05-07T09:55:16.43025Z","iopub.status.idle":"2022-05-07T09:55:16.465366Z","shell.execute_reply.started":"2022-05-07T09:55:16.430203Z","shell.execute_reply":"2022-05-07T09:55:16.464506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:55:16.903663Z","iopub.execute_input":"2022-05-07T09:55:16.904432Z","iopub.status.idle":"2022-05-07T09:55:16.92069Z","shell.execute_reply.started":"2022-05-07T09:55:16.904384Z","shell.execute_reply":"2022-05-07T09:55:16.91955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop('Unnamed: 32', axis=1)\ndf = df.drop('id', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:55:17.392711Z","iopub.execute_input":"2022-05-07T09:55:17.393398Z","iopub.status.idle":"2022-05-07T09:55:17.400068Z","shell.execute_reply.started":"2022-05-07T09:55:17.393345Z","shell.execute_reply":"2022-05-07T09:55:17.39955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(df['diagnosis'])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:55:17.849588Z","iopub.execute_input":"2022-05-07T09:55:17.849985Z","iopub.status.idle":"2022-05-07T09:55:18.06043Z","shell.execute_reply.started":"2022-05-07T09:55:17.849953Z","shell.execute_reply":"2022-05-07T09:55:18.059613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output variables donot have a common distribution hence while splitting them I will use stratify","metadata":{}},{"cell_type":"markdown","source":"# Cluster analysis with the data","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:55:18.159321Z","iopub.execute_input":"2022-05-07T09:55:18.160213Z","iopub.status.idle":"2022-05-07T09:55:18.171736Z","shell.execute_reply.started":"2022-05-07T09:55:18.160166Z","shell.execute_reply":"2022-05-07T09:55:18.171142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.drop('id',axis=1)\ntrain_data = train_data.drop('Unnamed: 32',axis=1)\n\ntrain_data['diagnosis'] = train_data['diagnosis'].map({'M':1,'B':0})\n\n# Scaling the dataset\ndatas = pd.DataFrame(preprocessing.scale(train_data.iloc[:,1:32]))\ndatas.columns = list(train_data.iloc[:,1:32].columns)\n\nX = datas\ny = train_data['diagnosis']","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:55:18.264831Z","iopub.execute_input":"2022-05-07T09:55:18.265631Z","iopub.status.idle":"2022-05-07T09:55:18.279567Z","shell.execute_reply.started":"2022-05-07T09:55:18.265585Z","shell.execute_reply":"2022-05-07T09:55:18.278896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nY = pca.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:55:18.358266Z","iopub.execute_input":"2022-05-07T09:55:18.358709Z","iopub.status.idle":"2022-05-07T09:55:18.372218Z","shell.execute_reply.started":"2022-05-07T09:55:18.358677Z","shell.execute_reply":"2022-05-07T09:55:18.370834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### KMeans cluster analysis\ndatas['diagnosis'] = train_data['diagnosis']\n\n\nfrom sklearn.cluster import KMeans\n\nkmns = KMeans(n_clusters=2, algorithm='full')\nKY = kmns.fit_predict(X)\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n\nax1.scatter(Y[:,0],Y[:,1],  c=KY, cmap = \"jet\", edgecolor = \"None\", alpha=0.35)\nax1.set_title('k-means clustering plot')\n\nax2.scatter(Y[:,0],Y[:,1],  c = datas['diagnosis'], cmap = \"jet\", edgecolor = \"None\", alpha=0.35)\nax2.set_title('Actual clusters')","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:55:18.449479Z","iopub.execute_input":"2022-05-07T09:55:18.449877Z","iopub.status.idle":"2022-05-07T09:55:19.764815Z","shell.execute_reply.started":"2022-05-07T09:55:18.449832Z","shell.execute_reply":"2022-05-07T09:55:19.763691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Agglomerative Clustering analysis\n\nfrom sklearn.cluster import AgglomerativeClustering\naggC = AgglomerativeClustering()\nkY = aggC.fit_predict(X)\n\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n\n\nax1.scatter(Y[:,0],Y[:,1],  c=kY, cmap = \"jet\", edgecolor = \"None\", alpha=0.35)\nax1.set_title('Hierarchical clustering plot')\n\nax2.scatter(Y[:,0],Y[:,1],  c = datas['diagnosis'], cmap = \"jet\", edgecolor = \"None\", alpha=0.35)\nax2.set_title('Actual clusters')","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:55:19.766758Z","iopub.execute_input":"2022-05-07T09:55:19.767155Z","iopub.status.idle":"2022-05-07T09:55:20.352175Z","shell.execute_reply.started":"2022-05-07T09:55:19.767106Z","shell.execute_reply":"2022-05-07T09:55:20.351345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['diagnosis',\n        'radius_mean', \n        'texture_mean', \n        'perimeter_mean', \n        'area_mean', \n        'smoothness_mean', \n        'compactness_mean', \n        'concavity_mean',\n        'concave points_mean', \n        'symmetry_mean', \n        'fractal_dimension_mean']\n\nsns.pairplot(data=df[cols], hue='diagnosis', palette='RdBu')","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:55:20.353196Z","iopub.execute_input":"2022-05-07T09:55:20.353426Z","iopub.status.idle":"2022-05-07T09:56:27.837984Z","shell.execute_reply.started":"2022-05-07T09:55:20.353399Z","shell.execute_reply":"2022-05-07T09:56:27.836268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Perimeter mean, Area Mean and Radius Mean are implying multicollinearity\n2. Concavity_mean, concavePoints_means, Compactness_mean imply multicollinearity","metadata":{}},{"cell_type":"code","source":"corr_mat = df.corr()\ncols = corr_mat.index\n\nf, ax = plt.subplots(figsize=(25, 25))\nhm = sns.heatmap(corr_mat,vmin=-1, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, yticklabels=cols.values, xticklabels=cols.values)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:56:27.840609Z","iopub.execute_input":"2022-05-07T09:56:27.841004Z","iopub.status.idle":"2022-05-07T09:56:32.552625Z","shell.execute_reply.started":"2022-05-07T09:56:27.84096Z","shell.execute_reply":"2022-05-07T09:56:32.551978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. There is high correlation between radius_mean with perimeter_mean and area_mean.(similar for radius_se,perimeter_se, and area_se and radius_worst,perimeter_worst,area_worst)\n2. There is high correlation between compactness, concavity and concave_points\n3. There is high correlation between mean values and worst values","metadata":{}},{"cell_type":"code","source":"### Standardizing the data\n\nX = df[cols].values\n\ny = df['diagnosis'].map({'M':1,'B':0})","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:56:37.251146Z","iopub.execute_input":"2022-05-07T09:56:37.251975Z","iopub.status.idle":"2022-05-07T09:56:37.259353Z","shell.execute_reply.started":"2022-05-07T09:56:37.251932Z","shell.execute_reply":"2022-05-07T09:56:37.258545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using stratify to split the data into training and test set","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state = 21,stratify=y)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:56:37.260456Z","iopub.execute_input":"2022-05-07T09:56:37.261012Z","iopub.status.idle":"2022-05-07T09:56:37.274202Z","shell.execute_reply.started":"2022-05-07T09:56:37.26097Z","shell.execute_reply":"2022-05-07T09:56:37.273274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Running a bunch of classification models with default values to pick out the classifier. We will fine tune the model with the most accuracy","metadata":{}},{"cell_type":"code","source":"key = ['LogisticRegression','KNeighborsClassifier','SVC','DecisionTreeClassifier','RandomForestClassifier','GradientBoostingClassifier','AdaBoostClassifier','XGBClassifier']\nvalue = [LogisticRegression(),KNeighborsClassifier(n_neighbors = 2),SVC(random_state=15),DecisionTreeClassifier(random_state=10), RandomForestClassifier(n_estimators=60, random_state=0), GradientBoostingClassifier(random_state=20), AdaBoostClassifier(), xgb.XGBClassifier(random_state=0,booster=\"gbtree\")]\n\nmodels = dict(zip(key,value))\nmodels","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:56:37.275312Z","iopub.execute_input":"2022-05-07T09:56:37.275559Z","iopub.status.idle":"2022-05-07T09:56:37.287769Z","shell.execute_reply.started":"2022-05-07T09:56:37.275531Z","shell.execute_reply":"2022-05-07T09:56:37.28726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\naccurarcy = []\n\nfor name,algo in models.items():\n    model = algo\n    model.fit(X_train_scaled,y_train)\n    predict = model.predict(X_test_scaled)\n    acc = accuracy_score(y_test, predict)\n    accurarcy.append(acc)\n\nperformance = dict(zip(key,accurarcy))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:56:37.289819Z","iopub.execute_input":"2022-05-07T09:56:37.290453Z","iopub.status.idle":"2022-05-07T09:56:38.503779Z","shell.execute_reply.started":"2022-05-07T09:56:37.290421Z","shell.execute_reply":"2022-05-07T09:56:38.503112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"performance","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:56:38.504963Z","iopub.execute_input":"2022-05-07T09:56:38.505585Z","iopub.status.idle":"2022-05-07T09:56:38.514546Z","shell.execute_reply.started":"2022-05-07T09:56:38.505544Z","shell.execute_reply":"2022-05-07T09:56:38.513859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVC and Logistic Regression seem to have high performance(other models have similar or lower performance).","metadata":{}},{"cell_type":"code","source":"### KFOLD cross validation and hyperparameter tunning for LOGISTIC REGRESSION\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\n\npenalty_values = ['l1', 'l2', 'elasticnet']\nclass_weight_values = ['balanced']\nsolver_values = ['liblinear']\n\nparam_grid = dict(penalty=penalty_values,class_weight=class_weight_values,solver=solver_values)\nmodel = LogisticRegression()\n\nkFold = KFold(n_splits=10)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy',cv=kFold) \n\n## Fit the model \ngrid_result = grid.fit(X_train_scaled,y_train)\nprint(\"Best score: %f\" % (grid_result.best_score_))\nprint(grid_result.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:56:38.515601Z","iopub.execute_input":"2022-05-07T09:56:38.519457Z","iopub.status.idle":"2022-05-07T09:56:38.626108Z","shell.execute_reply.started":"2022-05-07T09:56:38.519404Z","shell.execute_reply":"2022-05-07T09:56:38.625338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nmodel = LogisticRegression(class_weight='balanced', penalty='l2', solver='liblinear')\nmodel.fit(X_train_scaled,y_train)\n\npredictions_logisticRegression = model.predict(X_test_scaled)\nprint(\"Accuracy score %f\" % accuracy_score(y_test, predictions_logisticRegression))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:56:38.627191Z","iopub.execute_input":"2022-05-07T09:56:38.6275Z","iopub.status.idle":"2022-05-07T09:56:38.638879Z","shell.execute_reply.started":"2022-05-07T09:56:38.627468Z","shell.execute_reply":"2022-05-07T09:56:38.638272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### KFOLD cross validation and hyperparameter tunning for SVC\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\n\nc_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\nkernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n\nparam_grid = dict(C=c_values, kernel=kernel_values)\nmodel = SVC()\n\nkFold = KFold(n_splits=10)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=kFold)\n\n## Fit the model\ngrid_result = grid.fit(rescaledX,y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:56:38.639964Z","iopub.execute_input":"2022-05-07T09:56:38.640701Z","iopub.status.idle":"2022-05-07T09:56:40.809072Z","shell.execute_reply.started":"2022-05-07T09:56:38.640658Z","shell.execute_reply":"2022-05-07T09:56:40.808165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nmodel = SVC(C=1.5, kernel='rbf')\nmodel.fit(X_train_scaled,y_train)\n\npredictions_SVC = model.predict(X_test_scaled)\nprint(\"Accuracy score %f\" % accuracy_score(y_test, predictions_SVC))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T09:57:17.844941Z","iopub.execute_input":"2022-05-07T09:57:17.845203Z","iopub.status.idle":"2022-05-07T09:57:17.859802Z","shell.execute_reply.started":"2022-05-07T09:57:17.845174Z","shell.execute_reply":"2022-05-07T09:57:17.859047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Having a look at misclassified points for SVC\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nprint(classification_report(y_test, predictions_SVC, digits=3))\n\ncfm = confusion_matrix(y_test, predictions_SVC)\n\ntrue_negative = cfm[0][0]\nfalse_positive = cfm[0][1]\nfalse_negative = cfm[1][0]\ntrue_positive = cfm[1][1]\n\nprint('Confusion Matrix: \\n', cfm, '\\n')\n\nprint('True Negative:', true_negative)\nprint('False Positive:', false_positive)\nprint('False Negative:', false_negative)\nprint('True Positive:', true_positive)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T10:05:07.776348Z","iopub.execute_input":"2022-05-07T10:05:07.776965Z","iopub.status.idle":"2022-05-07T10:05:07.793112Z","shell.execute_reply.started":"2022-05-07T10:05:07.776926Z","shell.execute_reply":"2022-05-07T10:05:07.792145Z"},"trusted":true},"execution_count":null,"outputs":[]}]}