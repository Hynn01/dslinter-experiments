{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport warnings\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NaN Values\nThe data given by this competition has many NaN values and upper outliers in it. It was crucial to use an imputer to handle these values. We used the imputer implementation in here and edited it slightly:\n\nhttps://github.com/analokmaus/kuma_utils/blob/master/preprocessing/imputer.py\n\nThanks [@analokamus](https://www.kaggle.com/analokamus) for the neat implementation!\n\nBut to reduce notebook runtime, this notebook will use pre-imputed dataframes. You can also run your own tests with them.\n\nYou can access the imputed data: https://www.kaggle.com/datasets/nlztrk/imputed-dataset-enerjisa-retim","metadata":{}},{"cell_type":"code","source":"IMPUTE = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def analyze_column(input_series: pd.Series) -> str:\n    if pd.api.types.is_numeric_dtype(input_series):\n        return 'numeric'\n    else:\n        return 'categorical'\n    \n\n\nclass LGBMImputer:\n    '''\n    Regression imputer using LightGBM\n    '''\n\n    def __init__(self, cat_features=[], n_iter=15000, verbose=False):\n        self.n_iter = n_iter\n        self.cat_features = cat_features\n        self.verbose = verbose\n        self.n_features = None\n        self.feature_names = None\n        self.feature_with_missing = None\n        self.imputers = {}\n        self.offsets = {}\n        self.objectives = {}\n        \n    def fit_transform(self, X, y=None):\n        output_X = X.copy()\n        self.n_features = X.shape[1]\n        if isinstance(X, pd.DataFrame):\n            self.feature_names = X.columns.tolist()\n        else:\n            self.feature_names = [f'f{i}' for i in range(self.n_features)]\n            X = pd.DataFrame(X, columns=self.feature_names)\n        self.feature_with_missing = [col for col in self.feature_names if X[col].isnull().sum() > 0]\n\n        for icol, col in enumerate(self.feature_with_missing):\n            if icol in self.cat_features:\n                nuni = X[col].dropna().nunique()\n                if nuni == 2:\n                    params = {\n                        'objective': 'binary'\n                    }\n                elif nuni > 2:\n                    params = {\n                        'objective': 'multiclass',\n                        'num_class': nuni + 1\n                    }\n            else: # automatic analyze column\n                if analyze_column(X[col]) == 'numeric':\n                    params = {\n                        'objective': 'regression'\n                    }\n                else:\n                    nuni = X[col].dropna().nunique()\n                    if nuni == 2:\n                        params = {\n                            'objective': 'binary'\n                        }\n                    elif nuni > 2:\n                        params = {\n                            'objective': 'multiclass',\n                            'num_class': nuni + 1\n                        }\n                    else:\n                        print(f'column {col} has only one unique value.')\n                        continue\n          \n            params['verbosity'] = -1\n            \n            null_idx = X[col].isnull()\n            x_train = X.loc[~null_idx].drop(col, axis=1)\n            x_test = X.loc[null_idx].drop(col, axis=1)\n            y_offset = X[col].min()\n            y_train = X.loc[~null_idx, col].astype(int) - y_offset\n            dtrain = lgb.Dataset(\n                data=x_train,\n                label=y_train\n            )\n\n            early_stopping_rounds = 50\n            model = lgb.train(\n                params, dtrain, valid_sets=[dtrain], \n                num_boost_round=self.n_iter,\n                early_stopping_rounds=early_stopping_rounds,\n                verbose_eval=0,\n            )\n\n            y_test = model.predict(x_test)\n            if params['objective'] == 'multiclass':\n                y_test = np.argmax(y_test, axis=1).astype(float)\n            elif params['objective'] == 'binary':\n                y_test = (y_test > 0.5).astype(float)\n            y_test += y_offset\n            output_X.loc[null_idx, col] = y_test\n            if params['objective'] in ['multiclass', 'binary']:\n                output_X[col] = output_X[col].astype(int)\n            self.imputers[col] = model\n            self.offsets[col] = y_offset\n            self.objectives[col] = params['objective']\n            if self.verbose:\n                print(f'{col}:\\t{self.objectives[col]}...iter{model.best_iteration}/{self.n_iter}')\n        \n        return output_X    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IMPUTE:\n    FEATURE_CSV_PATH = \"../input/enerjisa-uretim-hackathon/features.csv\"\n    POWER_CSV_PATH = \"../input/enerjisa-uretim-hackathon/power.csv\"\n\n    feature_df = pd.read_csv(FEATURE_CSV_PATH)\n    power_df = pd.read_csv(POWER_CSV_PATH)\n    feature_df = feature_df.merge(power_df, how=\"left\", on=\"Timestamp\")\n    \n    feature_df = feature_df.replace(99999.0,np.nan)\n\n    imputer = LGBMImputer(verbose=True)\n    feature_df.loc[:, feature_df.columns[1:-1]] = imputer.fit_transform(feature_df[feature_df.columns[1:-1]])\n\n    feature_df[\"month\"] = pd.to_datetime(feature_df[\"Timestamp\"]).dt.month\n    feature_df[\"year\"] = pd.to_datetime(feature_df[\"Timestamp\"]).dt.year\n    feature_df[\"hour\"] = pd.to_datetime(feature_df[\"Timestamp\"]).dt.hour\n    feature_df[\"week\"] = pd.to_datetime(feature_df[\"Timestamp\"]).dt.week\n\n    test_mask = pd.to_datetime(feature_df[\"Timestamp\"]) > \"2021-08-14 23:50:00\"\n\n    train_df = feature_df[~test_mask].copy().reset_index(drop=True)\n    test_df = feature_df[test_mask].copy().reset_index(drop=True)\n    \nelse:\n    TRAIN_CSV_PATH = \"../input/imputed-dataset-enerjisa-retim/train_imputed.csv\"\n    TEST_CSV_PATH = \"../input/imputed-dataset-enerjisa-retim/test_imputed.csv\"\n    SUBM_CSV_PATH = \"../input/enerjisa-uretim-hackathon/sample_submission.csv\"\n\n    train_df = pd.read_csv(TRAIN_CSV_PATH).set_index(\"Timestamp\")\n    test_df = pd.read_csv(TEST_CSV_PATH).set_index(\"Timestamp\")\n    subm_df = pd.read_csv(SUBM_CSV_PATH).drop(columns=\"Power(kW)\", axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating training and test dataframes","metadata":{}},{"cell_type":"code","source":"label = \"Power(kW)\"\nexcept_cols = [\"year\", \"month\", \"week\", \"hour\"]\nX_train = train_df.drop(labels = except_cols + [label], axis=1)\ny_train = train_df[label]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating CV splits\nWe are being evaluated with RMSE in this competition. So how far our predicted values are from the ground truths are crucial. We need to keep that in mind while executing our cross-validations. Let's look at the distribution of our label.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\n_ = sns.histplot(data=y_train, kde=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that most values are clustered at two opposite ends of the distribution. It may be advantageous to maintain this distribution in all our folds.","metadata":{}},{"cell_type":"code","source":"def create_cont_folds(df, n_s=8, n_grp=1000):\n    \n    skf = StratifiedKFold(n_splits=n_s, shuffle=True, random_state=1337)\n    grp = pd.cut(df, n_grp, labels=False)\n    target = grp\n    \n    fold_nums = np.zeros(len(df))\n    for fold_no, (t, v) in enumerate(skf.split(target, target)):\n        fold_nums[v] = fold_no\n        \n    return fold_nums","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_of_folds = 8\nnum_of_groups = 40\n\nfoldnums = create_cont_folds(y_train, n_s=num_of_folds, n_grp=num_of_groups)\ncv_splits = []\n\nfor i in range(num_of_folds):\n    test_indices = np.argwhere(foldnums==i).flatten()\n    train_indices = list(set(range(len(y_train))) - set(test_indices))\n    cv_splits.append((train_indices, test_indices))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PLOT_FOLD_NUM = 2\n\nfig, axs = plt.subplots(2, figsize=(12,10))\nsns.histplot(data=y_train.iloc[cv_splits[PLOT_FOLD_NUM][0]], kde=True, ax=axs[0]).set(title='Train Set Distribution')\nsns.histplot(data=y_train.iloc[cv_splits[PLOT_FOLD_NUM][1]], kde=True, ax=axs[1]).set(title='Validation Set Distribution')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like we are preserving the label distribution over all folds! You can also try different fold IDs to validate that behaviour. Doing this didn't improved our CV or Kaggle scores, but it definitely is a more robust CV splitting method.","metadata":{}},{"cell_type":"markdown","source":"### Level 1: Prediction with LGBM & CatBoost\nWe ran predictions for the Kaggle test set with each CV models separately.","metadata":{}},{"cell_type":"code","source":"cv_models = []\nrms_errs = []\n\ncat_preds = []\nlgb_preds = []\n\ncat_final_preds = []\nlgb_final_preds = []\n\nlgb_params = {\n    \"learning_rate\": 0.02,\n    \"num_leaves\": 64,\n    \"colsample_bytree\": 0.9,\n    \"subsample\": 0.9,\n    \"verbosity\": -1,\n    \"n_estimators\": 7000,\n    \"early_stopping_rounds\": 50,\n    \"random_state\": 42,\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n}\n\nfor split_train, split_val in tqdm(cv_splits):\n    split_train = X_train.index[split_train]\n    split_val = X_train.index[split_val]\n\n    model1 = CatBoostRegressor(\n        iterations=5000,\n        random_state=42,\n        early_stopping_rounds=50,\n    )\n\n    model2 = lgb.LGBMRegressor(**lgb_params)\n \n    train_x, train_y = X_train.loc[split_train], y_train.loc[split_train]\n    test_x, test_y = X_train.loc[split_val], y_train.loc[split_val]\n\n    model1.fit(\n        train_x,\n        train_y,\n        eval_set=(test_x, test_y),\n        verbose=500,\n    )\n    model2.fit(\n        train_x,\n        train_y,\n        eval_set=(test_x, test_y),\n        verbose=500,\n    )\n\n    preds = (model1.predict(test_x) + model2.predict(test_x)) / 2\n    rms = mean_squared_error(test_y, preds, squared=False)\n    rms_errs.append(rms)\n\n    cat_preds.append(model1.predict(test_x))\n    lgb_preds.append(model2.predict(test_x))\n    \n    cat_final_preds.append(\n        model1.predict((test_df.drop(labels=except_cols + [label], axis=1)))\n    )\n    \n    lgb_final_preds.append(\n        model2.predict((test_df.drop(labels=except_cols + [label], axis=1)))\n    )\n\n    print(\"RMSE:\", rms)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We stored the prediction of the models for both CV folds and the Kaggle test sets for the next step.","metadata":{}},{"cell_type":"code","source":"cat_final_preds = [pd.DataFrame(cat_final_preds).mean(axis=0).values]\nlgb_final_preds = [pd.DataFrame(lgb_final_preds).mean(axis=0).values]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"two_stage_feats = (\n    pd.DataFrame(\n        {\"cat_pred\": np.concatenate(cat_preds), \"lgb_pred\": np.concatenate(lgb_preds)},\n        index=pd.concat(\n            [\n                pd.Series(X_train.loc[X_train.index[test_indices]].index)\n                for (_, test_indices) in cv_splits\n            ]\n        ),\n    )\n    .reset_index()\n    .set_index(\"Timestamp\")\n)\n\ntwo_stage_preds = (\n    pd.DataFrame(\n        {\n            \"cat_pred\": np.concatenate(cat_final_preds),\n            \"lgb_pred\": np.concatenate(lgb_final_preds),\n        },\n        index=(test_df.drop(labels=except_cols + [label], axis=1)).index,\n    )\n    .reset_index()\n    .set_index(\"Timestamp\")\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Level 2: Stacking with LGBM & CatBoost\nWe also used stacking and used LGBM and CatBoost as meta-regressors. We used the equally blended predictions of them as our final predictions. We have seen that this approach improved both our CV and Kaggle score. ***(Public 19.46 -> 18.33)***","metadata":{}},{"cell_type":"code","source":"rms_errs = []\n\nfinal_preds = []\ncat_models = []\nlgb_models = []\n\nlgb_params = {\n    \"learning_rate\": 0.02,\n    \"num_leaves\": 64,\n    \"colsample_bytree\": 0.9,\n    \"subsample\": 0.9,\n    \"verbosity\": -1,\n    \"n_estimators\": 7000,\n    \"early_stopping_rounds\": 50,\n    \"random_state\": 42,\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n}\n\n\nfor split_train, split_val in tqdm(cv_splits):\n    split_train = X_train.index[split_train]\n    split_val = X_train.index[split_val]\n\n    model1 = CatBoostRegressor(\n        iterations=5000,\n        random_state=42,\n        early_stopping_rounds=50,\n    )\n\n    model2 = lgb.LGBMRegressor(**lgb_params)\n\n    train_x, train_y = (\n        pd.concat([X_train.loc[split_train], two_stage_feats.loc[split_train]], axis=1),\n        y_train.loc[split_train],\n    )\n    test_x, test_y = (\n        pd.concat([X_train.loc[split_val], two_stage_feats.loc[split_val]], axis=1),\n        y_train.loc[split_val],\n    )\n\n    model1.fit(\n        train_x,\n        train_y,\n        eval_set=(test_x, test_y),\n        verbose=500,\n    )\n    \n    model2.fit(\n        train_x,\n        train_y,\n        eval_set=(test_x, test_y),\n        verbose=500,\n    )\n\n    preds = (model1.predict(test_x) + model2.predict(test_x)) / 2\n    rms = mean_squared_error(test_y, preds, squared=False)\n    rms_errs.append(rms)\n\n    final_preds.append(\n        (\n            model1.predict(\n                pd.concat(\n                    [\n                        (test_df.drop(labels=except_cols + [label], axis=1)),\n                        two_stage_preds,\n                    ],\n                    axis=1,\n                )\n            )\n            + model2.predict(\n                pd.concat(\n                    [\n                        (test_df.drop(labels=except_cols + [label], axis=1)),\n                        two_stage_preds,\n                    ],\n                    axis=1,\n                )\n            )\n        )\n        / 2\n    )\n\n    print(\"RMSE:\", rms)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Current Submission CV:\", rms_errs)\nprint(\"Current Submission CV Mean:\", np.array(rms_errs).mean())\nprint(\"Current Submission CV Std:\", np.array(rms_errs).std())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating the submission file","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(final_preds, columns=test_df.index).mean(axis=0).to_frame(\n    \"Power(kW)\"\n).reset_index().to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}