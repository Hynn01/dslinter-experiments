{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# JPX: LGBM Regressor (Fit in 1 Min)\n","metadata":{"papermill":{"duration":0.018454,"end_time":"2022-04-21T17:14:46.650744","exception":false,"start_time":"2022-04-21T17:14:46.63229","status":"completed"},"tags":[],"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"<center><img src=\"https://lightgbm.readthedocs.io/en/latest/_images/LightGBM_logo_black_text.svg\" height=250 width=250></center>\n<hr>\n<center>LightGBM = üå≥ + üöÄ + ‚ò¢Ô∏è</center>","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"\nLightGBM is the current \"Meta\" on kaggle and it doesn't look like it is going to get Nerfed anytime soon!\nIt is basiclly a \"light\" version of gradient boosting machines framework that aims to increases efficiency and reduces memory usage.\n\n**It is usually THE Algorithm everyone on Kaggle try when facing a tabular dataset**\n\n><h4>TL;DR: What makes LightGBM so great:</h4>\n>\n>1. LGBM was developed and maintained by Microsoft themselves so it gets constant maintenance and support.\n>2. Easy to use\n>3. Faster than nearly all other gradient boosting algorithms.\n>4. Usually the most powerful gradient boosting.\n\n\nIt is a **gradient boosting** model that makes use of tree based learning algorithms. It is considered to be a fast processing algorithm.\n\nWhile other algorithms trees grow horizontally, LightGBM algorithm grows vertically, meaning it grows leaf-wise and other algorithms grow level-wise. LightGBM chooses the leaf with large loss to grow. It can lower down more loss than a level wise algorithm when growing the same leaf.\n\n![img](https://i.imgur.com/pzOP2Lb.png)\n\n[Source of Image](https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/)\n\nLight GBM is prefixed as Light because of its high speed. Light GBM can handle the large size of data and takes lower memory to run.\n\nAnother reason why Light GBM is so popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development.\n\n<h4>Leaf growth technique in LightGBM</h4>\n\nLightGBM uses leaf-wise (best-first) tree growth. It chooses to grow the leaf that minimizes the loss, allowing a growth of an imbalanced tree. Because it doesn‚Äôt grow level-wise, but leaf-wise, over-fitting can happen when data is small. In these cases, it is important to control the tree depth.\n\n<h4>LightGBM vs XGBoost</h4>\n\nbase learner of almost all of the competitions that have structured datasets right now. This is mostly because of LightGBM's implementation; it doesn't do exact searches for optimal splits like XGBoost does in it's default setting but rather through histogram approximations (XGBoost now has this functionality as well but it's still not as fast as LightGBM).\n\nThis results in slight decrease of predictive performance buy much larger increase of speed. This means more opportunity for feature engineering/experimentation/model tuning which inevitably yields larger increases in predictive performance. (Feature engineering are the key to winning most Kaggle competitions)\n\n\n<h4>LightGBM vs Catboost</h4>\n\nCatBoost is not used as much, mostly because it tends to be much slower than LightGBM and XGBoost. That being said, CatBoost is very different when it comes to the implementation of gradient boosting. This can give slightly more accurate predictions, in particular if you have large amounts of categorical features. Because rapid experimentation is vital in Kaggle competitions, LightGBM tends to be the go-to algorithm when first creating strong base learners.\n\nIn general, it is important to note that a large amount of approaches involves combining all three boosting algorithms in an ensemble. LightGBM, CatBoost, and XGBoost might be thrown together in a mix to create a strong ensemble. This is done to really squeeze spots on the leaderboard and it usually works.\n\n<div class=\"alert alert-block alert-info\">\n<b>Read More:</b>\n<ul>\n    <li><a href = \"https://github.com/microsoft/LightGBM/tree/master/python-package\">LightGBM Github Documentation</a></li>\n    <li><a href = \"https://lightgbm.readthedocs.io/en/latest/Features.html\">All features of LightGBM</a></li>\n    <li><a href = \"https://lightgbm.readthedocs.io/en/latest/index.html\">Official Documentation</a></li>\n</ul>\n</div>\n\n____\n\n<h3>Hyper-Parameter Tuning in LightGBM</h3>\n\n____\n\nParameter Tuning is an important part that is usually done by data scientists to achieve a good accuracy, fast result and to deal with overfitting. Let us see quickly some of the parameter tuning you can do for better results.\nWhile, LightGBM has more than 100 parameters that are given in the [documentation of LightGBM](https://github.com/microsoft/LightGBM), we are going to check the most important ones.\n\n**num_leaves**: This parameter is responsible for the complexity of the model. I normally start by trying values in the range [10,100]. But if you have a solid heuristic to choose tree depth you can always use it and set num_leaves to 2^tree_depth - 1\n\n[LightGBM Documentation](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html) says in respect -\nThis is the main parameter to control the complexity of the tree model. Theoretically, we can set num_leaves = 2^(max_depth) to obtain the same number of leaves as depth-wise tree. However, this simple conversion is not good in practice. The reason is that a leaf-wise tree is typically much deeper than a depth-wise tree for a fixed number of leaves. Unconstrained depth can induce over-fitting. Thus, when trying to tune the num_leaves, we should let it be smaller than 2^(max_depth). For example, when the max_depth=7 the depth-wise tree can get good accuracy, but setting num_leaves to 127 may cause over-fitting, and setting it to 70 or 80 may get better accuracy than depth-wise.\n\n**Min_data_in_leaf**: Assigning bigger value to this parameter can result in underfitting of the model. Giving it a value of 100 or 1000 is sufficient for a large dataset.\n\n**Max_depth**: Controls the depth of the individual trees. Typical values range from a depth of 3‚Äì8 but it is not uncommon to see a tree depth of 1. Smaller depth trees are computationally efficient (but require more trees); however, higher depth trees allow the algorithm to capture unique interactions but also increase the risk of over-fitting. Larger training data sets are more tolerable to deeper trees.\n\n**num_iterations**: Num_iterations specifies the number of boosting iterations (trees to build). The more trees you build the more accurate your model can be at the cost of:\n    - Longer training time\n    - Higher chance of over-fitting\nSo typically start with a lower number of trees to build a baseline and increase it later when you want to squeeze the last % out of your model.\n\nIt is recommended to use smaller `learning_rate` with larger `num_iterations`. Also, we should use `early_stopping_rounds` if we go for higher `num_iterations` to stop your training when it is not learning anything useful.\n\n**early_stopping_rounds** - \"early stopping\" refers to stopping the training process if the model's performance on a given validation set does not improve for several consecutive iterations. This parameter will stop training if the validation metric is not improving after the last early stopping round. It should be defined in pair with a number of iterations. If we set it too large we increase the chance of over-fitting. **The rule of thumb is to have it at 10% of your `num_iterations`**.\n\n____\n\n<h3>Other Parameters Overview</h3>\n\n____\n\n**Parameters that control the trees of LightGBM**\n\n- num_leaves: controls the number of decision leaves in a single tree. there will be multiple trees in pool.\n- min_data_in_leaf: the minimum number of data/sample/count per leaf (default is 20; lower min_data_in_leaf means less conservative/control, potentially overfitting).\n- max_depth: this the height of a decision tree. if its more possibility of overfitting but too low may underfit.\n>**NOTE:** max_depth directly impacts:\n>1. The best value for the num_leaves parameter\n>2. Model Performance\n>3. Training Time\n\n____\n\n**Parameters For Better Accuracy**\n\n- Use large max_bin (may be slower)\n\n Use small learning_rate with large num_iterations\n\n- Use large num_leaves (may cause over-fitting)\n\n- Use bigger training data\n\n- Try dart\n\n____\n\n**Parameters for Dealing with Over-fitting**\n\n- Use small max_bin\n\n- Use small num_leaves\n\n- Use min_data_in_leaf and min_sum_hessian_in_leaf\n\n- Use bagging by set bagging_fraction and bagging_freq\n\n- Use feature sub-sampling by set feature_fraction\n\n- Use bigger training data\n\n- Try lambda_l1, lambda_l2 and min_gain_to_split for regularization\n\n- Try max_depth to avoid growing deep tree\n\n- Try extra_trees\n\n- Try increasing path_smooth\n\n____\n\n\n<h3>How to tune LightGBM like a boss?</h3>\n\nHyperparameters tuning guide:\n\n**objective**\n * When you change it affects other parameters\tSpecify the type of ML model\n * default- value regression\n * aliases- Objective_type\n\n**boosting**\n * If you set it RF, that would be a bagging approach\n * default- gbdt\n * Range- [gbdt, rf, dart, goss]\n * aliases- boosting_type\n\n**lambda_l1**\n * regularization parameter\n * default- 0.0\n * Range- [0, ‚àû]\n * aliases- reg_alpha\n * constraints- lambda_l1 >= 0.0\n\n**bagging_fraction**\n * randomly select part of data without resampling\n * default-1.0\n * range- [0, 1]\n * aliases- Subsample\n * constarints- 0.0 < bagging_fraction <= 1.0\n\n**bagging_freq**\n * default- 0.0\n * range- [0, ‚àû]\n * aliases- subsample_freq\n * bagging_fraction should be set to value smaller than 1.0 as well 0 means disable bagging\n\n**num_leaves**\n * max number of leaves in one tree\n * default- 31\n * Range- [1, ‚àû]\n * Note- 1 < num_leaves <= 131072\n\n**feature_fraction**\n * if you set it to 0.8, LightGBM will select 80% of features\n * default- 1.0\n * Range- [0, 1]\n * aliases- sub_feature\n * constarint- 0.0 < feature_fraction <= 1.0\n\n**max_depth**\n * default- [-1]\n * range- [-1, ‚àû]m\n * Larger is usually better, but overfitting speed increases.\n * limit the max depth Forr tree model\n\n**max_bin**\n * deal with over-fitting\n * default- 255\n * range- [2, ‚àû]\n * aliases- Histogram Binning\n * max_bin > 1\n\n**num_iterations**\n * number of boosting iterations\n * default- 100\n * range- [1, ‚àû]\n * AKA- Num_boost_round, n_iter\n * constarints- num_iterations >= 0\n\n**learning_rate**\n * default- 0.1\n * range- [0 1]\n * aliases- eta\n * general values- learning_rate > 0.0Typical: 0.05.\n\n**early_stopping_round**\n * will stop training if validation doesn‚Äôt improve in last early_stopping_round\n * Model Performance, Number of Iterations, Training Time\n * default- 0\n * Range- [0, ‚àû]\n\n**categorical_feature**\n * to sepecify or Handle categorical features\n * i.e LGBM automatically handels categorical variable we dont need to one hot encode them.\n\n**bagging_freq**\n * default-0.0\n * Range-[0, ‚àû]\n * aliases- subsample_freq\n * note- 0 means disable bagging; k means perform bagging at every k iteration\n * enable    bagging, bagging_fraction should be set to value smaller than 1.0 as well\n\n**verbosity**\n * default- 0\n * range- [-‚àû, ‚àû]\n * aliases- verbose\n * constraints- {< 0: Fatal, = 0: Error (Warning), = 1: Info, > 1}\n\n**min_data_in_leaf**\n * Can be used to deal with over-fitting:\n * default- 20\n * constarint-min_data_in_leaf >= 0\n\n\n<div class=\"alert alert-block alert-warning\">\n<b>Introduction Credits:</b>\n<ul>\n    <li><a href = \"https://www.kaggle.com/shivansh002/your-friendly-neighbour-lightgbm\">Your Friendly Neighbour LightGBM</a> By @shivansh002. Thank you @shivansh002 for a great introduction! </li>\n    <li><a href = \"https://www.kaggle.com/paulrohan2020/tutorial-lightgbm-xgboost-catboost-top-11\">Tutorial LightGBM + XGBoost + CatBoost</a> By @paulrohan2020. Thank you @paulrohan2020 for a great tutorial! </li>\n</ul>\n</div>\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"outline\">Libraries üìö</span>\n<hr>","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"#### Code starts here ‚¨á","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{"papermill":{"duration":0.019048,"end_time":"2022-04-21T17:14:46.694252","exception":false,"start_time":"2022-04-21T17:14:46.675204","status":"completed"},"tags":[],"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"import os\nimport traceback\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport lightgbm as lgb\nimport jpx_tokyo_market_prediction\nimport warnings; warnings.filterwarnings(\"ignore\")","metadata":{"papermill":{"duration":1.08262,"end_time":"2022-04-21T17:14:47.794744","exception":false,"start_time":"2022-04-21T17:14:46.712124","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:08:49.840081Z","iopub.execute_input":"2022-04-23T10:08:49.840412Z","iopub.status.idle":"2022-04-23T10:08:52.979833Z","shell.execute_reply.started":"2022-04-23T10:08:49.840323Z","shell.execute_reply":"2022-04-23T10:08:52.979078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_list = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv\")\nprices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\nsupplemental_prices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv\")\ndf_train = pd.concat([prices, supplemental_prices])\ndf_train = pd.merge(df_train, stock_list[['SecuritiesCode', 'Name']], left_on = 'SecuritiesCode', right_on = 'SecuritiesCode', how = 'left')\nstock_list = stock_list.loc[stock_list['SecuritiesCode'].isin(prices['SecuritiesCode'].unique())]\nprint(list(df_train.columns))\nprint(len(list(df_train['SecuritiesCode'].unique())))\ndf_train.head()","metadata":{"papermill":{"duration":6.576157,"end_time":"2022-04-21T17:14:54.386385","exception":false,"start_time":"2022-04-21T17:14:47.810228","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:08:52.981467Z","iopub.execute_input":"2022-04-23T10:08:52.981733Z","iopub.status.idle":"2022-04-23T10:08:59.372525Z","shell.execute_reply.started":"2022-04-23T10:08:52.981699Z","shell.execute_reply":"2022-04-23T10:08:59.37178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"papermill":{"duration":0.013446,"end_time":"2022-04-21T17:14:54.416282","exception":false,"start_time":"2022-04-21T17:14:54.402836","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"markdown","source":"**Feature Extraction**","metadata":{}},{"cell_type":"code","source":"def upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\ndef lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']\n\n# A utility function to build features from the original df\n# It works for rows to, so we can reutilize it.\ndef get_features(df):\n    df_feat = df[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n    df_feat['Upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['Lower_Shadow'] = lower_shadow(df_feat)\n    return df_feat","metadata":{"papermill":{"duration":0.021307,"end_time":"2022-04-21T17:14:54.50521","exception":false,"start_time":"2022-04-21T17:14:54.483903","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:08:59.374105Z","iopub.execute_input":"2022-04-23T10:08:59.374636Z","iopub.status.idle":"2022-04-23T10:08:59.381667Z","shell.execute_reply.started":"2022-04-23T10:08:59.374572Z","shell.execute_reply":"2022-04-23T10:08:59.380843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Main Training Function**","metadata":{"papermill":{"duration":0.01295,"end_time":"2022-04-21T17:14:54.531224","exception":false,"start_time":"2022-04-21T17:14:54.518274","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"def get_Xy_and_model(df_train):\n\n    df_proc = get_features(df_train)\n    df_proc['y'] = df_train['Target']\n    df_proc = df_proc.dropna(how = \"any\")\n    \n    X = df_proc.drop(\"y\", axis=1)\n    y = df_proc[\"y\"]\n    \n    model = lgb.LGBMRegressor(device_type = 'gpu')\n    model.fit(X, y)\n    return X, y, model","metadata":{"papermill":{"duration":0.021361,"end_time":"2022-04-21T17:14:54.565666","exception":false,"start_time":"2022-04-21T17:14:54.544305","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:08:59.385641Z","iopub.execute_input":"2022-04-23T10:08:59.385916Z","iopub.status.idle":"2022-04-23T10:08:59.392933Z","shell.execute_reply.started":"2022-04-23T10:08:59.385881Z","shell.execute_reply":"2022-04-23T10:08:59.392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loop over all securities","metadata":{"papermill":{"duration":0.012924,"end_time":"2022-04-21T17:14:54.59239","exception":false,"start_time":"2022-04-21T17:14:54.579466","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"print(f\"Training model\")\nX, y, model = get_Xy_and_model(df_train)\nXs, ys, models = X, y, model","metadata":{"papermill":{"duration":27.190996,"end_time":"2022-04-21T17:15:21.79649","exception":false,"start_time":"2022-04-21T17:14:54.605494","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:08:59.394688Z","iopub.execute_input":"2022-04-23T10:08:59.394976Z","iopub.status.idle":"2022-04-23T10:09:11.324426Z","shell.execute_reply.started":"2022-04-23T10:08:59.394938Z","shell.execute_reply":"2022-04-23T10:09:11.323812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = get_features(df_train.iloc[1])\ny_pred = models.predict(pd.DataFrame([x]))\ny_pred[0]","metadata":{"papermill":{"duration":0.070417,"end_time":"2022-04-21T17:15:21.891605","exception":false,"start_time":"2022-04-21T17:15:21.821188","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:09:11.325497Z","iopub.execute_input":"2022-04-23T10:09:11.325925Z","iopub.status.idle":"2022-04-23T10:09:11.340462Z","shell.execute_reply.started":"2022-04-23T10:09:11.325887Z","shell.execute_reply":"2022-04-23T10:09:11.339781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{"execution":{"iopub.status.busy":"2021-11-02T20:57:49.349459Z","iopub.status.idle":"2021-11-02T20:57:49.349757Z","shell.execute_reply":"2021-11-02T20:57:49.349613Z","shell.execute_reply.started":"2021-11-02T20:57:49.349596Z"},"papermill":{"duration":0.028085,"end_time":"2022-04-21T17:15:21.949349","exception":false,"start_time":"2022-04-21T17:15:21.921264","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"env = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()","metadata":{"papermill":{"duration":0.031026,"end_time":"2022-04-21T17:15:22.003088","exception":false,"start_time":"2022-04-21T17:15:21.972062","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:09:11.341686Z","iopub.execute_input":"2022-04-23T10:09:11.342047Z","iopub.status.idle":"2022-04-23T10:09:11.348429Z","shell.execute_reply.started":"2022-04-23T10:09:11.342013Z","shell.execute_reply":"2022-04-23T10:09:11.347883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (df_test, options, financials, trades, secondary_prices, df_pred) in iter_test:\n    df_pred['row_id'] = (df_pred['Date'].astype(str) + '_' + df_pred['SecuritiesCode'].astype(str))\n    df_test['row_id'] = (df_test['Date'].astype(str) + '_' + df_test['SecuritiesCode'].astype(str))\n    model = models\n    x_test = get_features(df_test)\n    y_pred = model.predict(x_test)\n    df_pred['Target'] = y_pred\n    df_pred = df_pred.sort_values(by = \"Target\", ascending = False)\n    df_pred['Rank'] = np.arange(0,2000)\n    df_pred = df_pred.sort_values(by = \"SecuritiesCode\", ascending = True)\n    df_pred.drop([\"Target\"], axis = 1)\n    submission = df_pred[[\"Date\", \"SecuritiesCode\", \"Rank\"]]    \n    env.predict(submission)","metadata":{"papermill":{"duration":0.343962,"end_time":"2022-04-21T17:15:22.369514","exception":false,"start_time":"2022-04-21T17:15:22.025552","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:09:11.351795Z","iopub.execute_input":"2022-04-23T10:09:11.353349Z","iopub.status.idle":"2022-04-23T10:09:11.585545Z","shell.execute_reply.started":"2022-04-23T10:09:11.353311Z","shell.execute_reply":"2022-04-23T10:09:11.584841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_pred.columns)","metadata":{"papermill":{"duration":0.022059,"end_time":"2022-04-21T17:15:22.406321","exception":false,"start_time":"2022-04-21T17:15:22.384262","status":"completed"},"tags":[],"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-04-23T10:09:11.587531Z","iopub.execute_input":"2022-04-23T10:09:11.587747Z","iopub.status.idle":"2022-04-23T10:09:11.595372Z","shell.execute_reply.started":"2022-04-23T10:09:11.587721Z","shell.execute_reply":"2022-04-23T10:09:11.594635Z"},"trusted":true},"execution_count":null,"outputs":[]}]}