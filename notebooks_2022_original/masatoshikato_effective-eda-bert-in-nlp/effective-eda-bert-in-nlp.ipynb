{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Effective EDA & BERT in NLP\nNatural Language Processing with Disaster TweetsðŸš’","metadata":{}},{"cell_type":"markdown","source":"#### In my kernel\n1. Points to note\n2. Text Cleaning (regular expression)\n3. BERT (using PyTorch)\n4. Reference","metadata":{}},{"cell_type":"markdown","source":"I explained the methods I have learned about Natural Language Processing. \nI especially focused on EDA (text cleaning) and model design with BERT. After removing noise (such as http://- or HTML or punctuation) in the text and extracting text features, I created models using BERT. This resulted in a high score. I explained in detail what I struggled with and what stopped me, to not leave anything out!","metadata":{}},{"cell_type":"markdown","source":"## 1. Points to note","metadata":{}},{"cell_type":"markdown","source":"* To use a GPU, go to Settingsâ†’Accelerator and set the GPU.\n\n* If you see the error **CUDA error: out of memory**, check GPU Memory in the upper right corner of the kernel. (It is shown to the right of Draft Session (-m)).ã€€      \nIn some cases, stopping the session and resetting it may solve the problem.\n\n* When assembling the model with the class, I set in_features=768. Any other number will cause an error. error: **mat1 and mat2 shapes cannot be multiplied (16x768 and 500x20).** In this case, the number of batches is 16 and in_features is 768.","metadata":{}},{"cell_type":"markdown","source":"### Import libraries and basic EDA","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest  = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsubmission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(1,3,figsize=(14,4))\nplt.subplot(1,3,1); sns.countplot(train['target'], palette='gray', alpha=1.0); plt.xlabel(''), plt.ylabel(''); plt.title('target')\nplt.subplot(1,3,2); plt.hist(train['text'].str.len(), color='black', alpha=0.6); plt.ylabel(''); plt.title('Words in sentences (train)')\nplt.subplot(1,3,3); plt.hist(test['text'].str.len(), color='black', alpha=0.6); plt.ylabel(''); plt.title('Words in sentences (test)')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(50):\n    print(train['text'][i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Cleaning (regular expression)\n\nI converted all text to lowercase, converted http://--- to httpsmark. I also pre-processed HTML(< h >), other symbols(@, #) and punctuation. I thought that some number in the text were meaningless, so I converted numbers such as 2000 or 15000 into \"number\" so that we could see there was some kind of number in the text.","metadata":{}},{"cell_type":"markdown","source":"Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b (for the pre-processing of Emoji)","metadata":{}},{"cell_type":"code","source":"import re\nimport string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+')\n    return url.sub(r' httpsmark ', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)\n\n\ndef remove_atsymbol(text):\n    name = re.compile(r'@\\S+')\n    return name.sub(r' atsymbol ', text)\n\n\ndef remove_hashtag(text):\n    hashtag = re.compile(r'#')\n    return hashtag.sub(r' hashtag ', text)\n\n\ndef remove_exclamation(text):\n    exclamation = re.compile(r'!')\n    return exclamation.sub(r' exclamation ', text)\n\n\ndef remove_question(text):\n    question = re.compile(r'?')\n    return question.sub(r' question ', text)\n\n\ndef remove_punc(text):\n    return text.translate(str.maketrans('','',string.punctuation))\n\n\ndef remove_number(text):\n    number = re.compile(r'\\d+')\n    return number.sub(r' number ', text)\n\n\ndef remove_emoji(string):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               u\"\\U0001f926-\\U0001f937\"\n                               u\"\\U00010000-\\U0010ffff\"\n                               u\"\\u2640-\\u2642\"\n                               u\"\\u2600-\\u2B55\"\n                               u\"\\u200d\"\n                               u\"\\u23cf\"\n                               u\"\\u23e9\"\n                               u\"\\u231a\"\n                               u\"\\ufe0f\"  # dingbats\n                               u\"\\u3030\"\n                               \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r' emoji ', string)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text'] = train['text'].str.lower()\ntrain['text'] = train['text'].apply(lambda text: remove_URL(text))\ntrain['text'] = train['text'].apply(lambda text: remove_html(text))\ntrain['text'] = train['text'].apply(lambda text: remove_atsymbol(text))\ntrain['text'] = train['text'].apply(lambda text: remove_hashtag(text))\ntrain['text'] = train['text'].apply(lambda text: remove_exclamation(text))\ntrain['text'] = train['text'].apply(lambda text: remove_punc(text))\ntrain['text'] = train['text'].apply(lambda text: remove_number(text))\ntrain['text'] = train['text'].apply(lambda text: remove_emoji(text))\n\n\n\n\ntest['text']  = test['text'].str.lower()\ntest['text']  = test['text'].apply(lambda text: remove_URL(text))\ntest['text']  = test['text'].apply(lambda text: remove_html(text))\ntest['text']  = test['text'].apply(lambda text: remove_atsymbol(text))\ntest['text']  = test['text'].apply(lambda text: remove_hashtag(text))\ntest['text']  = test['text'].apply(lambda text: remove_exclamation(text))\ntest['text']  = test['text'].apply(lambda text: remove_punc(text))\ntest['text']  = test['text'].apply(lambda text: remove_number(text))\ntest['text']  = test['text'].apply(lambda text: remove_emoji(text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(50):\n    print(test['text'][i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(1,2,figsize=(14,4))\nplt.subplot(1,2,1); plt.hist(train['text'].str.len(), color='black', alpha=0.6); plt.ylabel(''); plt.title('Number of words in text (train)')\nplt.subplot(1,2,2); plt.hist(test['text'].str.len(), color='black', alpha=0.6); plt.ylabel(''); plt.title('Number of words in text (test)')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING_SIZE = 6090\ntrain_shuffle = train.sample(frac=1, random_state=0)\n\n\ntrain = train_shuffle[0:TRAINING_SIZE]\nvalid = train_shuffle[TRAINING_SIZE:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT (using PyTorch)\n\nWe need to design Dataset, DataLoader and Model using PyTorch.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Data(Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n    def __len__(self):\n        return self.data.shape[0]\n    def __getitem__(self, idx):\n        text = self.data['text'].values[idx]\n        label = self.data['target'].values[idx]\n        return text, torch.tensor(label, dtype=torch.float)\n    \n    \n    \nclass TestData(Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n    def __len__(self):\n        return self.data.shape[0]\n    def __getitem__(self, idx):\n        text = self.data['text'].values[idx]\n        return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = Data(train)\nvalid_ds = Data(valid)\ntest_ds  = TestData(test)\n\n\n\nprint(train_ds[0])\nprint('-'*85)\nprint(train_ds[0][0])\nprint('-'*85)\nprint(train_ds[0][1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dl = DataLoader(train_ds, batch_size=16, shuffle=True, drop_last=False)\nvalid_dl = DataLoader(valid_ds, batch_size=16*2, shuffle=True, drop_last=False)\ntest_dl  = DataLoader(test_ds, batch_size=16, shuffle=False, drop_last=False)\n\n\n\nbatch = next(iter(train_dl))\nprint(len(batch[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer\n\n!pip install transformers -q\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I used a tokenizer to break the text into tokens and convert it into a form that could be entered into BERT.\n\n\n\n* **input_ids** : Token converted to ID  (ex. myâ†’101, brothernlawâ†’2026)\n\n    [CLS] is added to the beginning of the token sequence and [SEP] to the end by default.\n\n* **token_type_ids** : Distinguish between tokens used and not used in learning.\n\n* **attention_mask** : Used to distinguish between two sentences when they are entered as a pair. In this case, it has no meaning.\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:11:42.392492Z","iopub.execute_input":"2022-05-08T02:11:42.39289Z","iopub.status.idle":"2022-05-08T02:11:42.399022Z","shell.execute_reply.started":"2022-05-08T02:11:42.392861Z","shell.execute_reply":"2022-05-08T02:11:42.397947Z"}}},{"cell_type":"code","source":"for text in train['text'].values:\n    encoded = tokenizer.encode_plus(text.lower())\n    \n    \nprint(encoded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoded = tokenizer.decode(encoded['input_ids'])\n\n\nprint(decoded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 50\nencoded = tokenizer.encode_plus(text, padding='max_length', max_length=MAX_LEN, truncation=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nfrom transformers import AutoModel\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained('bert-base-uncased')\n        self.classifier = nn.Linear(in_features=768, out_features=2)\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        pooler_output = outputs.pooler_output\n        logits = self.classifier(pooler_output).squeeze(-1)\n        return logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model().cuda()\n\n\noptimizer = torch.optim.AdamW(model.parameters())\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.train()\ntrain_loss=0\n\nfor batch in train_dl:\n    optimizer.zero_grad()\n    text = batch[0]\n    label = batch[1].long().cuda()\n    encoded = tokenizer.batch_encode_plus(\n                  list(text),\n                  padding='max_length',\n                  max_length=MAX_LEN,\n                  truncation=True,\n                  return_tensors='pt',\n                  return_attention_mask=True,  \n                  return_token_type_ids=True)\n    input_ids=encoded['input_ids'].cuda()\n    attention_mask=encoded['attention_mask'].cuda()\n    token_type_ids=encoded['token_type_ids'].cuda()\n    preds=model(input_ids, attention_mask, token_type_ids)\n    loss=criterion(preds, label)\n    loss.backward()\n    optimizer.step()\n    train_loss += loss.item()\ntrain_loss/=len(train_dl)\nprint(train_loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nvalid_loss=0\n\nwith torch.no_grad():\n    for batch in valid_dl:\n        optimizer.zero_grad()\n        text = batch[0]\n        label = batch[1].long().cuda()\n        encoded = tokenizer.batch_encode_plus(\n                      list(text),\n                      padding='max_length',\n                      max_length=MAX_LEN,\n                      truncation=True,\n                      return_tensors='pt',\n                      return_attention_mask=True,  \n                      return_token_type_ids=True)\n        input_ids=encoded['input_ids'].cuda()\n        attention_mask=encoded['attention_mask'].cuda()\n        token_type_ids=encoded['token_type_ids'].cuda()\n        preds=model(input_ids, attention_mask, token_type_ids)\n        loss=criterion(preds, label)\n        valid_loss += loss.item()\n    valid_loss/=len(valid_dl)\nprint(valid_loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model().cuda()\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\ncriterion = nn.CrossEntropyLoss()\n\n\n\nbest_loss=np.inf\nfor epoch in range(5):\n    model.train()\n    train_loss=0\n    for batch in train_dl:\n        optimizer.zero_grad()\n        text = batch[0]\n        label = batch[1].long().cuda()\n        encoded = tokenizer.batch_encode_plus(\n                      list(text),\n                      padding='max_length',\n                      max_length=MAX_LEN,\n                      truncation=True,\n                      return_tensors='pt',\n                      return_attention_mask=True,  \n                      return_token_type_ids=True)\n        input_ids=encoded['input_ids'].cuda()\n        attention_mask=encoded['attention_mask'].cuda()\n        token_type_ids=encoded['token_type_ids'].cuda()\n        preds=model(input_ids, attention_mask, token_type_ids)\n        loss=criterion(preds, label)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    train_loss/=len(train_dl)\n\n    \n    \n    model.eval()\n    valid_loss=0\n    with torch.no_grad():\n        for batch in valid_dl:\n            optimizer.zero_grad()\n            text = batch[0]\n            label = batch[1].long().cuda()\n            encoded = tokenizer.batch_encode_plus(\n                          list(text),\n                          padding='max_length',\n                          max_length=MAX_LEN,\n                          truncation=True,\n                          return_tensors='pt',\n                          return_attention_mask=True,  \n                          return_token_type_ids=True)\n            input_ids=encoded['input_ids'].cuda()\n            attention_mask=encoded['attention_mask'].cuda()\n            token_type_ids=encoded['token_type_ids'].cuda()\n            preds=model(input_ids, attention_mask, token_type_ids)\n            loss=criterion(preds, label)\n            valid_loss+=loss.item()\n        valid_loss/=len(valid_dl)\n        \n    print(f\"EPOCH[{epoch}]\")\n    print(train_loss)\n    print(valid_loss)\n    if valid_loss<best_loss:\n        best_loss=valid_loss\n        torch.save(model.state_dict(), \"bert.pth\")\n        print('saved.....')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"bert.pth\", map_location=\"cpu\"))\nmodel.eval()\npred_valid = []\nwith torch.no_grad():\n    for batch in valid_dl:\n        optimizer.zero_grad()\n        text = batch[0]\n        encoded = tokenizer.batch_encode_plus(\n                      list(text),\n                      padding='max_length',\n                      max_length=MAX_LEN,\n                      truncation=True,\n                      return_tensors='pt',\n                      return_attention_mask=True,  \n                      return_token_type_ids=True)\n        input_ids = encoded['input_ids'].cuda()\n        attention_mask = encoded['attention_mask'].cuda()\n        token_type_ids = encoded['token_type_ids'].cuda()\n        preds = model(input_ids, attention_mask, token_type_ids)\n        pred_valid.append(preds.cpu().numpy())\npred_valid = np.concatenate(pred_valid, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_valid.argmax(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nacc = accuracy_score(valid['target'], pred_valid.argmax(axis=1))\nprint(acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testset","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"bert.pth\", map_location=\"cpu\"))\nmodel.eval()\npred = []\nwith torch.no_grad():\n    for batch in test_dl:\n        optimizer.zero_grad()\n        text = batch\n        encoded = tokenizer.batch_encode_plus(\n                      list(text),\n                      padding='max_length',\n                      max_length=MAX_LEN,\n                      truncation=True,\n                      return_tensors='pt',\n                      return_attention_mask=True,  \n                      return_token_type_ids=True)\n        input_ids = encoded['input_ids'].cuda()\n        attention_mask = encoded['attention_mask'].cuda()\n        token_type_ids = encoded['token_type_ids'].cuda()\n        preds = model(input_ids, attention_mask, token_type_ids)\n        pred.append(preds.cpu().numpy())\npred = np.concatenate(pred, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pred.argmax(axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['target'] = pred.argmax(axis=1)\ntest['target'] = test['target'].astype(int)\ntest = test[['id', 'target']]\ntest.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reference\n* https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304bã€€\n\n* ã€Pythonã€‘BERTã®å®Ÿè£…æ–¹æ³•ï½œæ–‡ç« åˆ†é¡ž, pytorchã€€https://htomblog.com/python-bert  HTOMblog \n\n* BERTã«ã‚ˆã‚‹è‡ªç„¶è¨€èªžå‡¦ç†å…¥é–€ Transformersã‚’ä½¿ã£ãŸå®Ÿè·µãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° ã‚¹ãƒˆãƒƒã‚¯ãƒžãƒ¼ã‚¯æ ªå¼ä¼šç¤¾ã€€è¿‘æ±Ÿå´‡å®,é‡‘ç”°å¥å¤ªéƒŽ,æ£®é•·èª ,æ±Ÿé–“è¦‹äºœåˆ© å…±è‘— ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T15:10:51.431158Z","iopub.execute_input":"2022-05-07T15:10:51.431509Z","iopub.status.idle":"2022-05-07T15:10:51.439638Z","shell.execute_reply.started":"2022-05-07T15:10:51.431477Z","shell.execute_reply":"2022-05-07T15:10:51.438272Z"}}},{"cell_type":"markdown","source":"## Thank you!!","metadata":{"execution":{"iopub.status.busy":"2022-05-07T15:14:48.489527Z","iopub.execute_input":"2022-05-07T15:14:48.489899Z","iopub.status.idle":"2022-05-07T15:14:48.496604Z","shell.execute_reply.started":"2022-05-07T15:14:48.489867Z","shell.execute_reply":"2022-05-07T15:14:48.495553Z"}}}]}