{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom transformers import AutoTokenizer,TFBertModel\nimport tensorflow as tf\nfrom sklearn.model_selection import StratifiedKFold\nimport re\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nimport gensim\nfrom nltk.data import find\nfrom tensorflow.keras.layers import LSTM, Dense, Masking, Dropout\nfrom tensorflow.keras import Sequential, Input, optimizers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import LearningRateSchedule\nimport numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-29T11:58:37.902087Z","iopub.execute_input":"2022-04-29T11:58:37.902372Z","iopub.status.idle":"2022-04-29T11:58:37.914931Z","shell.execute_reply.started":"2022-04-29T11:58:37.902342Z","shell.execute_reply":"2022-04-29T11:58:37.91384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_path = '/kaggle/input/nlp-getting-started/train.csv'\ntest_data_path = '/kaggle/input/nlp-getting-started/test.csv'\nsubmission_path = '/kaggle/input/nlp-getting-started/sample_submission.csv'\n\noriginal_data = pd.read_csv(train_data_path)\ntest_data = pd.read_csv(test_data_path)\nsubmission_data = pd.read_csv(submission_path)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:24:49.022241Z","iopub.execute_input":"2022-04-29T09:24:49.022949Z","iopub.status.idle":"2022-04-29T09:24:49.096278Z","shell.execute_reply.started":"2022-04-29T09:24:49.022909Z","shell.execute_reply":"2022-04-29T09:24:49.095529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(original_data.head())\ndisplay(original_data.info(show_counts=True))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:24:53.295752Z","iopub.execute_input":"2022-04-29T09:24:53.296312Z","iopub.status.idle":"2022-04-29T09:24:53.336962Z","shell.execute_reply.started":"2022-04-29T09:24:53.296271Z","shell.execute_reply":"2022-04-29T09:24:53.336234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Pre-processing\nSimple data cleaning functions that may be useful","metadata":{}},{"cell_type":"code","source":"\n# %20 is the URL encoding of space, let's replace them with '_'\ndef re_encode_space(input_string):\n    return None if pd.isna(input_string) else input_string.replace('%20', '_')\n\n\n# Let's try to find hastags\nimport re\n\ndef find_hash_tags(input_string):\n    hash_tags = re.findall(r\"#(\\w+)\", str(input_string))\n    return ','.join(hash_tags)\n\n\n# Let's turn hashtags to normal words\ndef re_encode_hashtags(input_string):\n    return None if pd.isna(input_string) else input_string.replace('#', '')\n\n\n# Let's remove URLs from the tweets\ndef remove_links(input_string):\n    res = input_string\n    urls = re.findall(r'(https?://[^\\s]+)', res)\n    for link in urls:\n        res = res.strip(link)\n    return res\n\n\n# Let's remove the state abbreviations\ndef state_renaming(input_string):\n\n    states = {\n        'AK': 'Alaska',\n        'AL': 'Alabama',\n        'AR': 'Arkansas',\n        'AZ': 'Arizona',\n        'CA': 'California',\n        'CO': 'Colorado',\n        'CT': 'Connecticut',\n        'DC': 'District_of_Columbia',\n        'DE': 'Delaware',\n        'FL': 'Florida',\n        'GA': 'Georgia',\n        'HI': 'Hawaii',\n        'IA': 'Iowa',\n        'ID': 'Idaho',\n        'IL': 'Illinois',\n        'IN': 'Indiana',\n        'KS': 'Kansas',\n        'KY': 'Kentucky',\n        'LA': 'Louisiana',\n        'MA': 'Massachusetts',\n        'MD': 'Maryland',\n        'ME': 'Maine',\n        'MI': 'Michigan',\n        'MN': 'Minnesota',\n        'MO': 'Missouri',\n        'MS': 'Mississippi',\n        'MT': 'Montana',\n        'NC': 'North_Carolina',\n        'ND': 'North_Dakota',\n        'NE': 'Nebraska',\n        'NH': 'New_Hampshire',\n        'NJ': 'New_Jersey',\n        'NM': 'New_Mexico',\n        'NV': 'Nevada',\n        'NY': 'New_York',\n        'OH': 'Ohio',\n        'OK': 'Oklahoma',\n        'OR': 'Oregon',\n        'PA': 'Pennsylvania',\n        'RI': 'Rhode_Island',\n        'SC': 'South_Carolina',\n        'SD': 'South_Dakota',\n        'TN': 'Tennessee',\n        'TX': 'Texas',\n        'UT': 'Utah',\n        'VA': 'Virginia',\n        'VT': 'Vermont',\n        'WA': 'Washington',\n        'WI': 'Wisconsin',\n        'WV': 'West_Virginia',\n        'WY': 'Wyoming'\n    }\n\n    result = input_string\n    \n    if isinstance(input_string, str):\n        input_candidates = input_string.split(', ')\n        \n        if len(input_candidates) > 1:\n            for candidate in input_candidates:\n                if candidate in states.keys():\n                    result = states[candidate]\n                \n    if input_string in states.keys():\n        result = states[input_string]\n\n    return result\n","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:24:56.299056Z","iopub.execute_input":"2022-04-29T09:24:56.299564Z","iopub.status.idle":"2022-04-29T09:24:56.31771Z","shell.execute_reply.started":"2022-04-29T09:24:56.299525Z","shell.execute_reply":"2022-04-29T09:24:56.316914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's wrap the preprocessing functions so it's easier to\n# process both train and test dataset\ndef preprocess_data(input_data):\n    input_df = input_data.copy()\n    input_df['keyword'] = input_df['keyword'].map(re_encode_space)\n    input_df['keyword'].fillna('Missing', inplace=True)\n    input_df['hashtags'] = input_df['text'].map(find_hash_tags)\n    input_df['text'] = input_df['text'].map(re_encode_hashtags)\n    input_df['text'] = input_df['text'].map(remove_links)\n    input_df['location'] = input_df['location'].map(state_renaming)\n    return input_df","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:24:57.685346Z","iopub.execute_input":"2022-04-29T09:24:57.68588Z","iopub.status.idle":"2022-04-29T09:24:57.69152Z","shell.execute_reply.started":"2022-04-29T09:24:57.685839Z","shell.execute_reply":"2022-04-29T09:24:57.69056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_data = preprocess_data(original_data)\ntest_data = preprocess_data(test_data)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:25:04.165528Z","iopub.execute_input":"2022-04-29T09:25:04.166253Z","iopub.status.idle":"2022-04-29T09:25:04.261471Z","shell.execute_reply.started":"2022-04-29T09:25:04.166197Z","shell.execute_reply":"2022-04-29T09:25:04.260767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We notice that keyword field has missing values\n# We notice location has missing values\n\n# Let's Visualize the keyword field in a word cloud to get an idea of what it is\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\nkeyword_words = str(original_data['keyword']\n    .dropna()\n    .unique()\n    .tolist()\n)\n\nlocation_words = str(original_data['location']\n    .dropna()\n    .unique()\n    .tolist()\n)\n\nhashtag_words = str(original_data['hashtags']\n    .dropna()\n    .unique()\n    .tolist()\n)\n\nkeyword_wordcloud = WordCloud(\n    background_color='white',\n    stopwords=None,\n    max_words=200,\n    max_font_size=40, \n    random_state=42\n).generate(keyword_words)\n\nlocation_wordcloud = WordCloud(\n    background_color='white',\n    stopwords=None,\n    max_words=200,\n    max_font_size=40, \n    random_state=42\n).generate(location_words)\n\nhashtag_wordcloud = WordCloud(\n    background_color='white',\n    stopwords=None,\n    max_words=200,\n    max_font_size=40, \n    random_state=42\n).generate(hashtag_words)\n\nfig, ax = plt.subplots(1,3, figsize=(16,9), constrained_layout=True)\nax[0].set_title(\"Keywords\")\nax[0].imshow(keyword_wordcloud)\nax[0].axis(False)\nax[1].set_title('Location')\nax[1].imshow(location_wordcloud)\nax[1].axis(False)\nax[2].set_title('Hashtags')\nax[2].imshow(hashtag_wordcloud)\nax[2].axis(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:25:05.582217Z","iopub.execute_input":"2022-04-29T09:25:05.58257Z","iopub.status.idle":"2022-04-29T09:25:07.18062Z","shell.execute_reply.started":"2022-04-29T09:25:05.582527Z","shell.execute_reply":"2022-04-29T09:25:07.178379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def na_proportions(data, column_name, as_pct):\n    if column_name in data.columns:\n        na_counts = len(data[pd.isna(data[column_name])])\n        non_na_counts = len(data[~pd.isna(data[column_name])])\n    else:\n        na_counts = None\n        non_na_counts = None\n        \n    if as_pct:\n        na_counts /= data.shape[0]\n        non_na_counts /= data.shape[0]\n    return (column_name, na_counts, non_na_counts)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:25:07.18215Z","iopub.execute_input":"2022-04-29T09:25:07.182604Z","iopub.status.idle":"2022-04-29T09:25:07.189505Z","shell.execute_reply.started":"2022-04-29T09:25:07.182557Z","shell.execute_reply":"2022-04-29T09:25:07.188912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keyword_props = na_proportions(data=original_data, column_name='keyword', as_pct=True)\nprint(f'The {keyword_props[0]} variable has: NA={keyword_props[1]:.3f} NON-NA={keyword_props[2]:.3f}')\nlocation_props = na_proportions(data=original_data, column_name='location', as_pct=True)\nprint(f'The {location_props[0]} variable has: NA={location_props[1]:.3f} NON-NA={location_props[2]:.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:25:09.493885Z","iopub.execute_input":"2022-04-29T09:25:09.494459Z","iopub.status.idle":"2022-04-29T09:25:09.51172Z","shell.execute_reply.started":"2022-04-29T09:25:09.494418Z","shell.execute_reply":"2022-04-29T09:25:09.510687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## First approach: TF-IDF + Logistic Regression\nA simple model to use as baseline","metadata":{}},{"cell_type":"code","source":"X = original_data['text']\ny = original_data['target']","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:02:28.534397Z","iopub.execute_input":"2022-04-29T12:02:28.534684Z","iopub.status.idle":"2022-04-29T12:02:28.539431Z","shell.execute_reply.started":"2022-04-29T12:02:28.534654Z","shell.execute_reply":"2022-04-29T12:02:28.538349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nskf = StratifiedKFold(n_splits=5)\ntrain_average_score = 0\nvalidation_average_score = 0\nvalidation_oof_predictions = np.zeros((len(X)))\n\nfor fold_n, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_test, y_test = X[test_idx], y[test_idx]\n    \n    model = Pipeline([\n        ('Encoder', TfidfVectorizer(max_features=None)),\n        ('Clf', LogisticRegression(penalty='l2', C=1, solver='liblinear'))\n    ])\n    \n    model.fit(X_train, y_train)\n    \n    train_predictions = model.predict_proba(X_train)[:, 1]\n    validation_predictions = model.predict_proba(X_test)[:, 1]\n    \n    train_score = roc_auc_score(y_train, train_predictions)\n    validation_score = roc_auc_score(y_test, validation_predictions)\n    \n    train_average_score += train_score / 5\n    validation_average_score += validation_score / 5\n    validation_oof_predictions[test_idx,] = (validation_predictions > 0.5).astype(int)\n    \n    print(f'Fold: {fold_n}, train auc: {train_score:.3f}, validation auc: {validation_score:.3f}')\nprint(f'Train average: {train_average_score:.3f}, validation average: {validation_average_score:.3f}')\nprint(f'OOF Accuracy Score: {accuracy_score(y, validation_oof_predictions)}')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:03:50.890369Z","iopub.execute_input":"2022-04-29T12:03:50.890629Z","iopub.status.idle":"2022-04-29T12:03:53.054837Z","shell.execute_reply.started":"2022-04-29T12:03:50.890594Z","shell.execute_reply":"2022-04-29T12:03:53.054106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A simple linear model appears to have an idea of what is going on.\nWe'll use these scores as baseline when trying more advanced models","metadata":{}},{"cell_type":"markdown","source":"## Second approach: Word2Vec embeddings and a simple LSTM-based network\nLet's slightly increase the complexity of the model:\n* Word2Vec embeddings using a pre-trained model for vector representation of words\n* LSTM based model as a classifier","metadata":{}},{"cell_type":"code","source":"# The tokenizer is responsible to turn a string of words\n# into a list of tokens (words) for which we'll get their\n# vector representation (embeddings)\ntknzr = TweetTokenizer(\n    preserve_case=False,\n    reduce_len=True,\n    strip_handles=True,\n)\n\n\ndef tokenize_tweets(tokenizer, input_text):\n    tokens = list(tokenizer.tokenize(input_text))\n    tokens = [re.sub('[^A-Za-z0-9]+', '', i) for i in tokens]\n    return tokens\n\noriginal_data['tokens'] = original_data['text']\noriginal_data['tokens'] = original_data['tokens'].apply(lambda x: tokenize_tweets(tknzr, x))\n\ntest_data['tokens'] = test_data['text'].apply(lambda x: tokenize_tweets(tknzr, x))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:25:18.314398Z","iopub.execute_input":"2022-04-29T09:25:18.314655Z","iopub.status.idle":"2022-04-29T09:25:20.098605Z","shell.execute_reply.started":"2022-04-29T09:25:18.314626Z","shell.execute_reply":"2022-04-29T09:25:20.097859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Our dataset is quite small so if we train the word2vec model the\n# resulting embeddings will be poor in quality. Therefore we use a\n# pre-trained model\nword2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\nword2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)\nfeatures = 300","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:25:20.100152Z","iopub.execute_input":"2022-04-29T09:25:20.100418Z","iopub.status.idle":"2022-04-29T09:25:29.332043Z","shell.execute_reply.started":"2022-04-29T09:25:20.10038Z","shell.execute_reply":"2022-04-29T09:25:29.331294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll pad all embeddings to match the length of the biggest tweet\n# in order to account for the variability in tweet length\n# Later on the model is going to mask the padded values, so that\n# they won't influence the result\nmax_tweet_length = max(original_data['tokens'].apply(lambda x: len(x)).max(), \n                       test_data['tokens'].apply(lambda x: len(x)).max())","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:25:29.333722Z","iopub.execute_input":"2022-04-29T09:25:29.333969Z","iopub.status.idle":"2022-04-29T09:25:29.345986Z","shell.execute_reply.started":"2022-04-29T09:25:29.33393Z","shell.execute_reply":"2022-04-29T09:25:29.345035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's compute the embeddings for every word that the pre-trained model\n# has in its vocabulary.\ndef vectorize_tokens(data_, vec_model, max_seq, num_features):\n    data_in = data_.copy()\n    # List to save all text embeddings\n    all_vectors = []\n    # Iterate over each text\n    for _, row in data_in.iterrows():\n        # Initialize a 2D matrix with zeros. Equivalent to 0 padding\n        # in the 1st dimension, to accomondate for variable text length\n        text_vectors = np.zeros((max_seq, num_features))\n        # If the word exists in the model vocabulary add its embeddings\n        # else keep the zeros as unknown words\n        for i, item in enumerate(row['tokens']):\n            try:\n                text_vectors[i, :] = vec_model[item]\n            except:\n                continue\n        all_vectors.append(text_vectors)\n    \n    return all_vectors","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:25:29.347663Z","iopub.execute_input":"2022-04-29T09:25:29.348334Z","iopub.status.idle":"2022-04-29T09:25:29.357264Z","shell.execute_reply.started":"2022-04-29T09:25:29.348183Z","shell.execute_reply":"2022-04-29T09:25:29.356534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_data['vectors'] = vectorize_tokens(data_=original_data, \n                                            vec_model=word2vec_model, \n                                            max_seq=max_tweet_length, \n                                            num_features=features)\n\ntest_data['vectors'] = vectorize_tokens(data_=test_data, \n                                        vec_model=word2vec_model, \n                                        max_seq=max_tweet_length, \n                                        num_features=features)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:25:29.359218Z","iopub.execute_input":"2022-04-29T09:25:29.359472Z","iopub.status.idle":"2022-04-29T09:25:30.867779Z","shell.execute_reply.started":"2022-04-29T09:25:29.359436Z","shell.execute_reply":"2022-04-29T09:25:30.86702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logic to have the training dataset as a 3D array of (text_count, max_sequence_length, embedding_size)\nX = np.asarray(original_data['vectors'].tolist()).astype(np.float32)\ny = np.asarray(original_data['target'].tolist()).astype(np.float32)\n\ntest_array = np.asarray(test_data['vectors'].tolist()).astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:04:49.113097Z","iopub.execute_input":"2022-04-29T12:04:49.114067Z","iopub.status.idle":"2022-04-29T12:04:49.755221Z","shell.execute_reply.started":"2022-04-29T12:04:49.11402Z","shell.execute_reply":"2022-04-29T12:04:49.754354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel = Sequential([\n    Input(shape=(max_tweet_length, features)),\n    Masking(),\n    LSTM(32),\n    Dropout(0.1),\n    Dense(1, activation='sigmoid')\n])\n\n\nskf = StratifiedKFold(n_splits=5)\ntrain_average_score = 0\nvalidation_average_score = 0\nvalidation_oof_predictions = np.zeros((len(X)))\n\n# It's a good practice to predict the test set on every fold\n# And average the predictions over the folds\naveraged_test_predictions = np.zeros((test_array.shape[0]))\n\n# It's standard practice to use Stratified k-fold cross validation\n# so we're also using it here\nfor fold_n, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_test, y_test = X[test_idx], y[test_idx]\n    \n    # Re-compile the model at every fold to \"reset\" it\n    model.compile(optimizer='adam', loss='binary_crossentropy')\n    \n    # Simple training strategy, hyper-parameters haven't been tuned\n    model.fit(x=X_train, y=y_train, batch_size=32, epochs=3)\n    \n    train_predictions = model.predict(X_train)\n    validation_predictions = model.predict(X_test)\n    \n    train_score = roc_auc_score(y_train, train_predictions)\n    validation_score = roc_auc_score(y_test, validation_predictions)\n    \n    train_average_score += train_score / 5\n    validation_average_score += validation_score / 5\n    validation_oof_predictions[test_idx,] = (validation_predictions > 0.5).astype(int).flatten()\n    \n    print(f'Fold: {fold_n}, train auc: {train_score:.3f}, validation auc: {validation_score:.3f}')\n    \n    test_predictions = model.predict(test_array).flatten()\n    averaged_test_predictions += test_predictions / 5\n    \nprint(f'Train average: {train_average_score:.3f}, validation average: {validation_average_score:.3f}')\nprint(f'OOF Accuracy Score: {accuracy_score(y, validation_oof_predictions)}')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:07:40.415526Z","iopub.execute_input":"2022-04-29T12:07:40.41628Z","iopub.status.idle":"2022-04-29T12:14:31.466012Z","shell.execute_reply.started":"2022-04-29T12:07:40.416242Z","shell.execute_reply":"2022-04-29T12:14:31.465235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The new model has a much better performance than the baseline\nIn addition, the train-validation overfitting is greatly reduced\nincreasing the probability of getting the model to generalize well","metadata":{}},{"cell_type":"markdown","source":"## Third approach: Bert Embeddings\nLet's use the Powerful Bert Model and see if it performs better than our word2vec model.\nWe'll follow the code from the TF tutorial","metadata":{}},{"cell_type":"code","source":"X = original_data['text'].tolist()\ny = np.asarray(original_data['target'].tolist()).astype(np.float32)\n\ntest_array = test_data['text'].tolist()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:16:08.594552Z","iopub.execute_input":"2022-04-29T12:16:08.594815Z","iopub.status.idle":"2022-04-29T12:16:08.601107Z","shell.execute_reply.started":"2022-04-29T12:16:08.594786Z","shell.execute_reply":"2022-04-29T12:16:08.600405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nbert = TFBertModel.from_pretrained('bert-base-uncased')\n\nX = bert_tokenizer(\n    text=X,\n    add_special_tokens=True,\n    max_length=max_tweet_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\ntest_array = bert_tokenizer(\n    text=test_array,\n    add_special_tokens=True,\n    max_length=max_tweet_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:16:14.525271Z","iopub.execute_input":"2022-04-29T12:16:14.52602Z","iopub.status.idle":"2022-04-29T12:16:21.555997Z","shell.execute_reply.started":"2022-04-29T12:16:14.525977Z","shell.execute_reply":"2022-04-29T12:16:21.555219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nepochs = 5\nsteps_per_epoch = X['input_ids'].numpy().shape[0]\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\nclass BertLrSchedule(LearningRateSchedule):\n\n    @tf.function\n    def __init__(self, initial_learning_rate, num_warmups, num_train_steps):\n        self.overshoot = 1000\n        self.initial_learning_rate = initial_learning_rate\n        self.num_warmups = num_warmups\n        self.num_train_steps = num_train_steps\n        self.angle_warm = self.initial_learning_rate / self.num_warmups\n        self.angle_decay = - self.initial_learning_rate / \\\n            (self.num_train_steps - self.num_warmups - self.overshoot)\n    \n    @tf.function\n    def __call__(self, step):\n        if step <= self.num_warmups:\n            return (tf.cast(step, tf.float32) + 1) * self.angle_warm\n        else:\n            return self.initial_learning_rate + (tf.cast(step, tf.float32) - self.num_warmups + 1 + self.overshoot) * self.angle_decay\n        \n        \nschedule = BertLrSchedule(initial_learning_rate=2e-5, \n                          num_warmups=num_warmup_steps, \n                          num_train_steps=num_train_steps)\n\nsteps = np.arange(num_train_steps)\nlrs = [schedule.__call__(i) for i in steps]\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1,1)\nax.plot(steps, lrs)\nax.set_xlabel('Train steps')\nax.set_ylabel('Learning Rate')\nax.set_title('Bert scheduler')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T11:57:00.76395Z","iopub.execute_input":"2022-04-29T11:57:00.764864Z","iopub.status.idle":"2022-04-29T11:57:31.265835Z","shell.execute_reply.started":"2022-04-29T11:57:00.764817Z","shell.execute_reply":"2022-04-29T11:57:31.265066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef build_bert_classifier():\n    input_ids = Input(shape=(max_tweet_length,), dtype=tf.int32, name=\"input_ids\")\n    input_mask = Input(shape=(max_tweet_length,), dtype=tf.int32, name=\"attention_mask\")\n    embeddings = bert(input_ids,attention_mask = input_mask)['pooler_output']\n    net = tf.keras.layers.Dropout(0.1)(embeddings)\n    net = tf.keras.layers.Dense(128, activation='relu', name='pre-clf')(net)\n    net = tf.keras.layers.Dropout(0.1)(net)\n    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n    return tf.keras.Model(inputs=[input_ids, input_mask], outputs=net)\n\nskf = StratifiedKFold(n_splits=5)\ntrain_average_score = 0\nvalidation_average_score = 0\nvalidation_oof_predictions = np.zeros((len(X['input_ids'].numpy())))\n\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\noptimizer = Adam(learning_rate=schedule)\nepochs = 5\n\n# It's a good practice to predict the test set on every fold\n# And average the predictions over the folds\naveraged_test_predictions = np.zeros((test_array['input_ids'].shape[0]))\n\n# It's standard practice to use Stratified k-fold cross validation\n# so we're also using it here\nfor fold_n, (train_idx, test_idx) in enumerate(skf.split(X['input_ids'].numpy(), y)):\n    X_train_ids = X['input_ids'].numpy()[train_idx]\n    X_train_att = X['attention_mask'].numpy()[train_idx]\n    y_train = y[train_idx]\n    \n    X_test_ids = X['input_ids'].numpy()[test_idx]\n    X_test_att = X['attention_mask'].numpy()[test_idx]\n    y_test = y[test_idx]\n    \n    # Re-build the model at every fold to \"reset\" it\n    model = build_bert_classifier()\n    model.layers[2].trainable = True\n    \n    model.compile(optimizer=optimizer,\n                  loss=loss)\n    \n    model.fit(x={'input_ids':X_train_ids,'attention_mask':X_train_att}, \n              y=y_train, batch_size=32, epochs=epochs)\n    \n    train_predictions = model.predict({'input_ids':X_train_ids,'attention_mask':X_train_att})\n    validation_predictions = model.predict({'input_ids':X_test_ids,'attention_mask':X_test_att})\n    \n    train_score = roc_auc_score(y_train, train_predictions)\n    validation_score = roc_auc_score(y_test, validation_predictions)\n    \n    train_average_score += train_score / 5\n    validation_average_score += validation_score / 5\n    validation_oof_predictions[test_idx,] = (validation_predictions > 0.5).astype(int).flatten()\n    \n    print(f'Fold: {fold_n}, train auc: {train_score:.3f}, validation auc: {validation_score:.3f}')\n    \n    test_predictions = model.predict({'input_ids':test_array['input_ids'],\n                                      'attention_mask':test_array['attention_mask']}).flatten()\n    averaged_test_predictions += test_predictions / 5\n    \nprint(f'Train average: {train_average_score:.3f}, validation average: {validation_average_score:.3f}')\nprint(f'OOF Accuracy Score: {accuracy_score(y, validation_oof_predictions)}')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:16:28.572416Z","iopub.execute_input":"2022-04-29T12:16:28.572827Z","iopub.status.idle":"2022-04-29T12:36:51.306625Z","shell.execute_reply.started":"2022-04-29T12:16:28.57279Z","shell.execute_reply":"2022-04-29T12:36:51.305746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's save the test predictions and submit them\nsubmission_data['target'] = (averaged_test_predictions > 0.5).astype(int)\nsubmission_data.to_csv('model_predictions.csv', index=False)\nsubmission_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}