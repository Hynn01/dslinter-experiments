{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<h1> This notebooks aims to present different callbacks with learning rate function and customized early stopping","metadata":{"_kg_hide-input":true,"papermill":{"duration":6.092024,"end_time":"2022-05-06T09:16:37.066226","exception":false,"start_time":"2022-05-06T09:16:30.974202","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-07T15:39:23.301297Z","iopub.execute_input":"2022-05-07T15:39:23.301761Z","iopub.status.idle":"2022-05-07T15:39:30.333405Z","shell.execute_reply.started":"2022-05-07T15:39:23.301671Z","shell.execute_reply":"2022-05-07T15:39:30.33265Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nfrom cycler import cycler\nfrom IPython.display import display\nimport datetime\nimport scipy.stats\n\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping, Callback\nfrom tensorflow.keras.layers import Dense, Input, InputLayer, Add, Dropout, Embedding, Conv1D, Flatten, Concatenate,BatchNormalization,Reshape,Activation\nfrom tensorflow.keras.utils import get_custom_objects\nimport tensorflow_addons as tfa\nfrom keras import backend as K\n\nimport math\nfrom math import pi\nfrom math import cos\nfrom math import floor\n\nimport gc","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:46:52.514558Z","iopub.execute_input":"2022-05-07T20:46:52.515226Z","iopub.status.idle":"2022-05-07T20:46:52.523003Z","shell.execute_reply.started":"2022-05-07T20:46:52.51519Z","shell.execute_reply":"2022-05-07T20:46:52.521599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration :\nBATCH_SIZE = 2048 \nEPOCHS = 150\nVERBOSE = 0","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:46:57.724363Z","iopub.execute_input":"2022-05-07T20:46:57.724978Z","iopub.status.idle":"2022-05-07T20:46:57.729206Z","shell.execute_reply.started":"2022-05-07T20:46:57.724935Z","shell.execute_reply":"2022-05-07T20:46:57.728271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv')\n\nfor df in [train, test]:\n    for i in range(10):\n        df[f'ch{i}'] = df.f_27.str.get(i).apply(ord) - ord('A')\n    # Next feature is from https://www.kaggle.com/code/cabaxiom/tps-may-22-eda-lgbm-model\n    df[\"unique_caracters\"] = df.f_27.apply(lambda s: len(set(s)))\nfeatures = [f for f in test.columns if f != 'id' and f != 'f_27']","metadata":{"papermill":{"duration":30.367823,"end_time":"2022-05-06T09:17:07.444543","exception":false,"start_time":"2022-05-06T09:16:37.07672","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-07T20:46:59.01859Z","iopub.execute_input":"2022-05-07T20:46:59.019045Z","iopub.status.idle":"2022-05-07T20:47:28.927695Z","shell.execute_reply.started":"2022-05-07T20:46:59.019008Z","shell.execute_reply":"2022-05-07T20:47:28.92697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#features = [f for f in test.columns if f != 'id' and f != 'f_27']\nrob = StandardScaler()\ntrain[features] = pd.DataFrame(rob.fit_transform(train[features]),columns = features)\ntest[features] = pd.DataFrame(rob.transform(test[features]),columns = features)\ntrain[features].shape,test[features].shape","metadata":{"papermill":{"duration":2.942709,"end_time":"2022-05-06T09:17:10.397487","exception":false,"start_time":"2022-05-06T09:17:07.454778","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-07T20:47:28.929508Z","iopub.execute_input":"2022-05-07T20:47:28.929769Z","iopub.status.idle":"2022-05-07T20:47:31.860434Z","shell.execute_reply.started":"2022-05-07T20:47:28.929733Z","shell.execute_reply":"2022-05-07T20:47:31.859763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> History display function","metadata":{}},{"cell_type":"code","source":"# from https://www.kaggle.com/code/ambrosm/tpsmay22-keras-quickstart\n# Plot training history\ndef plot_history(history, *, n_epochs=None, plot_lr=False, title=None, bottom=None, top=None):\n    \"\"\"Plot (the last n_epochs epochs of) the training history\n    \n    Plots loss and optionally val_loss and lr.\"\"\"\n    plt.figure(figsize=(12, 6))\n    from_epoch = 0 if n_epochs is None else max(len(history['loss']) - n_epochs, 0)\n    \n    # Plot training and validation losses\n    plt.plot(np.arange(from_epoch, len(history['loss'])), history['loss'][from_epoch:], label='Training loss')\n    try:\n        plt.plot(np.arange(from_epoch, len(history['loss'])), history['val_loss'][from_epoch:], label='Validation loss')\n        best_epoch = np.argmin(np.array(history['val_loss']))\n        best_val_loss = history['val_loss'][best_epoch]\n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_loss], c='r', label=f'Best val_loss = {best_val_loss:.5f}')\n        if best_epoch > 0:\n            almost_epoch = np.argmin(np.array(history['val_loss'])[:best_epoch])\n            almost_val_loss = history['val_loss'][almost_epoch]\n            if almost_epoch >= from_epoch:\n                plt.scatter([almost_epoch], [almost_val_loss], c='orange', label='Second best val_loss')\n    except KeyError:\n        pass\n    if bottom is not None: plt.ylim(bottom=bottom)\n    if top is not None: plt.ylim(top=top)\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='lower left')\n    if title is not None: plt.title(title)\n        \n    # Plot learning rate\n    if plot_lr and 'lr' in history:\n        ax2 = plt.gca().twinx()\n        ax2.plot(np.arange(from_epoch, len(history['lr'])), np.array(history['lr'][from_epoch:]), color='g', label='Learning rate')\n        ax2.set_ylabel('Learning rate')\n        ax2.legend(loc='upper right')\n        \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:47:31.861787Z","iopub.execute_input":"2022-05-07T20:47:31.862053Z","iopub.status.idle":"2022-05-07T20:47:31.874519Z","shell.execute_reply.started":"2022-05-07T20:47:31.862019Z","shell.execute_reply":"2022-05-07T20:47:31.873682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> Model function","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:55:36.151386Z","iopub.execute_input":"2022-05-07T17:55:36.15174Z","iopub.status.idle":"2022-05-07T17:55:36.157998Z","shell.execute_reply.started":"2022-05-07T17:55:36.151705Z","shell.execute_reply":"2022-05-07T17:55:36.156703Z"}}},{"cell_type":"code","source":"def NN_model():\n    \n    inputs = Input(shape=(len(features)))\n    \n    A = 64\n    REGUL = 6e-5\n    activation = 'swish'\n    INIT = \"glorot_uniform\"\n    BIAS = True\n\n    x1 =  Dense(A, \n                kernel_regularizer=tf.keras.regularizers.l2(REGUL),\n                use_bias=BIAS,\n                kernel_initializer = INIT,\n                activation=activation\n                )(inputs)\n\n    x2 = Dense(A, \n                kernel_regularizer=tf.keras.regularizers.l2(REGUL),\n                use_bias=BIAS,\n                kernel_initializer = INIT,\n                activation=activation\n                )(x1)\n    \n    x21 = Concatenate()([x1,x2])\n    x21 = Dropout(0.1)(x21)\n    x21 = BatchNormalization()(x21)\n    \n    x3 = Dense(A, \n                kernel_regularizer=tf.keras.regularizers.l2(REGUL),\n                use_bias=BIAS,\n                kernel_initializer = INIT,\n                activation=activation\n                )(x21)\n\n    x4 = Dense(16, \n                kernel_regularizer=tf.keras.regularizers.l2(REGUL),\n                use_bias=BIAS,\n                kernel_initializer = INIT,\n                activation=activation\n                )(x3)\n\n    x5 = Dense(1, \n                use_bias=BIAS,\n                activation='sigmoid',\n                )(x4)\n    \n    model = Model(inputs, x5)\n    \n    LOSS = tf.keras.losses.BinaryCrossentropy()\n    METRIC = tf.keras.metrics.AUC(name = 'auc')\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n                          metrics= METRIC,\n                          loss=tf.keras.losses.BinaryCrossentropy())\n    \n    return model\n","metadata":{"papermill":{"duration":0.025704,"end_time":"2022-05-06T09:17:10.483097","exception":false,"start_time":"2022-05-06T09:17:10.457393","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-07T20:47:31.876523Z","iopub.execute_input":"2022-05-07T20:47:31.876981Z","iopub.status.idle":"2022-05-07T20:47:31.889634Z","shell.execute_reply.started":"2022-05-07T20:47:31.876946Z","shell.execute_reply":"2022-05-07T20:47:31.888969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> Training function","metadata":{}},{"cell_type":"code","source":"def training(callbacks,BREAK = True, Cosine_Annealing = False):\n    \n    LOSS = tf.keras.losses.BinaryCrossentropy()\n    METRIC = tf.keras.metrics.AUC(name = 'auc')\n    nn_oof = np.zeros(train.shape[0])\n\n    split = 5\n    cv = StratifiedKFold(n_splits=split, shuffle=True, random_state=2)\n\n    for fold, (idx_train, idx_valid) in enumerate(cv.split(train, train.target)):\n\n        X_train, y_train = train.loc[idx_train][features], train.target.iloc[idx_train]\n        X_valid, y_valid = train.loc[idx_valid][features], train.target.iloc[idx_valid]\n        \n        #---------  Model instantiation ------------------------\n\n        model = NN_model()\n        model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n                          metrics= METRIC,\n                          loss=tf.keras.losses.BinaryCrossentropy())\n        \n        # Model Training :\n        \n        history = model.fit(X_train, y_train, \n                            validation_data=(X_valid,y_valid),\n                            epochs=EPOCHS,\n                            verbose=VERBOSE,\n                            batch_size=BATCH_SIZE,\n                            shuffle=True,\n                            callbacks=[callbacks]\n                           )\n                \n        history_list = []\n        history_list.append(history.history)\n\n        print(f\"\\nTraining loss    \", np.round((history_list[0].get('loss')[-1]),5))\n        print(f\"Training val_loss\", np.round((history_list[0].get('val_loss')[-1]),5))\n        print(f\"Training delta   \", np.round(100 * (history_list[0].get('loss')[-1] - history_list[0].get('val_loss')[-1])/history_list[0].get('loss')[-1],2),'%')\n        \n        if Cosine_Annealing :\n            pred_list =[]\n            for i in range(1, int(n_cycles+1)):\n                file_name ='snapshot_model_'+str(i)+'.h5'\n                model.load_weights(file_name)\n                pred = model.predict(train[features]).squeeze()\n                pred_list.append(pred)\n                score = roc_auc_score(train.target, pred)\n                print(f\"cycle {i} Score: {score}\")\n        else : \n            model.load_weights('best_model.h5')\n            pred_oof_nn = model.predict(X_valid)\n            score = roc_auc_score(y_valid, pred_oof_nn)\n            nn_oof[idx_valid] = pred_oof_nn.squeeze()\n            print(f\"\\nFold: {fold + 1} NN1 Score: {score}\")\n        \n        tf.keras.backend.clear_session()\n        \n        if BREAK :\n            break\n\n    if not BREAK :\n        print(f\"\\n TOTAL oof NN1 = {roc_auc_score(train.target,nn_oof)}\")\n    \n    return history_list, score","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:47:31.890745Z","iopub.execute_input":"2022-05-07T20:47:31.891178Z","iopub.status.idle":"2022-05-07T20:47:31.909867Z","shell.execute_reply.started":"2022-05-07T20:47:31.891141Z","shell.execute_reply":"2022-05-07T20:47:31.909128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> Early Stopping customized","metadata":{}},{"cell_type":"markdown","source":"The usual earlystopping is based upon loss or val_loss (better) or the metric choiced (auc..).\nThe idea was to introduce an earlystopping based upon the difference between loss and val_loss. There is a loss limit value when start the patience calculation.\nBelow the limit, no patience is recorded and when the loss has reached the limit (small value) the patience calculation starts : every val_loss - loss > 0 is recorded and after the number of patience has been reached : earlystopping.\nThe goal is to avoid an overfitting risk when the val_loss increases or decreases very slowly and the loss still decreases quickly.\nThe earlystopping can (verbose = 1) display the difference (val_loss - loss) at each epoch end.","metadata":{}},{"cell_type":"code","source":"# From Keras documentation :\nclass Early_Stopping_the_war(tf.keras.callbacks.Callback):\n\n    def __init__(self,patience = 0,verbose = 1,loss_limit = 0.075 ):\n        super(Early_Stopping_the_war, self).__init__()\n        self.patience = patience\n        self.best_weights = None\n        self.verbose = verbose\n        self.loss_limit = loss_limit\n\n    def on_train_begin(self, logs=None):\n        self.wait = 0\n        self.stopped_epoch = 0\n        self.best_loss = np.Inf\n        self.best_val_loss = np.Inf\n\n    def on_epoch_end(self, epoch, logs=None):\n        \n        current_loss = logs.get(\"loss\")\n        current_val_loss = logs.get(\"val_loss\")\n        \n        if np.less(current_loss, self.best_loss):\n            self.best_loss = current_loss\n            self.best_weights = self.model.get_weights()\n\n        if self.verbose == 1 :\n            print(15*' ','val_loss-loss:{}'.format(np.round((current_val_loss - current_loss),5)))\n\n        if np.less(current_loss,current_val_loss) and current_loss < self.loss_limit :\n            self.wait += 1\n            self.best_val_loss = current_val_loss\n            if self.verbose == 1 :\n                print(15*' ',\"val_loss > loss => patience :\",self.wait) \n\n            if self.wait >= self.patience:\n                self.stopped_epoch = epoch\n                self.model.stop_training = True\n                print(\"Restoring model weights from the end of the best epoch.\")\n                self.model.set_weights(self.best_weights)\n                    \n    def on_train_end(self, logs=None):\n        if self.stopped_epoch > 0:\n            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:47:31.912691Z","iopub.execute_input":"2022-05-07T20:47:31.912896Z","iopub.status.idle":"2022-05-07T20:47:31.923532Z","shell.execute_reply.started":"2022-05-07T20:47:31.912872Z","shell.execute_reply":"2022-05-07T20:47:31.922642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> Training with Early_stopping_The_War monitored by val_loss-loss","metadata":{}},{"cell_type":"code","source":"\n# plateau monitored by val_loss if X_val only\nplateau_val_loss = tf.keras.callbacks.ReduceLROnPlateau(\n                                        monitor = 'val_loss', \n                                        factor = 0.9, \n                                        patience = 3, \n                                        verbose = VERBOSE, \n                                        mode = 'auto')\n# plateau monitored by loss \nplateau_loss = tf.keras.callbacks.ReduceLROnPlateau(\n                                        monitor ='loss', \n                                        factor = 0.95, \n                                        patience = 4, \n                                        verbose = VERBOSE, \n                                        mode ='max')\n\nModelCheckpoint = tf.keras.callbacks.ModelCheckpoint(\n                                        'best_model.h5', \n                                        monitor = 'val_loss',\n                                        save_weights_only = True,\n                                        save_best_only = True,\n                                        mode = 'auto')\n\n\nes = Early_Stopping_the_war(patience = 5,verbose = VERBOSE, loss_limit = 0.074)\ncallbacks = [es, plateau_val_loss,ModelCheckpoint]\nhistory_list_plateau, score_plateau = training(callbacks)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:47:31.925024Z","iopub.execute_input":"2022-05-07T20:47:31.92566Z","iopub.status.idle":"2022-05-07T20:48:15.743676Z","shell.execute_reply.started":"2022-05-07T20:47:31.925625Z","shell.execute_reply":"2022-05-07T20:48:15.742971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> Cosine learning rate decay","metadata":{}},{"cell_type":"code","source":"# from https://www.kaggle.com/code/ambrosm/tpsmay22-keras-quickstart\nepochs = EPOCHS\nlr_start=0.01\nlr_end=0.0002\ndef cosine_decay(epoch):\n    if epochs > 1:\n        w = (1 + math.cos(epoch / (epochs-1) * math.pi)) / 2\n    else:\n        w = 1\n    return w * lr_start + (1 - w) * lr_end\n\nLearning_Rate_Scheduler = LearningRateScheduler(cosine_decay, verbose=0)\ncallbacks = [Learning_Rate_Scheduler, tf.keras.callbacks.TerminateOnNaN(),ModelCheckpoint]\nhistory_list_cosine, score_cosine = training(callbacks)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:48:44.434329Z","iopub.execute_input":"2022-05-07T20:48:44.434599Z","iopub.status.idle":"2022-05-07T20:49:32.961542Z","shell.execute_reply.started":"2022-05-07T20:48:44.434568Z","shell.execute_reply":"2022-05-07T20:49:32.960953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> Cosine annealing learning rate","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:34:10.196596Z","iopub.execute_input":"2022-05-07T17:34:10.196935Z","iopub.status.idle":"2022-05-07T17:34:10.202768Z","shell.execute_reply.started":"2022-05-07T17:34:10.196905Z","shell.execute_reply":"2022-05-07T17:34:10.201656Z"}}},{"cell_type":"code","source":"# From Jason Brownlee : https://machinelearningmastery.com/snapshot-ensemble-deep-learning-neural-network/\n\nclass Cosine_Annealing(Callback):\n\n    def __init__(self, n_epochs, n_cycles, lrate_max):\n        self.epochs = n_epochs\n        self.cycles = n_cycles\n        self.lr_max = lrate_max\n        self.lrates = list()\n \n    # calculate learning rate for epoch\n    def cosine_annealing(self, epoch, n_epochs, n_cycles, lrate_max):\n        epochs_per_cycle = floor(n_epochs/n_cycles)\n        cos_inner = (pi * (epoch % epochs_per_cycle)) / (epochs_per_cycle)\n        return lrate_max/2 * (cos(cos_inner) + 1)\n\n    # calculate and set learning rate at the start of the epoch\n    def on_epoch_begin(self, epoch, logs={}):\n        # calculate learning rate\n        lr = self.cosine_annealing(epoch, self.epochs, self.cycles, self.lr_max)\n        K.set_value(self.model.optimizer.lr, lr)\n        # set learning rate\n        #backend.set_value(self.model.optimizer.lr, lr)\n        #log value\n        self.lrates.append(lr)\n \n    # save models at the end of each cycle\n    def on_epoch_end(self, epoch, logs={}):\n        # check if we can save model\n        epochs_per_cycle = floor(self.epochs / self.cycles)\n        if epoch != 0 and (epoch + 1) % epochs_per_cycle == 0:\n            # save model to file\n            filename = \"snapshot_model_%d.h5\" % int((epoch + 1) / epochs_per_cycle)\n            self.model.save(filename)\n            \ndef load_all_models(n_models):\n    all_models = list()\n    for i in range(n_models):\n        # define filename for this ensemble\n        filename = 'snapshot_model_' + str(i + 1) + '.h5'\n        # load model from file\n        MODEL = model.load_weights(filename)\n        # add to list of members\n        all_models.append(MODEL)\n    print('>loaded %s' % filename)\n    return all_models\n            ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:49:48.81844Z","iopub.execute_input":"2022-05-07T20:49:48.818971Z","iopub.status.idle":"2022-05-07T20:49:48.83035Z","shell.execute_reply.started":"2022-05-07T20:49:48.818916Z","shell.execute_reply":"2022-05-07T20:49:48.829054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 150\nn_cycles = EPOCHS /50 \nca = Cosine_Annealing(EPOCHS, n_cycles, 0.01)\ncallbacks = [ca]\nhistory_list_Cosine_Annealing, score_Cosine_Annealing = training(callbacks,Cosine_Annealing = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:52:31.355558Z","iopub.execute_input":"2022-05-07T20:52:31.35582Z","iopub.status.idle":"2022-05-07T20:55:46.207119Z","shell.execute_reply.started":"2022-05-07T20:52:31.355793Z","shell.execute_reply":"2022-05-07T20:55:46.206309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> Let's compare the training history","metadata":{}},{"cell_type":"code","source":"plot_history(\n    history_list_plateau[0],\n    title=f\"Early_stopping_The_War is monitored by val_loss-loss, AUC  = {score_plateau:.5f}\",\n    plot_lr=True, n_epochs=EPOCHS) \n\nplot_history(\n    history_list_cosine[0],\n    title=f\"Cosine learning rate decay, AUC  = {score_cosine:.5f}\",\n    plot_lr=True, n_epochs=EPOCHS) \n\nplot_history(\n    history_list_Cosine_Annealing[0],\n    title=f\"Cosine Annealing, AUC  = {score_Cosine_Annealing:.5f}\",\n    plot_lr=True, n_epochs=EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:55:52.481943Z","iopub.execute_input":"2022-05-07T20:55:52.482218Z","iopub.status.idle":"2022-05-07T20:55:53.335498Z","shell.execute_reply.started":"2022-05-07T20:55:52.48219Z","shell.execute_reply":"2022-05-07T20:55:53.334783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Each strategy is effective, but depending on each dataset, there is sometimes a better one.\n    Cosine annealing is my favorite but takes a lot of time when you select many cycles...","metadata":{}},{"cell_type":"markdown","source":"<h1> Training for submission","metadata":{}},{"cell_type":"code","source":"RUN = 3\n\nEPOCHS = 400\n\nblend_train_best = np.zeros((train.shape[0],1))\nblend_test = np.zeros((test.shape[0],1))\nblend_train = np.zeros((train.shape[0],1))\n\nfor i in range(RUN): \n    \n    print('\\n_______________ RUN {}  _____________\\n'.format(i+1))\n    \n    #--------- NN1 -------------------------\n    model = NN_model()\n    n_cycles = EPOCHS / 100\n    ca = Cosine_Annealing(EPOCHS, n_cycles, 0.01)\n    \n    model.fit(train[features], train.target,  \n                epochs=EPOCHS,\n                verbose=0,\n                batch_size=BATCH_SIZE,\n                shuffle=True,\n                 callbacks=[ca]\n             )\n    \n    pred_list =[]\n    for i in range(1, int(n_cycles+1)):\n        file_name ='snapshot_model_'+str(i)+'.h5'\n        model.load_weights(file_name)\n        pred = model.predict(train[features]).squeeze()\n        pred_list.append(pred)\n        score = roc_auc_score(train.target, pred)\n        print(f\"cycle {i} Score: {score}\")\n        \n        # --------- test prediction --------------\n        pred_test = model.predict(test[features]).squeeze()\n        blend_test[:,0] += (pred_test/n_cycles)/RUN\n        \n        # ------ train prediction (no oof) -------\n        pred_train = model.predict(train[features]).squeeze()\n        blend_train[:,0] += (pred_train/n_cycles)/RUN\n        \n    score = roc_auc_score(train.target, np.mean(pred_list,axis=0))\n    print(f\"All cycles Score: {score}\")\n    \nprint('\\nFINAL non oof AUC = ',roc_auc_score(train.target,blend_train[:,0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/tabular-playground-series-may-2022/sample_submission.csv')\nsub['target'] = blend_test\nsub.to_csv('submission.csv', index = False)\npd.read_csv('submission.csv')","metadata":{"papermill":{"duration":36.03386,"end_time":"2022-05-06T10:54:46.60917","exception":false,"start_time":"2022-05-06T10:54:10.57531","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-07T18:50:37.345291Z","iopub.status.idle":"2022-05-07T18:50:37.345918Z","shell.execute_reply.started":"2022-05-07T18:50:37.345665Z","shell.execute_reply":"2022-05-07T18:50:37.345691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}