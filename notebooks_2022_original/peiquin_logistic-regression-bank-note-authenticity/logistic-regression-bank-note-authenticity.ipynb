{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Logistic Regression ","metadata":{}},{"cell_type":"markdown","source":"## Table of content ","metadata":{}},{"cell_type":"markdown","source":"- [Task 1 - Preprocessing of data](#t1)\n- [Task 2 - Feature normalization](#t2)\n- [Task 3 - Logistic regression equation](#t3)\n- [Task 4 - Split dataset into train and test](#t4)\n- [Task 5 - SGDClassifier](#t5)\n- [Task 6 - Classification report for predictions from task 5](#t6)\n- [Task 7 & 8 - Regularization & classification reports](#t7_8)\n- [Task 9 - K-Nearest Neighbours (KNN)](#t9)\n- [References](#references)","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"t1\"> Task 1</a>","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport plotly.express as px \nimport seaborn as sns","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:49:10.394303Z","start_time":"2022-04-25T10:49:09.454123Z"},"execution":{"iopub.status.busy":"2022-05-07T12:40:39.17795Z","iopub.execute_input":"2022-05-07T12:40:39.178284Z","iopub.status.idle":"2022-05-07T12:40:41.554952Z","shell.execute_reply.started":"2022-05-07T12:40:39.178196Z","shell.execute_reply":"2022-05-07T12:40:41.554158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ignore warnings \nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(\"ignore\", category=ConvergenceWarning)","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:49:10.456181Z","start_time":"2022-04-25T10:49:10.39518Z"},"execution":{"iopub.status.busy":"2022-05-07T12:40:41.998877Z","iopub.execute_input":"2022-05-07T12:40:41.999372Z","iopub.status.idle":"2022-05-07T12:40:42.237507Z","shell.execute_reply.started":"2022-05-07T12:40:41.99934Z","shell.execute_reply":"2022-05-07T12:40:42.236613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create header based on details provided by UCI website\nheader = ['variance', 'skewness', 'kurtosis', 'entropy', 'class']\n# Load data from txt file (comma seperated) with header\ndf = pd.read_csv('../input/bank-note-authentication-uci-data/BankNote_Authentication.csv')\ndf.head()","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:49:10.465154Z","start_time":"2022-04-25T10:49:10.456755Z"},"execution":{"iopub.status.busy":"2022-05-07T12:40:44.98375Z","iopub.execute_input":"2022-05-07T12:40:44.984121Z","iopub.status.idle":"2022-05-07T12:40:45.021038Z","shell.execute_reply.started":"2022-05-07T12:40:44.984089Z","shell.execute_reply":"2022-05-07T12:40:45.020027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Last 5 rows of data set\ndf.tail()","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:49:10.470067Z","start_time":"2022-04-25T10:49:10.466337Z"},"execution":{"iopub.status.busy":"2022-05-07T12:40:47.98315Z","iopub.execute_input":"2022-05-07T12:40:47.983425Z","iopub.status.idle":"2022-05-07T12:40:47.995683Z","shell.execute_reply.started":"2022-05-07T12:40:47.983398Z","shell.execute_reply":"2022-05-07T12:40:47.995064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shape of the data set\ndf.shape","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:49:10.472725Z","start_time":"2022-04-25T10:49:10.470832Z"},"execution":{"iopub.status.busy":"2022-05-07T12:40:50.187993Z","iopub.execute_input":"2022-05-07T12:40:50.18896Z","iopub.status.idle":"2022-05-07T12:40:50.195424Z","shell.execute_reply.started":"2022-05-07T12:40:50.18892Z","shell.execute_reply":"2022-05-07T12:40:50.194288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Information of data set\ndf.info()","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:49:10.478903Z","start_time":"2022-04-25T10:49:10.473294Z"},"execution":{"iopub.status.busy":"2022-05-07T12:40:53.421001Z","iopub.execute_input":"2022-05-07T12:40:53.421347Z","iopub.status.idle":"2022-05-07T12:40:53.447214Z","shell.execute_reply.started":"2022-05-07T12:40:53.421304Z","shell.execute_reply":"2022-05-07T12:40:53.446413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistical summary of the data set \ndf.describe()","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:49:10.487834Z","start_time":"2022-04-25T10:49:10.479574Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-05-07T12:40:55.977062Z","iopub.execute_input":"2022-05-07T12:40:55.977491Z","iopub.status.idle":"2022-05-07T12:40:56.010236Z","shell.execute_reply.started":"2022-05-07T12:40:55.977454Z","shell.execute_reply":"2022-05-07T12:40:56.00932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Class count for unforged and forged bank notes.\ndf['class'].value_counts()","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:49:10.492392Z","start_time":"2022-04-25T10:49:10.488668Z"},"execution":{"iopub.status.busy":"2022-05-07T12:40:58.225911Z","iopub.execute_input":"2022-05-07T12:40:58.226401Z","iopub.status.idle":"2022-05-07T12:40:58.233692Z","shell.execute_reply.started":"2022-05-07T12:40:58.226342Z","shell.execute_reply":"2022-05-07T12:40:58.232821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the information above, we can see that there is no missing data. The shape of the data is 1372 and it matches the non-null values of 1372 for all columns. As such, data imputation for missing values need not be conducted. \n\nAll input features (i.e., variance, skewness, kurtosis, entropy) are numerical. Thus, encoding is not required to be performed for such case. Class is the target (or dependent) variable that is classified as 0 and 1 if bank note is not forged and forged respectively. ","metadata":{}},{"cell_type":"code","source":"# Distributions of numerical inputs\ncols = ['variance', 'skewness', 'curtosis', 'entropy']\n\nfor col in cols: \n    fig = px.histogram(df, x=col, color='class', marginal='box', opacity=0.7, \n                        title=f\"Distribution of input feature {col}\")\n    fig.show()","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:49:10.920879Z","start_time":"2022-04-25T10:49:10.493125Z"},"execution":{"iopub.status.busy":"2022-05-07T12:41:23.601349Z","iopub.execute_input":"2022-05-07T12:41:23.601889Z","iopub.status.idle":"2022-05-07T12:41:23.996506Z","shell.execute_reply.started":"2022-05-07T12:41:23.601849Z","shell.execute_reply":"2022-05-07T12:41:23.995661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With reference to the distribution plots above, we note the following observations. \n\n1. There were a number of outliers for the following features based on the marginal box plot for variance, kurtosis, and entropy. However, they may contribute in patterns to identify if a bank note is forged. Hence, outliers need not be remove. \n2. We note that the variance and skewness of the forged notes have a rather different distribution as compared to the unforged notes in the first two histograms. \n3. The forged notes have the long right tail for kurtosis as shown in the third histogram.","metadata":{}},{"cell_type":"markdown","source":"## <a id='t2'> Task 2 - Feature Normalization </a>    ","metadata":{}},{"cell_type":"code","source":"# Normalise the numercial input features using Sklearn MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler() \ndf_norm = scaler.fit_transform(df)\ndf_norm = pd.DataFrame(df_norm, columns=header)\ndf_norm.describe()","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:49:10.951418Z","start_time":"2022-04-25T10:49:10.922805Z"},"execution":{"iopub.status.busy":"2022-05-07T12:41:32.1021Z","iopub.execute_input":"2022-05-07T12:41:32.10271Z","iopub.status.idle":"2022-05-07T12:41:32.164674Z","shell.execute_reply.started":"2022-05-07T12:41:32.102671Z","shell.execute_reply":"2022-05-07T12:41:32.163808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate input features into variable X and target output to variable y\nX = df_norm.drop(columns=['class'])\ny = df_norm['class']\nprint(X.shape)\nprint(y.shape)","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:49:10.957045Z","start_time":"2022-04-25T10:49:10.953352Z"},"execution":{"iopub.status.busy":"2022-05-07T12:41:34.570928Z","iopub.execute_input":"2022-05-07T12:41:34.571598Z","iopub.status.idle":"2022-05-07T12:41:34.578225Z","shell.execute_reply.started":"2022-05-07T12:41:34.57155Z","shell.execute_reply":"2022-05-07T12:41:34.577617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As shown in the statistical summmary in task 1, the input variables are of different scale. Hence, normalisation will be done before model training to ensure equal importance/contribution of the features to the model. It may also help to reduces the training time depending on the algorithm used to train the model. ","metadata":{}},{"cell_type":"markdown","source":"## <a id='t3'>Task 3 - Logistic Regression Equation</a>","metadata":{}},{"cell_type":"markdown","source":"__Logistic Regression__ is a statistical method for predicting binary (or two) classes. The target variable is dichotomous in nature. In this case, there are only two possible classes: whether the bank note is forged or not. \n\nThe logistic regression graph is in the form of a sigmoid function such that the prediction above 0.5 (by default) would be classified as forged. Those below 0.5 would be classified as unforged. The threshold probability of 0.5 may be changed depending on the problem statement. \n\n__Logistic Regression Equation for Bank Notes Classification Problem (4 input features):__\n$ P(forged bank note) = P(x)= \\Large\\frac{1}{1 + e^{- (\\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{3} + \\beta_{4}x_{4})}}$ \n\nwhere\n- $\\beta_{0}$ - bias term, and\n- $\\beta_{1}, \\beta_{2}, \\beta_{3}, \\beta_{4}$ - weights for the input features\n\n<br>\nThe parameters that needs to be estimated are $\\beta_{0}, \\beta_{1}, \\beta_{2}, \\beta_{3}, \\beta_{4}$ such that it  maximises the log-likelihood function as follows.\n$ l = \\frac{1}{4} \\sum \\limits _{k=1} ^{4}y_{k} \\log {P(x_{k})} + \\sum \\limits _{k=1} ^{4}(1-y_{k}) \\log{(1-P(x_{k})}) $\n\nwhere \n- M = 4 as there are 4 inputs features,  \n- $y_{k}$ - categorical outcome of the prediction of k-th observation, and \n- $x_{k}$ - input features / explanatory variables of k-th observation.","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"t4\"> Task 4 - Split dataset into train and test </a>","metadata":{}},{"cell_type":"code","source":"# Split dataset into train and test using Sklearn train_test_split\n# As the API includes a shuffle attribute with a default value of True, additional manual shuffling of the \n# data is deemed unrequired.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of y_train:\", y_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)\nprint(\"Shape of y_test:\", y_test.shape)","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:49:10.980044Z","start_time":"2022-04-25T10:49:10.957762Z"},"execution":{"iopub.status.busy":"2022-05-07T12:41:38.579629Z","iopub.execute_input":"2022-05-07T12:41:38.579958Z","iopub.status.idle":"2022-05-07T12:41:38.651124Z","shell.execute_reply.started":"2022-05-07T12:41:38.579924Z","shell.execute_reply":"2022-05-07T12:41:38.650165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id='t5'>Task 5 - SGDClassifier</a>","metadata":{}},{"cell_type":"code","source":"# import required library\nfrom sklearn.utils.fixes import loguniform\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# model and parameters\nmodel = SGDClassifier(loss='log', learning_rate='optimal', eta0=0.00001, random_state=42)\nspace = dict()\nspace['alpha'] = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\nspace['max_iter'] = [500, 1000, 2500, 5000, 10000]\nspace['tol'] = [0.00001, 0.00005, 0.0005, 0.001, 0.005, 0.01]\n\n# GridSearchCV for best params\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\nsearch = GridSearchCV(model, space, scoring='accuracy', n_jobs=-1, cv=cv)\nresult = search.fit(X, y)","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:52:18.842981Z","start_time":"2022-04-25T10:49:10.980692Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-05-07T12:45:00.644346Z","iopub.execute_input":"2022-05-07T12:45:00.644666Z","iopub.status.idle":"2022-05-07T12:45:30.569563Z","shell.execute_reply.started":"2022-05-07T12:45:00.644627Z","shell.execute_reply":"2022-05-07T12:45:30.568894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print best score and params for GridSearchCV\nprint(f'Best score: {result.best_score_}')\nprint(f'Best Hyperparameters: {result.best_params_}')","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:52:18.845636Z","start_time":"2022-04-25T10:52:18.843904Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Initial Grid Search Hyperparameter Tuning Findings\n\nAs shown above, we initial the random values in the hyperparameters *alpha*, *max_iter*, *tol*, *learning_rate* that we want to tuned in a dictionary and execute the GridSearchCV algorithm from Scikit Learn library. The difference between the initialised random values were intentionally set larger as further tuning of the values can be done after the initial best parameters are found. \n\nThe inital best hyperparameters are summarised as follows. \n* alpha: 5e-05\n* learning_rate = 'optimal'\n* max_iter = 500 \n* tol = 0.0005\n\nFurther tuning is conducted below with a lower range of values for each hyperparameter to check and verify if any further improvement is possible.","metadata":{}},{"cell_type":"code","source":"# Further tuning of GridSearchCV parameters based on initial findings\n# model and parameters\nmodel3 = SGDClassifier(loss='log', random_state=42)\nspace3 = dict()\nspace3['alpha'] = [0.00001, 0.00002, 0.00003, 0.00004, 0.00005, 0.00006, 0.00007]\nspace3['max_iter'] = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\nspace3['tol'] = [0.00025, 0.0005, 0.00075, 0.001, 0.00125, 0.0025, 0.00275, 0.003]\nspace3['learning_rate'] = ['optimal']\n\n# GridSearchCV for best params\ncv3 = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\nsearch3 = GridSearchCV(model3, space3, scoring='accuracy', n_jobs=-1, cv=cv)\nresult3 = search3.fit(X, y)","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:52:29.975186Z","start_time":"2022-04-25T10:52:18.846402Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print best score and params for RandomSearchCV\nprint(f'Best score: {result3.best_score_}')\nprint(f'Best Hyperparameters: {result3.best_params_}')","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:52:29.977935Z","start_time":"2022-04-25T10:52:29.975992Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Finalised Grid Search Hyperparameters \n\nThe finalised hyperparameters used to train the model are as follows. \n\n* __alpha__ (val: 2e-05): Constant which multiplies with the regularization term and used to compute the learning_rate when learning_rate is set to optimal. The higher the value, the stronger the regularization.\n* __learning_rate__ (val: 'optimal'): Value that determine step the algorithm takes to optimise the loss function. Learning rate schedule includes 4 options: constant, optimal, invscaling and adpative. \n* __max_iter__ (val: 50): Maximum number of passes over the training data. \n* __tol__ (val: 0.00275): Stopping criterion for the training. If none is set, then the training will stop when loss > best_loss - tol. ","metadata":{}},{"cell_type":"code","source":"# Train model using parameters found by GridSearchCV \n\nsgdcls = SGDClassifier(loss='log', random_state=42, alpha=2e-05, \n                       learning_rate='optimal', max_iter=50, tol=0.00275)\n\nsgdcls.fit(X_train, y_train)\ny_pred = sgdcls.predict(X_test)","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:52:29.984212Z","start_time":"2022-04-25T10:52:29.978869Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the finalised hyperparameters as summarised above, we will build and train the model to predict if the bank notes is forged or not. Refer to task 6 for the classification report and confusion matrix. ","metadata":{}},{"cell_type":"markdown","source":"#### Extra Random Search Hyperparameter Tuning\n\nAs the values are very close, we will use the parameters found using GridSearchCV in this practicum.","metadata":{}},{"cell_type":"code","source":"# model and parameters\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import expon as sp_expon\n\nmodel2 = SGDClassifier(loss='log', eta0=0.00001, random_state=42)\nspace2 = dict()\nspace2['alpha'] = loguniform(1e-6, 1e1)\nspace2['max_iter'] = sp_randint(100,2000)\nspace2['tol'] = loguniform(1e-5, 1e-1)\nspace2['learning_rate'] = ['optimal']\n\n# RandomSearchCV for best params\ncv2 = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\nsearch2 = RandomizedSearchCV(model2, space2, n_iter=2000, scoring='accuracy', n_jobs=-1, cv=cv, random_state=42)\nresult2 = search2.fit(X, y)","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:54:37.163992Z","start_time":"2022-04-25T10:52:29.98504Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print best score and params for RandomSearchCV\nprint(f'Best score: {result2.best_score_}')\nprint(f'Best Hyperparameters: {result2.best_params_}')","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:54:37.16752Z","start_time":"2022-04-25T10:54:37.165136Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id='t6'>Task 6: Classification report for above predictions</a> ","metadata":{}},{"cell_type":"code","source":"# import relevant metrics and print classification report\nfrom sklearn.metrics import plot_confusion_matrix, classification_report\nprint(classification_report(y_test, y_pred))","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:54:37.173322Z","start_time":"2022-04-25T10:54:37.168203Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix\nplot_confusion_matrix(sgdcls, X_test, y_test)\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:54:37.283695Z","start_time":"2022-04-25T10:54:37.174057Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The trained model has a high F1 score of 0.99. There were only 6 misclassified bank notes which were not forged but highlighted as forged by the model. As the impact of false negative (i.e., loss of profit/revenue by not identifying the forged bank notes) is greater than false positive, the impact of the misclassification above is deemed to be acceptable in this case. In the next task, we will look into the different regularization techniques. ","metadata":{}},{"cell_type":"markdown","source":"## <a id='t7_8'>Task 7 & 8 - Regularization and classification reports</a>","metadata":{}},{"cell_type":"markdown","source":"#### L1-norm: Lasso Regression","metadata":{}},{"cell_type":"code","source":"# SGDclassifier with L1-norm (Lasso Regression) and classification report\nsgdcls_l1 = SGDClassifier(loss='log', random_state=42, alpha=2e-05, \n                       learning_rate='optimal', max_iter=50, tol=0.00275, penalty='l1')\n\nsgdcls_l1.fit(X_train, y_train)\ny_pred_l1 = sgdcls_l1.predict(X_test)\nprint(classification_report(y_test, y_pred_l1))","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:54:37.292246Z","start_time":"2022-04-25T10:54:37.28461Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix of SGDclassifier with l1-norm regularization\nplot_confusion_matrix(sgdcls_l1, X_test, y_test)\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:54:37.355704Z","start_time":"2022-04-25T10:54:37.293155Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above model, we add in the __penalty__ parameter and set to $l_{1}$, which is the lasso regression, which uses the $l_{1}norm$. \n\n* Lasso Regression cost function: $J(\\theta) = MSE(\\theta) + \\alpha \\sum \\limits_{i=1}^{n} \\left\\lvert{\\theta_{i}}\\right\\rvert$\n\nwhere \n- $\\alpha$ is the hyperparameter to control how much to regularize the model. For $\\alpha$ = 0, original regression model is used. For large $\\alpha$, the result is close to a flat line at the dataset's mean.\n\nLasso regression tends to eliminate the weight of insignicant or non-important features (i.e., setting the weight to zero). In other words, it performs feature selection automatically and return a sparse model with only the features deemed as important. \n\n#### Lasso Regression Findings \nBased on the classification report and confusion matrix, we note an improvement in accuracy as the number of misclassification decrease from 6 to 4. There were 2 forged bank notes predicted as not forged, and 2 original bank notes predicted as forged. \n\nHowever, this may not be ideal for this particular problem as the false negative (type 2 error) has more impact (i.e., result in potential loss of profit and revenue) as the forged bank note was not highlighted. ","metadata":{}},{"cell_type":"markdown","source":"#### L2-norm: Ridge Regression","metadata":{}},{"cell_type":"code","source":"# SGDclassifier with L2-norm (Ridge Regression) and classification report\nsgdcls_l2 = SGDClassifier(loss='log', random_state=42, alpha=2e-05, \n                       learning_rate='optimal', max_iter=50, tol=0.00275, penalty='l2')\n\nsgdcls_l2.fit(X_train, y_train)\ny_pred_l2 = sgdcls_l2.predict(X_test)\nprint(classification_report(y_test, y_pred_l2))","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:54:37.364308Z","start_time":"2022-04-25T10:54:37.356756Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(sgdcls_l2, X_test, y_test)\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:54:37.43678Z","start_time":"2022-04-25T10:54:37.36559Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For ridge regression, we add the **penalty** parameter and set it to $l_{2}$, which uses the $l_{2} norm$. \n* Ridge regression cost function: $J(\\theta) = MSE(\\theta) + \\alpha(\\frac{1}{2} \\sum \\limits_{i=1}^{n} \\theta_{i}^{2})$\n\nwhere \n- $\\alpha$ is the hyperparameter to control how much to regularize the model. For $\\alpha$ = 0, original regression model is used. For large $\\alpha$, the result is close to a flat line at the dataset's mean.\n\nFor ridge regression, it restricts the weights of the features that are not important or insignificant (by assigning values that are close to 0). Hence, the features contribution to the output is less significant. However, the features would need remain to contribute to the output in the final model. In other word, feature selection is not automatically conducted in ridge regression. \n\n#### Ridge regression findings\n\nBased on the classification report and confusion matrix, we note that there is no difference from the initial model. \n","metadata":{}},{"cell_type":"markdown","source":"#### L1-norm & L2-norm: Elastic Net","metadata":{}},{"cell_type":"code","source":"# SGDclassifier with L1-norm and L2-norm (Elastic Net) and classification report\nsgdcls_en = SGDClassifier(loss='log', random_state=42, alpha=2e-05, \n                       learning_rate='optimal', max_iter=50, tol=0.00275, penalty='elasticnet')\n\nsgdcls_en.fit(X_train, y_train)\ny_pred_en = sgdcls_en.predict(X_test)\nprint(classification_report(y_test, y_pred_en))","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:54:37.446454Z","start_time":"2022-04-25T10:54:37.438247Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(sgdcls_en, X_test, y_test)\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:54:37.519547Z","start_time":"2022-04-25T10:54:37.447538Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For elastic net, we add the **penalty** parameter and set it to `elasticnet`, which uses both $l_{1} norm$ and $l_{2} norm$. \n\n* Elastic net cost function: $J(\\theta) = MSE(\\theta) + \\alpha \\sum \\limits_{i=1}^{n} \\left\\lvert{\\theta_{i}}\\right\\rvert + \\alpha(\\frac{1}{2} \\sum \\limits_{i=1}^{n} \\theta_{i}^{2})$\n\nElastic net is the combination of lasso and ridge regression. \n\n#### Elastic net regularization findings\n\nBased on the classification report and confusion matrix, we note that there is no difference from the initial model.","metadata":{}},{"cell_type":"markdown","source":"## <a id='t9'>Task 9: K-Nearest Neighbour (KNN)</a>","metadata":{}},{"cell_type":"code","source":"# K-nearest neighbours and classification report\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier()\nneigh.fit(X_train, y_train)\ny_pred_knn = neigh.predict(X_test)\nprint(classification_report(y_test, y_pred_knn))","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:54:37.561234Z","start_time":"2022-04-25T10:54:37.525147Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(neigh, X_test, y_test)\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2022-04-25T10:54:37.629842Z","start_time":"2022-04-25T10:54:37.56238Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comparison of accuracy of KNN and SGD models\n\nAs shown in the classification report and confusion matrix, KNN model has a 100% accuracy for the test/validation set whereas SGD model has an accuracy of 99%. This could mean the the KNN model fits the dataset better than the SGD model where it's possible to accurately predict whether the bank note is forged by analysis the mode of the 5 nearest neighbour. However, the accuracy for SGD model is also considered respectable at 99%. \n","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"references\">References</a>\n- [Wikipedia: Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)\n- [Machine Learning Mastery: Hyperparameter optimization](https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/)\n- [Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html)","metadata":{}}]}