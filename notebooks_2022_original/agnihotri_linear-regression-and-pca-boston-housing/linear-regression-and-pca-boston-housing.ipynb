{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Linear Regression and PCA - Boston Housing"},{"metadata":{},"cell_type":"markdown","source":"I  recently learned Regression and Principal Component Analysis and was very eager to try my hands on some intreasting dataset. I found the Boston housing data set as a perfect place to get my hands dirty on this.\n\nInitially i did the analysis using SAS and when the results where really good, i thought of implementing the same in Python and publish my work.\n\nAlthough i faced a few issues in Python as there were no easy implementation of analysis techniques like Variance Inflation Factor and Feature Selection, so therefore i tried to code them in myself.\n\nLet me know your feedback on this, as this would help me improve and put more good quality work in it."},{"metadata":{},"cell_type":"markdown","source":"### Upvote if you find it helpful"},{"metadata":{},"cell_type":"markdown","source":"# About the Data"},{"metadata":{},"cell_type":"markdown","source":"This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. It was obtained from the StatLib archive (http://lib.stat.cmu.edu/datasets/boston), and has been used extensively throughout the literature to benchmark algorithms. However, these comparisons were primarily done outside of Delve and are thus somewhat suspect. The dataset is small in size with only 506 cases."},{"metadata":{},"cell_type":"markdown","source":"-Variables\nThere are 14 attributes in each case of the dataset. They are:\n\nCRIM - per capita crime rate by town\n\nZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n\nINDUS - proportion of non-retail business acres per town.\n\n\nCHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n\nNOX - nitric oxides concentration (parts per 10 million)\n\nRM - average number of rooms per dwelling\n\nAGE - proportion of owner-occupied units built prior to 1940\n\nDIS - weighted distances to five Boston employment centres\n\nRAD - index of accessibility to radial highways\n\nTAX - full-value property-tax rate per 10,000\n\nPTRATIO - pupil-teacher ratio by town\n\nB - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n\nLSTAT - per lower status of the population\n\nMEDV - Median value of owner-occupied homes in $1000's"},{"metadata":{},"cell_type":"markdown","source":"# Data Loading"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Libraries needed to load the data\nimport pandas as pd\nfrom sklearn.datasets import load_boston","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data from sklearn module\ndf = pd.DataFrame(load_boston().data,columns=load_boston().feature_names)\ndf['MEDV'] = pd.DataFrame(load_boston().target)\nprint('Shape of Data is : {} rows and {} columns'.format(df.shape[0],df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at the null values of the data\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Good! no null values in the  data.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at the datatype of the features\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All numeric, great!!!"},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import libraries needed to do EDA\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at the distribution plot of the features\npos = 1\nfig = plt.figure(figsize=(16,24))\nfor i in df.columns:\n    ax = fig.add_subplot(7,2,pos)\n    pos = pos + 1\n    sns.distplot(df[i],ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Except RM and MEDV, nothing else is normally distributed, this might be an issue, as most statistical assumptions hold true only when our data is normally distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets look at some descriptive stats of our features\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scale of our features are very different from each other, therefore we might have to rescale our data to improve our data quality, as we cannot apply PCA or Linear Regression on this data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at the correlation matrix of our data.\nfig = plt.figure(figsize=(16,12))\nax = fig.add_subplot(111)\nsns.heatmap(df.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note :\nOur target variable, seems to be highly correlated, with LSTAT and RM, which makes sense, as these two are very important factors for house pricing, but there seems to be a lot of multicollinearity as well.\n\nThe issue here is, that there is a lot of collinearity between our predictor variables, for example DIS is highly correlated to INUDS, INOX and AGE.\n\nThis is not good, as multicollinearity can make our model unstable, we need to look at it a little more, before modeling our data,  I have explained, the probem of multicollinearity below."},{"metadata":{},"cell_type":"markdown","source":"## Variance Inflation Factor"},{"metadata":{},"cell_type":"markdown","source":"A variance inflation factor(VIF) detects multicollinearity in regression analysis. Multicollinearity is when there’s correlation between predictors (i.e. independent variables) in a model; it’s presence can adversely affect your regression results. The VIF estimates how much the variance of a regression coefficient is inflated due to multicollinearity in the model."},{"metadata":{},"cell_type":"markdown","source":"\\begin{align*}\nVIF = 1/(1 - R^2)\n\\end{align*}"},{"metadata":{},"cell_type":"markdown","source":"Where <b>R Squared</b> is <b>coefficient of determination</b>, in simple terms, it is the proportion of variance in independent variable, which is explained by dependent variable. Formula of r squared is as follows"},{"metadata":{},"cell_type":"markdown","source":"\\begin{align*}\nR^2 = 1 - (Residual sum of Squares)/(Total Sum of Squares)\n\\end{align*}"},{"metadata":{},"cell_type":"markdown","source":"So what we do is, we perform Linear Regression using each variable as target and others as predictors and the calculate ther R-Squared, then calculate the VIF for them.\n\nIf VIF < 4, its okay to be used, other wise we need to find a way to remove collinearrity from these features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import libraries needed for this.\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets get the VIF value to understand the multi collinearity\nvifdf = []\nfor i in df.columns:\n    X = np.array(df.drop(i,axis=1))\n    y = np.array(df[i])\n    lr = LinearRegression()\n    lr.fit(X,y)\n    y_pred = lr.predict(X)\n    r2 = r2_score(y,y_pred)\n    vif = 1/(1-r2)\n    vifdf.append((i,vif))\n\nvifdf = pd.DataFrame(vifdf,columns=['Features','Variance Inflation Factor'])\nvifdf.sort_values(by='Variance Inflation Factor')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Note :\nWhat we can see  is that, almost half of our features are having either VIF value greater than or near to 4, and TAX and RAD have VIF almost double of our threshold.\n\n### Problem of Multicollinearity\n\nIn the presence of multicollinearity, the estimate of one variable's impact on the dependent variable Y while controlling for the others tends to be less precise than if predictors were uncorrelated with one another.\n\nThe usual interpretation of a regression coefficient is that it provides an estimate of the effect of a one unit change in an independent variable, holding the other variables constant.\n\nIf X1 is highly correlated with another independent variable, X2 in the given data set, then we have a set of observations for which X1 and X2 have a particular linear stochastic relationship.\n\nWe don't have a set of observations for which all changes in X1 are independent of changes in X2, so we have an imprecise estimate of the effect of independent changes in X1."},{"metadata":{},"cell_type":"markdown","source":"# Standardiztion of Data"},{"metadata":{},"cell_type":"markdown","source":"### Rescaling the data\n\nAs our data comprises of many kinds of features, all of which have different scale. This is okay for Analysis, but not for data  modelling. As different scales can cause our model to be unstable and vary more than we would want."},{"metadata":{},"cell_type":"markdown","source":"#### Z-score Normalization"},{"metadata":{},"cell_type":"markdown","source":"\\begin{align*}\nx' = (x - mean)/std\n\\end{align*}"},{"metadata":{},"cell_type":"markdown","source":"This will give us data with mean = 0 and std = 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets build our function which will perform the normaliztion\ndef rescale(X):\n    mean = X.mean()\n    std = X.std()\n    scaled_X = [(i - mean)/std for i in X]\n    return pd.Series(scaled_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will build a new dataframe\ndf_std = pd.DataFrame(columns=df.columns)\nfor i in df.columns:\n    df_std[i] = rescale(df[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at the descriptive stats now\ndf_std.describe().iloc[1:3:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Note : Shape of Data does not changes when rescaling, it just scales the data to give mean at 0, and standard deviation as 1 for all the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets look at the shape of data after scaling\npos = 1\nfig = plt.figure(figsize=(16,24))\nfor i in df_std.columns:\n    ax = fig.add_subplot(7,2,pos)\n    pos = pos + 1\n    sns.distplot(df_std[i],ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As you can see, shape did not change, only the mean value shifted to 0"},{"metadata":{},"cell_type":"markdown","source":"# Principal Component Analysis"},{"metadata":{},"cell_type":"markdown","source":"In simple words, PCA is a mathematical procedure, which takes a few linearly correlated features and returns few uncorrelated features.\n\nIt is often used in dimensionality reduction for reducing complexity of learning models or to visualize the multidimensional data into 2D or 3D data, making to easy to visualize.\n\nBut to say that, PCA is just a dimensionality reduction technique is like saying Java and Javascript are same."},{"metadata":{},"cell_type":"markdown","source":"<b>Wiki</b>\n\nPrincipal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components.\nThis transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.\nThe resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables."},{"metadata":{},"cell_type":"markdown","source":"### Why we need it here ?\n\nWell we do not need it for dimesionality reduction of course, as our model is not that complex,\nWe need to remove the multicollinearity problem in our data.\n\nWe are going to feed in our standardized predictor variables into the the PCA transformation and get a set of  uncorrelated features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import libraries for PCA\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=13)\nX = df_std.drop('MEDV',axis=1)\nX_pca = pca.fit_transform(X)\ndf_std_pca = pd.DataFrame(X_pca,columns=['PCA1','PCA2','PCA3','PCA4','PCA5','PCA6','PCA7','PCA8','PCA9','PCA10','PCA11','PCA12','PCA13'])\ndf_std_pca['MEDV'] = df_std['MEDV']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at the correlation matrix now.\nfig = plt.figure(figsize=(16,12))\nax = fig.add_subplot(111)\nsns.heatmap(df_std_pca.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Note : As you can see there is no correlation between predictor variables, thus removing multicollinearity."},{"metadata":{},"cell_type":"markdown","source":"#### Note : The reason correlation between predictor variable and target varaiable is in sorted order is because, PCA takes all the explained variation and puts it into first components, and repeats the  process. The new feature are in no way related to the old ones, therfore it would be wrong to use the same name for them.\n\n#### Note : PCA is often used to make the data Anonymous as it makes the data completely different from the original one, while still keeping the information in the data intact."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at the distribution of our features after applying PCA\npos = 1\nfig = plt.figure(figsize=(16,24))\nfor i in df_std_pca.columns:\n    ax = fig.add_subplot(7,2,pos)\n    pos = pos + 1\n    sns.distplot(df_std_pca[i],ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Modelling"},{"metadata":{},"cell_type":"markdown","source":"### Now that our data is ready, we can apply our modelling techniques to it."},{"metadata":{},"cell_type":"markdown","source":"## Simple Linear Regression"},{"metadata":{},"cell_type":"markdown","source":"\\begin{align*}\ny' = theta0 + theta1*x1 +theta2*x2 ...... + theta13*x13\n\\end{align*}"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import libraires needed to perform our Regression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data into Training and testing\nX = np.array(df_std_pca.drop('MEDV',axis=1))\ny = np.array(df_std_pca['MEDV'])\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\nfor i in [X_train,X_test,y_train,y_test]:\n    print(\"Shape of Data is {}\".format(i.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets train our model on training data and predict also on training to see results\nlr = LinearRegression()\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_train)\nr2 = r2_score(y_train,y_pred)\nrmse = np.sqrt(mean_squared_error(y_train,y_pred))\nprint('R-Squared Score is : {} | Root Mean Square Error is : {}'.format(r2,rmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets train our model on training data and predict on testing to see results\nlr = LinearRegression()\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\nr2 = r2_score(y_test,y_pred)\nrmse = np.sqrt(mean_squared_error(y_test,y_pred))\nprint('R2 Score is : {} | Root Mean Square Error is : {}'.format(r2,rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This is better than I anticipated !!"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Regression Analysis has taken a back sit in todays world of Deep Learning and Complex Learning techniques, but I strongly believe that that these procedures can help explain your data quickly and give more explaination of the results.\n\nThrough this notebook I tried to explain the power of simple mathematical concepts and show there usage.\n\nBuilding this notebook has helped me learn and clarify few key concepts like PCA and VIF, hope it does the same for you."},{"metadata":{},"cell_type":"markdown","source":"I would try to improve the notebook if i get time, your suggestions would really help."},{"metadata":{},"cell_type":"markdown","source":"## Hope you liked the notebook, please leave a comment and upvote."},{"metadata":{},"cell_type":"markdown","source":"## Alright Folks, thats all for today!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}