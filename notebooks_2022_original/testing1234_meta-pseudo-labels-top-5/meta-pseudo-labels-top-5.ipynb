{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hyperparameter Tuning with Vizier #\n\nIn the second half of this notebook, we'll demonstrate Vertex Vizier, Vertex AI's hyperparameter optimization service. Among its capabilities are a Bayesian Optimization algorithm to search efficiently within a hyperparameter space, transfer learning to make use of information from previous hyperparameter studies, and automated early stopping when tuning models that train incrementally, like neural nets with stochastic gradient descent or gradient boosted trees. Google Research has a great whitepaper describing the capabilities of Vizier in detail: Google Vizier: A Service for Black-Box Optimization. Also see the Vizier guide for a nice overview.","metadata":{"papermill":{"duration":0.014983,"end_time":"2022-03-23T14:43:22.098982","exception":false,"start_time":"2022-03-23T14:43:22.083999","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X_test = df_test.loc[:, ['latitude', 'longitude']]\n\nidx_pred = nhbrs.kneighbors(X_test, return_distance=False).squeeze()\nidx_pred = idx_pred[:, 1:]  # don't include self","metadata":{"collapsed":false,"papermill":{"duration":40.294184,"end_time":"2022-03-23T14:44:27.128909","exception":false,"start_time":"2022-03-23T14:43:46.834725","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-13T21:44:23.928208Z","iopub.execute_input":"2022-04-13T21:44:23.928494Z","iopub.status.idle":"2022-04-13T21:44:24.038656Z","shell.execute_reply.started":"2022-04-13T21:44:23.928466Z","shell.execute_reply":"2022-04-13T21:44:24.037616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(data_dir / 'train.csv', index_col='id')\ndf_pairs = pd.read_csv(data_dir / 'pairs.csv')\n\ndf_test = pd.read_csv(data_dir / 'test.csv', index_col='id')\nsubmission = pd.read_csv(data_dir / 'sample_submission.csv', index_col='id')","metadata":{"collapsed":false,"papermill":{"duration":16.069474,"end_time":"2022-03-23T14:43:38.183883","exception":false,"start_time":"2022-03-23T14:43:22.114409","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-13T21:36:10.01169Z","iopub.execute_input":"2022-04-13T21:36:10.012203Z","iopub.status.idle":"2022-04-13T21:36:27.510555Z","shell.execute_reply.started":"2022-04-13T21:36:10.012127Z","shell.execute_reply":"2022-04-13T21:36:27.509563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_pairs.join(y_pairs).groupby('match').mean()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T21:45:07.815563Z","iopub.execute_input":"2022-04-13T21:45:07.815885Z","iopub.status.idle":"2022-04-13T21:45:07.855264Z","shell.execute_reply.started":"2022-04-13T21:45:07.815843Z","shell.execute_reply":"2022-04-13T21:45:07.854384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select matching pairs for submission\nsubmission = (\n    matches\n    .groupby('id_1')['id_2']\n    .apply(lambda x: ' '.join(x))\n    .rename_axis('id')\n    .rename('matches')\n)\ndisplay(submission)","metadata":{"collapsed":false,"papermill":{"duration":0.03912,"end_time":"2022-03-23T14:48:28.546384","exception":false,"start_time":"2022-03-23T14:48:28.507264","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-13T21:47:38.71956Z","iopub.execute_input":"2022-04-13T21:47:38.719873Z","iopub.status.idle":"2022-04-13T21:47:38.730201Z","shell.execute_reply.started":"2022-04-13T21:47:38.719838Z","shell.execute_reply":"2022-04-13T21:47:38.729322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Study #\n\nNow we'll create a study. A study conducts trials in order to optimize one or more metrics. A trial is a selection of hyperparameter values together with the outcome they produce. In our case, the hyperparameters define a neural net architechture and training regimen, and will produce a validation loss, the metric we hope to minimize.","metadata":{}},{"cell_type":"code","source":"display(df_train)","metadata":{"collapsed":false,"papermill":{"duration":0.049726,"end_time":"2022-03-23T14:43:38.249487","exception":false,"start_time":"2022-03-23T14:43:38.199761","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-13T21:36:27.511662Z","iopub.execute_input":"2022-04-13T21:36:27.511875Z","iopub.status.idle":"2022-04-13T21:36:27.545833Z","shell.execute_reply.started":"2022-04-13T21:36:27.511849Z","shell.execute_reply":"2022-04-13T21:36:27.544912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create submission\nsubmission.to_csv('submission.csv')","metadata":{"collapsed":false,"papermill":{"duration":0.028842,"end_time":"2022-03-23T14:48:28.594013","exception":false,"start_time":"2022-03-23T14:48:28.565171","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-13T21:47:55.99224Z","iopub.execute_input":"2022-04-13T21:47:55.993125Z","iopub.status.idle":"2022-04-13T21:47:55.998732Z","shell.execute_reply.started":"2022-04-13T21:47:55.993067Z","shell.execute_reply":"2022-04-13T21:47:55.998128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Key: Entry id, Value: Neighbor ids\nneighbors = {entry: df_test.index[nhbr].to_list() for entry, nhbr in zip(df_test.index, idx_pred)}\nneighbors = pd.Series(neighbors, name='id_2').rename_axis('id_1').explode().reset_index()\nneighbors","metadata":{"execution":{"iopub.status.busy":"2022-04-13T21:44:25.717846Z","iopub.execute_input":"2022-04-13T21:44:25.718127Z","iopub.status.idle":"2022-04-13T21:44:25.731773Z","shell.execute_reply.started":"2022-04-13T21:44:25.718098Z","shell.execute_reply":"2022-04-13T21:44:25.731204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_pairs = make_similarity_df(df_pairs)\ny_pairs = df_pairs.loc[:, 'match'].astype(int)\n\nX_pairs.join(y_pairs)","metadata":{"collapsed":false,"papermill":{"duration":27.991671,"end_time":"2022-03-23T14:44:55.228711","exception":false,"start_time":"2022-03-23T14:44:27.23704","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-13T21:44:34.755287Z","iopub.execute_input":"2022-04-13T21:44:34.755579Z","iopub.status.idle":"2022-04-13T21:45:07.813128Z","shell.execute_reply.started":"2022-04-13T21:44:34.75555Z","shell.execute_reply":"2022-04-13T21:45:07.812112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def category_similarity(df):\n    X = df['categories_1'].fillna('').str.split(',').combine(\n        df['categories_2'].fillna('').str.split(','),\n        lambda c1, c2: len(set(c1) & set(c2)),\n    ).rename('category_similarity')\n    return X.mask(\n        df[['categories_1', 'categories_2']].isna().any(axis=1),\n        0,\n    )\n\n\ndef string_similarity(df, attr, fn):\n    X = df[f'{attr}_1'].fillna('').combine(\n        df[f'{attr}_2'].fillna(''),\n        lambda n1, n2: fn(n1, n2)\n    ).rename(f'{attr}_similarity')\n    return X.mask(\n        df[[f'{attr}_1', f'{attr}_2']].isna().any(axis=1),\n        0,\n    )\n        ","metadata":{"collapsed":false,"papermill":{"duration":0.030503,"end_time":"2022-03-23T14:44:27.175873","exception":false,"start_time":"2022-03-23T14:44:27.14537","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-13T21:44:31.165336Z","iopub.execute_input":"2022-04-13T21:44:31.166213Z","iopub.status.idle":"2022-04-13T21:44:31.175607Z","shell.execute_reply.started":"2022-04-13T21:44:31.166166Z","shell.execute_reply":"2022-04-13T21:44:31.174602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Notebook Setup #\n\n1. Download this Notebook\nStart by creating your own copy of this notebook. Click the Copy and Edit button to the upper right. Now, in the menubar above, click File -> Download Notebook and save a copy of the notebook to your computer. We will reupload this in an AI Notebooks instance to take advantage of the Explainable AI service.\n\n2. Download Kaggle API Key\nWe'll use the Kaggle API to download the competition data to the notebook instance. You'll need a copy of your Kaggle credentials to authenticate your account.\n\nFrom the site header, click on your user profile picture, then on “My Account” from the dropdown menu. This will take you to your account settings at https://www.kaggle.com/account. Scroll down to the section of the page labelled API.\n\nTo create a new token, click on the “Create New API Token” button. This will download a fresh authentication token onto your machine.\n\n3. Sign up for Google Cloud Platform\nIf you don't have a GCP account already, go to https://cloud.google.com/ and click on “Get Started For Free\". This is a two step sign up process where you will need to provide your name, address and a credit card. The starter account is free and it comes with $300 credit that you can use. For this step you will need to provide a Google Account (i.e. your Gmail account) to sign in.\n\n4. Create a Project and Enable the Notebook API\nFollow the directions at https://cloud.google.com/notebooks/docs/before-you-begin to setup a notebook project.\n\n5. Create a Notebook Instance\nNext, go to https://notebook.new. Enter an Instance name of your choice and then click the blue CREATE button at the end of the page. Be sure to keep the default TensorFlow Enterprise environment. You'll be redirected to a page with a list of your notebook instances. It may take a few minutes for the instance you just created to start up.\n\nOnce the notebook instance is running, click OPEN JUPYTERLAB just to the right of the instance name. You should be redirected to a JupyterLab environment.\n\n6. Upload MLB Notebook and API Key\nFrom inside JupyterLab, click the \"Upload Files\" (up arrow) button in the file browser on the left and upload the files kaggle.json and vertex-ai-with-mlb-player-digital-engagement.ipynb.\n\n7. Authenticate Kaggle API and Download MLB Date\nRun the next cell to download the competition data.","metadata":{}},{"cell_type":"code","source":"def make_similarity_df(df_pairs):\n    from fuzzywuzzy import fuzz\n    X_pairs = pd.DataFrame(index=df_pairs.index)\n    # Categories\n    X_pairs = X_pairs.join(category_similarity(df_pairs))\n    # Names\n    X_pairs = X_pairs.join(string_similarity(df_pairs, attr='name', fn=fuzz.partial_ratio))\n    return X_pairs","metadata":{"collapsed":false,"papermill":{"duration":0.027347,"end_time":"2022-03-23T14:44:27.219792","exception":false,"start_time":"2022-03-23T14:44:27.192445","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-13T21:44:31.548239Z","iopub.execute_input":"2022-04-13T21:44:31.548558Z","iopub.status.idle":"2022-04-13T21:44:31.554915Z","shell.execute_reply.started":"2022-04-13T21:44:31.548527Z","shell.execute_reply":"2022-04-13T21:44:31.554027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MLB Getting Started #\n\nThe first part of this notebook reproduces the data and model setup of the Getting Started notebook.\n\nThe results of Explainable AI will be easier to understand if we restrict our analysis to a single player. The next cell has a helper function to load data for only a single player, by default Aaron Judge of the NY Yankees, who had the highest overall engagement during the training period.\n\nWe've picked out a few features from the playerBoxScores dataframe, but there are lots more you could try (see the data documentation for a complete description). Increase the number of Fourier components to model seasonality with in more detail. You could also look at explanations for other players -- the players dataframe can tell you the playerId for each player.","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\n\nX_test = df_test.loc[:, ['latitude', 'longitude']]\n\nnhbrs = NearestNeighbors(\n    n_neighbors=5,\n    n_jobs=-1,\n).fit(X_test)","metadata":{"collapsed":false,"papermill":{"duration":8.486797,"end_time":"2022-03-23T14:43:46.818161","exception":false,"start_time":"2022-03-23T14:43:38.331364","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-13T21:36:27.547742Z","iopub.execute_input":"2022-04-13T21:36:27.547983Z","iopub.status.idle":"2022-04-13T21:36:28.768044Z","shell.execute_reply.started":"2022-04-13T21:36:27.547954Z","shell.execute_reply":"2022-04-13T21:36:28.767167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\ndata_dir = Path('../input/foursquare-location-matching')","metadata":{"collapsed":false,"papermill":{"duration":0.029398,"end_time":"2022-03-23T14:43:22.067968","exception":false,"start_time":"2022-03-23T14:43:22.03857","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-13T21:36:09.981881Z","iopub.execute_input":"2022-04-13T21:36:09.982266Z","iopub.status.idle":"2022-04-13T21:36:10.010007Z","shell.execute_reply.started":"2022-04-13T21:36:09.982171Z","shell.execute_reply":"2022-04-13T21:36:10.009375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def metric_fn(params):\n    # Parse hyperparameters\n    units = int(params['units'])\n    dropout = params['dropout']\n    optimizer = params['optimizer']\n    batch_size = int(params['batch_size'])\n    # Create and train model\n    INPUTS = X_train.shape[-1]\n    OUTPUTS = y_train.shape[-1]\n    early_stopping = keras.callbacks.EarlyStopping(patience=10,\n                                                   restore_best_weights=True)\n    model = keras.Sequential([\n        layers.InputLayer(name='numpy_inputs', input_shape=(INPUTS, )),\n        layers.Dense(units, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(dropout),\n        layers.Dense(units, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(dropout),\n        layers.Dense(units, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(dropout),\n        layers.Dense(OUTPUTS),\n    ])\n    model.compile(\n        optimizer=optimizer,\n        loss='mae',\n        metrics=['mae'],\n    )\n    model.fit(X_train,\n              y_train,\n              validation_data=(X_valid, y_valid),\n              batch_size=batch_size,\n              epochs=50,\n              callbacks=[early_stopping],\n              verbose=0)\n    # Optimize the metric monitored by `early_stopping` (`val_loss` by default)\n    # The metric needs to be reported in this format\n    return {'metric_id': early_stopping.monitor, 'value': early_stopping.best}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explain #\nExplainable AI on Vertex AI Notebooks lets you compute feature attributions for neural networks. Feature attributions describe the contribution each features makes to the final prediction relative to a baseline. Feature attributions can help you tune your model by indicating which features are important and which are not. Features with little importance you could consider dropping from your feature set.\n\nRead more about Vertex Explainable AI here: Introduction to Vertex Explainable AI for Vertex AI. In JupyterLab on Vertex AI Notebooks, you can also review a tutorial on XAI in the tutorials > explainable_ai > sdk_tutorial.ipynb file\n\nNow we can look at explanations using the explainable_ai_sdk library. Run the following cell on AI Notebooks with a Cloud TF image to see model explanations.","metadata":{}},{"cell_type":"code","source":"matches = neighbors.loc[model.predict(X_pairs_test).astype('bool'), :].append(\n    pd.DataFrame({'id_1': df_test.index, 'id_2': df_test.index})  # Everything matches itself\n)\nmatches","metadata":{"execution":{"iopub.status.busy":"2022-04-13T21:47:20.617725Z","iopub.execute_input":"2022-04-13T21:47:20.618243Z","iopub.status.idle":"2022-04-13T21:47:20.642135Z","shell.execute_reply.started":"2022-04-13T21:47:20.618208Z","shell.execute_reply":"2022-04-13T21:47:20.641253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier(\n    use_label_encoder=False,\n    eval_metric='logloss',\n).fit(X_pairs, y_pairs)","metadata":{"collapsed":false,"papermill":{"duration":213.038011,"end_time":"2022-03-23T14:48:28.283892","exception":false,"start_time":"2022-03-23T14:44:55.245881","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-13T21:45:07.856826Z","iopub.execute_input":"2022-04-13T21:45:07.857316Z","iopub.status.idle":"2022-04-13T21:45:19.754744Z","shell.execute_reply.started":"2022-04-13T21:45:07.857273Z","shell.execute_reply":"2022-04-13T21:45:19.753767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Getting Started on Vertex AI Notebooks #\n\nThis notebook demonstrates how to do the following on Vertex AI, Google's powerful new machine learning platform:\n\nrun the getting started notebook on Vertex AI Notebooks, to load the data, create a model & generate predictions\nexplore explainable AI on Vertex AI to refine your features\ntune hyperparameters with Vizier\nIt is a complement to the Getting Started with MLB Digital Engagement tutorial which was designed to be run on Kaggle Notebooks.\n\nThis tutorial uses Cloud Notebooks, a billable component of Google Cloud. Learn more about Notebooks pricing.","metadata":{}}]}