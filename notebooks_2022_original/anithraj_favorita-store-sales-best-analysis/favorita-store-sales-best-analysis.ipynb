{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n#         df = pd.read_csv(os.path.join(dirname, filename))\n#         print(df.columns)\n#         print(df.head(5))\n\n\ndef reduce_mem_usage(train_data):\n    start_mem = train_data.memory_usage().sum() / 1024**2\n#     print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in train_data.columns:\n        col_type = train_data[col].dtype\n        if col_type != object:\n            c_min = train_data[col].min()\n            c_max = train_data[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    train_data[col] = train_data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    train_data[col] = train_data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    train_data[col] = train_data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    train_data[col] = train_data[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    train_data[col] = train_data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    train_data[col] = train_data[col].astype(np.float32)\n                else:\n                    train_data[col] = train_data[col].astype(np.float64)\n        else:\n            train_data[col] = train_data[col].astype('category')\n    \n    end_mem = train_data.memory_usage().sum() / 1024**2\n#     print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n#     print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return train_data\n\n\ndf_train = reduce_mem_usage(pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/train.csv'))\n# print(df_train.info(verbose=False, memory_usage=\"deep\"))\n\n# df_test = reduce_mem_usage(pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/test.csv'))\n# df_submission = reduce_mem_usage(pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/sample_submission.csv'))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-06T14:26:35.033211Z","iopub.execute_input":"2022-05-06T14:26:35.033509Z","iopub.status.idle":"2022-05-06T14:26:37.485151Z","shell.execute_reply.started":"2022-05-06T14:26:35.033481Z","shell.execute_reply":"2022-05-06T14:26:37.484291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset**\n","metadata":{}},{"cell_type":"code","source":"# Merge the data with other tables\ndf_stores = reduce_mem_usage(pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv'))\ndf_trainM = df_train.merge(df_stores, on='store_nbr', how='left')\n\ndf_trans = reduce_mem_usage(pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv'))\ndf_trainM = df_trainM.merge(df_trans, on=['date', 'store_nbr'], how='left')\n# df_trainM['date'] = pd.to_datetime(df_trainM['date'])\ndf_oil = reduce_mem_usage(pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv'))\ndf_trainM = df_trainM.merge(df_oil, on='date', how='left')\ndf_trainM = df_trainM.rename(columns = {\"type\" : \"store_type\"})\n\ndf_holiEve = reduce_mem_usage(pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv'))\n# df_holiEve['date'] = pd.to_datetime(df_holiEve['date'])\ndf_holiEve.drop(df_holiEve[pd.to_datetime(df_holiEve['date']) < '2013-01-01'].index, inplace=True)\n# df_holiEve.drop(df_holiEve[df_holiEve['date'] < '2013-01-01'].index, inplace=True)\ndf_holiEve.drop(df_holiEve[df_holiEve['type'] == 'Transfer'].index, inplace=True)\n\ndf_holiEve.drop_duplicates(subset=['date'], inplace=True, ignore_index=True)\n# df_holiEve['date'] = df_holiEve['date'].astype('category')\ndf_holiEve['type'] = pd.Categorical(df_holiEve['type']).remove_categories('Transfer')\n\ndf_trainM = df_trainM.merge(df_holiEve, on='date', how='left')\n\ndf_trainM['date'] = pd.to_datetime(df_trainM['date'])\ndf_trainM['year'] = df_trainM['date'].dt.year\ndf_trainM['month'] = df_trainM['date'].dt.month\ndf_trainM['monthname'] = df_trainM['date'].dt.month_name()\ndf_trainM['week'] = df_trainM['date'].dt.isocalendar().week\ndf_trainM['quarter'] = df_trainM['date'].dt.quarter\ndf_trainM['day_of_week'] = df_trainM['date'].dt.day_name()\ndf_trainM = df_trainM.rename(columns = {\"type\" : \"holiday_type\"})\n\ndf_trainM.drop(columns = ['locale', 'locale_name', 'description', 'transferred'], axis = 1, inplace=True)\ndf_trainM['holiday_type'].fillna('Work Day', inplace=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-06T14:26:42.332926Z","iopub.execute_input":"2022-05-06T14:26:42.333189Z","iopub.status.idle":"2022-05-06T14:26:50.136814Z","shell.execute_reply.started":"2022-05-06T14:26:42.333163Z","shell.execute_reply":"2022-05-06T14:26:50.135997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sales Analysis**\n\nCustomer (C)> Hey, What do you sell here?\n\nSales Manger (SM)> What do you want?\n\nC> Don't know.\n\nSM> Come, I will walk you through it.\nWe have 33 products, 54 stores across 16 states, and we are into this business for the past 56 months.\nHere are the top 10 products we sell the most.","metadata":{}},{"cell_type":"code","source":"\n# Intialize the matplotlib figure\nf, ax = plt.subplots(figsize=(20,20))\nsns.set_context(\"notebook\", font_scale=2)\n\n# Load the dataset\nda_fs = df_trainM.groupby('family').agg(mxSum = ('sales',sum), \n                                         meSales = ('sales',np.mean)).reset_index().sort_values(by='mxSum',\n                                                                                                ascending=False)\nda_fs['pSales'] = (da_fs['mxSum']/sum(df_trainM['sales'])*100)\n\nremlFam = da_fs['family'].tail(23).tolist()\n\nda_fs['family'] = pd.Categorical(da_fs['family']).remove_categories(da_fs['family'].tail(23).tolist())\n# print(da_fs.tail(23).index.tolist())\nda_fs = da_fs.drop(da_fs.tail(23).index.tolist())\n\n# Plot the sales\nax = sns.barplot(x='pSales', y='family', data=da_fs, label='Family',\n             order=da_fs['family'], palette=\"Blues_r\")\n\n# label each bar in barplot\nfor p in ax.patches:\n height = p.get_height() # height of each horizontal bar is the same\n width = p.get_width() # width (average number of passengers)\n # adding text to each bar\n ax.text(x = width+0.3, # x-coordinate position of data label, padded 3 to right of bar\n y = p.get_y()+(height/2), # # y-coordinate position of data label, padded to be in the middle of the bar\n s = '{:.0f}%'.format(width), # data label, formatted to ignore decimals\n va = 'center', # sets vertical alignment (va) to center\n fontsize=16)\n    \n# for name, sales in zip(da_fs['family'], da_fs['pSales']):\n#     ax.text(name, sales, round(sales, 2), color='white', ha='center')\n# Add a legend and informative axis label\n# ax.legend(ncol=2, loc='lower right', frameon=True)\nax.set(xlim=(0,max(da_fs['pSales'])+1), ylabel=\"Goods Family\",\n       xlabel=\"% Amount spent on products\")\n\nax.set_title(\"Overall Sales on 53 Stores, 10/33 Products, 16 States and 56 Months\", fontdict={'fontsize':24})\nsns.despine(left=True, bottom=True)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-06T14:28:06.04152Z","iopub.execute_input":"2022-05-06T14:28:06.04277Z","iopub.status.idle":"2022-05-06T14:28:06.905702Z","shell.execute_reply.started":"2022-05-06T14:28:06.042706Z","shell.execute_reply":"2022-05-06T14:28:06.904821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"C> I think your business might be affected due to the change in oil prices?\n\nSM > In fact, our sales have increased each year, irrespective of oil price fluctuations.\n\nThis our growth from past 5 years still sales are high albeit we are in September month of 2017\n","metadata":{}},{"cell_type":"code","source":"df_ys = df_trainM.groupby(['year', 'family']).agg(mxSum = ('sales',sum), \n                                         meSales = ('sales',np.mean)).reset_index().sort_values(by=['year', 'mxSum'],\n                                                                                                ascending=False)\n\n\ndf_oilP = df_trainM.groupby(['year']).agg(mxPrice = ('dcoilwtico',np.mean)).reset_index().sort_values(by=['year'],ascending=False)\n\n# print(da_fs['family'].tolist())\n# print(df_ys)\n# for x in da_fs['family'].tolist():\n#     df_ys = df_ys.filter(like=x, axis=0)\ndf_ys = df_ys[(df_ys['family'] == 'GROCERY I') | (df_ys['family'] == 'BEVERAGES') | \n        (df_ys['family'] == 'PRODUCE') | (df_ys['family'] == 'CLEANING') |\n        (df_ys['family'] == 'DAIRY') | (df_ys['family'] == 'BREAD/BAKERY') |\n        (df_ys['family'] == 'POULTRY') | (df_ys['family'] == 'MEATS') |\n        (df_ys['family'] == 'PERSONAL CARE') | (df_ys['family'] == 'DELI')]\n\n\ndf_ys['family'] = pd.Categorical(df_ys['family']).remove_categories(remlFam)\ndf_ys['mxSum'] = df_ys['mxSum']/1000000\ndf_ys['meSales'] = df_ys['meSales']\n\n\nf, ax = plt.subplots(figsize=(20,20))\nsns.set_context(\"notebook\", font_scale=2)\nax = sns.pointplot(x=\"year\", y=\"meSales\",hue=\"family\",  data=df_ys)\nax = sns.barplot(x='year', y='mxPrice', data=df_oilP, palette=\"gist_gray\")\nfor p in ax.patches:\n height = p.get_height() # height of each horizontal bar is the same\n width = p.get_width() # width (average number of passengers)\n # adding text to each bar\n ax.text(x = p.get_x()+(width/2), # x-coordinate position of data label, padded 3 to right of bar\n y = height+8,# # y-coordinate position of data label, padded to be in the middle of the bar\n s = '{:.0f}'.format(height), # data label, formatted to ignore decimals\n va = 'center', # sets vertical alignment (va) to center\n fontsize=18)\n    \n# for name, sales in zip(da_fs['family'], da_fs['pSales']):\n#     ax.text(name, sales, round(sales, 2), color='white', ha='center')\n# Add a legend and informative axis label\n# ax.legend(ncol=2, loc='lower right', frameon=True)\n\nax.set_title(\"Average Sales of Products each Year and Oil Price\", fontdict={'fontsize':24})\n\nax.set(ylabel=\"Average Sales\",\n       xlabel=\"Year\")\nsns.despine(left=True, bottom=True)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-06T14:28:23.887279Z","iopub.execute_input":"2022-05-06T14:28:23.887563Z","iopub.status.idle":"2022-05-06T14:28:24.971695Z","shell.execute_reply.started":"2022-05-06T14:28:23.887533Z","shell.execute_reply":"2022-05-06T14:28:24.97069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"C>  Good, I would like to visit your store, let me check my calendar.\n\nSM> You can plan your visit on Workdays.  Stores are overcrowded on holidays.\n\nC> Thank you.\n\nSM> Happy shoping. \n\nAverage sales on working and non-working days.","metadata":{}},{"cell_type":"code","source":"df_ho = df_trainM.groupby(['family','holiday_type']).agg(mxSum = ('sales',sum), \n                                         meSales = ('sales',np.mean)).reset_index().sort_values(by=['holiday_type','meSales'], ascending=False)\n# print(df_ho)\n\ndf_ho = df_ho[(df_ho['family'] == 'GROCERY I') | (df_ho['family'] == 'BEVERAGES') | \n        (df_ho['family'] == 'PRODUCE') | (df_ho['family'] == 'CLEANING') |\n        (df_ho['family'] == 'DAIRY') | (df_ho['family'] == 'BREAD/BAKERY') |\n        (df_ho['family'] == 'POULTRY') | (df_ho['family'] == 'MEATS') |\n        (df_ho['family'] == 'PERSONAL CARE') | (df_ho['family'] == 'DELI')]\n\n# ms = set(df_trainM['family'])\n# s = ['HOME AND KITCHEN II','GROCERY II','HOME AND KITCHEN I','GROCERY I','BEVERAGES','PRODUCE','CLEANING','DAIRY','BREAD/BAKERY','POULTRY','MEATS','PERSONAL CARE','DELI','HOME CARE','EGGS',\n#      'FROZEN FOODS','PREPARED FOODS','LIQUOR,WINE,BEER','SEAFOOD','GROCERY II','HOME AND KITCHEN I','HOME AND KITCHEN II']\n# ss = set(s)\n\ndf_ho['family'] = pd.Categorical(df_ho['family']).remove_categories(remlFam)\n# df_ho['family'] = pd.Categorical(df_ho['family']).remove_categories(list(ms.symmetric_difference(ss)))\n\nf, ax = plt.subplots(figsize=(20,20))\nsns.set_context(\"notebook\", font_scale=2)\nax = sns.pointplot(x=\"holiday_type\", y=\"meSales\",hue=\"family\",  data=df_ho)\n\nax.set_title(\"Average Sales on Working and Non-Working Days\", fontdict={'fontsize':24})\nax.set(ylabel=\"Average Sales\",\n       xlabel=\"Working and Non Working Days\")\nsns.despine(left=True, bottom=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-06T14:41:00.976023Z","iopub.execute_input":"2022-05-06T14:41:00.976436Z","iopub.status.idle":"2022-05-06T14:41:02.217302Z","shell.execute_reply.started":"2022-05-06T14:41:00.976398Z","shell.execute_reply":"2022-05-06T14:41:02.216261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Annual Sales meeting**\n\nStakeholder(SH)> Hey guys, How are we doing?\n\nSM > We are consistently good in business our sales have increased month on month each year. Especially Christmas (December)","metadata":{}},{"cell_type":"code","source":"df_yrmnth = df_trainM.groupby(['year', 'month']).agg(mxSum = ('sales',sum), \n                                         meSales = ('sales',np.mean)).reset_index().sort_values(by=['month','meSales'], ascending=False)\n\ndf_yrmnth['meSales'] = np.round(df_yrmnth['meSales']).astype('int')\ndf_yrmnth['mxSum'] = np.round(df_yrmnth['mxSum']).astype('int')\n# print(df_yrmnth)\nsns.set_context(\"notebook\", font_scale=2)\nf, ax = plt.subplots(figsize=(20,20))\ndf_HM = df_yrmnth.pivot(\"month\", \"year\", \"meSales\")\n\n# m = {1:'j',2:'f',3:'m',4:'a',5:'ma',6:'j',7:'ju',8:'Au',9:'Se',10:'Oct',11:'Nov',12:'Dec'}\nax.set_title(\"Average sales by monthly, each year\", fontdict={'fontsize':24})\nax.set(ylabel=\"Months\", xlabel=\"Year\")\n# Draw a heatmap with the numeric values in each cell\nsns.heatmap(df_HM, annot=True, fmt=\"f\", linewidths=.1, ax=ax, cmap=\"YlGnBu\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-06T14:47:49.419833Z","iopub.execute_input":"2022-05-06T14:47:49.420501Z","iopub.status.idle":"2022-05-06T14:47:50.40392Z","shell.execute_reply.started":"2022-05-06T14:47:49.420455Z","shell.execute_reply":"2022-05-06T14:47:50.403007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SH>  What are the sales in each store?\n\nSM > Our Average sales in Store \"A\" are exceptionally high, but we have to concentrate on B, C, D and E stores.","metadata":{}},{"cell_type":"code","source":"df_stSales = df_trainM.groupby(['store_type']).agg(mxSum = ('sales',sum), \n                                         meSales = ('sales',np.mean)).reset_index().sort_values(by=['meSales'], ascending=False)\n\n# print(df_stSales)\nf, ax = plt.subplots(figsize=(20,20))\nsns.set_context(\"notebook\", font_scale=2)\n# ax = sns.pointplot(x=\"store_type\", y=\"meSales\",hue=\"family\",  data=df_stfa)\nax = sns.barplot(x='store_type', y='meSales', data=df_stSales, palette=\"gist_gray\")\n\n# label each bar in barplot\nfor p in ax.patches:\n height = p.get_height() # height of each horizontal bar is the same\n width = p.get_width() # width (average number of passengers)\n # adding text to each bar\n ax.text(x = p.get_x()+(width/2), # x-coordinate position of data label, padded 3 to right of bar\n y = height+8,# # y-coordinate position of data label, padded to be in the middle of the bar\n s = '{:.0f}'.format(height), # data label, formatted to ignore decimals\n va = 'center', # sets vertical alignment (va) to center\n fontsize=16)\n    \n# for name, sales in zip(da_fs['family'], da_fs['pSales']):\n#     ax.text(name, sales, round(sales, 2), color='white', ha='center')\n# Add a legend and informative axis label\n# ax.legend(ncol=2, loc='lower right', frameon=True)\nax.set(ylabel=\"Average Sales\",\n       xlabel=\"Store Type\")\nax.set_title(\"Average sales on each store type\", fontdict={'fontsize':24})\nsns.despine(left=True, bottom=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-06T15:01:06.603387Z","iopub.execute_input":"2022-05-06T15:01:06.603722Z","iopub.status.idle":"2022-05-06T15:01:06.993092Z","shell.execute_reply.started":"2022-05-06T15:01:06.60369Z","shell.execute_reply":"2022-05-06T15:01:06.991984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SH> What are the stats on the \"Promotion\" strategy?\n\nSM> There is 65% growth in sales.","metadata":{}},{"cell_type":"code","source":"df_trainM['promoted'] = np.where(df_trainM['onpromotion']> 0, True, False)\n\ndf_cl = df_trainM.groupby(['promoted']).agg(Sales = ('sales',np.sum)).reset_index().sort_values(by=['Sales'],\n                                                                                                   ascending=True)\ndf_cl['Sales'] = np.round(df_cl['Sales']).astype('int')\ndf_cl['pSales'] = (df_cl['Sales']/sum(df_trainM['sales'])*100)\n# print(df_cl)\n\nf, ax = plt.subplots(figsize=(12,10))\nsns.set_context(\"notebook\", font_scale=2)\nax = sns.barplot(x='promoted', y='pSales', data=df_cl, palette=\"BuGn\")\nfor p in ax.patches:\n height = p.get_height() # height of each horizontal bar is the same\n width = p.get_width() # width (average number of passengers)\n # adding text to each bar\n ax.text(x = p.get_x()+(width/2), # x-coordinate position of data label, padded 3 to right of bar\n y = height+1,# # y-coordinate position of data label, padded to be in the middle of the bar\n s = '{:.0f}%'.format(height), # data label, formatted to ignore decimals\n va = 'center', # sets vertical alignment (va) to center\n fontsize=16)\n    \n# for name, sales in zip(da_fs['family'], da_fs['pSales']):\n#     ax.text(name, sales, round(sales, 2), color='white', ha='center')\n# Add a legend and informative axis label\n# ax.legend(ncol=2, loc='lower right', frameon=True)\nax.set(ylabel=\"% Sales\",\n       xlabel=\"On Promotion\")\nax.set_title(\"% Sales when being promoted\", fontdict={'fontsize':24})\nsns.despine(left=True, bottom=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-06T15:14:39.091846Z","iopub.execute_input":"2022-05-06T15:14:39.092438Z","iopub.status.idle":"2022-05-06T15:14:39.734603Z","shell.execute_reply.started":"2022-05-06T15:14:39.092389Z","shell.execute_reply":"2022-05-06T15:14:39.733771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion**\n\nSH> Now, we have the data and stats. Let's predict our sales next a couple of years and plan strategies accordingly.\n\n","metadata":{}}]}