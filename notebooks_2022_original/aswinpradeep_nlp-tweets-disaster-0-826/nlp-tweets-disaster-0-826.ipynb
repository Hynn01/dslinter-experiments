{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-08T03:01:33.884976Z","iopub.execute_input":"2022-05-08T03:01:33.885335Z","iopub.status.idle":"2022-05-08T03:01:33.912175Z","shell.execute_reply.started":"2022-05-08T03:01:33.885248Z","shell.execute_reply":"2022-05-08T03:01:33.911513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing all the libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom wordcloud import WordCloud\n\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\nimport folium \nfrom folium import plugins \n\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom collections import Counter\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.callbacks import ReduceLROnPlateau\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:01:52.157724Z","iopub.execute_input":"2022-05-08T03:01:52.158438Z","iopub.status.idle":"2022-05-08T03:01:57.216137Z","shell.execute_reply.started":"2022-05-08T03:01:52.158391Z","shell.execute_reply":"2022-05-08T03:01:57.215405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reading the data","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:42:35.119979Z","iopub.execute_input":"2022-05-08T03:42:35.120535Z","iopub.status.idle":"2022-05-08T03:42:35.149004Z","shell.execute_reply.started":"2022-05-08T03:42:35.120494Z","shell.execute_reply":"2022-05-08T03:42:35.14825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.isnull().sum())\nprint(\"--------------------------\")\nprint(f\"Total missing values: {df.isnull().sum().sum()}\")\nprint(\"--------------------------\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:42:38.061776Z","iopub.execute_input":"2022-05-08T03:42:38.062058Z","iopub.status.idle":"2022-05-08T03:42:38.079324Z","shell.execute_reply.started":"2022-05-08T03:42:38.062027Z","shell.execute_reply":"2022-05-08T03:42:38.078599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**EDA**","metadata":{}},{"cell_type":"code","source":"msno.bar(df, color = (0, 0, 0), sort = \"ascending\", figsize = (15, 10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:42:42.619412Z","iopub.execute_input":"2022-05-08T03:42:42.619811Z","iopub.status.idle":"2022-05-08T03:42:43.155709Z","shell.execute_reply.started":"2022-05-08T03:42:42.619765Z","shell.execute_reply":"2022-05-08T03:42:43.153749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see how our class distribution look like","metadata":{}},{"cell_type":"code","source":"custom_colors = ['#000000', '#E31E33', '#4A53E1', '#F5AD02', '#94D5EA', '#F6F8F7']\ncustom_palette = sns.set_palette(sns.color_palette(custom_colors))\nsns.palplot(sns.color_palette(custom_colors), size = 1)\nplt.tick_params(axis = 'both', labelsize = 0, length = 0)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:41:45.284268Z","iopub.execute_input":"2022-05-08T03:41:45.284534Z","iopub.status.idle":"2022-05-08T03:41:45.356829Z","shell.execute_reply.started":"2022-05-08T03:41:45.284505Z","shell.execute_reply":"2022-05-08T03:41:45.35605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15, 12))\nax = plt.axes()\nax.set_facecolor('black')\nax = sns.countplot(x = 'target', data = df, palette = [custom_colors[2], custom_colors[1]], edgecolor = 'white', linewidth = 1.2)\nplt.title('Disaster Count', fontsize = 25)\nplt.xlabel('Disaster', fontsize = 20)\nplt.ylabel('Count', fontsize = 20)\nax.xaxis.set_tick_params(labelsize = 15)\nax.yaxis.set_tick_params(labelsize = 15)\nbbox_args = dict(boxstyle = 'round', fc = '0.9')\nfor p in ax.patches:\n        ax.annotate('{:.0f} = {:.2f}%'.format(p.get_height(), (p.get_height() / len(df['target'])) * 100), (p.get_x() + 0.25, p.get_height() + 60), \n                   color = 'black',\n                   bbox = bbox_args,\n                   fontsize = 15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:42:49.197562Z","iopub.execute_input":"2022-05-08T03:42:49.197835Z","iopub.status.idle":"2022-05-08T03:42:49.383974Z","shell.execute_reply.started":"2022-05-08T03:42:49.197787Z","shell.execute_reply":"2022-05-08T03:42:49.383283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"target\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:42:54.225275Z","iopub.execute_input":"2022-05-08T03:42:54.225979Z","iopub.status.idle":"2022-05-08T03:42:54.23361Z","shell.execute_reply.started":"2022-05-08T03:42:54.225945Z","shell.execute_reply":"2022-05-08T03:42:54.232879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#label 0 = non diaster\n#label 1 = disaster","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:08:55.280453Z","iopub.execute_input":"2022-05-07T17:08:55.280778Z","iopub.status.idle":"2022-05-07T17:08:55.286126Z","shell.execute_reply.started":"2022-05-07T17:08:55.280722Z","shell.execute_reply":"2022-05-07T17:08:55.285473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We have data imbalance so im undersampling the majority class**","metadata":{}},{"cell_type":"code","source":"new_df0=df[df['target']==0]\nnew_df1=df[df['target']==1]","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:03:07.765101Z","iopub.execute_input":"2022-05-08T03:03:07.765744Z","iopub.status.idle":"2022-05-08T03:03:07.774282Z","shell.execute_reply.started":"2022-05-08T03:03:07.765703Z","shell.execute_reply":"2022-05-08T03:03:07.773448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df1.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:43:52.84427Z","iopub.execute_input":"2022-05-08T03:43:52.844533Z","iopub.status.idle":"2022-05-08T03:43:52.851331Z","shell.execute_reply.started":"2022-05-08T03:43:52.844504Z","shell.execute_reply":"2022-05-08T03:43:52.850668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df0=new_df0.sample(new_df1.shape[0]) #generating random samples from majority class same as of the minority class","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:44:28.808143Z","iopub.execute_input":"2022-05-08T03:44:28.808408Z","iopub.status.idle":"2022-05-08T03:44:28.814246Z","shell.execute_reply.started":"2022-05-08T03:44:28.808379Z","shell.execute_reply":"2022-05-08T03:44:28.813535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.concat([new_df0,new_df1],axis=0)   #appending both the classes with equal number of observations","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:44:57.828559Z","iopub.execute_input":"2022-05-08T03:44:57.828841Z","iopub.status.idle":"2022-05-08T03:44:57.836824Z","shell.execute_reply.started":"2022-05-08T03:44:57.828784Z","shell.execute_reply":"2022-05-08T03:44:57.836037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle   #shuffling the values\ndf = shuffle(df)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:45:11.19804Z","iopub.execute_input":"2022-05-08T03:45:11.198328Z","iopub.status.idle":"2022-05-08T03:45:11.205873Z","shell.execute_reply.started":"2022-05-08T03:45:11.198299Z","shell.execute_reply":"2022-05-08T03:45:11.205101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:45:14.215292Z","iopub.execute_input":"2022-05-08T03:45:14.215908Z","iopub.status.idle":"2022-05-08T03:45:14.221289Z","shell.execute_reply.started":"2022-05-08T03:45:14.215869Z","shell.execute_reply":"2022-05-08T03:45:14.220322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:45:18.013297Z","iopub.execute_input":"2022-05-08T03:45:18.013868Z","iopub.status.idle":"2022-05-08T03:45:18.029047Z","shell.execute_reply.started":"2022-05-08T03:45:18.013825Z","shell.execute_reply":"2022-05-08T03:45:18.028179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Top 10 locations of tweets","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 13))\nax = plt.axes()\nax.set_facecolor('black')\nax = ((df.location.value_counts())[:10]).plot(kind = 'bar', color = custom_colors[2], linewidth = 2, edgecolor = 'white')\nplt.title('Location Count', fontsize = 30)\nplt.xlabel('Location', fontsize = 25)\nplt.ylabel('Count', fontsize = 25)\nax.xaxis.set_tick_params(labelsize = 15, rotation = 30)\nax.yaxis.set_tick_params(labelsize = 15)\nbbox_args = dict(boxstyle = 'round', fc = '0.9')\nfor p in ax.patches:\n        ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x() + 0.15, p.get_height() + 2),\n                   bbox = bbox_args,\n                   color = custom_colors[2],\n                   fontsize = 15)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:45:30.297226Z","iopub.execute_input":"2022-05-08T03:45:30.297532Z","iopub.status.idle":"2022-05-08T03:45:30.606797Z","shell.execute_reply.started":"2022-05-08T03:45:30.297504Z","shell.execute_reply":"2022-05-08T03:45:30.606095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"# %20 is the URL encoding of space, let's replace them with '_'\ndef re_encode_space(input_string):\n    return None if pd.isna(input_string) else input_string.replace('%20', '_')\n\n\n# Let's try to find hastags\nimport re\n\ndef find_hash_tags(input_string):\n    hash_tags = re.findall(r\"#(\\w+)\", str(input_string))\n    return ','.join(hash_tags)\n\n\n# Let's turn hashtags to normal words\ndef re_encode_hashtags(input_string):\n    return None if pd.isna(input_string) else input_string.replace('#', '')\n\n\n# Let's remove URLs from the tweets\ndef remove_links(input_string):\n    res = input_string\n    urls = re.findall(r'(https?://[^\\s]+)', res)\n    for link in urls:\n        res = res.strip(link)\n    return res\n\n\n# Let's remove the state abbreviations\ndef state_renaming(input_string):\n\n    states = {\n        'AK': 'Alaska',\n        'AL': 'Alabama',\n        'AR': 'Arkansas',\n        'AZ': 'Arizona',\n        'CA': 'California',\n        'CO': 'Colorado',\n        'CT': 'Connecticut',\n        'DC': 'District_of_Columbia',\n        'DE': 'Delaware',\n        'FL': 'Florida',\n        'GA': 'Georgia',\n        'HI': 'Hawaii',\n        'IA': 'Iowa',\n        'ID': 'Idaho',\n        'IL': 'Illinois',\n        'IN': 'Indiana',\n        'KS': 'Kansas',\n        'KY': 'Kentucky',\n        'LA': 'Louisiana',\n        'MA': 'Massachusetts',\n        'MD': 'Maryland',\n        'ME': 'Maine',\n        'MI': 'Michigan',\n        'MN': 'Minnesota',\n        'MO': 'Missouri',\n        'MS': 'Mississippi',\n        'MT': 'Montana',\n        'NC': 'North_Carolina',\n        'ND': 'North_Dakota',\n        'NE': 'Nebraska',\n        'NH': 'New_Hampshire',\n        'NJ': 'New_Jersey',\n        'NM': 'New_Mexico',\n        'NV': 'Nevada',\n        'NY': 'New_York',\n        'OH': 'Ohio',\n        'OK': 'Oklahoma',\n        'OR': 'Oregon',\n        'PA': 'Pennsylvania',\n        'RI': 'Rhode_Island',\n        'SC': 'South_Carolina',\n        'SD': 'South_Dakota',\n        'TN': 'Tennessee',\n        'TX': 'Texas',\n        'UT': 'Utah',\n        'VA': 'Virginia',\n        'VT': 'Vermont',\n        'WA': 'Washington',\n        'WI': 'Wisconsin',\n        'WV': 'West_Virginia',\n        'WY': 'Wyoming'\n    }\n\n    result = input_string\n    \n    if isinstance(input_string, str):\n        input_candidates = input_string.split(', ')\n        \n        if len(input_candidates) > 1:\n            for candidate in input_candidates:\n                if candidate in states.keys():\n                    result = states[candidate]\n                \n    if input_string in states.keys():\n        result = states[input_string]\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:45:49.417573Z","iopub.execute_input":"2022-05-08T03:45:49.41786Z","iopub.status.idle":"2022-05-08T03:45:49.433032Z","shell.execute_reply.started":"2022-05-08T03:45:49.417827Z","shell.execute_reply":"2022-05-08T03:45:49.431837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's wrap the preprocessing functions so it's easier to\n# process both train and test dataset\ndef preprocess_data(input_data):\n    input_df = input_data.copy()\n    input_df['keyword'] = input_df['keyword'].map(re_encode_space)\n    input_df['keyword'].fillna('Missing', inplace=True)\n    input_df['hashtags'] = input_df['text'].map(find_hash_tags)\n    input_df['text'] = input_df['text'].map(re_encode_hashtags)\n    input_df['text'] = input_df['text'].map(remove_links)\n    input_df['location'] = input_df['location'].map(state_renaming)\n    return input_df","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:45:52.483304Z","iopub.execute_input":"2022-05-08T03:45:52.483562Z","iopub.status.idle":"2022-05-08T03:45:52.490046Z","shell.execute_reply.started":"2022-05-08T03:45:52.483533Z","shell.execute_reply":"2022-05-08T03:45:52.488807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# our training data is df","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:09:00.483991Z","iopub.execute_input":"2022-05-07T17:09:00.484164Z","iopub.status.idle":"2022-05-07T17:09:00.490127Z","shell.execute_reply.started":"2022-05-07T17:09:00.484143Z","shell.execute_reply":"2022-05-07T17:09:00.48934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"importing test data","metadata":{}},{"cell_type":"code","source":"test_data=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:08:05.293754Z","iopub.execute_input":"2022-05-08T03:08:05.294075Z","iopub.status.idle":"2022-05-08T03:08:05.324663Z","shell.execute_reply.started":"2022-05-08T03:08:05.294041Z","shell.execute_reply":"2022-05-08T03:08:05.323786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we are going to rename train data as original_data and test as test_data","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:09:00.517554Z","iopub.execute_input":"2022-05-07T17:09:00.517826Z","iopub.status.idle":"2022-05-07T17:09:00.522309Z","shell.execute_reply.started":"2022-05-07T17:09:00.51779Z","shell.execute_reply":"2022-05-07T17:09:00.521323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_data = preprocess_data(df)\ntest_data = preprocess_data(test_data)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:46:22.507146Z","iopub.execute_input":"2022-05-08T03:46:22.507412Z","iopub.status.idle":"2022-05-08T03:46:22.594025Z","shell.execute_reply.started":"2022-05-08T03:46:22.507383Z","shell.execute_reply":"2022-05-08T03:46:22.593305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We notice that keyword field has missing values\n# We notice location has missing values\n\n# Let's Visualize the keyword field in a word cloud to get an idea of what it is\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\nkeyword_words = str(original_data['keyword']\n    .dropna()\n    .unique()\n    .tolist()\n)\n\nlocation_words = str(original_data['location']\n    .dropna()\n    .unique()\n    .tolist()\n)\n\nhashtag_words = str(original_data['hashtags']\n    .dropna()\n    .unique()\n    .tolist()\n)\n\nkeyword_wordcloud = WordCloud(\n    background_color='white',\n    stopwords=None,\n    max_words=200,\n    max_font_size=40, \n    random_state=42\n).generate(keyword_words)\n\nlocation_wordcloud = WordCloud(\n    background_color='white',\n    stopwords=None,\n    max_words=200,\n    max_font_size=40, \n    random_state=42\n).generate(location_words)\n\nhashtag_wordcloud = WordCloud(\n    background_color='white',\n    stopwords=None,\n    max_words=200,\n    max_font_size=40, \n    random_state=42\n).generate(hashtag_words)\n\nfig, ax = plt.subplots(1,3, figsize=(16,9), constrained_layout=True)\nax[0].set_title(\"Keywords\")\nax[0].imshow(keyword_wordcloud)\nax[0].axis(False)\nax[1].set_title('Location')\nax[1].imshow(location_wordcloud)\nax[1].axis(False)\nax[2].set_title('Hashtags')\nax[2].imshow(hashtag_wordcloud)\nax[2].axis(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:46:30.165504Z","iopub.execute_input":"2022-05-08T03:46:30.165764Z","iopub.status.idle":"2022-05-08T03:46:31.160318Z","shell.execute_reply.started":"2022-05-08T03:46:30.165734Z","shell.execute_reply":"2022-05-08T03:46:31.15954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TF-IDF + LogistiRegression","metadata":{}},{"cell_type":"code","source":"X = original_data['text']\ny = original_data['target']","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:08:59.954475Z","iopub.execute_input":"2022-05-08T03:08:59.954731Z","iopub.status.idle":"2022-05-08T03:08:59.958823Z","shell.execute_reply.started":"2022-05-08T03:08:59.954702Z","shell.execute_reply":"2022-05-08T03:08:59.958118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:09:04.935064Z","iopub.execute_input":"2022-05-08T03:09:04.935354Z","iopub.status.idle":"2022-05-08T03:09:04.942269Z","shell.execute_reply.started":"2022-05-08T03:09:04.935323Z","shell.execute_reply":"2022-05-08T03:09:04.94149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5)\ntrain_average_score = 0\nvalidation_average_score = 0\nvalidation_oof_predictions = np.zeros((len(X)))\n\nfor fold_n, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_test, y_test = X[test_idx], y[test_idx]\n    \n    model = Pipeline([\n        ('Encoder', TfidfVectorizer(max_features=None)),\n        ('Clf', LogisticRegression(penalty='l2', C=1, solver='liblinear'))\n    ])\n    \n    model.fit(X_train, y_train)\n    \n    train_predictions = model.predict_proba(X_train)[:, 1]\n    validation_predictions = model.predict_proba(X_test)[:, 1]\n    \n    train_score = roc_auc_score(y_train, train_predictions)\n    validation_score = roc_auc_score(y_test, validation_predictions)\n    \n    train_average_score += train_score / 5\n    validation_average_score += validation_score / 5\n    validation_oof_predictions[test_idx,] = (validation_predictions > 0.5).astype(int)\n    \n    print(f'Fold: {fold_n}, train auc: {train_score:.3f}, validation auc: {validation_score:.3f}')\nprint(f'Train average: {train_average_score:.3f}, validation average: {validation_average_score:.3f}')\nprint(f'OOF Accuracy Score: {accuracy_score(y, validation_oof_predictions)}')","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:09:01.63077Z","iopub.execute_input":"2022-05-07T17:09:01.631228Z","iopub.status.idle":"2022-05-07T17:09:03.448279Z","shell.execute_reply.started":"2022-05-07T17:09:01.631194Z","shell.execute_reply":"2022-05-07T17:09:03.447053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tokeninzing","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:09:58.134104Z","iopub.execute_input":"2022-05-07T17:09:58.134735Z","iopub.status.idle":"2022-05-07T17:09:58.138283Z","shell.execute_reply.started":"2022-05-07T17:09:58.134693Z","shell.execute_reply":"2022-05-07T17:09:58.137305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import TweetTokenizer\n# The tokenizer is responsible to turn a string of words\n# into a list of tokens (words) for which we'll get their\n# vector representation (embeddings)\ntknzr = TweetTokenizer(\n    preserve_case=False,\n    reduce_len=True,\n    strip_handles=True,\n)\n\n\ndef tokenize_tweets(tokenizer, input_text):\n    tokens = list(tokenizer.tokenize(input_text))\n    tokens = [re.sub('[^A-Za-z0-9]+', '', i) for i in tokens]\n    return tokens\n\noriginal_data['tokens'] = original_data['text']\noriginal_data['tokens'] = original_data['tokens'].apply(lambda x: tokenize_tweets(tknzr, x))\n\ntest_data['tokens'] = test_data['text'].apply(lambda x: tokenize_tweets(tknzr, x))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:47:11.665247Z","iopub.execute_input":"2022-05-08T03:47:11.665514Z","iopub.status.idle":"2022-05-08T03:47:13.316037Z","shell.execute_reply.started":"2022-05-08T03:47:11.665485Z","shell.execute_reply":"2022-05-08T03:47:13.315194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll pad all embeddings to match the length of the biggest tweet\n# in order to account for the variability in tweet length\n# Later on the model is going to mask the padded values, so that\n# they won't influence the result\nmax_tweet_length = max(original_data['tokens'].apply(lambda x: len(x)).max(), \n                       test_data['tokens'].apply(lambda x: len(x)).max())","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:47:19.057209Z","iopub.execute_input":"2022-05-08T03:47:19.057759Z","iopub.status.idle":"2022-05-08T03:47:19.072677Z","shell.execute_reply.started":"2022-05-08T03:47:19.05772Z","shell.execute_reply":"2022-05-08T03:47:19.071989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"USING BERT Embeddings","metadata":{}},{"cell_type":"code","source":"X = original_data['text'].tolist()\ny = np.asarray(original_data['target'].tolist()).astype(np.float32)\n\ntest_array = test_data['text'].tolist()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:47:22.403783Z","iopub.execute_input":"2022-05-08T03:47:22.404479Z","iopub.status.idle":"2022-05-08T03:47:22.45067Z","shell.execute_reply.started":"2022-05-08T03:47:22.404443Z","shell.execute_reply":"2022-05-08T03:47:22.449939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoConfig,TFBertModel","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:09:25.209611Z","iopub.execute_input":"2022-05-08T03:09:25.210159Z","iopub.status.idle":"2022-05-08T03:09:26.948748Z","shell.execute_reply.started":"2022-05-08T03:09:25.210121Z","shell.execute_reply":"2022-05-08T03:09:26.948037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nbert = TFBertModel.from_pretrained('bert-base-uncased')\n\nX = bert_tokenizer(\n    text=X,\n    add_special_tokens=True,\n    max_length=max_tweet_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\ntest_array = bert_tokenizer(\n    text=test_array,\n    add_special_tokens=True,\n    max_length=max_tweet_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:47:44.830941Z","iopub.execute_input":"2022-05-08T03:47:44.831202Z","iopub.status.idle":"2022-05-08T03:47:51.75308Z","shell.execute_reply.started":"2022-05-08T03:47:44.831174Z","shell.execute_reply":"2022-05-08T03:47:51.752289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nsteps_per_epoch = X['input_ids'].numpy().shape[0]\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\nclass BertLrSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n\n    @tf.function\n    def __init__(self, initial_learning_rate, num_warmups, num_train_steps):\n        self.overshoot = 1000\n        self.initial_learning_rate = initial_learning_rate\n        self.num_warmups = num_warmups\n        self.num_train_steps = num_train_steps\n        self.angle_warm = self.initial_learning_rate / self.num_warmups\n        self.angle_decay = - self.initial_learning_rate / \\\n            (self.num_train_steps - self.num_warmups - self.overshoot)\n    \n    @tf.function\n    def __call__(self, step):\n        if step <= self.num_warmups:\n            return (tf.cast(step, tf.float32) + 1) * self.angle_warm\n        else:\n            return self.initial_learning_rate + (tf.cast(step, tf.float32) - self.num_warmups + 1 + self.overshoot) * self.angle_decay\n        \n        \nschedule = BertLrSchedule(initial_learning_rate=2e-5, \n                          num_warmups=num_warmup_steps, \n                          num_train_steps=num_train_steps)\n\nsteps = np.arange(num_train_steps)\nlrs = [schedule.__call__(i) for i in steps]\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1,1)\nax.plot(steps, lrs)\nax.set_xlabel('Train steps')\nax.set_ylabel('Learning Rate')\nax.set_title('Bert scheduler')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:48:08.810697Z","iopub.execute_input":"2022-05-08T03:48:08.811401Z","iopub.status.idle":"2022-05-08T03:48:33.875003Z","shell.execute_reply.started":"2022-05-08T03:48:08.811361Z","shell.execute_reply":"2022-05-08T03:48:33.874277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Building**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:48:38.975486Z","iopub.execute_input":"2022-05-08T03:48:38.975743Z","iopub.status.idle":"2022-05-08T03:48:38.979261Z","shell.execute_reply.started":"2022-05-08T03:48:38.975714Z","shell.execute_reply":"2022-05-08T03:48:38.978584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_bert_classifier():\n    input_ids = tf.keras.layers.Input(shape=(max_tweet_length,), dtype=tf.int32, name=\"input_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_tweet_length,), dtype=tf.int32, name=\"attention_mask\")\n    embeddings = bert(input_ids,attention_mask = input_mask)['pooler_output']\n    net = tf.keras.layers.Dropout(0.1)(embeddings)\n    net = tf.keras.layers.Dense(128, activation='relu', name='pre-clf')(net)\n    net = tf.keras.layers.Dropout(0.1)(net)\n    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n    return tf.keras.Model(inputs=[input_ids, input_mask], outputs=net)\n\nskf = StratifiedKFold(n_splits=5)\ntrain_average_score = 0\nvalidation_average_score = 0\nvalidation_oof_predictions = np.zeros((len(X['input_ids'].numpy())))\n\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\noptimizer = Adam(learning_rate=schedule)\nepochs = 5\n\n# It's a good practice to predict the test set on every fold\n# And average the predictions over the folds\naveraged_test_predictions = np.zeros((test_array['input_ids'].shape[0]))\n\n# It's standard practice to use Stratified k-fold cross validation\n# so we're also using it here\nfor fold_n, (train_idx, test_idx) in enumerate(skf.split(X['input_ids'].numpy(), y)):\n    X_train_ids = X['input_ids'].numpy()[train_idx]\n    X_train_att = X['attention_mask'].numpy()[train_idx]\n    y_train = y[train_idx]\n    \n    X_test_ids = X['input_ids'].numpy()[test_idx]\n    X_test_att = X['attention_mask'].numpy()[test_idx]\n    y_test = y[test_idx]\n    \n    # Re-build the model at every fold to \"reset\" it\n    model = build_bert_classifier()\n    model.layers[2].trainable = True\n    \n    model.compile(optimizer=optimizer,\n                  loss=loss)\n    \n    model.fit(x={'input_ids':X_train_ids,'attention_mask':X_train_att}, \n              y=y_train, batch_size=32, epochs=epochs)\n    \n    train_predictions = model.predict({'input_ids':X_train_ids,'attention_mask':X_train_att})\n    validation_predictions = model.predict({'input_ids':X_test_ids,'attention_mask':X_test_att})\n    \n    train_score = roc_auc_score(y_train, train_predictions)\n    validation_score = roc_auc_score(y_test, validation_predictions)\n    \n    train_average_score += train_score / 5\n    validation_average_score += validation_score / 5\n    validation_oof_predictions[test_idx,] = (validation_predictions > 0.5).astype(int).flatten()\n    \n    print(f'Fold: {fold_n}, train auc: {train_score:.3f}, validation auc: {validation_score:.3f}')\n    \n    test_predictions = model.predict({'input_ids':test_array['input_ids'],\n                                      'attention_mask':test_array['attention_mask']}).flatten()\n    averaged_test_predictions += test_predictions / 5\n    \nprint(f'Train average: {train_average_score:.3f}, validation average: {validation_average_score:.3f}')\nprint(f'OOF Accuracy Score: {accuracy_score(y, validation_oof_predictions)}')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:13:00.795599Z","iopub.execute_input":"2022-05-08T03:13:00.795896Z","iopub.status.idle":"2022-05-08T03:31:49.786286Z","shell.execute_reply.started":"2022-05-08T03:13:00.795862Z","shell.execute_reply":"2022-05-08T03:31:49.785477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creating the submission file from predictions**","metadata":{}},{"cell_type":"code","source":"sub=pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsub['target']=(averaged_test_predictions > 0.5).astype(int)\nsub.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:50:24.256253Z","iopub.execute_input":"2022-05-08T03:50:24.256532Z","iopub.status.idle":"2022-05-08T03:50:24.274878Z","shell.execute_reply.started":"2022-05-08T03:50:24.256503Z","shell.execute_reply":"2022-05-08T03:50:24.274153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:51:04.325249Z","iopub.execute_input":"2022-05-08T03:51:04.325531Z","iopub.status.idle":"2022-05-08T03:51:04.337471Z","shell.execute_reply.started":"2022-05-08T03:51:04.325501Z","shell.execute_reply":"2022-05-08T03:51:04.336711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:32:27.942187Z","iopub.execute_input":"2022-05-08T03:32:27.942935Z","iopub.status.idle":"2022-05-08T03:32:27.955073Z","shell.execute_reply.started":"2022-05-08T03:32:27.942889Z","shell.execute_reply":"2022-05-08T03:32:27.954346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-05-07T17:41:24.247062Z","iopub.execute_input":"2022-05-07T17:41:24.247322Z","iopub.status.idle":"2022-05-07T17:41:24.253952Z","shell.execute_reply.started":"2022-05-07T17:41:24.247291Z","shell.execute_reply":"2022-05-07T17:41:24.253261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}