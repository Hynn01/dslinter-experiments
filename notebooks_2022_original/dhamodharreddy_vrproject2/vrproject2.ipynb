{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Import libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle as pkl\nimport string","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:09:35.523123Z","iopub.execute_input":"2022-05-08T09:09:35.523796Z","iopub.status.idle":"2022-05-08T09:09:35.528775Z","shell.execute_reply.started":"2022-05-08T09:09:35.52376Z","shell.execute_reply":"2022-05-08T09:09:35.527885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import keras libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import Input, layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import add\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout","metadata":{"execution":{"iopub.status.busy":"2022-05-08T08:53:24.264706Z","iopub.execute_input":"2022-05-08T08:53:24.264981Z","iopub.status.idle":"2022-05-08T08:53:29.421355Z","shell.execute_reply.started":"2022-05-08T08:53:24.264945Z","shell.execute_reply":"2022-05-08T08:53:29.420593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import train,test,val image names from given datafiles  ","metadata":{}},{"cell_type":"code","source":"train_image_names = open('../input/image-captioning/flickr-8k/flickr-8k/Flickr8k_text-20220427T180132Z-001/Flickr8k_text/Flickr_8k.trainImages.txt','r').read().splitlines()\nval_image_names = open('../input/image-captioning/flickr-8k/flickr-8k/Flickr8k_text-20220427T180132Z-001/Flickr8k_text/Flickr_8k.valImages.txt','r').read().splitlines()\ntest_image_names = open('../input/image-captioning/flickr-8k/flickr-8k/Flickr8k_text-20220427T180132Z-001/Flickr8k_text/Flickr_8k.testImages.txt','r').read().splitlines()\nimages_path = '../input/image-captioning/flickr-8k/flickr-8k/Flicker8k_Images-20220427T175615Z-001/Flicker8k_Images/'","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:09:38.823341Z","iopub.execute_input":"2022-05-08T09:09:38.823686Z","iopub.status.idle":"2022-05-08T09:09:38.833916Z","shell.execute_reply.started":"2022-05-08T09:09:38.823646Z","shell.execute_reply":"2022-05-08T09:09:38.833122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import lemmatized text decriptions from datafiles","metadata":{}},{"cell_type":"code","source":"file = open('../input/image-captioning/flickr-8k/flickr-8k/Flickr8k_text-20220427T180132Z-001/Flickr8k_text/Flickr8k.lemma.token.txt','r')\ndoc = file.read()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:09:40.493486Z","iopub.execute_input":"2022-05-08T09:09:40.494169Z","iopub.status.idle":"2022-05-08T09:09:40.50562Z","shell.execute_reply.started":"2022-05-08T09:09:40.494132Z","shell.execute_reply":"2022-05-08T09:09:40.504738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning the text descriptions","metadata":{}},{"cell_type":"code","source":"descriptors = dict()\n    # process lines\nfor line in doc.split('\\n'):\n    # split line by white space\n    tokens = line.split()\n    if len(line) < 2:\n        continue\n    # take the first token as the image id, the rest as the description\n    image_id, image_desc = tokens[0], tokens[1:]\n    # extract filename from image id\n    image_id = image_id.split('#')[0]\n    # convert description tokens back to string\n    image_desc = ' '.join(image_desc)\n    # create the list if needed\n    if image_id not in descriptors:\n        descriptors[image_id] = list()\n    descriptors[image_id].append(image_desc)\nlen(descriptors)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:11:08.661748Z","iopub.execute_input":"2022-05-08T09:11:08.662461Z","iopub.status.idle":"2022-05-08T09:11:08.786427Z","shell.execute_reply.started":"2022-05-08T09:11:08.662425Z","shell.execute_reply":"2022-05-08T09:11:08.785574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare translation table for removing punctuation\ntable = str.maketrans('', '', string.punctuation)\nfor key, desc_list in descriptors.items():\n    for i in range(len(desc_list)):\n        desc = desc_list[i]\n        # tokenize\n        desc = desc.split()\n        # convert to lower case\n        desc = [word.lower() for word in desc]\n        # remove punctuation from each token\n        desc = [w.translate(table) for w in desc]\n        # remove hanging 's' and 'a'\n        desc = [word for word in desc if len(word)>1]\n        # remove tokens with numbers in them\n        desc = [word for word in desc if word.isalpha()]\n        # store as string\n        desc_list[i] =  ' '.join(desc)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:11:20.525983Z","iopub.execute_input":"2022-05-08T09:11:20.526565Z","iopub.status.idle":"2022-05-08T09:11:20.973733Z","shell.execute_reply.started":"2022-05-08T09:11:20.526524Z","shell.execute_reply":"2022-05-08T09:11:20.97299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = set()\nfor key in descriptors.keys():\n    [vocab.update(d.split()) for d in descriptors[key]]\nlen(vocab)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:11:22.379404Z","iopub.execute_input":"2022-05-08T09:11:22.37968Z","iopub.status.idle":"2022-05-08T09:11:22.444921Z","shell.execute_reply.started":"2022-05-08T09:11:22.37965Z","shell.execute_reply":"2022-05-08T09:11:22.44404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_name_list = list(descriptors.keys())\n# image_name = image_name_list[4]\n# x = plt.imread(images_path+image_name)\n# plt.imshow(x)\n# plt.show()\n# for i in descriptors[image_name]:\n#     print(i)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:11:24.031934Z","iopub.execute_input":"2022-05-08T09:11:24.032258Z","iopub.status.idle":"2022-05-08T09:11:24.037173Z","shell.execute_reply.started":"2022-05-08T09:11:24.03222Z","shell.execute_reply":"2022-05-08T09:11:24.03635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Append startseq and endseq to each sentence to distinguish the end of the sentence","metadata":{}},{"cell_type":"code","source":"for image_name in image_name_list:\n    for i in range(len(descriptors[image_name])):\n        descriptors[image_name][i] = 'startseq ' + descriptors[image_name][i] + ' endseq'","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-08T09:11:31.516803Z","iopub.execute_input":"2022-05-08T09:11:31.517772Z","iopub.status.idle":"2022-05-08T09:11:31.549242Z","shell.execute_reply.started":"2022-05-08T09:11:31.517731Z","shell.execute_reply":"2022-05-08T09:11:31.548441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Split descriptors into train,val and test descriptors  ","metadata":{}},{"cell_type":"code","source":"train_text = {}\nval_text = {}\ntest_text = {}\nfor i in train_image_names:\n    if i not in descriptors:\n        descriptors[i] = list()\n    train_text[i] = descriptors[i]\nfor i in val_image_names:\n    if i not in descriptors:\n        descriptors[i] = list()\n    val_text[i] = descriptors[i]\nfor i in test_image_names:\n    if i not in descriptors:\n        descriptors[i] = list()\n    test_text[i] = descriptors[i]","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:11:35.215811Z","iopub.execute_input":"2022-05-08T09:11:35.21627Z","iopub.status.idle":"2022-05-08T09:11:35.227808Z","shell.execute_reply.started":"2022-05-08T09:11:35.216235Z","shell.execute_reply":"2022-05-08T09:11:35.226926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Check the maximum length before training","metadata":{}},{"cell_type":"code","source":"max_length = 0\nfor filename,texts in descriptors.items():\n    for i in texts:\n        if(max_length < len(i.split())):\n            max_length = len(i.split())\n            max_string = i\n            max_list = i.split()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:11:44.236396Z","iopub.execute_input":"2022-05-08T09:11:44.236672Z","iopub.status.idle":"2022-05-08T09:11:44.278509Z","shell.execute_reply.started":"2022-05-08T09:11:44.236641Z","shell.execute_reply":"2022-05-08T09:11:44.277727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:11:46.435927Z","iopub.execute_input":"2022-05-08T09:11:46.43662Z","iopub.status.idle":"2022-05-08T09:11:46.442109Z","shell.execute_reply.started":"2022-05-08T09:11:46.436581Z","shell.execute_reply":"2022-05-08T09:11:46.441134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Delete the words which are not repeated for more than a certain number of times. (No usefule information from these words to train) ","metadata":{}},{"cell_type":"code","source":"word_counts = {}\nnsents = 0\nfor key,values in train_text.items():\n    for i in values:\n        nsents += 1\n        for w in i.split(' '):\n            word_counts[w] = word_counts.get(w, 0) + 1\nvocabulary = [w for w in word_counts if word_counts[w] >= 10]\nprint(len(vocabulary))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:11:48.895911Z","iopub.execute_input":"2022-05-08T09:11:48.896168Z","iopub.status.idle":"2022-05-08T09:11:49.041128Z","shell.execute_reply.started":"2022-05-08T09:11:48.896138Z","shell.execute_reply":"2022-05-08T09:11:49.040243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Import all the word embeddings from GloVe 6b","metadata":{}},{"cell_type":"code","source":"embeddings_index = {} \nglove_file = open(os.path.join('../input/image-captioning/glove.6B.200d.txt/glove.6B.200d.txt'), encoding=\"utf-8\")\nfor line in glove_file:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:11:52.050645Z","iopub.execute_input":"2022-05-08T09:11:52.05147Z","iopub.status.idle":"2022-05-08T09:12:14.789366Z","shell.execute_reply.started":"2022-05-08T09:11:52.051422Z","shell.execute_reply":"2022-05-08T09:12:14.788571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Assign indices to the words and vice-versa ","metadata":{}},{"cell_type":"code","source":"ixtoword = {}\nwordtoix = {}\nix = 1\nfor w in vocabulary:\n    wordtoix[w] = ix\n    ixtoword[ix] = w\n    ix += 1\n\nvocab_size = len(ixtoword) + 1","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:12:26.619039Z","iopub.execute_input":"2022-05-08T09:12:26.619584Z","iopub.status.idle":"2022-05-08T09:12:26.625194Z","shell.execute_reply.started":"2022-05-08T09:12:26.619545Z","shell.execute_reply":"2022-05-08T09:12:26.624166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Convert the words into corresponding word embedding vectors","metadata":{}},{"cell_type":"code","source":"embedding_dim = 200\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in wordtoix.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n    break","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:12:52.658269Z","iopub.execute_input":"2022-05-08T09:12:52.658543Z","iopub.status.idle":"2022-05-08T09:12:52.663639Z","shell.execute_reply.started":"2022-05-08T09:12:52.658512Z","shell.execute_reply":"2022-05-08T09:12:52.662902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN model (ResNet50) for feature extraction from images","metadata":{}},{"cell_type":"code","source":"resnet_model = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')\nresnet_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:12:56.023828Z","iopub.execute_input":"2022-05-08T09:12:56.024531Z","iopub.status.idle":"2022-05-08T09:13:04.859811Z","shell.execute_reply.started":"2022-05-08T09:12:56.024487Z","shell.execute_reply":"2022-05-08T09:13:04.858936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def img_preprocess(img_path):\n    im = cv2.imread(images_path  + img_path)   \n    im_res = cv2.resize(im,(224,224))\n    im_res = np.expand_dims(im_res, axis=0)\n    return im_res","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:13:45.716767Z","iopub.execute_input":"2022-05-08T09:13:45.717634Z","iopub.status.idle":"2022-05-08T09:13:45.72318Z","shell.execute_reply.started":"2022-05-08T09:13:45.717586Z","shell.execute_reply":"2022-05-08T09:13:45.721992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Predict the feature vectors for each training image ","metadata":{}},{"cell_type":"code","source":"# train_data = {}\n# ctr=0\n# for ix in train_image_names:\n#     if ix == \"\":\n#         continue\n#     ctr+=1\n#     if ctr%500==0:\n#         print(ctr)\n#     path = ix\n#     img = img_preprocess(path)\n#     pred = resnet_model.predict(img).reshape(2048)\n#     train_data[ix] = pred","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:15:47.562806Z","iopub.execute_input":"2022-05-08T09:15:47.563512Z","iopub.status.idle":"2022-05-08T09:21:36.421554Z","shell.execute_reply.started":"2022-05-08T09:15:47.563466Z","shell.execute_reply":"2022-05-08T09:21:36.420791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Store and load the image feature vectors","metadata":{}},{"cell_type":"code","source":"filename = '../input/imagefeatures/cnn_train_features.pickle'\nfile = open(filename, 'rb')\ntrainImg_features = pkl.load(file)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-08T09:27:41.519259Z","iopub.execute_input":"2022-05-08T09:27:41.519516Z","iopub.status.idle":"2022-05-08T09:27:42.708904Z","shell.execute_reply.started":"2022-05-08T09:27:41.519488Z","shell.execute_reply":"2022-05-08T09:27:42.708158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filename = 'cnn_train_features.pickle'\n# file = open(filename, 'wb')\n# pkl.dump(train_data,file)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:23:47.823346Z","iopub.execute_input":"2022-05-08T09:23:47.823651Z","iopub.status.idle":"2022-05-08T09:23:47.972355Z","shell.execute_reply.started":"2022-05-08T09:23:47.823619Z","shell.execute_reply":"2022-05-08T09:23:47.971542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN model InceptionV3 for feature extraction from images","metadata":{}},{"cell_type":"code","source":"# from time import time\n# from tensorflow.keras.applications.inception_v3 import InceptionV3\n# from tensorflow.keras.applications.inception_v3 import preprocess_input","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inc_model = InceptionV3(weights='imagenet')\n# model_new = Model(inc_model.input, inc_model.layers[-2].output)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Function to encode a given image into a vector of size (2048, )\n# def encode(image):\n#     image = img_preprocess(image) # preprocess the image\n#     fea_vec = model_new.predict(image) # Get the encoding vector for the image\n#     fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) # reshape from (1, 2048) to (2048, )\n#     return fea_vec","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start = time()\n# encoding_train = {}\n# x = 0\n# for img in train_image_names:\n#     encoding_train[img] = encode(img)\n#     if x %100 == 0:\n#         print(x)\n#     x+=1\n# print(\"Time taken in seconds =\", time()-start)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sequential LSTM model to predict captions","metadata":{}},{"cell_type":"code","source":"inputs1 = Input(shape=(2048,))\nfe1 = Dropout(0.5)(inputs1)\nfe2 = Dense(256, activation='relu')(fe1)\n\ninputs2 = Input(shape=(max_length,))\nse1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\nse2 = Dropout(0.5)(se1)\nse3 = LSTM(256)(se2)\n\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:26:23.684593Z","iopub.execute_input":"2022-05-08T09:26:23.684917Z","iopub.status.idle":"2022-05-08T09:26:24.484646Z","shell.execute_reply.started":"2022-05-08T09:26:23.684867Z","shell.execute_reply":"2022-05-08T09:26:24.483922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Setting the embedding layer weights to the weights we predicted from the word embeddings","metadata":{}},{"cell_type":"code","source":"model.layers[2].set_weights([embedding_matrix])\nmodel.layers[2].trainable = False","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:26:50.822516Z","iopub.execute_input":"2022-05-08T09:26:50.822784Z","iopub.status.idle":"2022-05-08T09:26:50.829585Z","shell.execute_reply.started":"2022-05-08T09:26:50.822753Z","shell.execute_reply":"2022-05-08T09:26:50.828694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:26:55.418655Z","iopub.execute_input":"2022-05-08T09:26:55.419029Z","iopub.status.idle":"2022-05-08T09:26:55.433107Z","shell.execute_reply.started":"2022-05-08T09:26:55.418992Z","shell.execute_reply":"2022-05-08T09:26:55.432389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n    X1, X2, y = list(), list(), list()\n    n=0\n    # loop for ever over images\n    while 1:\n        for key, desc_list in descriptions.items():\n            n+=1\n            # retrieve the photo feature\n            photo = photos[key]\n            for desc in desc_list:\n                # encode the sequence\n                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n                # split one sequence into multiple X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pair\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # pad input sequence\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    # encode output sequence\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                    # store\n                    X1.append(photo)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n\n            if n==num_photos_per_batch:\n                yield ([np.array(X1), np.array(X2)], np.array(y))\n                X1, X2, y = list(), list(), list()\n                n=0","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:27:00.781552Z","iopub.execute_input":"2022-05-08T09:27:00.78182Z","iopub.status.idle":"2022-05-08T09:27:00.793668Z","shell.execute_reply.started":"2022-05-08T09:27:00.78179Z","shell.execute_reply":"2022-05-08T09:27:00.792825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 15\nbatch_size = 3\nsteps = len(train_text)//batch_size\n\ngenerator = data_generator(train_text, trainImg_features, wordtoix, max_length, batch_size)\nmodel.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:27:49.748214Z","iopub.execute_input":"2022-05-08T09:27:49.74882Z","iopub.status.idle":"2022-05-08T09:30:27.701332Z","shell.execute_reply.started":"2022-05-08T09:27:49.748781Z","shell.execute_reply":"2022-05-08T09:30:27.699904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generator = data_generator(train_text, trainImg_features, wordtoix, max_length, batch_size)\n# model.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T21:28:35.841969Z","iopub.execute_input":"2022-05-07T21:28:35.84254Z","iopub.status.idle":"2022-05-07T21:28:35.846929Z","shell.execute_reply.started":"2022-05-07T21:28:35.842499Z","shell.execute_reply":"2022-05-07T21:28:35.846149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\ndef greedySearch(photo):\n    in_text = 'startseq'\n    for i in range(max_length):\n        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        \n        yhat = model.predict([photo,sequence], verbose=0)\n        \n        yhat = np.argmax(yhat)\n        word = ixtoword[yhat]\n        in_text += ' ' + word\n        if word == 'endseq':\n            break\n\n    final = in_text.split()\n    final = final[1:-1]\n    final = ' '.join(final)\n    return final","metadata":{"execution":{"iopub.status.busy":"2022-05-08T04:01:30.989346Z","iopub.execute_input":"2022-05-08T04:01:30.98965Z","iopub.status.idle":"2022-05-08T04:01:31.013392Z","shell.execute_reply.started":"2022-05-08T04:01:30.989574Z","shell.execute_reply":"2022-05-08T04:01:31.012716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z = 0","metadata":{"execution":{"iopub.status.busy":"2022-05-08T04:02:05.557017Z","iopub.execute_input":"2022-05-08T04:02:05.557674Z","iopub.status.idle":"2022-05-08T04:02:05.561886Z","shell.execute_reply.started":"2022-05-08T04:02:05.557636Z","shell.execute_reply":"2022-05-08T04:02:05.560607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z+=1\nimg_name = test_image_names[z]\nimg = img_preprocess(img_name)\npred = resnet_model.predict(img).reshape(1,2048)\nx=plt.imread(images_path+img_name)\nplt.imshow(x)\nplt.show()\ncandidate = greedySearch(pred)\nprint(\"Greedy Search:\",candidate)\n\nreference = test_text[img_name]\nprint('BLEU score -> {}'.format(sentence_bleu(reference, candidate)))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T04:02:06.215297Z","iopub.execute_input":"2022-05-08T04:02:06.216104Z","iopub.status.idle":"2022-05-08T04:02:06.232716Z","shell.execute_reply.started":"2022-05-08T04:02:06.216059Z","shell.execute_reply":"2022-05-08T04:02:06.231738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model\nmodel.save('./model_2')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:47:03.146146Z","iopub.execute_input":"2022-05-08T06:47:03.146417Z","iopub.status.idle":"2022-05-08T06:47:03.169447Z","shell.execute_reply.started":"2022-05-08T06:47:03.146388Z","shell.execute_reply":"2022-05-08T06:47:03.167396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=tensorflow.keras.models.load_model('.../input/model/model_2')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T06:47:09.605455Z","iopub.execute_input":"2022-05-08T06:47:09.606014Z","iopub.status.idle":"2022-05-08T06:47:09.62937Z","shell.execute_reply.started":"2022-05-08T06:47:09.605974Z","shell.execute_reply":"2022-05-08T06:47:09.628358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing import sequence\ndef beam_search_predictions(image, beam_index = 3):\n    start = [wordtoix[\"startseq\"]]\n    start_word = [[start, 0.0]]\n    while len(start_word[0][0]) < max_length:\n        temp = []\n        for s in start_word:\n            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n            preds = model.predict([image,par_caps], verbose=0)\n            word_preds = np.argsort(preds[0])[-beam_index:]\n            # Getting the top <beam_index>(n) predictions and creating a \n            # new list so as to put them via the model again\n            for w in word_preds:\n                next_cap, prob = s[0][:], s[1]\n                next_cap.append(w)\n                prob += preds[0][w]\n                temp.append([next_cap, prob])\n                    \n        start_word = temp\n        # Sorting according to the probabilities\n        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n        # Getting the top words\n        start_word = start_word[-beam_index:]\n    \n    start_word = start_word[-1][0]\n    intermediate_caption = [ixtoword[i] for i in start_word]\n    final_caption = []\n    \n    for i in intermediate_caption:\n        if i != 'endseq':\n            final_caption.append(i)\n        else:\n            break\n\n    final_caption = ' '.join(final_caption[1:])\n    return final_caption","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}