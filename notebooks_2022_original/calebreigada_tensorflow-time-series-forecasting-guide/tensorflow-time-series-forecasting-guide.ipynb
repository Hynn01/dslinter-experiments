{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TensorFlow Time Series Forecasting Guide\n\n**In this notebook, I will demonstrate how to predict future values of univariate time series data\nwith models in Tensorflow.**\n\n --------------------------------------------------------------------------------------------------\n**Notebook Prerequisites:**\n- Python                               > https://www.kaggle.com/learn/python\n- Data Visualization with Matplotlib   > https://www.kaggle.com/learn/data-visualization\n- Basic Linear Algebra                 > https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab\n- Basic Knowledge of Machine Learning  > https://www.kaggle.com/learn/intro-to-machine-learning\n\n\n---------------------------------------------------------------------------------------------------\n\n\n**This notebook will cover the following topics:**\n- Visualizing time series data in matplotlib\n- Using windows to preprocess time series data\n- Creating custom callbacks\n- Building LSTM based time series forecasting models\n- Building Convolution-layer based time seires forecasting models\n- Building mixed architecture models\n- Evaluting time series forecast predictions\n\n\n*Note: You should research anything in this notebook that you do not understand. Some links will be provided*","metadata":{}},{"cell_type":"markdown","source":"# Read In Pollution Dataset\n\nThe dataset we will be using comes from air quality sensors across South Korea. The sensors measure and record all types of air pollutants/particles in the air, but for this tutorial we will only look at PM<sub>2.5</sub> (fine dust).\n\nFor more details on the dataset, see here -> https://www.kaggle.com/datasets/calebreigada/south-korean-pollution","metadata":{}},{"cell_type":"code","source":"#Import Libraries\nimport tensorflow as tf\nimport numpy as np #Linear Algebra\nimport matplotlib.pyplot as plt #Data visualization\nimport pandas as pd #data manipulation\n\nimport warnings\nwarnings.filterwarnings('ignore') #Ignore warnings\n\n#Make sure Tensorflow is version 2.0 or higher\nprint('Tensorflow Version:', tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T11:38:22.33914Z","iopub.execute_input":"2022-04-27T11:38:22.339992Z","iopub.status.idle":"2022-04-27T11:38:26.558801Z","shell.execute_reply.started":"2022-04-27T11:38:22.339881Z","shell.execute_reply":"2022-04-27T11:38:26.557477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reads in Pollution csv\npollution = pd.read_csv(\"../input/south-korean-pollution/south-korean-pollution-data.csv\",\n                       parse_dates=['date'],\n                       index_col='date')\n#Filters for only pm25 values in Jeongnim-Dong City, sorted by date\npollution = pollution[pollution.City == 'Jeongnim-Dong'].pm25.sort_index()\n#starts the dataset at 2018 and ends in 2022(due to breaks in data in previous years)\nstart = pd.to_datetime('2018-01-01')\nend = pd.to_datetime('2022-01-01')\npollution = pollution[start:end]\nprint('SAMPLE OF TIME SERIES DATA:')\npollution.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T11:38:26.560559Z","iopub.execute_input":"2022-04-27T11:38:26.560798Z","iopub.status.idle":"2022-04-27T11:38:26.687256Z","shell.execute_reply.started":"2022-04-27T11:38:26.560763Z","shell.execute_reply":"2022-04-27T11:38:26.686461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Impute Missing Dates\n\nTime series data is especially sensitive to missing values. For this reason, we need to impute any missing time steps. In this dataset there are a handful of missing time steps which we will impute the value of the previous time step.","metadata":{}},{"cell_type":"code","source":"#Checks for and imputes missing dates\na = pd.date_range(start=\"2018-01-01\", end=\"2022-01-01\", freq=\"D\") #continous dates\nb = pollution.index #our time series\ndiff_dates = a.difference(b) # finds what in 'a' is not in 'b'\n\ntd = pd.Timedelta(1, \"d\") #1 day\nfor date in diff_dates:\n    prev_val = pollution[date-td] #takes the previous value\n    pollution[date] = prev_val #imputes previous value\n\npollution.sort_index(inplace=True)\n#sets the time index frequency as daily\npollution.freq = \"D\"\n","metadata":{"execution":{"iopub.status.busy":"2022-04-27T11:38:26.688873Z","iopub.execute_input":"2022-04-27T11:38:26.689423Z","iopub.status.idle":"2022-04-27T11:38:26.702929Z","shell.execute_reply.started":"2022-04-27T11:38:26.689381Z","shell.execute_reply":"2022-04-27T11:38:26.701943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#displays a plot of the pm25 values since 2018\nfig = plt.figure(figsize=(15,5))\nplt.plot(pollution, color='blue')\nplt.xlabel('Date')\nplt.ylabel('PM 2.5 Value')\nplt.title('Jeongnim-Dong PM 2.5 Values 2018-2022')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T11:38:26.705454Z","iopub.execute_input":"2022-04-27T11:38:26.705709Z","iopub.status.idle":"2022-04-27T11:38:27.010731Z","shell.execute_reply.started":"2022-04-27T11:38:26.705674Z","shell.execute_reply":"2022-04-27T11:38:27.010086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split Dataset Into Train/Test\n\nYou may have done a train/test split before using random subsets of the entire dataset. In time series forecasting problems, the test data needs to be a block of the most recent time steps. This is in order to prevent data lookahead in our model (meaning the model would already know the future).","metadata":{}},{"cell_type":"code","source":"#Split the time series data into a train and test set\nend_train_ix = pd.to_datetime('2020-12-31')\ntrain = pollution[:end_train_ix] # Jan 2018-2021\ntest = pollution[end_train_ix:] # Jan 2021-2022","metadata":{"execution":{"iopub.status.busy":"2022-04-27T11:38:27.012033Z","iopub.execute_input":"2022-04-27T11:38:27.012394Z","iopub.status.idle":"2022-04-27T11:38:27.019274Z","shell.execute_reply.started":"2022-04-27T11:38:27.012361Z","shell.execute_reply":"2022-04-27T11:38:27.018398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#displays a plot of the train/test split\nfig = plt.figure(figsize=(15,5))\nplt.plot(train, color='purple', label='Training')\nplt.plot(test, color='orange', label='Testing')\nplt.xlabel('Date')\nplt.ylabel('PM 2.5 Value')\nplt.title('Train-Test Split')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T11:38:27.020895Z","iopub.execute_input":"2022-04-27T11:38:27.02131Z","iopub.status.idle":"2022-04-27T11:38:27.43332Z","shell.execute_reply.started":"2022-04-27T11:38:27.021261Z","shell.execute_reply":"2022-04-27T11:38:27.432583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Process Dataset with Windows\n\nTime series data needs to be sliced into windows before being sent to a ML model. A window is essentially a limited range snapshot of our time series data. Say we have an array of values:\n\n`[1, 2, 3, 4, 5, 6, 7, 8, 9]`\n\nIf we have a window size of 4 and we want to forecast 1 timestep in advance, this would be the result:\n\n```\n[1, 2, 3, 4] [5]\n[2, 3, 4, 5] [6]\n[3, 4, 5, 6] [7]\n[4, 5, 6, 7] [8]\n[5, 6, 7, 8] [9]\n```","metadata":{}},{"cell_type":"code","source":"#Creates a windowed dataset from the time series data\nWINDOW = 14 #the window value... 14 days\n\n#converts values to TensorSliceDataset\ntrain_data = tf.data.Dataset.from_tensor_slices(train.values) \n\n#takes window size + 1 slices of the dataset\ntrain_data = train_data.window(WINDOW+1, shift=1, drop_remainder=True)\n\n#flattens windowed data by batching \ntrain_data = train_data.flat_map(lambda x: x.batch(WINDOW+1))\n\n#creates features and target tuple\ntrain_data = train_data.map(lambda x: (x[:-1], x[-1]))\n\n#shuffles dataset\ntrain_data = train_data.shuffle(1_000)\n\n#creates batches of windows\ntrain_data = train_data.batch(32).prefetch(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T13:32:28.468204Z","iopub.execute_input":"2022-04-27T13:32:28.468772Z","iopub.status.idle":"2022-04-27T13:32:28.496064Z","shell.execute_reply.started":"2022-04-27T13:32:28.468714Z","shell.execute_reply":"2022-04-27T13:32:28.495414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `train_data` will now have batches of windowed time series data. The batches are filled with arrays of 14 pm25 values and their target values.","metadata":{}},{"cell_type":"markdown","source":"# Custom Callback\n\nCallbacks are used in Tensorflow to allow user intervention during model training. A callback can be executed at a number of specific intances during model training. \nFor example: \n- `on_batch_begin`/`end`\n- `on_epoch_begin`/`end`\n- `on_predict_batch_begin`/`end`\n- `on_predict_begin`/`end`\n- `on_test_batch_begin`/`end`\n- `on_test_begin`/`end`\n- `on_train_batch_begin`/`end`\n- `on_train_begin`/`end`\n\nWe will create `CustomCallback` which will stop the model from training once the model reaches under 10 mean absolute error on the training set.\n\nLink: https://keras.io/api/callbacks/","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback\n\nclass CustomCallback(Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if logs.get('mae') < 10.0:\n            print(\"MAE under 10.0... Stopping training\")\n            self.model.stop_training = True\n\nmy_callback = CustomCallback()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T11:38:29.761047Z","iopub.execute_input":"2022-04-27T11:38:29.761305Z","iopub.status.idle":"2022-04-27T11:38:30.613273Z","shell.execute_reply.started":"2022-04-27T11:38:29.76127Z","shell.execute_reply":"2022-04-27T11:38:30.612488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predefined Callback - `LearningRateScheduler`\n\nThere are also a number of predefined callbacks. We will use the `LearningRateScheduler` to dynamically update the learning rate of our optimizer. This predefined callback takes a funtion that updates the learning rate as an argument.\n\nLink: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import LearningRateScheduler\n\n#creates a function that updates the learning rate based on the epoch number\ndef scheduler(epoch, lr):\n    if epoch < 2:\n        return 0.01\n    else:\n        return lr * 0.99\n\nlr_scheduler = LearningRateScheduler(scheduler)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T11:38:30.614694Z","iopub.execute_input":"2022-04-27T11:38:30.614975Z","iopub.status.idle":"2022-04-27T11:38:30.624085Z","shell.execute_reply.started":"2022-04-27T11:38:30.61494Z","shell.execute_reply":"2022-04-27T11:38:30.61918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM Model\n\nLSTM (Long Short-Term Memory) layers are sequence aware and can carry meaning from previous time steps. These layers are very commonly used in all machine learning tasks where order is important -- to include time series forecasting. \n\n**Pros:**\n- Can retain information from far in the past to make its prediction\n- Proven to be highly effective\n\n**Cons:**\n- Many parameters\n- Long training time\n\nSee here for more -> https://en.wikipedia.org/wiki/Long_short-term_memory","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout, Lambda, Bidirectional\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import Huber\n\n\nlstm_model = Sequential([\n    # add extra axis to input data\n    Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[WINDOW]), \n    Bidirectional(LSTM(128, return_sequences=True)),\n    Bidirectional(LSTM(128)),\n    Dense(256, activation='relu'),\n    Dropout(0.4),\n    Dense(1)\n])\n\nlstm_model.compile(\n    loss=Huber(),\n    optimizer=Adam(),\n    metrics=['mae']\n)\n\nlstm_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T11:38:30.62793Z","iopub.execute_input":"2022-04-27T11:38:30.628305Z","iopub.status.idle":"2022-04-27T11:38:31.548375Z","shell.execute_reply.started":"2022-04-27T11:38:30.628276Z","shell.execute_reply":"2022-04-27T11:38:31.547655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Trains LSTM Model\nlstm_history = lstm_model.fit(\n    train_data,\n    epochs=100,\n    callbacks=[lr_scheduler, my_callback],\n    verbose=0\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T11:38:31.549502Z","iopub.execute_input":"2022-04-27T11:38:31.550179Z","iopub.status.idle":"2022-04-27T11:39:28.427734Z","shell.execute_reply.started":"2022-04-27T11:38:31.550138Z","shell.execute_reply":"2022-04-27T11:39:28.426927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plots training history\n#Plots history of model training\nplt.rcParams[\"figure.figsize\"] = (15,5)\nfig, axs = plt.subplots(1, 2)\n\naxs[0].plot(lstm_history.history['loss'], color='red')\naxs[0].set_xlabel('Epoch')\naxs[0].set_ylabel('Loss')\naxs[0].set_title('Training Loss')\n\naxs[1].plot(lstm_history.history['mae'])\naxs[1].set_xlabel('Epoch')\naxs[1].set_ylabel('MAE')\naxs[1].set_title('Training MAE')\n\nfig.text(0.425,1, 'LSTM MODEL', {'size':25})\nplt.show()\n\nprint(\"\\t\\t\\t\\t\\tFINAL LOSS: {} | FINAL MAE: {}\".format(\n                                                round(lstm_history.history['loss'][-1], 2),\n                                                 round(lstm_history.history['mae'][-1] ,2)))","metadata":{"execution":{"iopub.status.busy":"2022-04-27T12:42:59.104393Z","iopub.execute_input":"2022-04-27T12:42:59.104658Z","iopub.status.idle":"2022-04-27T12:42:59.383572Z","shell.execute_reply.started":"2022-04-27T12:42:59.104628Z","shell.execute_reply":"2022-04-27T12:42:59.38281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convolution Model\n\n2D Convolution layers are commonly used in computer vision tasks. Their 1D version may be used for time series data. The 1D convolutional layer tries to find patterns in different segments of the time series data. \n\n**Pros:**\n- Low parameters\n- Fast training time\n\n**Cons:**\n- Tends to put more weight on recent values\n\nSee here for more -> https://boostedml.com/2020/04/1-d-convolutional-neural-networks-for-time-series-basic-intuition.html","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, Flatten\n\n\ncnn_model = Sequential([\n    # add extra axis to input data\n    Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[WINDOW]),\n    Conv1D(filters=32, kernel_size=3, strides=1,\n           padding='causal', activation='relu'),\n    Conv1D(filters=64, kernel_size=3, strides=1, \n           padding='causal', activation='relu'),\n    GlobalAveragePooling1D(),\n    Flatten(),\n    Dropout(0.3),\n    Dense(512, activation='relu'),\n    Dropout(0.4),\n    Dense(1)\n])\n\ncnn_model.compile(\n    loss=Huber(),\n    optimizer=Adam(),\n    metrics=['mae']\n)\n\ncnn_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T12:01:07.378783Z","iopub.execute_input":"2022-04-27T12:01:07.379309Z","iopub.status.idle":"2022-04-27T12:01:07.454091Z","shell.execute_reply.started":"2022-04-27T12:01:07.379269Z","shell.execute_reply":"2022-04-27T12:01:07.453302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Trains CNN Model\ncnn_history = cnn_model.fit(\n    train_data,\n    epochs=100,\n    callbacks=[lr_scheduler, my_callback],\n    verbose=0\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T12:01:13.461173Z","iopub.execute_input":"2022-04-27T12:01:13.461429Z","iopub.status.idle":"2022-04-27T12:01:40.766784Z","shell.execute_reply.started":"2022-04-27T12:01:13.461402Z","shell.execute_reply":"2022-04-27T12:01:40.766053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plots training history\n#Plots history of model training\nplt.rcParams[\"figure.figsize\"] = (15,5)\nfig, axs = plt.subplots(1, 2)\n\naxs[0].plot(cnn_history.history['loss'], color='red')\naxs[0].set_xlabel('Epoch')\naxs[0].set_ylabel('Loss')\naxs[0].set_title('Training Loss')\n\naxs[1].plot(cnn_history.history['mae'])\naxs[1].set_xlabel('Epoch')\naxs[1].set_ylabel('MAE')\naxs[1].set_title('Training MAE')\n\nfig.text(0.425,1, 'CNN MODEL', {'size':25})\nplt.show()\n\nprint(\"\\t\\t\\t\\t\\tFINAL LOSS: {} | FINAL MAE: {}\".format(\n                                                round(cnn_history.history['loss'][-1], 2),\n                                                 round(cnn_history.history['mae'][-1], 2)))","metadata":{"execution":{"iopub.status.busy":"2022-04-27T12:43:15.963045Z","iopub.execute_input":"2022-04-27T12:43:15.963314Z","iopub.status.idle":"2022-04-27T12:43:16.258738Z","shell.execute_reply.started":"2022-04-27T12:43:15.963286Z","shell.execute_reply":"2022-04-27T12:43:16.257963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mixed Architecture Model\n\nIf we are trying to get the best of both worlds, we can mix the Convolution layers with the LSTM layers. ","metadata":{}},{"cell_type":"code","source":"mixed_model = Sequential([\n    # add extra axis to input data\n    Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[WINDOW]),\n    Conv1D(filters=64, kernel_size=3, strides=1,\n           padding='causal', activation='relu'),\n    Bidirectional(LSTM(128, return_sequences=True)),\n    Bidirectional(LSTM(128)),\n    Dropout(0.3),\n    Dense(512, activation='relu'),\n    Dropout(0.4),\n    Dense(1)\n])\n\n\nmixed_model.compile(\n    loss=Huber(),\n    optimizer=Adam(),\n    metrics=['mae']\n)\n\nmixed_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T12:24:10.964877Z","iopub.execute_input":"2022-04-27T12:24:10.965602Z","iopub.status.idle":"2022-04-27T12:24:11.863557Z","shell.execute_reply.started":"2022-04-27T12:24:10.965563Z","shell.execute_reply":"2022-04-27T12:24:11.862791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Trains Mixed Model\nmixed_history = mixed_model.fit(\n    train_data,\n    epochs=100,\n    callbacks=[lr_scheduler, my_callback],\n    verbose=0\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T12:24:32.186702Z","iopub.execute_input":"2022-04-27T12:24:32.186986Z","iopub.status.idle":"2022-04-27T12:25:30.059274Z","shell.execute_reply.started":"2022-04-27T12:24:32.186956Z","shell.execute_reply":"2022-04-27T12:25:30.058539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plots training history\n#Plots history of model training\nplt.rcParams[\"figure.figsize\"] = (15,5)\nfig, axs = plt.subplots(1, 2)\n\naxs[0].plot(mixed_history.history['loss'], color='red')\naxs[0].set_xlabel('Epoch')\naxs[0].set_ylabel('Loss')\naxs[0].set_title('Training Loss')\n\naxs[1].plot(mixed_history.history['mae'])\naxs[1].set_xlabel('Epoch')\naxs[1].set_ylabel('MAE')\naxs[1].set_title('Training MAE')\n\nfig.text(0.425,1, 'MIXED MODEL', {'size':25})\nplt.show()\n\nprint(\"\\t\\t\\t\\t\\tFINAL LOSS: {} | FINAL MAE: {}\".format(\n                                                round(mixed_history.history['loss'][-1], 2),\n                                                 round(mixed_history.history['mae'][-1], 2)))","metadata":{"execution":{"iopub.status.busy":"2022-04-27T12:43:46.386985Z","iopub.execute_input":"2022-04-27T12:43:46.387442Z","iopub.status.idle":"2022-04-27T12:43:46.699636Z","shell.execute_reply.started":"2022-04-27T12:43:46.387405Z","shell.execute_reply":"2022-04-27T12:43:46.698931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate Models on Test Set\n\nUnlike other machine learning tasks, we cannot directly predict and evaluate the model's performance from the test data. First, we must create windows (similar to how we did on the training data) on the last 14 days of the training set and the test set. These length-14 windows will each correspond with a timestep of the test data. Predicting the next value for each of these windows will be our forecast.\n\n\n*Note: Current state of the art machine learning models achieve around **13 MAE** score for this problem*\n","metadata":{}},{"cell_type":"code","source":"#Gets forecasts of models\n\nall_models = [('LSTM MODEL', lstm_model),\n              ('CNN MODEL', cnn_model),\n              ('MIXED MODEL', mixed_model)]\n\nmodel_forecasts = {\n    'LSTM MODEL': [],\n    'CNN MODEL': [],\n    'MIXED MODEL': []\n}\n\n#chunck of data to be windowed so that each window associated to a value in test set\nforecast_data = train[-WINDOW:].append(test[:-1]).values\n\nfor name, model in all_models:\n    #converts values to TensorSliceDataset\n    test_data = tf.data.Dataset.from_tensor_slices(forecast_data) \n    #takes window size  slices of the dataset\n    test_data = test_data.window(WINDOW, shift=1, drop_remainder=True)\n    #flattens windowed data by batching \n    test_data = test_data.flat_map(lambda x: x.batch(WINDOW+1))\n    #creates batches of windows\n    test_data = test_data.batch(32).prefetch(1)\n    #gets model prediction \n    preds = model.predict(test_data)\n    #append to forecast dict\n    model_forecasts[name].append(preds)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T12:58:56.822526Z","iopub.execute_input":"2022-04-27T12:58:56.823148Z","iopub.status.idle":"2022-04-27T12:58:57.065004Z","shell.execute_reply.started":"2022-04-27T12:58:56.823111Z","shell.execute_reply":"2022-04-27T12:58:57.064275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Gets MAE score of model forecasts\n\nN = test.values.shape[0] #number of samples in test set\n\nlstm_mae = np.abs(test.values - model_forecasts['LSTM MODEL'][0].squeeze()).sum() / N\n\ncnn_mae = np.abs(test.values - model_forecasts['CNN MODEL'][0].squeeze()).sum() / N\n\nmix_mae = np.abs(test.values - model_forecasts['MIXED MODEL'][0].squeeze()).sum() / N\n\n\nprint('MODEL MAE SCORES')\nprint('=====================================')\nprint('LSTM MAE:', round(lstm_mae, 2))\nprint('CNN MAE:', round(cnn_mae, 2))\nprint('MIXED MAE:', round(mix_mae, 2))","metadata":{"execution":{"iopub.status.busy":"2022-04-27T13:12:34.705105Z","iopub.execute_input":"2022-04-27T13:12:34.705358Z","iopub.status.idle":"2022-04-27T13:12:34.71553Z","shell.execute_reply.started":"2022-04-27T13:12:34.70533Z","shell.execute_reply":"2022-04-27T13:12:34.71464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#displays forecasted data\nplt.rcParams[\"figure.figsize\"] = (15,20)\nfig, axs = plt.subplots(4, 1)\n\n#LSTM Forecast\naxs[0].plot(test.values, color='black', label='Actual Value')\naxs[0].plot(model_forecasts['LSTM MODEL'][0].squeeze(), color='green', label='LSTM')\naxs[0].set_title('LSTM MODEL FORECAST')\n\n#CNN Forcast\naxs[1].plot(test.values, color='black', label='Actual Value')\naxs[1].plot(model_forecasts['CNN MODEL'][0].squeeze(), color='blue', label='Convolution')\naxs[1].set_title('CNN MODEL FORECAST')\n\n#Mixed Model Forecast\naxs[2].plot(test.values, color='black', label='Actual Value')\naxs[2].plot(model_forecasts['MIXED MODEL'][0].squeeze(), color='red', label='Mixed')\naxs[2].set_title('MIXED MODEL FORECAST')\n\n#All forecasts\naxs[3].plot(test.values, color='black', label='Actual Value')\naxs[3].plot(model_forecasts['LSTM MODEL'][0].squeeze(), color='green', label='LSTM')\naxs[3].plot(model_forecasts['CNN MODEL'][0].squeeze(), color='blue', label='Convolution')\naxs[3].plot(model_forecasts['MIXED MODEL'][0].squeeze(), color='red', label='Mixed')\naxs[3].set_title('ALL MODEL FORECASTS')\n\n\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T13:06:52.703091Z","iopub.execute_input":"2022-04-27T13:06:52.703379Z","iopub.status.idle":"2022-04-27T13:06:53.329206Z","shell.execute_reply.started":"2022-04-27T13:06:52.703351Z","shell.execute_reply":"2022-04-27T13:06:53.328586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Similar Notebooks\n**TensorFlow Image Classification Guide**: https://www.kaggle.com/code/calebreigada/tensorflow-image-classification-guide\n\n**TensorFlow Natural Language Processing Guide:** https://www.kaggle.com/code/calebreigada/tensorflow-natural-language-processing-guide","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}