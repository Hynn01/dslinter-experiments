{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [training] 3rd place solution\n\nI will post a description of the model in discussion. Inference is done in another notebook.\n* Inference: https://www.kaggle.com/sugghi/inference-3rd-place-solution/\n\nI referred to various notebooks when coding. In particular, the following notebooks were used directly.\n* Submitting Lagged Features via API：\nhttps://www.kaggle.com/tomforbes/gresearch-submitting-lagged-features-via-api\n* EmbargoCV：\nhttps://www.kaggle.com/nrcjea001/lgbm-embargocv-weightedpearson-lagtarget/notebook\n\nIn addition, although not used in this notebook, local api published by @jagofc helped me a lot in coding.\n* local api：https://www.kaggle.com/code/jagofc/local-api/\n\nAs you can see from my code, I am a novice in machine learning and python.\nIf you see anything  to improve on or any mistakes, I'd be very happy to hear about them!","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport gresearch_crypto\nimport time\nimport datetime\n\nimport pickle\nimport gc\n\nfrom tqdm import tqdm\n\nn_fold = 7\nseed0 = 8586\nuse_supple_for_train = True\n\n# If True, the period used to evaluate Public LB will not be used for training.\n# Set to False on final submission.\nnot_use_overlap_to_train = False\n\nTRAIN_CSV = '/kaggle/input/g-research-crypto-forecasting/train.csv'\nSUPPLE_TRAIN_CSV = '/kaggle/input/g-research-crypto-forecasting/supplemental_train.csv'\nASSET_DETAILS_CSV = '/kaggle/input/g-research-crypto-forecasting/asset_details.csv'\n\npd.set_option('display.max_rows', 6)\npd.set_option('display.max_columns', 350)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:01:42.98654Z","iopub.execute_input":"2022-05-05T08:01:42.987217Z","iopub.status.idle":"2022-05-05T08:01:45.186484Z","shell.execute_reply.started":"2022-05-05T08:01:42.987087Z","shell.execute_reply":"2022-05-05T08:01:45.185777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lags = [60,300,900]","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:01:45.188229Z","iopub.execute_input":"2022-05-05T08:01:45.188749Z","iopub.status.idle":"2022-05-05T08:01:45.192609Z","shell.execute_reply.started":"2022-05-05T08:01:45.188705Z","shell.execute_reply":"2022-05-05T08:01:45.191842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'early_stopping_rounds': 50,\n    'objective': 'regression',\n    'metric': 'rmse',\n#     'metric': 'None',\n    'boosting_type': 'gbdt',\n    'max_depth': 5,\n    'verbose': -1,\n    'max_bin':600,\n    'min_data_in_leaf':50,\n    'learning_rate': 0.03,\n    'subsample': 0.7,\n    'subsample_freq': 1,\n    'feature_fraction': 1,\n    'lambda_l1': 0.5,\n    'lambda_l2': 2,\n    'seed':seed0,\n    'feature_fraction_seed': seed0,\n    'bagging_fraction_seed': seed0,\n    'drop_seed': seed0,\n    'data_random_seed': seed0,\n    'extra_trees': True,\n    'extra_seed': seed0,\n    'zero_as_missing': True,\n    \"first_metric_only\": True\n         }","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:01:45.193512Z","iopub.execute_input":"2022-05-05T08:01:45.193757Z","iopub.status.idle":"2022-05-05T08:01:45.205436Z","shell.execute_reply.started":"2022-05-05T08:01:45.193725Z","shell.execute_reply":"2022-05-05T08:01:45.204808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:01:45.207427Z","iopub.execute_input":"2022-05-05T08:01:45.207947Z","iopub.status.idle":"2022-05-05T08:01:45.224146Z","shell.execute_reply.started":"2022-05-05T08:01:45.207908Z","shell.execute_reply":"2022-05-05T08:01:45.223227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_asset_details = pd.read_csv(ASSET_DETAILS_CSV).sort_values(\"Asset_ID\")\n# df_asset_details","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:01:45.225264Z","iopub.execute_input":"2022-05-05T08:01:45.225607Z","iopub.status.idle":"2022-05-05T08:01:45.27193Z","shell.execute_reply.started":"2022-05-05T08:01:45.225472Z","shell.execute_reply":"2022-05-05T08:01:45.271358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"%%time\ndf_train = pd.read_csv(TRAIN_CSV, usecols=['timestamp','Asset_ID', 'Close', 'Target'])\n\nif use_supple_for_train:    \n    df_supple = pd.read_csv(SUPPLE_TRAIN_CSV, usecols=['timestamp','Asset_ID', 'Close', 'Target'])\n#     display(df_supple)\n    df_train = pd.concat([df_train, df_supple])\n    del df_supple\ndf_train = reduce_mem_usage(df_train)\n# df_train","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:01:45.272951Z","iopub.execute_input":"2022-05-05T08:01:45.273266Z","iopub.status.idle":"2022-05-05T08:02:44.907679Z","shell.execute_reply.started":"2022-05-05T08:01:45.273238Z","shell.execute_reply":"2022-05-05T08:02:44.906844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_merged = pd.DataFrame()\ntrain_merged[df_train.columns] = 0\nfor id in tqdm( range(14) ):\n    train_merged = train_merged.merge(df_train.loc[df_train[\"Asset_ID\"] == id, ['timestamp', 'Close','Target']].copy(), on=\"timestamp\", how='outer',suffixes=['', \"_\"+str(id)])\n        \ntrain_merged = train_merged.drop(df_train.columns.drop(\"timestamp\"), axis=1)\ntrain_merged = train_merged.sort_values('timestamp', ascending=True)\ndisplay(train_merged.head())","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:02:44.908919Z","iopub.execute_input":"2022-05-05T08:02:44.909629Z","iopub.status.idle":"2022-05-05T08:03:03.940142Z","shell.execute_reply.started":"2022-05-05T08:02:44.909586Z","shell.execute_reply":"2022-05-05T08:03:03.938904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# forward fill\n# Set an upper limit on the number of fills, since there may be long term gaps.\nfor id in range(14):\n#     print(id, train_merged[f'Close_{id}'].isnull().sum())   # Number of missing before forward fill\n    train_merged[f'Close_{id}'] = train_merged[f'Close_{id}'].fillna(method='ffill', limit=100)\n#     print(id, train_merged[f'Close_{id}'].isnull().sum())   # Number of missing after forward fill","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:03:03.941914Z","iopub.execute_input":"2022-05-05T08:03:03.942304Z","iopub.status.idle":"2022-05-05T08:03:04.188394Z","shell.execute_reply.started":"2022-05-05T08:03:03.942274Z","shell.execute_reply":"2022-05-05T08:03:04.187381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"def get_features(df, train=True):   \n    if train == True:\n        totimestamp = lambda s: np.int32(time.mktime(datetime.datetime.strptime(s, \"%d/%m/%Y\").timetuple()))\n        valid_window = [totimestamp(\"12/03/2021\")]\n#         valid_window = [totimestamp(\"15/08/2021\")]  #検証用\n        df['train_flg'] = np.where(df['timestamp']>=valid_window[0], 0,1)\n\n        supple_start_window = [totimestamp(\"22/09/2021\")]\n        if use_supple_for_train:\n            df['train_flg'] = np.where(df['timestamp']>=supple_start_window[0], 1 ,df['train_flg']  )\n\n   \n    for id in range(14):    \n        for lag in lags:\n            df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n            df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n    for lag in lags:\n        df[f'mean_close/mean_{lag}'] =  np.mean(df.iloc[:,df.columns.str.startswith(f'log_close/mean_{lag}_id')], axis=1)\n        df[f'mean_log_returns_{lag}'] = np.mean(df.iloc[:,df.columns.str.startswith(f'log_return_{lag}_id')] ,    axis=1)\n        for id in range(14):\n            df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n            df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n\n    if train == True:\n        for id in range(14):\n            df = df.drop([f'Close_{id}'], axis=1)\n        oldest_use_window = [totimestamp(\"12/01/2019\")]\n        df = df[  df['timestamp'] >= oldest_use_window[0]   ]\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:03:04.189874Z","iopub.execute_input":"2022-05-05T08:03:04.19008Z","iopub.status.idle":"2022-05-05T08:03:04.205407Z","shell.execute_reply.started":"2022-05-05T08:03:04.190054Z","shell.execute_reply":"2022-05-05T08:03:04.20446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfeat = get_features(train_merged)\nfeat","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:03:35.604791Z","iopub.execute_input":"2022-05-05T08:03:35.605085Z","iopub.status.idle":"2022-05-05T08:04:00.744287Z","shell.execute_reply.started":"2022-05-05T08:03:35.605044Z","shell.execute_reply":"2022-05-05T08:04:00.74361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define features for LGBM\nnot_use_features_train = ['timestamp', 'train_flg']\nfor id in range(14):\n    not_use_features_train.append(f'Target_{id}')\n\nfeatures = feat.columns \nfeatures = features.drop(not_use_features_train)\nfeatures = list(features)\n# display(features)  \nlen(features)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:03:04.209804Z","iopub.execute_input":"2022-05-05T08:03:04.210227Z","iopub.status.idle":"2022-05-05T08:03:35.603423Z","shell.execute_reply.started":"2022-05-05T08:03:04.210183Z","shell.execute_reply":"2022-05-05T08:03:35.602634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_merged\ndel df_train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:04:00.745343Z","iopub.execute_input":"2022-05-05T08:04:00.746059Z","iopub.status.idle":"2022-05-05T08:04:00.889495Z","shell.execute_reply.started":"2022-05-05T08:04:00.746012Z","shell.execute_reply":"2022-05-05T08:04:00.888726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# define the evaluation metric\ndef correlation(a, train_data):\n    \n    b = train_data.get_label()\n    \n    a = np.ravel(a)\n    b = np.ravel(b)\n\n    len_data = len(a)\n    mean_a = np.sum(a) / len_data\n    mean_b = np.sum(b) / len_data\n    var_a = np.sum(np.square(a - mean_a)) / len_data\n    var_b = np.sum(np.square(b - mean_b)) / len_data\n\n    cov = np.sum((a * b))/len_data - mean_a*mean_b\n    corr = cov / np.sqrt(var_a * var_b)\n\n    return 'corr', corr, True\n\n# For CV score calculation\ndef corr_score(pred, valid):\n    len_data = len(pred)\n    mean_pred = np.sum(pred) / len_data\n    mean_valid = np.sum(valid) / len_data\n    var_pred = np.sum(np.square(pred - mean_pred)) / len_data\n    var_valid = np.sum(np.square(valid - mean_valid)) / len_data\n\n    cov = np.sum((pred * valid))/len_data - mean_pred*mean_valid\n    corr = cov / np.sqrt(var_pred * var_valid)\n\n    return corr\n\n# For CV score calculation\ndef wcorr_score(pred, valid, weight):\n    len_data = len(pred)\n    sum_w = np.sum(weight)\n    mean_pred = np.sum(pred * weight) / sum_w\n    mean_valid = np.sum(valid * weight) / sum_w\n    var_pred = np.sum(weight * np.square(pred - mean_pred)) / sum_w\n    var_valid = np.sum(weight * np.square(valid - mean_valid)) / sum_w\n\n    cov = np.sum((pred * valid * weight)) / sum_w - mean_pred*mean_valid\n    corr = cov / np.sqrt(var_pred * var_valid)\n\n    return corr","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:04:00.890596Z","iopub.execute_input":"2022-05-05T08:04:00.890842Z","iopub.status.idle":"2022-05-05T08:04:00.903484Z","shell.execute_reply.started":"2022-05-05T08:04:00.890815Z","shell.execute_reply":"2022-05-05T08:04:00.902735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from: https://blog.amedama.jp/entry/lightgbm-cv-feature-importance\n# (used in nyanp's Optiver solution)\ndef plot_importance(importances, features_names = features, PLOT_TOP_N = 20, figsize=(10, 10)):\n    importance_df = pd.DataFrame(data=importances, columns=features)\n    sorted_indices = importance_df.median(axis=0).sort_values(ascending=False).index\n    sorted_importance_df = importance_df.loc[:, sorted_indices]\n    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n    _, ax = plt.subplots(figsize=figsize)\n    ax.grid()\n    ax.set_xscale('log')\n    ax.set_ylabel('Feature')\n    ax.set_xlabel('Importance')\n    sns.boxplot(data=sorted_importance_df[plot_cols],\n                orient='h',\n                ax=ax)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:04:00.905087Z","iopub.execute_input":"2022-05-05T08:04:00.905533Z","iopub.status.idle":"2022-05-05T08:04:00.916455Z","shell.execute_reply.started":"2022-05-05T08:04:00.905492Z","shell.execute_reply":"2022-05-05T08:04:00.915919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from: https://www.kaggle.com/code/nrcjea001/lgbm-embargocv-weightedpearson-lagtarget/\ndef get_time_series_cross_val_splits(data, cv = n_fold, embargo = 3750):\n    all_train_timestamps = data['timestamp'].unique()\n    len_split = len(all_train_timestamps) // cv\n    test_splits = [all_train_timestamps[i * len_split:(i + 1) * len_split] for i in range(cv)]\n    # fix the last test split to have all the last timestamps, in case the number of timestamps wasn't divisible by cv\n    rem = len(all_train_timestamps) - len_split*cv\n    if rem>0:\n        test_splits[-1] = np.append(test_splits[-1], all_train_timestamps[-rem:])\n\n    train_splits = []\n    for test_split in test_splits:\n        test_split_max = int(np.max(test_split))\n        test_split_min = int(np.min(test_split))\n        # get all of the timestamps that aren't in the test split\n        train_split_not_embargoed = [e for e in all_train_timestamps if not (test_split_min <= int(e) <= test_split_max)]\n        # embargo the train split so we have no leakage. Note timestamps are expressed in seconds, so multiply by 60\n        embargo_sec = 60*embargo\n        train_split = [e for e in train_split_not_embargoed if\n                       abs(int(e) - test_split_max) > embargo_sec and abs(int(e) - test_split_min) > embargo_sec]\n        train_splits.append(train_split)\n\n    # convenient way to iterate over train and test splits\n    train_test_zip = zip(train_splits, test_splits)\n    return train_test_zip","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:04:00.917583Z","iopub.execute_input":"2022-05-05T08:04:00.91822Z","iopub.status.idle":"2022-05-05T08:04:00.934408Z","shell.execute_reply.started":"2022-05-05T08:04:00.918186Z","shell.execute_reply":"2022-05-05T08:04:00.933796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_Xy_and_model_for_asset(df_proc, asset_id):\n    df_proc = df_proc.loc[  (df_proc[f'Target_{asset_id}'] == df_proc[f'Target_{asset_id}'])  ]\n    if not_use_overlap_to_train:\n        df_proc = df_proc.loc[  (df_proc['train_flg'] == 1)  ]\n    \n# EmbargoCV\n    train_test_zip = get_time_series_cross_val_splits(df_proc, cv = n_fold, embargo = 3750)\n    print(\"entering time series cross validation loop\")\n    importances = []\n    oof_pred = []\n    oof_valid = []\n    \n    for split, train_test_split in enumerate(train_test_zip):\n        gc.collect()\n        \n        print(f\"doing split {split+1} out of {n_fold}\")\n        train_split, test_split = train_test_split\n        train_split_index = df_proc['timestamp'].isin(train_split)\n        test_split_index = df_proc['timestamp'].isin(test_split)\n    \n        train_dataset = lgb.Dataset(df_proc.loc[train_split_index, features],\n                                    df_proc.loc[train_split_index, f'Target_{asset_id}'].values, \n                                    feature_name = features, \n                                   )\n        val_dataset = lgb.Dataset(df_proc.loc[test_split_index, features], \n                                  df_proc.loc[test_split_index, f'Target_{asset_id}'].values, \n                                  feature_name = features, \n                                 )\n\n        print(f\"number of train data: {len(df_proc.loc[train_split_index])}\")\n        print(f\"number of val data:   {len(df_proc.loc[test_split_index])}\")\n\n        model = lgb.train(params = params,\n                          train_set = train_dataset, \n                          valid_sets=[train_dataset, val_dataset],\n                          valid_names=['tr', 'vl'],\n                          num_boost_round = 5000,\n                          verbose_eval = 100,     \n                          feval = correlation,\n                         )\n        importances.append(model.feature_importance(importance_type='gain'))\n        \n        file = f'trained_model_id{asset_id}_fold{split}.pkl'\n        pickle.dump(model, open(file, 'wb'))\n        print(f\"Trained model was saved to 'trained_model_id{asset_id}_fold{split}.pkl'\")\n        print(\"\")\n            \n        oof_pred += list(  model.predict(df_proc.loc[test_split_index, features])        )\n        oof_valid += list(   df_proc.loc[test_split_index, f'Target_{asset_id}'].values    )\n    \n    \n    plot_importance(np.array(importances),features, PLOT_TOP_N = 20, figsize=(10, 5))\n\n    return oof_pred, oof_valid","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:04:00.935813Z","iopub.execute_input":"2022-05-05T08:04:00.936143Z","iopub.status.idle":"2022-05-05T08:04:00.95387Z","shell.execute_reply.started":"2022-05-05T08:04:00.936115Z","shell.execute_reply":"2022-05-05T08:04:00.953004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = [ [] for id in range(14)   ]\n\nall_oof_pred = []\nall_oof_valid = []\nall_oof_weight = []\n\nfor asset_id, asset_name in zip(df_asset_details['Asset_ID'], df_asset_details['Asset_Name']):\n    print(f\"Training model for {asset_name:<16} (ID={asset_id:<2})\")\n    \n    oof_pred, oof_valid = get_Xy_and_model_for_asset(feat, asset_id)\n    \n    weight_temp = float( df_asset_details.loc[  df_asset_details['Asset_ID'] == asset_id  , 'Weight'   ]  )\n    \n    all_oof_pred += oof_pred\n    all_oof_valid += oof_valid\n    all_oof_weight += [weight_temp] * len(oof_pred)\n    \n    oof[asset_id] = corr_score(     np.array(oof_pred)   ,    np.array(oof_valid)    )\n    \n    print(f'OOF corr score of {asset_name} (ID={asset_id}) is {oof[asset_id]:.5f}. (Weight: {float(weight_temp):.5f})')\n    print('')\n    print('')","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:13:45.506551Z","iopub.execute_input":"2022-05-05T08:13:45.507152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ls -lh","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"woof = 0\nfor id in range(14):\n    woof += oof[id] * float(  df_asset_details.loc[  df_asset_details['Asset_ID'] == id  , 'Weight'   ] )\nwoof = woof / df_asset_details['Weight'].sum()\n\nprint(f'OOF corr scores are;')\nfor oof_score in oof:\n    print(f'      {oof_score:.5f}')\nprint(f'  simple average corr score: {np.mean(oof):.5f}.')\nprint(f'weighted average corr score: {woof:.5f}.')\nprint(f'')\n\nall_oof_wcorr = wcorr_score(     np.array(all_oof_pred),    np.array(all_oof_valid),  np.array(all_oof_weight)   )\nprint(f'        weighted corr score: {all_oof_wcorr:.5f}.')","metadata":{"execution":{"iopub.status.busy":"2022-05-05T08:13:23.41753Z","iopub.status.idle":"2022-05-05T08:13:23.41864Z","shell.execute_reply.started":"2022-05-05T08:13:23.418327Z","shell.execute_reply":"2022-05-05T08:13:23.418359Z"},"trusted":true},"execution_count":null,"outputs":[]}]}