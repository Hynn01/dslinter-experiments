{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tractü©ªSegm: Statistic ‚öñÔ∏è predictions\n\nThis is a simple statistical kernel, where we assume that all bodies are approximately aligned...\n\nThen we reconstruct all annotations in their 3D form and scale them to an average shape, and accumulate label appearance per each class.\n\nLater, when we are asked for a prediction, we take an average segmentation, thresholding over a given level of presence.\n\nAs an extension, we can accumulate these stats over time, so  each day will have its own accumulation and we can use linear extrapolation if needed.","metadata":{}},{"cell_type":"code","source":"import os, glob\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nDATASET_FOLDER = \"/kaggle/input/uw-madison-gi-tract-image-segmentation\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-27T02:08:49.630492Z","iopub.execute_input":"2022-04-27T02:08:49.630916Z","iopub.status.idle":"2022-04-27T02:08:49.660782Z","shell.execute_reply.started":"2022-04-27T02:08:49.63081Z","shell.execute_reply":"2022-04-27T02:08:49.66001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(os.path.join(DATASET_FOLDER, \"train.csv\"))\nprint(f\"size: {len(df_train)}\")\ndisplay(df_train.head(3))\n\ndf_ssub = pd.read_csv(os.path.join(DATASET_FOLDER, \"sample_submission.csv\"))\nprint(f\"size: {len(df_ssub)}\")\ndisplay(df_ssub.head(3))","metadata":{"execution":{"iopub.status.busy":"2022-04-27T02:08:49.662088Z","iopub.execute_input":"2022-04-27T02:08:49.662519Z","iopub.status.idle":"2022-04-27T02:08:50.378805Z","shell.execute_reply.started":"2022-04-27T02:08:49.662482Z","shell.execute_reply":"2022-04-27T02:08:50.378149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def enrich_data(df, sdir=\"train\"):\n    imgs = glob.glob(os.path.join(DATASET_FOLDER, sdir, \"case*\", \"case*_day*\", \"scans\", \"*.png\"))\n    img_folders = [os.path.dirname(p).split(os.path.sep) for p in imgs]\n    img_names = [os.path.splitext(os.path.basename(p))[0].split(\"_\") for p in imgs]\n    img_keys = [f\"{f[-2]}_slice_{n[1]}\" for f, n in zip(img_folders, img_names)]\n\n    print(img_keys[:5])\n    df[\"img_path\"] = df[\"id\"].map({k: p for k, p in zip(img_keys, imgs)})\n    df[\"Case_Day\"] = df[\"id\"].map({k: f[-2] for k, f in zip(img_keys, img_folders)})\n    df[\"Case\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\")[0].replace(\"case\", \"\")))\n    df[\"Day\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\")[1].replace(\"day\", \"\")))\n    df[\"Slice\"] = df[\"id\"].map({k: int(n[1]) for k, n in zip(img_keys, img_names)})\n    df[\"width\"] = df[\"id\"].map({k: int(n[2]) for k, n in zip(img_keys, img_names)})\n    df[\"height\"] = df[\"id\"].map({k: int(n[3]) for k, n in zip(img_keys, img_names)})\n    df[\"spacing1\"] = df[\"id\"].map({k: float(n[4]) for k, n in zip(img_keys, img_names)})\n    df[\"spacing2\"] = df[\"id\"].map({k: float(n[5]) for k, n in zip(img_keys, img_names)})","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-27T02:08:50.379789Z","iopub.execute_input":"2022-04-27T02:08:50.380544Z","iopub.status.idle":"2022-04-27T02:08:50.395453Z","shell.execute_reply.started":"2022-04-27T02:08:50.380495Z","shell.execute_reply":"2022-04-27T02:08:50.394546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enrich_data(df_train, \"train\")\ndisplay(df_train.head())\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\ndf_train.drop_duplicates(\"Case_Day\")[[\"height\", \"width\"]].value_counts().plot.bar(ax=axes[0], grid=True)\ndf_train.drop_duplicates(\"Case_Day\")[[\"spacing1\", \"spacing2\"]].value_counts().plot.bar(ax=axes[1], grid=True)\ndf_train[df_train[\"class\"] == \"stomach\"].groupby(\"Case_Day\").size().value_counts().plot.pie(ax=axes[2])","metadata":{"execution":{"iopub.status.busy":"2022-04-27T02:08:50.397542Z","iopub.execute_input":"2022-04-27T02:08:50.397793Z","iopub.status.idle":"2022-04-27T02:08:55.497375Z","shell.execute_reply.started":"2022-04-27T02:08:50.397765Z","shell.execute_reply":"2022-04-27T02:08:55.496268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enrich_data(df_ssub, \"test\")\ndisplay(df_ssub.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-27T02:08:55.498642Z","iopub.execute_input":"2022-04-27T02:08:55.498908Z","iopub.status.idle":"2022-04-27T02:08:55.520666Z","shell.execute_reply.started":"2022-04-27T02:08:55.498877Z","shell.execute_reply":"2022-04-27T02:08:55.519957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Aggregate cumulative map","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef rle_decode(rle, img = None, label = 1):\n    seq = rle.split()\n    starts = np.array(list(map(int, seq[0::2])))\n    lengths = np.array(list(map(int, seq[1::2])))\n    ends = starts + lengths\n    img_shape = img.shape\n    img = img.flatten()\n    for begin, end in zip(starts, ends):\n        img[begin:end] = label\n    return img.reshape(img_shape)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-27T02:08:55.521798Z","iopub.execute_input":"2022-04-27T02:08:55.522236Z","iopub.status.idle":"2022-04-27T02:08:55.534755Z","shell.execute_reply.started":"2022-04-27T02:08:55.522204Z","shell.execute_reply":"2022-04-27T02:08:55.533601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef interpolate_volume(volume, vol_size):\n    vol_shape = tuple(volume.shape)\n    if not vol_size:\n        d_new = min(vol_shape[:2])\n        vol_size = (vol_shape[0], vol_shape[1], d_new)\n    # assert vol_shape[0] == vol_shape[1], f\"mixed shape: {vol_shape}\"\n    if vol_shape == vol_size:\n        return volume\n    vol = F.interpolate(volume.unsqueeze(0).unsqueeze(0), size=vol_size, mode=\"nearest\")\n    return vol[0, 0]","metadata":{"execution":{"iopub.status.busy":"2022-04-27T02:08:55.536621Z","iopub.execute_input":"2022-04-27T02:08:55.537152Z","iopub.status.idle":"2022-04-27T02:08:56.825739Z","shell.execute_reply.started":"2022-04-27T02:08:55.537104Z","shell.execute_reply":"2022-04-27T02:08:56.825065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom tqdm.auto import tqdm\n\nAVG_VOLUME_SIZE = (144, 266, 266)\nsegms, counts = {}, {}\n\nfor lb, dfg in df_train.groupby(\"class\"):\n    print(lb)\n    vol_acc, nb_acc = {}, {}\n    for cd, dfgg in tqdm(dfg.groupby(\"Case_Day\")):\n        day = dfgg[\"Day\"].iloc[0]\n        h, w = dfgg[[\"height\", \"width\"]].iloc[0]\n        vol = np.zeros((len(dfgg), h, w))\n        for _, row in dfgg.iterrows():\n            idx = int(row['Slice']) - 1\n            rle = row[\"segmentation\"]\n            if not rle or not isinstance(rle, str):\n                continue\n            vol[idx, :, :] = rle_decode(rle, img=vol[idx, :, :], label=1)\n        vol = interpolate_volume(torch.tensor(vol), vol_size=AVG_VOLUME_SIZE).numpy()\n        vol_acc[day] = vol_acc.get(day, np.zeros(AVG_VOLUME_SIZE, dtype=np.uint16)) + vol\n        nb_acc[day] = nb_acc.get(day, 0) + 1\n    for d in vol_acc:\n        vol_acc[d] = vol_acc[d] / float(nb_acc[d])\n    segms[lb] = vol_acc\n    counts[lb] = nb_acc","metadata":{"execution":{"iopub.status.busy":"2022-04-27T02:08:56.827397Z","iopub.execute_input":"2022-04-27T02:08:56.82769Z","iopub.status.idle":"2022-04-27T02:11:49.97862Z","shell.execute_reply.started":"2022-04-27T02:08:56.827658Z","shell.execute_reply":"2022-04-27T02:11:49.977818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"days = sorted(segms[\"stomach\"].keys())\nprint(days)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T02:11:49.980855Z","iopub.execute_input":"2022-04-27T02:11:49.981258Z","iopub.status.idle":"2022-04-27T02:11:49.986853Z","shell.execute_reply.started":"2022-04-27T02:11:49.981195Z","shell.execute_reply":"2022-04-27T02:11:49.985902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ipywidgets import interact, IntSlider\n\ndef show_volumes(segms, counts, z, y, x, day=10, fig_size=(12, 12)):\n    day = sorted(segms[\"stomach\"])[day]\n    fig, axarr = plt.subplots(nrows=3, ncols=3, figsize=fig_size)\n    for i, lb in enumerate(segms):\n        for j in range(3):\n            axarr[i, j].set_title(f\"{lb};\\n day:{day} with cases:{counts[lb][day]}\\n sum over axis {j}\")\n            im = axarr[i, j].imshow(np.sum(segms[lb][day], axis=j) / segms[lb][day].shape[j])\n            fig.colorbar(im, ax= axarr[i, j])\n\ndef interactive_show_cum(segms, counts):\n    interact(\n        lambda z, y, x, day: plt.show(show_volumes(segms, counts, z, y, x, day)),\n        z=IntSlider(min=0, max=AVG_VOLUME_SIZE[0], step=5, value=int(AVG_VOLUME_SIZE[0] / 2)),\n        y=IntSlider(min=0, max=AVG_VOLUME_SIZE[1], step=5, value=int(AVG_VOLUME_SIZE[1] / 2)),\n        x=IntSlider(min=0, max=AVG_VOLUME_SIZE[2], step=5, value=int(AVG_VOLUME_SIZE[2] / 2)),\n        day=IntSlider(min=0, max=len(days), step=1, value=0),\n    )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-27T02:11:49.988335Z","iopub.execute_input":"2022-04-27T02:11:49.988603Z","iopub.status.idle":"2022-04-27T02:11:50.002192Z","shell.execute_reply.started":"2022-04-27T02:11:49.988565Z","shell.execute_reply":"2022-04-27T02:11:50.000905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interactive_show_cum(segms, counts)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T02:11:50.004256Z","iopub.execute_input":"2022-04-27T02:11:50.005147Z","iopub.status.idle":"2022-04-27T02:11:51.856351Z","shell.execute_reply.started":"2022-04-27T02:11:50.005102Z","shell.execute_reply":"2022-04-27T02:11:51.855678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ipywidgets import interact, IntSlider, FloatSlider\n\nLABELS = sorted(df_train['class'].unique())\n\ndef show_volume(segms, z, y, x, day=0, thr=0.05, fig_size=(14, 14)):\n    day = sorted(segms[\"stomach\"])[day]\n    fig, axarr = plt.subplots(nrows=2, ncols=2, figsize=(9, 9))\n    \n    segms_lb = [np.ones(AVG_VOLUME_SIZE) * thr]\n    segms_lb += [segms[lb][day] for lb in LABELS]\n    segm = np.argmax(segms_lb, axis=0)\n    #print(segm.shape)\n    \n    _imshow_args = dict(cmap=\"jet\", interpolation=\"antialiased\", interpolation_stage=\"rgb\", vmin=0, vmax=len(segms))\n    axarr[0, 0].imshow(segm[z, :, :], **_imshow_args)\n    axarr[0, 1].set_title(f\"day:{day} with cases:{counts[lb][day]}\")\n    axarr[0, 1].imshow(segm[:, :, x], **_imshow_args)\n    axarr[1, 0].set_title(f\"day:{day} with cases:{counts[lb][day]}\")\n    axarr[1, 0].imshow(segm[:, y, :], **_imshow_args)\n    axarr[1, 1].set_axis_off()\n    fig.tight_layout()\n\n\ndef interactive_show_segm(segms):\n    interact(\n        lambda z, y, x, day, thr: plt.show(show_volume(segms, z, y, x, day, thr)),\n        z=IntSlider(min=0, max=AVG_VOLUME_SIZE[0], step=5, value=int(AVG_VOLUME_SIZE[0] / 2)),\n        y=IntSlider(min=0, max=AVG_VOLUME_SIZE[1], step=5, value=int(AVG_VOLUME_SIZE[1] / 2)),\n        x=IntSlider(min=0, max=AVG_VOLUME_SIZE[2], step=5, value=int(AVG_VOLUME_SIZE[2] / 2)),\n        day=IntSlider(min=0, max=len(days), step=1, value=0),\n        thr=FloatSlider(min=0, max=1, step=0.05, value=0.1),\n    )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-27T02:11:51.857394Z","iopub.execute_input":"2022-04-27T02:11:51.858107Z","iopub.status.idle":"2022-04-27T02:11:51.885489Z","shell.execute_reply.started":"2022-04-27T02:11:51.858063Z","shell.execute_reply":"2022-04-27T02:11:51.884549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interactive_show_segm(segms)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T02:11:51.886816Z","iopub.execute_input":"2022-04-27T02:11:51.88706Z","iopub.status.idle":"2022-04-27T02:11:53.012823Z","shell.execute_reply.started":"2022-04-27T02:11:51.887033Z","shell.execute_reply":"2022-04-27T02:11:53.011946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extrapolate üîé predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-24T08:20:53.125743Z","iopub.execute_input":"2022-04-24T08:20:53.126152Z","iopub.status.idle":"2022-04-24T08:20:53.130204Z","shell.execute_reply.started":"2022-04-24T08:20:53.126122Z","shell.execute_reply":"2022-04-24T08:20:53.128865Z"}}},{"cell_type":"code","source":"def rle_encode(mask, bg = 0) -> dict:\n    vec = mask.flatten()\n    nb = len(vec)\n    where = np.flatnonzero\n    starts = np.r_[0, where(~np.isclose(vec[1:], vec[:-1], equal_nan=True)) + 1]\n    lengths = np.diff(np.r_[starts, nb])\n    values = vec[starts]\n    assert len(starts) == len(lengths) == len(values)\n    rle = {}\n    for start, length, val in zip(starts, lengths, values):\n        if val == bg:\n            continue\n        rle[val] = rle.get(val, []) + [str(start), length]\n    # post-processing\n    rle = {lb: \" \".join(map(str, id_lens)) for lb, id_lens in rle.items()}\n    return rle","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-27T02:11:53.014039Z","iopub.execute_input":"2022-04-27T02:11:53.014304Z","iopub.status.idle":"2022-04-27T02:11:53.023097Z","shell.execute_reply.started":"2022-04-27T02:11:53.014269Z","shell.execute_reply":"2022-04-27T02:11:53.022439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def _process_slice(row, vol, lb):\n#     idx = int(row['Slice']) - 1\n#     mask = vol[idx, :, :]\n#     rle = rle_encode(mask)[1] if np.sum(mask) > 1 else None\n#     return {\"id\": row[\"id\"], \"class\": lb, \"predicted\": rle}\n\n\ndef _interpolate_day(segm_lb, day):\n    if day in segm_lb:\n        return segm_lb[day]\n    days_last = [d for d in segm_lb.keys() if d < day]\n    day_last = max(days_last)\n    days_next = [d for d in segm_lb.keys() if d > day]\n    if not days_next:\n        return segm_lb[day_last]\n    day_next = min(days_next)\n    dn = (day - day_last) / (day_next - day_last)\n    dl = (day_next - day) / (day_next - day_last)\n    return dl * segm_lb[day_last] + dn * segm_lb[day_next]\n\n\ndef _process_vol(dfgg, segm, thr, lb):\n    day = int(dfgg[[\"Day\"]].iloc[0])\n    h, w = dfgg[[\"height\", \"width\"]].iloc[0]\n    vol = interpolate_volume(\n        torch.tensor(_interpolate_day(segm, day) > thr, dtype=float),\n        vol_size=(len(dfgg), h, w),\n    ).numpy().astype(np.uint8)\n    rows = []\n    for _, row in dfgg.iterrows():\n        idx = int(row['Slice']) - 1\n        mask = vol[idx, :, :]\n        rle = rle_encode(mask)[1] if np.sum(mask) > 1 else \"\"\n        rows.append({\"id\": row[\"id\"], \"class\": lb, \"predicted\": rle})\n    return rows","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-27T02:11:53.024554Z","iopub.execute_input":"2022-04-27T02:11:53.024971Z","iopub.status.idle":"2022-04-27T02:11:53.044632Z","shell.execute_reply.started":"2022-04-27T02:11:53.02494Z","shell.execute_reply":"2022-04-27T02:11:53.043606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"segm_thr = 0.03\ndf_pred = df_train if df_ssub.empty else df_ssub\n\npreds = []\nfor _, dfg in tqdm(df_pred.groupby(\"Case_Day\")):\n    day = int(dfg[[\"Day\"]].iloc[0])\n    segms_lb = [np.ones(AVG_VOLUME_SIZE) * segm_thr]\n    segms_lb += [_interpolate_day(segms[lb], day) for lb in LABELS]\n    segm_sc = np.argmax(segms_lb, axis=0)\n    h, w = dfg[[\"height\", \"width\"]].iloc[0]\n    dfgg = dfg[dfg[\"class\"] == LABELS[0]]\n    vol = interpolate_volume(torch.tensor(segm_sc, dtype=float), vol_size=(len(dfgg), h, w)).numpy().astype(np.uint8)\n    for _, row in dfgg.iterrows():\n        idx = int(row['Slice']) - 1\n        mask = vol[idx, :, :]\n        rle = rle_encode(mask) if np.sum(mask) > 1 else {}\n        preds += [{\"id\": row[\"id\"], \"class\": lb, \"predicted\": rle.get(i + 1, \"\")} for i, lb in enumerate(LABELS)]\n\n\ndf_pred = pd.DataFrame(preds)\ndisplay(df_pred[df_pred['predicted'] != \"\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-27T02:11:53.045708Z","iopub.execute_input":"2022-04-27T02:11:53.04636Z","iopub.status.idle":"2022-04-27T02:16:42.467864Z","shell.execute_reply.started":"2022-04-27T02:11:53.046323Z","shell.execute_reply":"2022-04-27T02:16:42.466836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare üóÑÔ∏è submission","metadata":{}},{"cell_type":"code","source":"del df_ssub['predicted']\ndf_pred = df_ssub.merge(df_pred, on=['id','class'])\n\ndf_pred[['id', 'class', 'predicted']].to_csv(\"submission.csv\", index=False)\n\n!head submission.csv","metadata":{"execution":{"iopub.status.busy":"2022-04-27T02:16:42.469326Z","iopub.execute_input":"2022-04-27T02:16:42.469593Z","iopub.status.idle":"2022-04-27T02:16:43.312511Z","shell.execute_reply.started":"2022-04-27T02:16:42.469562Z","shell.execute_reply":"2022-04-27T02:16:43.31142Z"},"trusted":true},"execution_count":null,"outputs":[]}]}