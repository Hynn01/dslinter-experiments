{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-04T03:12:52.166807Z","iopub.execute_input":"2022-05-04T03:12:52.167385Z","iopub.status.idle":"2022-05-04T03:12:53.398838Z","shell.execute_reply.started":"2022-05-04T03:12:52.167269Z","shell.execute_reply":"2022-05-04T03:12:53.397079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. House Prices?\n\nHouse Prices Project is one of well known datasets. The target is to predict a house price with many explanatory variables, such as LotArea, YearBuilt, and so on. Basically, the dataset is made of train and test data sets. With 79 variables, I have to build my model to forecast individual house price and submit my prediction over test dataset. Let's start!\n\nFirst of all, I import the data set. Let's look at basic stats.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv', index_col='Id')\n\ntrain.info()\ntrain.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:12:56.678542Z","iopub.execute_input":"2022-05-04T03:12:56.678849Z","iopub.status.idle":"2022-05-04T03:12:56.900224Z","shell.execute_reply.started":"2022-05-04T03:12:56.678818Z","shell.execute_reply":"2022-05-04T03:12:56.899149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. First Model\n\nAt first, let's build basic model and get RMSE value. RMSE (Root Mean Square Error) will be my criteria, which will determine whether my treatment is valuable. If RMSE value gets higher after I make a adjustment, I have to drop the decision.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#print(train.info())\ny_1 = train[\"SalePrice\"]\ntrain_1 = train.copy()\ntrain_1.drop([\"SalePrice\"], axis=1, inplace=True)\n\nX_train_1, X_valid_1, y_train_1, y_valid_1 = train_test_split(train_1, y_1,\n                                              train_size=0.8, test_size=0.2,random_state=0)\nprint(\"\\nX_train_1 shape:\",X_train_1.shape)\nprint(\"\\nX_valid_1 shape:\",X_valid_1.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:01.603462Z","iopub.execute_input":"2022-05-04T03:13:01.603751Z","iopub.status.idle":"2022-05-04T03:13:01.699596Z","shell.execute_reply.started":"2022-05-04T03:13:01.603722Z","shell.execute_reply":"2022-05-04T03:13:01.698635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndef rmse_score(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    mse = mean_squared_error(y_valid, preds)\n    return np.sqrt(mse)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:04.142206Z","iopub.execute_input":"2022-05-04T03:13:04.142506Z","iopub.status.idle":"2022-05-04T03:13:04.375003Z","shell.execute_reply.started":"2022-05-04T03:13:04.142477Z","shell.execute_reply":"2022-05-04T03:13:04.373897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To obtain my first RMSE, I have to delete categorical columns. Unlike R, python panda has no tools for categorical data. For none numberic data, I have some options, but to start easily, at first I get my first RMSE without categorical data.","metadata":{}},{"cell_type":"code","source":"X_train_1 = X_train_1.select_dtypes(exclude=['object'])\nX_valid_1 = X_valid_1.select_dtypes(exclude=['object'])\n\n#X_train_1","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:06.765121Z","iopub.execute_input":"2022-05-04T03:13:06.76557Z","iopub.status.idle":"2022-05-04T03:13:06.777172Z","shell.execute_reply.started":"2022-05-04T03:13:06.765525Z","shell.execute_reply":"2022-05-04T03:13:06.776198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, I have to check whether there is any missing values. If there is any missing column, I have to consider an optional way to deal with.","metadata":{}},{"cell_type":"code","source":"missing_val_count_by_column = (X_train_1.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:08.754464Z","iopub.execute_input":"2022-05-04T03:13:08.754776Z","iopub.status.idle":"2022-05-04T03:13:08.764284Z","shell.execute_reply.started":"2022-05-04T03:13:08.754744Z","shell.execute_reply":"2022-05-04T03:13:08.762999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the check, I conclude that there are 3 columns with missing values. So, I have to consider a way - imputation. I will use \"simple Imputer\", which will fill missing values with the mean value along each column.","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer() \nX_train_1 =  pd.DataFrame(my_imputer.fit_transform(X_train_1))\nX_valid_1 =  pd.DataFrame(my_imputer.transform(X_valid_1))\n\n#X_train_1\n#X_valid_1","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:11.466603Z","iopub.execute_input":"2022-05-04T03:13:11.466911Z","iopub.status.idle":"2022-05-04T03:13:11.48794Z","shell.execute_reply.started":"2022-05-04T03:13:11.466879Z","shell.execute_reply":"2022-05-04T03:13:11.486915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"1st RMSE Score:\",rmse_score(X_train_1, X_valid_1, y_train_1, y_valid_1))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:14.206351Z","iopub.execute_input":"2022-05-04T03:13:14.206628Z","iopub.status.idle":"2022-05-04T03:13:15.458968Z","shell.execute_reply.started":"2022-05-04T03:13:14.206599Z","shell.execute_reply":"2022-05-04T03:13:15.458026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Dependent Variable\n\nIn my project, dependent variable is \"SalePrice\". The goal of this project is to predict the sale price of a housewith various data features. Let's check the \"SalePrice\" variable.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.histplot(y_1,kde=True)\nplt.show()\nprint(\"SalePrice's Mean :\",y_1.mean())\nprint(\"SalePrice's Median :\",y_1.median())\nprint(\"SalePrice's Mode :\",y_1.mode(dropna=True))\n#","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:17.440102Z","iopub.execute_input":"2022-05-04T03:13:17.440607Z","iopub.status.idle":"2022-05-04T03:13:17.94004Z","shell.execute_reply.started":"2022-05-04T03:13:17.44057Z","shell.execute_reply":"2022-05-04T03:13:17.938912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I think dependent variable, 'SalePrice' is right skewed. Mean value is much bigger thant mode, (180,921 > 140,000). Therefore I need some scaling process over 'SalePrice' variable. I will apply log transformation. Let's build model with transformation of y variable.","metadata":{}},{"cell_type":"code","source":"y_2 = np.log1p(y_1)\nsns.histplot(y_2,kde=True)\nplt.show()\nprint(\"Transformed SalePrice's Mean :\",y_2.mean())\nprint(\"Transformed SalePrice's Median :\",y_2.median())\nprint(\"Transformed SalePrice's Mode :\",y_2.mode(dropna=True))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:20.822781Z","iopub.execute_input":"2022-05-04T03:13:20.823074Z","iopub.status.idle":"2022-05-04T03:13:21.132267Z","shell.execute_reply.started":"2022-05-04T03:13:20.823044Z","shell.execute_reply":"2022-05-04T03:13:21.131234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After log transforamtion, mean, median, and mode value of \"SalePrice\" is close and the shape is much more like normal distribution.","metadata":{}},{"cell_type":"code","source":"# No change for X variables\nX_train_2 = X_train_1 \nX_valid_2 = X_valid_1 \n# log transformation for y variable\ny_train_2 = np.log1p(y_train_1)\ny_valid_2 = np.log1p(y_valid_1)\n\nprint(X_train_2.shape)\nprint(X_valid_2.shape)\nprint(y_train_2.shape)\nprint(y_valid_2.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:24.096745Z","iopub.execute_input":"2022-05-04T03:13:24.097129Z","iopub.status.idle":"2022-05-04T03:13:24.105112Z","shell.execute_reply.started":"2022-05-04T03:13:24.097089Z","shell.execute_reply":"2022-05-04T03:13:24.104469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"2nd RMSE Score:\",rmse_score(X_train_2, X_valid_2, y_train_2, y_valid_2))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:26.511366Z","iopub.execute_input":"2022-05-04T03:13:26.51225Z","iopub.status.idle":"2022-05-04T03:13:27.738744Z","shell.execute_reply.started":"2022-05-04T03:13:26.5122Z","shell.execute_reply":"2022-05-04T03:13:27.737726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Explanatory Variables\n\nIn my project, the dataset has total 79 independent variables. At first, I think categorical data has to be dealt with. Let's check how many non-numberic variables and null values.","metadata":{}},{"cell_type":"code","source":"print(train.dtypes.value_counts(),'\\n\\n')\n\nfor i in train.select_dtypes(\"object\"):\n    if(train[i].isnull().sum()>0):\n        print(i,train[i].isnull().sum())    \n    \nprint(\"Total number of train dataset:\",train.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:30.042551Z","iopub.execute_input":"2022-05-04T03:13:30.043519Z","iopub.status.idle":"2022-05-04T03:13:30.086166Z","shell.execute_reply.started":"2022-05-04T03:13:30.04347Z","shell.execute_reply":"2022-05-04T03:13:30.085252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After reviewing null values, I think that some features should be removed. For \"MiscFeature\" variable, 1,406 values are null. Considering that total count of train set is 1,460, I conclude that too many null value will not make any difference over my research. Therefore I have to delete some features, such as \"MiscFeature\", \"PoolQC\", \"Alley\" and \"Fence\". Let's move the next step without those four variables.  ","metadata":{}},{"cell_type":"code","source":"drop_columns = ['MiscFeature','PoolQC','Alley','Fence']\ntrain_3 = train.drop(drop_columns,axis=1).copy()\nprint(train_3.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:32.823971Z","iopub.execute_input":"2022-05-04T03:13:32.824478Z","iopub.status.idle":"2022-05-04T03:13:32.832305Z","shell.execute_reply.started":"2022-05-04T03:13:32.824443Z","shell.execute_reply":"2022-05-04T03:13:32.83116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. One-hot encoding for categorical data\n\nOf techniques to deal with categorical data, one-hot encoding is my choice. If I need to improve my model, I will consider other options, ordinal encoding. At first, I will start with one-hot encoding.","metadata":{}},{"cell_type":"code","source":"y_3 = y_2\ntrain_3.drop([\"SalePrice\"], axis=1, inplace=True)\n\nX_train_3, X_valid_3, y_train_3, y_valid_3 = train_test_split(train_3, y_3,\n                                       train_size=0.8, test_size=0.2, random_state=0)\n\nprint(X_train_3.shape)\nprint(X_valid_3.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:35.570719Z","iopub.execute_input":"2022-05-04T03:13:35.571124Z","iopub.status.idle":"2022-05-04T03:13:35.583765Z","shell.execute_reply.started":"2022-05-04T03:13:35.571089Z","shell.execute_reply":"2022-05-04T03:13:35.582938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nobject_cols = [col for col in X_train_3.columns if X_train_3[col].dtype == \"object\"]\nnumberic_cols = [col for col in X_train_3.columns if X_train_3[col].dtype != \"object\"]\n\n# imputer : fill missing value with mean of the column\nmy_imputer_2 = SimpleImputer()\nX_train_3[numberic_cols] = my_imputer_2.fit_transform(X_train_3[numberic_cols])\nX_valid_3[numberic_cols] = my_imputer_2.transform(X_valid_3[numberic_cols])\n\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train_3[object_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid_3[object_cols]))\n\nOH_cols_train.index = X_train_3.index\nOH_cols_valid.index = X_valid_3.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train_3.drop(object_cols, axis=1)\nnum_X_valid = X_valid_3.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\nX_train_3 = OH_X_train\nX_valid_3 = OH_X_valid\n\nprint(\"\\nX_train_3 shape:\",X_train_3.shape)\nprint(\"\\nX_valid_3 shape:\",X_valid_3.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:38.124666Z","iopub.execute_input":"2022-05-04T03:13:38.125143Z","iopub.status.idle":"2022-05-04T03:13:38.213629Z","shell.execute_reply.started":"2022-05-04T03:13:38.125109Z","shell.execute_reply":"2022-05-04T03:13:38.212954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RMSE with log-transformed y and one-hot Encoding\nprint(\"3rd RMSE Score:\",rmse_score(X_train_3, X_valid_3, y_train_3, y_valid_3))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:41.84152Z","iopub.execute_input":"2022-05-04T03:13:41.841995Z","iopub.status.idle":"2022-05-04T03:13:44.389539Z","shell.execute_reply.started":"2022-05-04T03:13:41.841962Z","shell.execute_reply":"2022-05-04T03:13:44.388586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparing between 2nd and 3rd RMSE, I will use 3rd RMSE as benchmark value. Because 2nd RMSE is calculated without any categorical variables, 3rd RMSE is a much better value, which is based on the model with none-numberic features.","metadata":{}},{"cell_type":"markdown","source":"# 6. Numberic Variables - Skewness\n\nLet's check number variables.","metadata":{}},{"cell_type":"code","source":"print(train.dtypes.value_counts(),'\\n\\n')\nfrom scipy.stats import skew\n\nfeatures_index = train.dtypes[train.dtypes != 'object'].index\nskew_values = train[features_index].apply(lambda x : skew(x))\nskew_values","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:48.778335Z","iopub.execute_input":"2022-05-04T03:13:48.779496Z","iopub.status.idle":"2022-05-04T03:13:48.80574Z","shell.execute_reply.started":"2022-05-04T03:13:48.779438Z","shell.execute_reply":"2022-05-04T03:13:48.805044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After reviewing skew values, I think some variables - MiscVal, LotArea, PoolArea - has huge positive skewness values. There are some negative values, but I think they are okay. I have to do something for some big positive-valued variables which are right skewed.","metadata":{}},{"cell_type":"code","source":"skew_value_more_than_3 = skew_values[skew_values >= 4]\nprint(skew_value_more_than_3.sort_values(ascending=False))\n\nskewed_variables = skew_value_more_than_3.index\nprint('\\n\\n',skewed_variables)\n\nX_train_4 = X_train_3.copy()\nX_valid_4 = X_valid_3.copy()\ny_train_4 = y_train_3.copy()\ny_valid_4 = y_valid_3.copy()\n\nX_train_4[skewed_variables] = np.log1p(X_train_4[skewed_variables])\nX_valid_4[skewed_variables] = np.log1p(X_valid_4[skewed_variables])","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:51.825221Z","iopub.execute_input":"2022-05-04T03:13:51.826132Z","iopub.status.idle":"2022-05-04T03:13:51.843261Z","shell.execute_reply.started":"2022-05-04T03:13:51.826081Z","shell.execute_reply":"2022-05-04T03:13:51.842339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, I think that 9 variables - MiscVal, PoolArea, LotArea, and so on - are right skewed so try to fix the problem with log transformation. Let's see what happen.","metadata":{}},{"cell_type":"code","source":"# RMSE : log-transformed y & one-hot Encoding & log-transformed skewed variables\nprint(\"4th RMSE Score:\",rmse_score(X_train_4, X_valid_4, y_train_4, y_valid_4))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:54.705191Z","iopub.execute_input":"2022-05-04T03:13:54.705566Z","iopub.status.idle":"2022-05-04T03:13:57.297129Z","shell.execute_reply.started":"2022-05-04T03:13:54.705518Z","shell.execute_reply":"2022-05-04T03:13:57.296226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparing with 3rd RMSE value, 0.138766, I think that 4th RMSE value, 0.138814 is slight high. I think, however, that the difference is quite small. So I will stick to this transformation. I hope the log transformation can improve my model.","metadata":{}},{"cell_type":"markdown","source":"# 7. Numberic Variables - Outliers\n\nLet's check scatter plot and outliers of number variables.","metadata":{}},{"cell_type":"code","source":"X_train_5 = X_train_4.copy()\nX_valid_5 = X_valid_4.copy()\ny_train_5 = y_train_4.copy()\ny_valid_5 = y_valid_4.copy()\n\n#for i in X_train_5[numberic_cols]:\n#    plt.scatter(x = X_train_5[i], y = y_train_5)\n#    plt.ylabel('SalePrice', fontsize=15)\n#    plt.xlabel(i, fontsize=15)\n#    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:13:59.944665Z","iopub.execute_input":"2022-05-04T03:13:59.945022Z","iopub.status.idle":"2022-05-04T03:13:59.951756Z","shell.execute_reply.started":"2022-05-04T03:13:59.94498Z","shell.execute_reply":"2022-05-04T03:13:59.950976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At first, let's look over \"LotFrontage\" variable.","metadata":{}},{"cell_type":"code","source":"plt.scatter(x = X_train_5[\"LotFrontage\"], y = y_train_5)\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel(\"LotFrontage\", fontsize=15)\nplt.show()\nprint(y_train_5[(X_train_5.LotFrontage>=150)])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:14:02.523079Z","iopub.execute_input":"2022-05-04T03:14:02.523859Z","iopub.status.idle":"2022-05-04T03:14:02.754267Z","shell.execute_reply.started":"2022-05-04T03:14:02.523722Z","shell.execute_reply":"2022-05-04T03:14:02.753292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I think that \"index number 1338\" item should be removed. I have few domain knowledge about property market, but I think the bigger lot frontage the higher sale price.\n\nLet's move to \"GrLivArea\" variable.","metadata":{}},{"cell_type":"code","source":"plt.scatter(x = X_train_5[\"GrLivArea\"], y = y_train_5)\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel(\"GrLivArea\", fontsize=15)\nplt.show()\nprint(y_train_5[(X_train_5.GrLivArea>=4000)])","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:14:05.96383Z","iopub.execute_input":"2022-05-04T03:14:05.964202Z","iopub.status.idle":"2022-05-04T03:14:06.16574Z","shell.execute_reply.started":"2022-05-04T03:14:05.964161Z","shell.execute_reply":"2022-05-04T03:14:06.164557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I think that \"index number 524\" item should be removed, because I think the bigger Ground Living Area the higher sale price.\n\nLet's move to \"OpenPorchSF\" variable.","metadata":{}},{"cell_type":"code","source":"plt.scatter(x = X_train_5[\"OpenPorchSF\"], y = y_train_5)\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel(\"OpenPorchSF\", fontsize=15)\nplt.show()\nprint(y_train_5[(X_train_5.OpenPorchSF>=500)])","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:14:09.301356Z","iopub.execute_input":"2022-05-04T03:14:09.301718Z","iopub.status.idle":"2022-05-04T03:14:09.520792Z","shell.execute_reply.started":"2022-05-04T03:14:09.301682Z","shell.execute_reply":"2022-05-04T03:14:09.519601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I think that \"index number 496\" item should be removed, because I think the open porch area the higher sale price.","metadata":{}},{"cell_type":"code","source":"#for i in train.select_dtypes(\"object\"):\n#    sns.catplot(x=i, y='SalePrice', data=train)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T00:40:48.033353Z","iopub.execute_input":"2022-05-04T00:40:48.034181Z","iopub.status.idle":"2022-05-04T00:40:48.038062Z","shell.execute_reply.started":"2022-05-04T00:40:48.034138Z","shell.execute_reply":"2022-05-04T00:40:48.037346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, through checking outlier process, I will delete total 3 items - 1338, 524, and 496 by index number. I remove those data to improve my model. Let's compare RMSE numbers!","metadata":{}},{"cell_type":"code","source":"#X_train_5 = X_train_4.copy()\n#X_valid_5 = X_valid_4.copy()\n#y_train_5 = y_train_4.copy()\n#y_valid_5 = y_valid_4.copy()\n\nprint(X_train_5.shape)\nprint(y_train_5.shape)\n\ndrop_index = [1338,524,496]\nX_train_5.drop(drop_index,axis=0,inplace=True)\ny_train_5.drop(drop_index,axis=0,inplace=True)\n\nprint(X_train_5.shape)\nprint(y_train_5.shape)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:14:13.615804Z","iopub.execute_input":"2022-05-04T03:14:13.616222Z","iopub.status.idle":"2022-05-04T03:14:13.627309Z","shell.execute_reply.started":"2022-05-04T03:14:13.616189Z","shell.execute_reply":"2022-05-04T03:14:13.626021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# log-trans Y & one-hot encoding & log-trans X's & deleting outliers\nprint(\"5th RMSE Score:\",rmse_score(X_train_5, X_valid_5, y_train_5, y_valid_5))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:14:16.423137Z","iopub.execute_input":"2022-05-04T03:14:16.423905Z","iopub.status.idle":"2022-05-04T03:14:19.08503Z","shell.execute_reply.started":"2022-05-04T03:14:16.42386Z","shell.execute_reply":"2022-05-04T03:14:19.084003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After deleting some outliers, I did not make any improvement RMSE 0.1435 - from previous 0.1388 (Model 4). In this case, I have to consider more carefully deleting outliers.","metadata":{}},{"cell_type":"markdown","source":"# 8. Final Model - 1\n\nI will prepare for my final model to submission.","metadata":{}},{"cell_type":"code","source":"#model = RandomForestRegressor(n_estimators=100, random_state=0)\n#model.fit(X_train_4, y_train_4)\n\n#X_test = test.drop(drop_columns,axis=1)\n#print(X_test.shape)\n\n#X_test[numberic_cols] = my_imputer_2.transform(X_test[numberic_cols])\n#OH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[object_cols]))\n#OH_cols_test.index = X_test.index\n#num_X_test = X_test.drop(object_cols, axis=1)\n\n#X_test_final = pd.concat([num_X_test, OH_cols_test], axis=1)\n#preds_test = model.predict(X_test_final)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:14:22.674067Z","iopub.execute_input":"2022-05-04T03:14:22.67461Z","iopub.status.idle":"2022-05-04T03:14:25.293354Z","shell.execute_reply.started":"2022-05-04T03:14:22.674574Z","shell.execute_reply":"2022-05-04T03:14:25.292174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(preds_test.shape)\n#test_result = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n#output = pd.DataFrame({'Id': test_result.Id,'SalePrice': preds_test})\n#output.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:22:01.70074Z","iopub.execute_input":"2022-04-29T16:22:01.701268Z","iopub.status.idle":"2022-04-29T16:22:01.746866Z","shell.execute_reply.started":"2022-04-29T16:22:01.701204Z","shell.execute_reply":"2022-04-29T16:22:01.746077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Regularization - Rigde & Lasso Regression\n\nRidge Regression was introduced to deal with the case in which linearly independent variables are highly correlated. I think regularization method can be applied to this project using trade-off between variance and bias. So, I will try Ridge Regression.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\n# To compare with the best model's value, I use X_train_4 data set\nridge = Ridge()\nridge.fit(X_train_4, y_train_4)\npred_ridge = ridge.predict(X_valid_4)\nmse_ridge = mean_squared_error(y_valid_4 , pred_ridge)\nrmse_rideg = np.sqrt(mse_ridge)\n\nprint(\"6th RMSE Score:\",rmse_rideg)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:14:28.49004Z","iopub.execute_input":"2022-05-04T03:14:28.490353Z","iopub.status.idle":"2022-05-04T03:14:28.546846Z","shell.execute_reply.started":"2022-05-04T03:14:28.490301Z","shell.execute_reply":"2022-05-04T03:14:28.545486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparams_ridge =  {'alpha':[0.05,0.1,0,5,7,10,15,20,30,50]}\nmodel_ridge_params= GridSearchCV(ridge,param_grid=params_ridge,scoring='neg_mean_squared_error')\nmodel_ridge_params.fit(X_train_4, y_train_4)\nprint('Best Score:',model_ridge_params.best_score_)\nprint('Best Parameter value:',model_ridge_params.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:14:32.528659Z","iopub.execute_input":"2022-05-04T03:14:32.52915Z","iopub.status.idle":"2022-05-04T03:14:36.222799Z","shell.execute_reply.started":"2022-05-04T03:14:32.529119Z","shell.execute_reply":"2022-05-04T03:14:36.221657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridge_1 = Ridge(alpha=10)\nridge_1.fit(X_train_4, y_train_4)\npred_ridge_1 = ridge_1.predict(X_valid_4)\nmse_ridge_1 = mean_squared_error(y_valid_4 , pred_ridge)\nrmse_rideg_1 = np.sqrt(mse_ridge_1)\n\nprint(\"6th RMSE Score:\",rmse_rideg_1)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:14:39.345987Z","iopub.execute_input":"2022-05-04T03:14:39.346349Z","iopub.status.idle":"2022-05-04T03:14:39.396648Z","shell.execute_reply.started":"2022-05-04T03:14:39.346298Z","shell.execute_reply":"2022-05-04T03:14:39.395487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After applying Ridge Regression, I get RMSE value, 0.2001, bigger than Model 4 value, 0.1388. At first, I tried with default parameters. Next, I try to find out the proper regularization strength with \"GridSearchCV\". After that, I tried once more ridge regression with alpha value, 10. The result, however, is almost the same.\n\nLet's try another Regularization method, Lasso Regression.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\n# To compare with the best model's value, I use X_train_4 data set\nlasso = Lasso()\nlasso.fit(X_train_4, y_train_4)\npred_lasso = lasso.predict(X_valid_4)\nmse_lasso = mean_squared_error(y_valid_4 , pred_lasso)\nrmse_lasso = np.sqrt(mse_lasso)\n\nprint(\"7th RMSE Score:\",rmse_lasso)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:14:42.834424Z","iopub.execute_input":"2022-05-04T03:14:42.834959Z","iopub.status.idle":"2022-05-04T03:14:42.906108Z","shell.execute_reply.started":"2022-05-04T03:14:42.834901Z","shell.execute_reply":"2022-05-04T03:14:42.904782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_lasso =  {'alpha':[0.001, 0.005, 0.01, 0.05, 0.03, 0.1, 0.5, 1]}\nmodel_lasso_params= GridSearchCV(lasso,param_grid=params_lasso,scoring='neg_mean_squared_error')\nmodel_lasso_params.fit(X_train_4, y_train_4)\nprint('Best Score:',model_lasso_params.best_score_)\nprint('Best Parameter value:',model_lasso_params.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:14:45.739372Z","iopub.execute_input":"2022-05-04T03:14:45.739717Z","iopub.status.idle":"2022-05-04T03:14:51.932246Z","shell.execute_reply.started":"2022-05-04T03:14:45.739682Z","shell.execute_reply":"2022-05-04T03:14:51.931198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lasso = Lasso(alpha=0.001)\nlasso.fit(X_train_4, y_train_4)\npred_lasso = lasso.predict(X_valid_4)\nmse_lasso = mean_squared_error(y_valid_4 , pred_lasso)\nrmse_lasso = np.sqrt(mse_lasso)\n\nprint(\"7th RMSE Score:\",rmse_lasso)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T03:14:54.314247Z","iopub.execute_input":"2022-05-04T03:14:54.314622Z","iopub.status.idle":"2022-05-04T03:14:54.629025Z","shell.execute_reply.started":"2022-05-04T03:14:54.314585Z","shell.execute_reply":"2022-05-04T03:14:54.628005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Final Lasso Regression's score is 0.1935. This value is still bigger than Model 4's value, 0.1388. ","metadata":{}},{"cell_type":"markdown","source":"# 10. Decision Tree - Gradient Boosting Regressor\n\nThis time, I will try a new regression model - Gradient Boosting Regressor.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\n#Let's start with default option\ngbr=GradientBoostingRegressor(learning_rate= 0.1, max_depth= 3, n_estimators= 100)\ngbr.fit(X_train_4, y_train_4)\npred_gbr = gbr.predict(X_valid_4)\nmse_gbr = mean_squared_error(y_valid_4 , pred_gbr)\nrmse_gbr = np.sqrt(mse_gbr)\n\nprint(\"8th RMSE Score:\",rmse_gbr)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:22:46.617597Z","iopub.execute_input":"2022-05-04T04:22:46.617871Z","iopub.status.idle":"2022-05-04T04:22:47.472843Z","shell.execute_reply.started":"2022-05-04T04:22:46.617841Z","shell.execute_reply":"2022-05-04T04:22:47.471661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's find out best parameters\nparams_gbr =  {'learning_rate':[0.1],\n               'max_depth':[2,3],\n               'n_estimators':[100,200,300,500,1000]}\nmodel_gbr_params= GridSearchCV(gbr,param_grid=params_gbr,scoring='neg_mean_squared_error')\nmodel_gbr_params.fit(X_train_4, y_train_4)\nprint('Best Score:',model_gbr_params.best_score_)\nprint('Best Parameter value:',model_gbr_params.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:27:49.317819Z","iopub.execute_input":"2022-05-04T04:27:49.318181Z","iopub.status.idle":"2022-05-04T04:29:55.268362Z","shell.execute_reply.started":"2022-05-04T04:27:49.318107Z","shell.execute_reply":"2022-05-04T04:29:55.26739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbr_1=GradientBoostingRegressor(learning_rate= 0.1, max_depth= 2, n_estimators= 500)\ngbr_1.fit(X_train_4, y_train_4)\npred_gbr_1 = gbr_1.predict(X_valid_4)\nmse_gbr_1 = mean_squared_error(y_valid_4 , pred_gbr_1)\nrmse_gbr_1 = np.sqrt(mse_gbr_1)\n\nprint(\"8th RMSE Score:\",rmse_gbr_1)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:30:51.538211Z","iopub.execute_input":"2022-05-04T04:30:51.539058Z","iopub.status.idle":"2022-05-04T04:30:54.545004Z","shell.execute_reply.started":"2022-05-04T04:30:51.539021Z","shell.execute_reply":"2022-05-04T04:30:54.543784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At first, I applied Gradient Boosting Regressor(GBR) with default parameters. Next, with GridSearchCV, I found out optimal parameter values. Finally my RMSE score with GBR is much lower than Model 4. 0.1254 < 0.1388.","metadata":{}},{"cell_type":"markdown","source":"# 11. Final Model - 2\n\nGradient Boosting Regressor(GBR) is my final model.","metadata":{}},{"cell_type":"code","source":"model = GradientBoostingRegressor(learning_rate= 0.1, max_depth= 2, n_estimators= 500)\nmodel.fit(X_train_4, y_train_4)\n\nX_test = test.drop(drop_columns,axis=1)\nprint(X_test.shape)\n\nX_test[numberic_cols] = my_imputer_2.transform(X_test[numberic_cols])\nOH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[object_cols]))\nOH_cols_test.index = X_test.index\nnum_X_test = X_test.drop(object_cols, axis=1)\n\nX_test_final = pd.concat([num_X_test, OH_cols_test], axis=1)\npreds_test = model.predict(X_test_final)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:35:26.659222Z","iopub.execute_input":"2022-05-04T04:35:26.660348Z","iopub.status.idle":"2022-05-04T04:35:29.611826Z","shell.execute_reply.started":"2022-05-04T04:35:26.660282Z","shell.execute_reply":"2022-05-04T04:35:29.610911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(preds_test.shape)\ntest_result = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\noutput = pd.DataFrame({'Id': test_result.Id,'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:35:35.15666Z","iopub.execute_input":"2022-05-04T04:35:35.156985Z","iopub.status.idle":"2022-05-04T04:35:35.19783Z","shell.execute_reply.started":"2022-05-04T04:35:35.156953Z","shell.execute_reply":"2022-05-04T04:35:35.196906Z"},"trusted":true},"execution_count":null,"outputs":[]}]}