{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TensorFlow Natural Language Processing Guide\n\n**In this notebook I will demonstrate how to process natural language and predict sentiment in TensorFlow.**\n\n-------------------------------------------------------------------------\n**Notebook Prerequisites:**\n- Python                               > https://www.kaggle.com/learn/python\n- Data Visualization with Matplotlib   > https://www.kaggle.com/learn/data-visualization\n- Basic Linear Algebra                 > https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab\n- Basic Knowledge of Machine Learning  > https://www.kaggle.com/learn/intro-to-machine-learning\n\n\n---------------------------------------------------------------------------------------------------\n\n**This notebook will cover the following topics:**\n- Loading datasets from the Tensorflow Datasets API\n- Tokenizing text data into a numerical representation\n- Creating sequences (with padding) out of tokenized data\n- Creating a sequence-aware LSTM model with text embedding\n- Using custom and premade callbacks\n- Evaluating a models performance\n\n\n*Note: You should research anything in this notebook that you do not understand. Some links will be provided*","metadata":{}},{"cell_type":"markdown","source":"# Load IMDB Dataset\n\nWe will load the `imdb_reviews` dataset from TensorFlow datasets API. This dataset contains 50,000 movie reviews that are categorized as either positive (1) or negative (0). We will extract just the first 20 words from each review to speed up training.\n\nLink: https://www.tensorflow.org/datasets/catalog/imdb_reviews","metadata":{}},{"cell_type":"code","source":"#Import libraries\nimport tensorflow as tf #TensorFlow\nimport tensorflow_datasets as tfds #Datasets\nimport numpy as np #linear algebra\nimport matplotlib.pyplot as plt #Data visualization\n\n#Make sure TensorFlow is version 2.0 or higher\nprint('TensorFlow Version:', tf.__version__)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Download dataset\n#dataset documentation -> https://www.tensorflow.org/datasets/catalog/imdb_reviews\ntext_data = tfds.load('imdb_reviews', split=['train', 'test']) ","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create empty lists to hold our training and test text and labels\ntrain_text = []\ntrain_labels = []\ntest_text = []\ntest_labels = []\n\n#Iterate over the imdb data set and add text + labels to their respective list variables\nfor row in text_data[0]: #training set\n    #takes just the first 20 words from the review\n    train_text.append(row['text'].numpy().decode('utf-8').split()[:20]) \n    train_labels.append(row['label'].numpy())\n    \nfor row in text_data[1]: #testing set\n    #takes just the first 20 words from the review\n    test_text.append(row['text'].numpy().decode('utf-8').split()[:20]) \n    test_labels.append(row['label'].numpy())\n    \n\n#prints a samples from the training set\nprint(\"FIRST 5 SAMPLES OF TRAINING DATA\")\nprint(\"============================================================\")\nfor i in range(5):\n    review = \"POSITIVE\" if train_labels[i] == 1 else \"NEGATIVE\"\n    print(\"REVIEW SENTIMENT:\", review)\n    print(\"REVIEW TEXT:\", \" \".join(train_text[i]))\n    print(\"============================================================\")","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transform Text into Numerical Representation\n\nWords have meaning to us but computers can only understand numbers. Because of this, we must somehow transform the words into a numerical representation. The TensorFlow `Tokenizer` class can do just that. The `Tokenizer` maps each new word it encounters (limited by the `num_words` attribute) to an number. From here, the numeric representations need to be padded with the `pad_sequences` function so that all inputs to the future model will be of equal length.\n\nLinks: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n       https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n#Creates and fits a TensorFlow Tokenizer\ntokenizer = Tokenizer(num_words=10_000, oov_token='<OOV>')\ntokenizer.fit_on_texts(train_text)\n\n#Creates sequences of numeric representations of words\ntraining_sequences = tokenizer.texts_to_sequences(train_text)\n#pads sequences so they all have the same length\ntraining_sequences = pad_sequences(training_sequences, maxlen=20)\n\n#Process test data in the same way for later evaluation\ntesting_sequences = tokenizer.texts_to_sequences(test_text)\ntesting_sequences = pad_sequences(testing_sequences, maxlen=20)\n\n        \n#prints a sample of the new sequences\nprint('PROCESSED TEXT DATA')\nprint('=========================')\nfor i in range(5):\n    print(training_sequences[i], '\\n')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Callback\n\nCallbacks are used in Tensorflow to allow user intervention during model training. A callback can be executed at a number of specific intances during model training. \nFor example: \n- `on_batch_begin`/`end`\n- `on_epoch_begin`/`end`\n- `on_predict_batch_begin`/`end`\n- `on_predict_begin`/`end`\n- `on_test_batch_begin`/`end`\n- `on_test_begin`/`end`\n- `on_train_batch_begin`/`end`\n- `on_train_begin`/`end`\n\nWe will create `CustomCallback` which will stop the model from training once the model reaches 95% acccuracy on the training set.\n\nLink: https://keras.io/api/callbacks/","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback\n\nclass CustomCallback(Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if logs.get('accuracy') > 0.95:\n            print(\"Accuracy over 95%... Stopping training\")\n            self.model.stop_training = True\n\nmy_callback = CustomCallback()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predefined Callback - `LearningRateScheduler`\n\nThere are also a number of predefined callbacks. We will use the `LearningRateScheduler` to dynamically update the learning rate of our optimizer. This predefined callback takes a funtion that updates the learning rate as an argument.\n\nLink: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import LearningRateScheduler\n\n#creates a function that updates the learning rate based on the epoch number\ndef scheduler(epoch, lr):\n    if epoch < 2:\n        return 0.01\n    else:\n        return lr * 0.99\n\nlr_scheduler = LearningRateScheduler(scheduler)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network for Sentiment Analysis\n\nWe will now create a neural network that takes the processed movie reviews and outputs its sentiment (0-1). \nBasic NLP models have a structure similar to the following:\n\n**Embedding Layer ->Bidirectional LSTM Layer -> Dense Layer -> Output**\n\nLets break down the role of each layer:\n\n- **Embedding**: This layer will transform the previous scalar representation of our words into an n-dimensional vector. This will put words more associated with negative/positive reviews closer with similar words in the n-dimensional space.\n- **Bidirectional LSTM** (*Long Short-term Memory*): This layer is sequence aware in both the forward and backward direction. This means that this layer can interpret meaning carried across a phrase which is very important in understanding language.\n- **Dense**: This is the most simple layer of a neural network. It applies multiplication and addition operators and a non-linear activation function to find non-linear patterns in the data.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n#input dimensions is equal to number of words tokenized (defined above)\ninput_dim = 10_000\n# input length will be the length of our padded sequences\ninput_length = 20\n\n\n#defines a text classifier model\nmodel = Sequential([\n    Embedding(input_dim=input_dim, output_dim=64, input_length=input_length),\n    Bidirectional(LSTM(150)),\n    Dropout(0.4),\n    Dense(512, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer=Adam(),\n    metrics=['accuracy']\n)\n\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trains the model\nhistory = model.fit(\n    np.array(training_sequences), #must convert to numpy array before sending to model\n    np.array(train_labels),       #must convert to numpy array before sending to model\n    epochs=100, \n    batch_size=128,                \n    callbacks=[my_callback, lr_scheduler], \n    verbose=0)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plots history of model training\nplt.rcParams[\"figure.figsize\"] = (20,5)\nfig, axs = plt.subplots(1, 2)\n\naxs[0].plot(history.history['loss'], color='red')\naxs[0].set_xlabel('Epoch')\naxs[0].set_ylabel('Loss')\naxs[0].set_title('Training Loss')\n\naxs[1].plot(history.history['accuracy'])\naxs[1].set_xlabel('Epoch')\naxs[1].set_ylabel('Accuracy')\naxs[1].set_title('Training Accuracy')\n\nplt.show()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate Model Performance\n\nThe model performed very well on training data, but it is likely overfitting. Here we will evaluate the model on test data and see what the model thinks of a review that we write.","metadata":{}},{"cell_type":"code","source":"#Measures models preformance on the testing data\nevaluation = model.evaluate(\n    np.array(testing_sequences), #must convert to numpy array before sending to model\n    np.array(test_labels),       #must convert to numpy array before sending to model\n    batch_size=128,\n    verbose=0\n)\n\n#Prints accuracy of model on testing data\nprint(\"MODEL ACCURACY ON TEST DATA: {}%\".format(round(evaluation[1], 3) * 100))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets see what the model thinks of a new review (keep in mind the review will be cut off after 20 words)\n#Enter your review here\nNEW_REVIEW =\\\n\"\"\"\nThis movie was garbage. I wish I never came to the theater to watch it.\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Process the new review the same way the test text was processed\nnew_review_sequence = tokenizer.texts_to_sequences([NEW_REVIEW])\nnew_review_sequence = pad_sequences(new_review_sequence, maxlen=20)\n\n#sends new review to be predicted by the model\nnew_review_prediction = round(model.predict(np.array(new_review_sequence))[0][0])\nsentiment = \"NEGATIVE\" if new_review_prediction == 0 else \"POSITIVE\"\n\n#displays what the model thinks the sentiment of the review was\nprint(\"MOVIE REVIEW:\", NEW_REVIEW)\nprint(\"MODEL PREDICTED SENTIMENT:\", sentiment)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now try and see if you can achieve a better accuracy!\n\n\n### Similar Notebooks\n**TensorFlow Image Classification Guide**: https://www.kaggle.com/code/calebreigada/tensorflow-image-classification-guide\n\n**TensorFlow Time Series Forecasting Guide**:\nhttps://www.kaggle.com/code/calebreigada/tensorflow-time-series-forecasting-guide","metadata":{}}]}