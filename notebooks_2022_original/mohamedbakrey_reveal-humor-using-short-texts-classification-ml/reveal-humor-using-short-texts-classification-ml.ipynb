{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# loading Library\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n# for calculating Polarity and Subjectivity\nfrom textblob import TextBlob\nimport seaborn as sns\nimport wordcloud\nimport re\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:13:39.887904Z","iopub.execute_input":"2022-04-29T19:13:39.888266Z","iopub.status.idle":"2022-04-29T19:13:48.130225Z","shell.execute_reply.started":"2022-04-29T19:13:39.88818Z","shell.execute_reply":"2022-04-29T19:13:48.129348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Read the data","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('../input/200k-short-texts-for-humor-detection/dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:13:48.131914Z","iopub.execute_input":"2022-04-29T19:13:48.132268Z","iopub.status.idle":"2022-04-29T19:13:48.61128Z","shell.execute_reply.started":"2022-04-29T19:13:48.132223Z","shell.execute_reply":"2022-04-29T19:13:48.610377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shape of data\nprint(\"Shape of Data:\",df.shape)\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:13:48.614585Z","iopub.execute_input":"2022-04-29T19:13:48.614878Z","iopub.status.idle":"2022-04-29T19:13:48.63744Z","shell.execute_reply.started":"2022-04-29T19:13:48.614847Z","shell.execute_reply":"2022-04-29T19:13:48.636451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# information of data\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:13:48.639322Z","iopub.execute_input":"2022-04-29T19:13:48.639664Z","iopub.status.idle":"2022-04-29T19:13:48.688944Z","shell.execute_reply.started":"2022-04-29T19:13:48.639627Z","shell.execute_reply":"2022-04-29T19:13:48.688025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the if we have null value\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:13:48.690308Z","iopub.execute_input":"2022-04-29T19:13:48.691158Z","iopub.status.idle":"2022-04-29T19:13:48.723597Z","shell.execute_reply.started":"2022-04-29T19:13:48.691105Z","shell.execute_reply":"2022-04-29T19:13:48.72253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'].value_counts().tail()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:13:48.725043Z","iopub.execute_input":"2022-04-29T19:13:48.725354Z","iopub.status.idle":"2022-04-29T19:13:48.919505Z","shell.execute_reply.started":"2022-04-29T19:13:48.725312Z","shell.execute_reply":"2022-04-29T19:13:48.918734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check how much we have of false and True\ndf['humor'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:13:48.920882Z","iopub.execute_input":"2022-04-29T19:13:48.921102Z","iopub.status.idle":"2022-04-29T19:13:48.929178Z","shell.execute_reply.started":"2022-04-29T19:13:48.921077Z","shell.execute_reply":"2022-04-29T19:13:48.928504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_count = df.humor.value_counts()\n\nplt.figure(figsize=(8,4))\nplt.bar(val_count.index, val_count.values)\nplt.title(\"Sentiment Data Distribution\")","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:13:48.930326Z","iopub.execute_input":"2022-04-29T19:13:48.931007Z","iopub.status.idle":"2022-04-29T19:13:49.194267Z","shell.execute_reply.started":"2022-04-29T19:13:48.930957Z","shell.execute_reply":"2022-04-29T19:13:49.193295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Find the Text Polarity","metadata":{}},{"cell_type":"code","source":"# Lets calculate the Polarity of the Reviews\ndef get_polarity(text):\n    textblob = TextBlob(str(text.encode('utf-8')))\n    pol = textblob.sentiment.polarity\n    return pol\n\n# lets apply the function\ndf['polarity'] = df['text'].apply(get_polarity)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:13:49.195547Z","iopub.execute_input":"2022-04-29T19:13:49.19578Z","iopub.status.idle":"2022-04-29T19:14:41.486774Z","shell.execute_reply.started":"2022-04-29T19:13:49.195754Z","shell.execute_reply":"2022-04-29T19:14:41.485944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets calculate the Subjectvity of the Reviews\ndef get_subjectivity(text):\n    textblob = TextBlob(str(text.encode('utf-8')))\n    subj = textblob.sentiment.subjectivity\n    return subj\n\n# lets apply the Function\ndf['subjectivity'] = df['text'].apply(get_subjectivity)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:14:41.489583Z","iopub.execute_input":"2022-04-29T19:14:41.489852Z","iopub.status.idle":"2022-04-29T19:15:33.361939Z","shell.execute_reply.started":"2022-04-29T19:14:41.489823Z","shell.execute_reply":"2022-04-29T19:15:33.360908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## lets summarize the Newly Created Features\ndf[['polarity','subjectivity']].describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:15:33.363591Z","iopub.execute_input":"2022-04-29T19:15:33.364206Z","iopub.status.idle":"2022-04-29T19:15:33.412205Z","shell.execute_reply.started":"2022-04-29T19:15:33.364158Z","shell.execute_reply":"2022-04-29T19:15:33.410528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Visualization","metadata":{}},{"cell_type":"code","source":"## Visualizing Polarity and Subjectivity\n\nplt.rcParams['figure.figsize'] = (10, 4)\n\nplt.subplot(1, 2, 1)\nsns.distplot(df['polarity'])\n\nplt.subplot(1, 2, 2)\nsns.distplot(df['subjectivity'])\n\nplt.suptitle('Distribution of Polarity and Subjectivity')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:15:33.413497Z","iopub.execute_input":"2022-04-29T19:15:33.413821Z","iopub.status.idle":"2022-04-29T19:15:35.429042Z","shell.execute_reply.started":"2022-04-29T19:15:33.413776Z","shell.execute_reply":"2022-04-29T19:15:35.428428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets check relation between Polarity and Subjectivity\n\nsns.scatterplot(df['polarity'], df['subjectivity'])\nplt.title('Polarity vs Subjectivity')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:15:35.430257Z","iopub.execute_input":"2022-04-29T19:15:35.430635Z","iopub.status.idle":"2022-04-29T19:15:36.039431Z","shell.execute_reply.started":"2022-04-29T19:15:35.430598Z","shell.execute_reply":"2022-04-29T19:15:36.038276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc = wordcloud.WordCloud(stopwords=wordcloud.STOPWORDS, max_font_size=80, max_words=5000,\n                      width = 800, height = 500,\n                      background_color='black').generate(' '.join(txt for txt in df[\"text\"]))\nfig, ax = plt.subplots(figsize=(10,7))\nax.imshow(wc, interpolation='bilinear')\nax.set_axis_off()\nplt.imshow(wc)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:15:36.040623Z","iopub.execute_input":"2022-04-29T19:15:36.04115Z","iopub.status.idle":"2022-04-29T19:15:52.866523Z","shell.execute_reply.started":"2022-04-29T19:15:36.041115Z","shell.execute_reply":"2022-04-29T19:15:52.86552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Visualizing the Most Frequent Words\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\ncv = CountVectorizer(stop_words = 'english')\nwords = cv.fit_transform(df.text)\nsum_words = words.sum(axis=0)\n\n\nwords_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\nfrequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n\nplt.style.use('fivethirtyeight')\ncolor = plt.cm.ocean(np.linspace(0, 1, 20))\nfrequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 6), color = color)\nplt.title(\"Most Frequently Occuring Words - Top 20\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:15:52.867873Z","iopub.execute_input":"2022-04-29T19:15:52.868517Z","iopub.status.idle":"2022-04-29T19:15:57.010304Z","shell.execute_reply.started":"2022-04-29T19:15:52.868455Z","shell.execute_reply":"2022-04-29T19:15:57.009325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Visualizing the Least Frequent Words\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\ncv = CountVectorizer(stop_words = 'english')\nwords = cv.fit_transform(df.text)\nsum_words = words.sum(axis=0)\n\n\nwords_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\nfrequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n\nplt.style.use('fivethirtyeight')\ncolor = plt.cm.ocean(np.linspace(0, 1, 20))\nfrequency.tail(20).plot(x='word', y='freq', kind='bar', figsize=(15, 6), color = color)\nplt.title(\"Least Frequently Occuring Words - Top 20\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:15:57.011635Z","iopub.execute_input":"2022-04-29T19:15:57.011889Z","iopub.status.idle":"2022-04-29T19:16:01.210293Z","shell.execute_reply.started":"2022-04-29T19:15:57.011861Z","shell.execute_reply":"2022-04-29T19:16:01.209579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data Cleaning","metadata":{}},{"cell_type":"code","source":"# Create a function to remove special characters\ndef remove_special_characters(text):\n    pat = r'[^a-zA-z0-9]' \n    return re.sub(pat, ' ', text)\n \n# lets apply this function\ndf['text'] = df.apply(lambda x: remove_special_characters(x['text']), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:16:01.211256Z","iopub.execute_input":"2022-04-29T19:16:01.212146Z","iopub.status.idle":"2022-04-29T19:16:05.946117Z","shell.execute_reply.started":"2022-04-29T19:16:01.212107Z","shell.execute_reply":"2022-04-29T19:16:05.945289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets make a function to remove Numbers from the reviews\nimport re\ndef drop_numbers(list_text):\n    list_text_new = []\n    for i in list_text:\n        if not re.search('\\d', i):\n            list_text_new.append(i)\n    return ''.join(list_text_new)\n\ndf['text'] = df['text'].apply(drop_numbers)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:16:05.947095Z","iopub.execute_input":"2022-04-29T19:16:05.947296Z","iopub.status.idle":"2022-04-29T19:16:21.4911Z","shell.execute_reply.started":"2022-04-29T19:16:05.94727Z","shell.execute_reply":"2022-04-29T19:16:21.490004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\nstop = set(stopwords.words(\"english\"))\n\n\ndef remove_stopwords(text):\n    text = [word.lower() for word in text.split() if word.lower() not in stop]\n\n    return \" \".join(text)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:16:21.493337Z","iopub.execute_input":"2022-04-29T19:16:21.493907Z","iopub.status.idle":"2022-04-29T19:16:21.510903Z","shell.execute_reply.started":"2022-04-29T19:16:21.493862Z","shell.execute_reply":"2022-04-29T19:16:21.509843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"text\"] = df[\"text\"].map(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:16:21.514174Z","iopub.execute_input":"2022-04-29T19:16:21.514931Z","iopub.status.idle":"2022-04-29T19:16:22.440018Z","shell.execute_reply.started":"2022-04-29T19:16:21.514879Z","shell.execute_reply":"2022-04-29T19:16:22.438827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.text","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:16:22.441615Z","iopub.execute_input":"2022-04-29T19:16:22.442056Z","iopub.status.idle":"2022-04-29T19:16:22.457346Z","shell.execute_reply.started":"2022-04-29T19:16:22.442007Z","shell.execute_reply":"2022-04-29T19:16:22.456172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Data Processing","metadata":{}},{"cell_type":"code","source":"# creating bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(max_features = 2500)\n\nx = cv.fit_transform(df['text']).toarray()\ny = df['humor'].values\nprint(x.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:16:22.458889Z","iopub.execute_input":"2022-04-29T19:16:22.459424Z","iopub.status.idle":"2022-04-29T19:16:27.648025Z","shell.execute_reply.started":"2022-04-29T19:16:22.459372Z","shell.execute_reply":"2022-04-29T19:16:27.647011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:16:27.649523Z","iopub.execute_input":"2022-04-29T19:16:27.64991Z","iopub.status.idle":"2022-04-29T19:16:29.885021Z","shell.execute_reply.started":"2022-04-29T19:16:27.649864Z","shell.execute_reply":"2022-04-29T19:16:29.883831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:16:29.886358Z","iopub.execute_input":"2022-04-29T19:16:29.886612Z","iopub.status.idle":"2022-04-29T19:16:37.762997Z","shell.execute_reply.started":"2022-04-29T19:16:29.886581Z","shell.execute_reply":"2022-04-29T19:16:37.762061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using ML Algorithms","metadata":{}},{"cell_type":"markdown","source":"#  DecisionTreeClassifier","metadata":{}},{"cell_type":"code","source":"# Fitting Decision Tree Classification to the Training set\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:16:37.764543Z","iopub.execute_input":"2022-04-29T19:16:37.764766Z","iopub.status.idle":"2022-04-29T19:32:11.790748Z","shell.execute_reply.started":"2022-04-29T19:16:37.76474Z","shell.execute_reply":"2022-04-29T19:32:11.789849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred=classifier.predict(X_test)\npred","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:32:11.792134Z","iopub.execute_input":"2022-04-29T19:32:11.792372Z","iopub.status.idle":"2022-04-29T19:32:12.216652Z","shell.execute_reply.started":"2022-04-29T19:32:11.792344Z","shell.execute_reply":"2022-04-29T19:32:12.215814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('model_DT Train Score is : ' , classifier.score(X_train, y_train))\nprint('model_DT Test Score is : ' , classifier.score(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:32:12.484428Z","iopub.execute_input":"2022-04-29T19:32:12.484746Z","iopub.status.idle":"2022-04-29T19:32:14.105191Z","shell.execute_reply.started":"2022-04-29T19:32:12.484717Z","shell.execute_reply":"2022-04-29T19:32:14.104335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint(\"The Score is: \",accuracy_score(y_test , pred)*100)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:32:14.106629Z","iopub.execute_input":"2022-04-29T19:32:14.106941Z","iopub.status.idle":"2022-04-29T19:32:14.118303Z","shell.execute_reply.started":"2022-04-29T19:32:14.1069Z","shell.execute_reply":"2022-04-29T19:32:14.117237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:32:14.119605Z","iopub.execute_input":"2022-04-29T19:32:14.119956Z","iopub.status.idle":"2022-04-29T19:32:14.219557Z","shell.execute_reply.started":"2022-04-29T19:32:14.119923Z","shell.execute_reply":"2022-04-29T19:32:14.218684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix #for model evaluation\nconfusion_matrix=confusion_matrix(y_test,pred)\nconfusion_matrix","metadata":{"execution":{"iopub.status.busy":"2022-04-29T20:07:41.970837Z","iopub.execute_input":"2022-04-29T20:07:41.971162Z","iopub.status.idle":"2022-04-29T20:07:41.990187Z","shell.execute_reply.started":"2022-04-29T20:07:41.971122Z","shell.execute_reply":"2022-04-29T20:07:41.989163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T20:09:57.271401Z","iopub.execute_input":"2022-04-29T20:09:57.271969Z","iopub.status.idle":"2022-04-29T20:09:57.510999Z","shell.execute_reply.started":"2022-04-29T20:09:57.271924Z","shell.execute_reply":"2022-04-29T20:09:57.510082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total=sum(sum(confusion_matrix))\n\nsensitivity = confusion_matrix[0,0]/(confusion_matrix[0,0]+confusion_matrix[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = confusion_matrix[1,1]/(confusion_matrix[1,1]+confusion_matrix[0,1])\nprint('Specificity : ', specificity)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T20:10:29.943736Z","iopub.execute_input":"2022-04-29T20:10:29.944525Z","iopub.status.idle":"2022-04-29T20:10:29.951952Z","shell.execute_reply.started":"2022-04-29T20:10:29.94446Z","shell.execute_reply":"2022-04-29T20:10:29.951276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total=sum(sum(confusion_matrix))\n\nppv = confusion_matrix[0,0]/(confusion_matrix[0,0]+confusion_matrix[0,1])\nprint('ppv : ', ppv )\n\nnpv = confusion_matrix[1,1]/(confusion_matrix[1,0]+confusion_matrix[1,1])\nprint('npv : ', npv)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T20:10:42.171177Z","iopub.execute_input":"2022-04-29T20:10:42.171898Z","iopub.status.idle":"2022-04-29T20:10:42.178266Z","shell.execute_reply.started":"2022-04-29T20:10:42.171841Z","shell.execute_reply":"2022-04-29T20:10:42.177662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc #for model evaluation\nfrom matplotlib import pyplot\nfig, (ax2) = plt.subplots(figsize = (8,6))\n        #roc-curve\nfpr, tpr, thresholds_roc = roc_curve(y_test,pred)\nroc_auc = auc(fpr,tpr)\nax2.plot(fpr,tpr, label = \" AUROC = {:0.2f}\".format(roc_auc))\nax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\nax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\nax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\nax2.set_title(\"ROC Curve\", fontsize = 18)\nax2.legend(loc = 'best')\nplt.title('ROC curve for ecisionTreeClassifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\n        #find default threshold\nclose_default = np.argmin(np.abs(thresholds_roc - 0.5))\nax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T20:11:29.112263Z","iopub.execute_input":"2022-04-29T20:11:29.113117Z","iopub.status.idle":"2022-04-29T20:11:29.431859Z","shell.execute_reply.started":"2022-04-29T20:11:29.113066Z","shell.execute_reply":"2022-04-29T20:11:29.431181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RandomForestClassifier","metadata":{}},{"cell_type":"code","source":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier1 = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 42)\nclassifier1.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:32:50.57871Z","iopub.execute_input":"2022-04-29T19:32:50.57909Z","iopub.status.idle":"2022-04-29T19:55:48.502855Z","shell.execute_reply.started":"2022-04-29T19:32:50.579056Z","shell.execute_reply":"2022-04-29T19:55:48.501971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicting the Test set results\ny_pred = classifier1.predict(X_test)\ny_pred","metadata":{"execution":{"iopub.status.busy":"2022-04-29T20:00:58.025306Z","iopub.execute_input":"2022-04-29T20:00:58.025616Z","iopub.status.idle":"2022-04-29T20:01:09.358507Z","shell.execute_reply.started":"2022-04-29T20:00:58.025585Z","shell.execute_reply":"2022-04-29T20:01:09.357542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('model_DT Train Score is : ' , classifier1.score(X_train, y_train))\nprint('model_DT Test Score is : ' , classifier1.score(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T20:01:46.194335Z","iopub.execute_input":"2022-04-29T20:01:46.194642Z","iopub.status.idle":"2022-04-29T20:02:29.782322Z","shell.execute_reply.started":"2022-04-29T20:01:46.194614Z","shell.execute_reply":"2022-04-29T20:02:29.781212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The Score is: \",accuracy_score(y_test , y_pred)*100)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T20:02:35.65994Z","iopub.execute_input":"2022-04-29T20:02:35.660506Z","iopub.status.idle":"2022-04-29T20:02:35.671404Z","shell.execute_reply.started":"2022-04-29T20:02:35.660419Z","shell.execute_reply":"2022-04-29T20:02:35.670448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T20:02:40.904316Z","iopub.execute_input":"2022-04-29T20:02:40.905024Z","iopub.status.idle":"2022-04-29T20:02:41.003817Z","shell.execute_reply.started":"2022-04-29T20:02:40.904973Z","shell.execute_reply":"2022-04-29T20:02:41.002813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix #for model evaluation\nconfusion_matrix=confusion_matrix(y_test,y_pred)\nconfusion_matrix","metadata":{"execution":{"iopub.status.busy":"2022-04-29T20:03:50.37379Z","iopub.execute_input":"2022-04-29T20:03:50.374325Z","iopub.status.idle":"2022-04-29T20:03:50.39557Z","shell.execute_reply.started":"2022-04-29T20:03:50.374264Z","shell.execute_reply":"2022-04-29T20:03:50.394832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib\n\nmatplotlib.rcParams['figure.figsize'] = (8,5)\nplot_confusion_matrix(classifier1,X_test,y_test);","metadata":{"execution":{"iopub.status.busy":"2022-04-29T20:04:43.055835Z","iopub.execute_input":"2022-04-29T20:04:43.056207Z","iopub.status.idle":"2022-04-29T20:04:55.197935Z","shell.execute_reply.started":"2022-04-29T20:04:43.056167Z","shell.execute_reply":"2022-04-29T20:04:55.196935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total=sum(sum(confusion_matrix))\n\nsensitivity = confusion_matrix[0,0]/(confusion_matrix[0,0]+confusion_matrix[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = confusion_matrix[1,1]/(confusion_matrix[1,1]+confusion_matrix[0,1])\nprint('Specificity : ', specificity)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T20:05:01.676811Z","iopub.execute_input":"2022-04-29T20:05:01.677123Z","iopub.status.idle":"2022-04-29T20:05:01.686475Z","shell.execute_reply.started":"2022-04-29T20:05:01.677088Z","shell.execute_reply":"2022-04-29T20:05:01.68521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total=sum(sum(confusion_matrix))\n\nppv = confusion_matrix[0,0]/(confusion_matrix[0,0]+confusion_matrix[0,1])\nprint('ppv : ', ppv )\n\nnpv = confusion_matrix[1,1]/(confusion_matrix[1,0]+confusion_matrix[1,1])\nprint('npv : ', npv)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T20:05:11.258902Z","iopub.execute_input":"2022-04-29T20:05:11.259408Z","iopub.status.idle":"2022-04-29T20:05:11.268846Z","shell.execute_reply.started":"2022-04-29T20:05:11.259358Z","shell.execute_reply":"2022-04-29T20:05:11.26799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc #for model evaluation\nfrom matplotlib import pyplot\nfig, (ax2) = plt.subplots(figsize = (8,6))\n        #roc-curve\nfpr, tpr, thresholds_roc = roc_curve(y_test,y_pred)\nroc_auc = auc(fpr,tpr)\nax2.plot(fpr,tpr, label = \" AUROC = {:0.2f}\".format(roc_auc))\nax2.plot([0,1], [0,1], 'r', linestyle = \"--\", lw = 2)\nax2.set_xlabel(\"False Positive Rate\", fontsize = 14)\nax2.set_ylabel(\"True Positive Rate\", fontsize = 14)\nax2.set_title(\"ROC Curve\", fontsize = 18)\nax2.legend(loc = 'best')\nplt.title('ROC curve for Using Random Forest')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\n        #find default threshold\nclose_default = np.argmin(np.abs(thresholds_roc - 0.5))\nax2.plot(fpr[close_default], tpr[close_default], 'o', markersize = 8)\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T20:05:54.745377Z","iopub.execute_input":"2022-04-29T20:05:54.745995Z","iopub.status.idle":"2022-04-29T20:05:55.072609Z","shell.execute_reply.started":"2022-04-29T20:05:54.745935Z","shell.execute_reply":"2022-04-29T20:05:55.071586Z"},"trusted":true},"execution_count":null,"outputs":[]}]}