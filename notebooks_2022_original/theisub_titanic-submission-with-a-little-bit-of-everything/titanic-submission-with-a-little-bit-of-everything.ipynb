{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Intro\n\nAs some kind of entry point I wanted to start with the classical Titanic dataset, I’ll try to cover different stages of modelling from EDA to ensembling suitable models. I’ll omit some details to make this notebook much easier to scroll and navigate. Hope you’ll like it. Maybe it can be a good tutorial for beginners. I might add some more references and more details of every aspect.","metadata":{}},{"cell_type":"markdown","source":"## Importing libraries\n","metadata":{}},{"cell_type":"code","source":"\n# Essential\nimport numpy as np\nimport pandas as pd\n\n# Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import PercentFormatter\n\n\n# Models\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\n# Model evaluation and tuning\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report\nimport shap\n\n\n# Imputing and scaling \nfrom sklearn.impute._knn import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:50.4597Z","iopub.execute_input":"2022-04-23T01:13:50.459989Z","iopub.status.idle":"2022-04-23T01:13:50.466849Z","shell.execute_reply.started":"2022-04-23T01:13:50.459961Z","shell.execute_reply":"2022-04-23T01:13:50.466087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing dataset\nWe will need to concatenate train and test data for future feature engineering. It might be not recommended in some cases and can cause a data leakage. But we will discuss some caveats later.\n\nLet's see what features we have","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('../input/titanic/train.csv')\n# train_data['Survived'] = train_data['Survived'].astype(int)\ntest_data = pd.read_csv('../input/titanic/test.csv')\nfull_data =  train_data.append(test_data)\n\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:50.48586Z","iopub.execute_input":"2022-04-23T01:13:50.48606Z","iopub.status.idle":"2022-04-23T01:13:50.514924Z","shell.execute_reply.started":"2022-04-23T01:13:50.486036Z","shell.execute_reply":"2022-04-23T01:13:50.514276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploring data \nWe have 3 categorical features:\n - `PClass`\n - `Sex`\n - `Embarked`\n\nWe also have 4 numerical features:\n - `Age`\n - `SibSp`\n - `Parch`\n - `Fare`\n\nAnd 3 nominal features:\n - `Name`\n - `Ticket`\n - `Cabin`\n\n   ","metadata":{}},{"cell_type":"markdown","source":"Let's see stats of numerical features in train dataset","metadata":{}},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:50.518124Z","iopub.execute_input":"2022-04-23T01:13:50.518308Z","iopub.status.idle":"2022-04-23T01:13:50.551123Z","shell.execute_reply.started":"2022-04-23T01:13:50.518286Z","shell.execute_reply":"2022-04-23T01:13:50.550442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There’s not much interesting data we can see now, but we can see that `count` in column `Age` varies from other features, which means that we have some missing values. Let’s see how many null values we have in the train dataset:","metadata":{}},{"cell_type":"code","source":"print('Number of rows ',len(train_data))\nprint(train_data.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:50.55246Z","iopub.execute_input":"2022-04-23T01:13:50.554623Z","iopub.status.idle":"2022-04-23T01:13:50.561864Z","shell.execute_reply.started":"2022-04-23T01:13:50.554596Z","shell.execute_reply":"2022-04-23T01:13:50.560994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 177 rows with missing `Age` and 687 rows with missing `Cabin`","metadata":{}},{"cell_type":"markdown","source":"I can see `PClass`,`Sex`,`Age`,`SibSp`,`Parch`,`Fare`,`Cabin` as potential important variables. \nAlso, we might combine some of those features (For example SibSp and Parch as they might be considered 'family')","metadata":{}},{"cell_type":"code","source":"full_data['FamilyMembers'] = full_data['SibSp'] + full_data['Parch']\ntrain_data['FamilyMembers'] = train_data['SibSp'] + train_data['Parch']","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:50.585798Z","iopub.execute_input":"2022-04-23T01:13:50.58609Z","iopub.status.idle":"2022-04-23T01:13:50.592053Z","shell.execute_reply.started":"2022-04-23T01:13:50.586062Z","shell.execute_reply":"2022-04-23T01:13:50.591131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After combining `SibSp` and `Parch` into the new feature `FamilyMembers` counting number of family members for each passenger, we will visualize everything we have for know. First, let’s see how many passengers survived based on how big is the family passenger is travelling with (Chart on the left). And how many passengers survived based on `Sex` and `Pclass` (Chart on the right).","metadata":{}},{"cell_type":"code","source":"\nfig,ax = plt.subplots(1,2,figsize=(10,6))\nsns.countplot(x='FamilyMembers',hue='Survived',data=train_data,ax=ax[0]).set(title='How many passengers survived? \\nGrouped by number of family members on board',ylabel='Survived',xlabel='Family members')\nsns.barplot(x='Sex',y='Survived',hue='Pclass',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')\nax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:50.611489Z","iopub.execute_input":"2022-04-23T01:13:50.611797Z","iopub.status.idle":"2022-04-23T01:13:51.186586Z","shell.execute_reply.started":"2022-04-23T01:13:50.61177Z","shell.execute_reply":"2022-04-23T01:13:51.185773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, so we can see that solo travellers died more often compared to the ones with family.\nAlso, there’s a strong sign that females have a higher chance to survive.\nAnd we can see that males from 1st class had a higher chance to survive than males from 2nd and 3rd class.","metadata":{}},{"cell_type":"markdown","source":"We also need to answer other questions info about the dataset:\n- How important is info about the port where passengers embarked?\n- Does `Age` distribution vary in groups of passengers who died and lived? Do we need to impute `Age` for the `177` passengers?\n\nWe can visualize those questions and try to answer them","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize=(10,6))\n\n\nsns.histplot(x='Age',hue='Survived',data=train_data,ax=ax[0],kde=True).set(title='How many passengers survived? (Age,Pclass) ')\nsns.barplot(x='Sex',y='Survived',hue='Embarked',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')\nax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:51.188481Z","iopub.execute_input":"2022-04-23T01:13:51.188757Z","iopub.status.idle":"2022-04-23T01:13:51.843795Z","shell.execute_reply.started":"2022-04-23T01:13:51.188721Z","shell.execute_reply":"2022-04-23T01:13:51.84306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's a small bump for passengers aged < 10 years. It is because children were prioritized during the evacuation.\n\nThere's no strong evidence if embark port affect the result since confidence intervals (black lines on bars) overlap with each other.","metadata":{}},{"cell_type":"markdown","source":"But what about imputing `177` rows for `Age` feature? Do we need to fill missing values or this feature is not that important?\nOverall, it is not that important, we can see it on distribution plots, only very young passengers had a higher chance to survive.\nWe can make `Age` feature a categorical feature and divide it in year bins.\n\nHowever, there’s a small detail about the dataset however - if we would examine `Name` feature, we might see that there is a title of the passenger (e.g. Mr,Mrs,Dr,etc.).","metadata":{}},{"cell_type":"markdown","source":"And one of the important titles is 'Master' whichб according to wikipedia, is used for boys:\n>  ... in the United States, unlike the UK, a boy can be addressed as Master only until age 12, then is addressed only by his name with no title until he turns 18, when he takes the title of Mr.\n\nTherefore, we can consider passengers with the title ‘Master’ as young boys for age feature, which would most likely fit in that orange bump on the left chart with distribution.\n\nI won't do it in this notebook, but it might be a good point for feature engineering or just to fill missing age values with consideration of the title of passenger.\n","metadata":{}},{"cell_type":"markdown","source":"##  Feature engineering","metadata":{}},{"cell_type":"markdown","source":"Additionally, to newly created combined feature `FamilyMembers` we also need to consider adding feature which identifies solo travellers (`IsSolo`). Also we'll fill empty values and change categorical string values to integer.  \nWe will ignore `Age` feature in training, since it is difficult to correctly predict how old is the passenger (except those with 'Master' title of course)","metadata":{}},{"cell_type":"code","source":"#Passenger considered solo if he has no family members on board\nfull_data['IsSolo'] = (full_data['FamilyMembers'] == 0).astype(int)\n\n# test_data['FamilyMembers'] = test_data['SibSp']+test_data['Parch']\n\n\n# Replace string value to numbers. There's 2 nan values in test data, we will change them to value of most common port ('S') \nfull_data['Embarked'] = full_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2,np.nan:0} ).astype(int)\n\n# Replace string value of sex to numbers 1 - female, 0 - male\nfull_data['Sex'] = (full_data['Sex'] == 'female').astype(int)\n\n\n\n#There's 1 missing value for Fare in test dataset, let's fill it with mean value\nfull_data['Fare'].fillna(full_data['Fare'].mean(), inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:51.844889Z","iopub.execute_input":"2022-04-23T01:13:51.845711Z","iopub.status.idle":"2022-04-23T01:13:51.856193Z","shell.execute_reply.started":"2022-04-23T01:13:51.845672Z","shell.execute_reply":"2022-04-23T01:13:51.855436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nIf we examine the dataset more carefully, we will see interesting details considering a group of travellers:\n- Families usually pay equal fare and obviously have the same last name. \n- Group of friends/relatives with different last names usually have the same ticket number\n\nWe can use those facts as a new feature that represents the chance of survival of this family/group, let's call it `Family_Survival`.\nI've changed the method used by [S.Xu's](https://www.kaggle.com/shunjiangxu/blood-is-thicker-than-water-friendship-forever), but changed his approach of minmaxing the family survival to average score, it seemed more precise.  ","metadata":{}},{"cell_type":"code","source":"# Extracting last name from Name feature\nfull_data['Last_Name'] = full_data['Name'].apply(lambda x: str.split(x, \",\")[0])\n\n# Filling default value of family/group survival as mean of individual survival \nfull_data['Family_Survival'] = train_data['Survived'].mean()\n\n\n# for loop to find family members (family with same surname)\nfor grp, grp_df in full_data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            #check if whole family doesn't have 'Survived' value  \n            if (np.isnan(grp_df['Survived']).all()):\n                continue\n            average_family_score = (grp_df.drop(ind)['Survived'].mean())\n            #check if average_family_score is nan, it happens when only current passenger has 'Survived' value\n            if np.isnan(average_family_score):\n                average_family_score = row['Survived']\n            average_family_score = round(average_family_score)   \n            passID = row['PassengerId']\n            full_data.loc[full_data['PassengerId'] == passID, 'Family_Survival'] = average_family_score\n\n# for loop to find group of passengers who bought ticket together (friends,relatives)\nfor _, grp_df in full_data.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival'] == 0.5):\n                if (np.isnan(grp_df['Survived']).all()):\n                    continue\n                average_family_score = (grp_df.drop(ind)['Survived'].mean())\n                if np.isnan(average_family_score):\n                    average_family_score = row['Survived']\n                average_family_score = round(average_family_score)   \n                passID = row['PassengerId']\n                full_data.loc[full_data['PassengerId'] == passID, 'Family_Survival'] = average_family_score","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:51.858637Z","iopub.execute_input":"2022-04-23T01:13:51.85911Z","iopub.status.idle":"2022-04-23T01:13:52.857407Z","shell.execute_reply.started":"2022-04-23T01:13:51.859068Z","shell.execute_reply":"2022-04-23T01:13:52.856696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good, now we can look at the updated dataset","metadata":{}},{"cell_type":"code","source":"full_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:52.85957Z","iopub.execute_input":"2022-04-23T01:13:52.859989Z","iopub.status.idle":"2022-04-23T01:13:52.876231Z","shell.execute_reply.started":"2022-04-23T01:13:52.859952Z","shell.execute_reply":"2022-04-23T01:13:52.875375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inspecting the models and features ","metadata":{}},{"cell_type":"markdown","source":"After adding new features, we can start trying to choose the best model to fit the data.\nLet's add new features to train and test data.","metadata":{}},{"cell_type":"code","source":"train_data = full_data[:len(train_data)]\ntrain_data.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-23T01:13:52.87787Z","iopub.execute_input":"2022-04-23T01:13:52.878168Z","iopub.status.idle":"2022-04-23T01:13:52.89921Z","shell.execute_reply.started":"2022-04-23T01:13:52.878131Z","shell.execute_reply":"2022-04-23T01:13:52.89835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As previously mentioned, we wanted to use only certain features to train. \nIt's time to start preparing our data to train our models","metadata":{}},{"cell_type":"code","source":"features = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']\ny = train_data['Survived'].ravel()\nX_train,X_val,y_train,y_val = train_test_split(train_data[features],y,test_size=0.20,random_state=111)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:52.901769Z","iopub.execute_input":"2022-04-23T01:13:52.902247Z","iopub.status.idle":"2022-04-23T01:13:52.916864Z","shell.execute_reply.started":"2022-04-23T01:13:52.902205Z","shell.execute_reply":"2022-04-23T01:13:52.915945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will test multiple types of models, such as:\n- Logistic regression\n- Support Vector Machine\n- Random Forest\n- Naive Bayes\n- KNN\n- XGBoosting ","metadata":{}},{"cell_type":"markdown","source":"To correctly choose the right model for our task, we need to evaluate each model. We will use cross-validation during training models with default parameters.\nHyperparameter tuning will be performed after we chose the most effective models.\n\nHere’s a basic wrapper to make this process easier","metadata":{}},{"cell_type":"code","source":"def test_models(model,X,y_train):\n    key = type(model).__name__\n    model.fit(X,y_train)\n    model_score =model.score(X,y_train)\n    model_score=cross_val_score(model,X,y_train,cv=5).mean()\n    if key not in summary:\n        summary[key] = []\n    summary[key].append(model_score)\n    return summary","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:52.918503Z","iopub.execute_input":"2022-04-23T01:13:52.918864Z","iopub.status.idle":"2022-04-23T01:13:52.927344Z","shell.execute_reply.started":"2022-04-23T01:13:52.918826Z","shell.execute_reply":"2022-04-23T01:13:52.926579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to standardize our training data since some models are very sensitive to unscaled data.\nWe'll do an experiment to showcase this: \n","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\n\nfeatures = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']\nsummary={}\nmodels_to_check= [SVC(),KNeighborsClassifier(),xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),LogisticRegression(solver='liblinear'),GaussianNB(),RandomForestClassifier()]\n\nfor item in models_to_check:\n    summary = test_models(item,X_train[features],y_train)\n\nprint(X_train[features].columns)\nX = scaler.fit_transform(X_train[features])\n\n\nfor item in models_to_check:\n    summary = test_models(item,X,y_train)\n\nsummary = pd.DataFrame.from_dict(summary,orient='index',columns=['Without scaler','With scaler'])\nprint(summary)\n;\n","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:52.928771Z","iopub.execute_input":"2022-04-23T01:13:52.929113Z","iopub.status.idle":"2022-04-23T01:13:56.694777Z","shell.execute_reply.started":"2022-04-23T01:13:52.929073Z","shell.execute_reply":"2022-04-23T01:13:56.694062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, all models increased score with scaled data.\nSolver also failed to converge on non-scaled training data, so there is an undoubted need to scale data for this dataset.","metadata":{}},{"cell_type":"markdown","source":"Ok, what about features, we can examine the importance of features. We will inspect the best classifier from our test - `XGBoost`.","metadata":{}},{"cell_type":"markdown","source":"We will describe model's features importance in bar chart","metadata":{}},{"cell_type":"code","source":"model = xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss')\nmodel.fit(X_train,y_train)\nfeature_importances = model.feature_importances_\n\n\nplt.yticks(range(len(feature_importances)), features[:len(feature_importances)])\nplt.xlabel('Relative Importance')\nplt.barh(range(len(feature_importances)), feature_importances[:len(feature_importances)], color='b', align='center')\nplt.title('Feature Importances')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:56.697769Z","iopub.execute_input":"2022-04-23T01:13:56.698017Z","iopub.status.idle":"2022-04-23T01:13:56.959702Z","shell.execute_reply.started":"2022-04-23T01:13:56.697991Z","shell.execute_reply":"2022-04-23T01:13:56.958938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we probably expected, `Sex` is the most important feature, after that we have `Pclass`, `Family_Survival` and `FamilyMembers`.\nSurprisingly, the new feature, `IsSolo` is practically useless.\n\nOkay,let's see how features affect our model's output.\nOne of the most useful and beautiful ways to plot the feature's output is in SHAP library in `summary_plot` method, by calculating Shapley values to explain the impact of the features on prediction.","metadata":{}},{"cell_type":"code","source":"model = model.fit(X_val,y_val)\nexplainer = shap.Explainer(model)\nshap_values = explainer(X_val)\n\nshap.summary_plot(shap_values)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:56.960977Z","iopub.execute_input":"2022-04-23T01:13:56.961339Z","iopub.status.idle":"2022-04-23T01:13:57.512438Z","shell.execute_reply.started":"2022-04-23T01:13:56.961295Z","shell.execute_reply":"2022-04-23T01:13:57.511757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just interpret this plot as - closer to the right side means more impact of that feature on prediction being 'Survived', closer to the left side 'Died'. \nRed means closer to the higher value of the described feature, blue means closer to the low value of the described feature (Pclass for example: 3 - red, 2 - purple, 1 - blue).\n\n\nWe'll break it down one by one:\n- `Sex` affects the chance of surviving, hence why red(bigger value, 1.0 in this case) are skewed to the right side.\n- `Fare` doesn't affect the output based on its value.\n- `Family_survival` does affect the output, passengers which families/group had bigger chance to live expected to survive, passengers which  families/groups had medium chance to live had smaller chance to live, and passengers with families/groups with smallest chance of survival expected to die with them(I guess...).\n- `Pclass` does  slightly affect the output, 1st class passengers are more likely to survive, than 2nd and 3rd class\n- `Embarked` is strange, but passengers from 'S'-Southampton are more likely to die\n- `Family members` does not affect that much\n- `IsSolo` does not affect at all\n","metadata":{}},{"cell_type":"markdown","source":"## Choosing the best model\nNow we need to get the best hyperparameters for our models. Different methods can help in fine tuning the hyperparameters. We will use Grid Search and combine it with Stratified KFold cross validation. We will try to find the best parameters for each model and stack them later.  \nSo I will omit the details to save valuable compiling time and set smaller parameter grids just to show the process.\nI will also set only 4 important features in training data, since I tried different combinations of them and these seemed most useful.","metadata":{}},{"cell_type":"markdown","source":"Here's another small wrapper for Grid Search","metadata":{}},{"cell_type":"code","source":"def GridSearchCVWrapper(model,parameters, X_train,X_val,y_train,y_val):\n\n    clf = GridSearchCV(estimator=model, param_grid=parameters,n_jobs=-1,\n                    cv=StratifiedKFold(n_splits=5), \n                    scoring=['accuracy','recall','f1','roc_auc'],\n                    verbose=1,refit='roc_auc')\n    clf.fit(X_train,y_train)          \n    preds = clf.best_estimator_.predict(X_val)\n    print(classification_report(preds,y_val))\n    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring = \"roc_auc\")\n    print(\"Scores:\", scores)\n    print(f\"Mean:{scores.mean()} ± {scores.std()}\")\n\n    return clf","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:57.513588Z","iopub.execute_input":"2022-04-23T01:13:57.514366Z","iopub.status.idle":"2022-04-23T01:13:57.522241Z","shell.execute_reply.started":"2022-04-23T01:13:57.514326Z","shell.execute_reply":"2022-04-23T01:13:57.521171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prepare our data for training","metadata":{}},{"cell_type":"code","source":"features = ['Pclass','Sex','FamilyMembers','Family_Survival']\n\ntest_data = full_data[len(train_data):]\ntest_data_x = test_data[features].copy(deep=True)\ntrain_data = full_data[:len(train_data)]\n\nscaler = StandardScaler()\nX = train_data[features].copy(deep=True)\nX= scaler.fit_transform(X)\n\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.20,random_state=111)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:57.524627Z","iopub.execute_input":"2022-04-23T01:13:57.525306Z","iopub.status.idle":"2022-04-23T01:13:57.544979Z","shell.execute_reply.started":"2022-04-23T01:13:57.525274Z","shell.execute_reply":"2022-04-23T01:13:57.544151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Logistic_model_params= {'penalty' : ['l1', 'l2'],\n                        'C' : np.logspace(-4, 4, 20),\n                        'solver' : ['liblinear']}\n\nLogistic_model = GridSearchCVWrapper(LogisticRegression(),Logistic_model_params,X_train,X_val,y_train,y_val)\nprint(f\"\\nBest params for Logistic Regression are:\")\nprint(Logistic_model.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:13:57.546529Z","iopub.execute_input":"2022-04-23T01:13:57.546844Z","iopub.status.idle":"2022-04-23T01:14:02.40266Z","shell.execute_reply.started":"2022-04-23T01:13:57.546804Z","shell.execute_reply":"2022-04-23T01:14:02.401936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVM_model_params = {'C':np.logspace(-2,1,4),\n                    'gamma':np.logspace(-2,1,4),}\n                    \nSVM_model = GridSearchCVWrapper(SVC(),SVM_model_params, X_train,X_val,y_train,y_val)\nprint(f\"\\nBest params for SVM are:\")\nprint(SVM_model.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:14:02.403926Z","iopub.execute_input":"2022-04-23T01:14:02.40432Z","iopub.status.idle":"2022-04-23T01:14:08.478727Z","shell.execute_reply.started":"2022-04-23T01:14:02.404283Z","shell.execute_reply":"2022-04-23T01:14:08.477948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nRF_model_params = { 'n_estimators': [200,350,500],\n               'max_features': ['auto'],\n               'max_depth': [2,5,None],\n               'min_samples_split': [5, 10],\n               'min_samples_leaf': [2, 4],\n               'bootstrap': [True],\n               'random_state':[1]}\nRF_model = GridSearchCVWrapper(RandomForestClassifier(),RF_model_params,X_train,X_val,y_train,y_val)\n\nprint(f\"\\nBest params for Random Forest are:\")\nprint(RF_model.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:14:08.480212Z","iopub.execute_input":"2022-04-23T01:14:08.480702Z","iopub.status.idle":"2022-04-23T01:22:54.854655Z","shell.execute_reply.started":"2022-04-23T01:14:08.480664Z","shell.execute_reply":"2022-04-23T01:22:54.8539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Gaussian_model_params = {'var_smoothing':np.logspace(0,-9,100)}\nGaussian_model = GridSearchCVWrapper(GaussianNB(),Gaussian_model_params, X_train,X_val,y_train,y_val)\n\nprint(f\"\\nBest params for Naive Bayes are:\")\nprint(Gaussian_model.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:22:54.85593Z","iopub.execute_input":"2022-04-23T01:22:54.856179Z","iopub.status.idle":"2022-04-23T01:23:07.823975Z","shell.execute_reply.started":"2022-04-23T01:22:54.856144Z","shell.execute_reply":"2022-04-23T01:23:07.823227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#! for some reason this cell runs horribly slow in kaggle, so I truncated most of the parameters. I used params which i got from run on my pc.\nXgb_model_parameters = {\n            'n_estimators': [200],\n            'colsample_bytree': [0.7],\n            'max_depth': [15],\n            'reg_alpha': [1.1],\n            'reg_lambda': [1.2],\n            'n_jobs':[-1]}\n\nXgb_model = GridSearchCVWrapper(xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),Xgb_model_parameters,X_train,X_val,y_train,y_val)\nprint(f\"\\nBest params for XGBoost are:\")\nprint(Xgb_model.best_estimator_)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:23:07.825242Z","iopub.execute_input":"2022-04-23T01:23:07.825477Z","iopub.status.idle":"2022-04-23T01:23:28.767679Z","shell.execute_reply.started":"2022-04-23T01:23:07.825443Z","shell.execute_reply":"2022-04-23T01:23:28.767106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KNN_model_params= {'n_neighbors':np.arange(1,30,2),\n                    'leaf_size':np.arange(1,15,2),\n                    'p':[1,2]}\nKNN_model = GridSearchCVWrapper(KNeighborsClassifier(),KNN_model_params,X_train,X_val,y_train,y_val)\n\nprint(f\"\\nBest params for K Neighbors are:\")\nprint(KNN_model.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:23:28.77067Z","iopub.execute_input":"2022-04-23T01:23:28.772379Z","iopub.status.idle":"2022-04-23T01:24:25.291882Z","shell.execute_reply.started":"2022-04-23T01:23:28.772347Z","shell.execute_reply":"2022-04-23T01:24:25.2911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacking models and getting results\nAfter completing training our models, it’s time to evaluate them and compare them one by one. We’ll do the last comparison and visualize the ROC AUC score of models on the heatmap to find a suitable combination of models for stacking.\n\nThe point is to combine the most accurate models to get a better score. We need to find models which have a little correlation between each other’s predictions.","metadata":{}},{"cell_type":"code","source":"data_of_classifier = pd.DataFrame()\nclassifiers = [SVM_model.best_estimator_,Xgb_model.best_estimator_, Logistic_model.best_estimator_, Gaussian_model.best_estimator_,RF_model.best_estimator_,KNN_model.best_estimator_]\nfor i in classifiers:\n    fit_classifier = i.fit(X_train,y_train)\n    data_of_classifier[type(i).__name__] = i.predict(X_val)\n    print('Score of',type(i).__name__,':')\n    print(cross_val_score(fit_classifier, X_train, y_train, cv=5, scoring = \"roc_auc\").mean())\nsns.heatmap(data_of_classifier.astype(float).corr(),annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:24:25.293063Z","iopub.execute_input":"2022-04-23T01:24:25.293557Z","iopub.status.idle":"2022-04-23T01:24:31.095658Z","shell.execute_reply.started":"2022-04-23T01:24:25.293499Z","shell.execute_reply":"2022-04-23T01:24:31.094981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results are pretty even, but we will consider the models with the highest ROC AUC scores and combine 3 of them.\n\nThe good idea is to find models with less correlation between each other and high scores.\n\nAfter some testing, the best combination I found was the Random Forest as a final estimator and will use KNeighbors and XGBoost as base estimators. I commented the different models in stacked classifier if you would want to test them.","metadata":{}},{"cell_type":"code","source":"data_to_test = scaler.transform(test_data[features])","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:24:31.09671Z","iopub.execute_input":"2022-04-23T01:24:31.09769Z","iopub.status.idle":"2022-04-23T01:24:31.105314Z","shell.execute_reply.started":"2022-04-23T01:24:31.097648Z","shell.execute_reply":"2022-04-23T01:24:31.104604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nestimators = [#('SVM',SVM_model.best_estimator_),\n              ('XGB',Xgb_model.best_estimator_),\n              ('Logistic',Logistic_model.best_estimator_)\n               # ('Random Forest',Gaussian_model.best_estimator_),\n               #('KNN',KNN_model.best_estimator_)\n]\n\nstacking_clf = StackingClassifier(estimators = estimators,final_estimator=RF_model.best_estimator_)\n\nstacking_clf.fit(X,y)\n\npredictions =  stacking_clf.predict(data_to_test)\npredictions =predictions.astype(int)\nfinal_results = pd.DataFrame({ 'PassengerId':test_data.PassengerId ,'Survived':predictions })\nfinal_results.to_csv('../working/submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:26:36.04363Z","iopub.execute_input":"2022-04-23T01:26:36.04394Z","iopub.status.idle":"2022-04-23T01:26:37.34401Z","shell.execute_reply.started":"2022-04-23T01:26:36.043911Z","shell.execute_reply":"2022-04-23T01:26:37.343312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\nI tried to do a little bit of everything on this notebook, so there's a lot of details I omitted, but I do appreciate your feedback","metadata":{}}]}