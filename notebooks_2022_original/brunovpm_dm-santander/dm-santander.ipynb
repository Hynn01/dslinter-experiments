{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nfrom sklearn.feature_selection import VarianceThreshold\n#warnings.filterwarnings(\"ignore\")\npd.options.display.max_columns = None\nfrom hyperopt import space_eval\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#plt.rcParams[\"figure.figsize\"] = (16,7)\nfrom sklearn.model_selection import train_test_split,cross_val_score,KFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_curve,roc_auc_score,precision_score,recall_score,confusion_matrix,make_scorer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scikitplot.helpers import binary_ks_curve\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import validation_curve\nfrom category_encoders import OneHotEncoder,TargetEncoder\nfrom yellowbrick.model_selection import RFECV\nimport shap\nimport pandas as pd\nimport numpy as np\nimport warnings\nfrom sklearn.feature_selection import VarianceThreshold\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_columns = None\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (16,7)\nfrom sklearn.model_selection import train_test_split,cross_val_score,KFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_curve,roc_auc_score,precision_score,recall_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scikitplot.helpers import binary_ks_curve\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import validation_curve\nfrom category_encoders import OneHotEncoder,TargetEncoder\nfrom yellowbrick.model_selection import RFECV\nfrom sklearn.model_selection import StratifiedKFold\nimport shap\nfrom hyperopt import fmin, tpe, hp,Trials","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-07T18:31:18.745497Z","iopub.execute_input":"2022-05-07T18:31:18.746007Z","iopub.status.idle":"2022-05-07T18:31:21.679382Z","shell.execute_reply.started":"2022-05-07T18:31:18.74592Z","shell.execute_reply":"2022-05-07T18:31:21.678484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lendo dataset disponivel","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/santander-customer-satisfaction/train.csv\")\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:31:21.681216Z","iopub.execute_input":"2022-05-07T18:31:21.681491Z","iopub.status.idle":"2022-05-07T18:31:24.265231Z","shell.execute_reply.started":"2022-05-07T18:31:21.681456Z","shell.execute_reply":"2022-05-07T18:31:24.264584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Definindo features\n\nAqui estamos dividindo o dataframe em 3 grupos:\n\n#### 1 - Colunas de identificação\n\n- Colunas que representam chaves ID \n\n#### 2 Target\n\n- Coluna que identifica a target do problema\n\n#### 3 Variáveis dependentes/explicativas\n\n- Aqui são todas as features disponiveis, esse grupo foi dividido em 2 subgrupos que são variaveis categoricas e variaveis continuas","metadata":{}},{"cell_type":"code","source":"id_columns = ['ID']\ntarget_column = ['TARGET']\n\nnum_vars = df_train.select_dtypes(include=['float64','int64'])\ncat_vars = df_train.select_dtypes(include=['object'])\n\nprint('initial numerical vars =',len(num_vars.columns))\nprint('initial categorical vars =',len(cat_vars.columns))\n\ny = df_train[target_column]\nx = df_train.drop(columns=id_columns + target_column).fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:31:24.266167Z","iopub.execute_input":"2022-05-07T18:31:24.266417Z","iopub.status.idle":"2022-05-07T18:31:24.540464Z","shell.execute_reply.started":"2022-05-07T18:31:24.266384Z","shell.execute_reply":"2022-05-07T18:31:24.539581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Qual o percentual de target do nosso problema?\n\nAo olharmos a distribuição de target da nossa base toda temos um problema que concentra apenas 3,95% de clientes com target positiva","metadata":{}},{"cell_type":"code","source":"y.value_counts()/len(y)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:31:24.54253Z","iopub.execute_input":"2022-05-07T18:31:24.542968Z","iopub.status.idle":"2022-05-07T18:31:24.558896Z","shell.execute_reply.started":"2022-05-07T18:31:24.542931Z","shell.execute_reply":"2022-05-07T18:31:24.558109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Separação em treino e teste \n\n- Foram usados 25% dos dados para teste e 75% para treino\n- O split foi feito de forma estratificada para preservarmos a distribuição da target em treino e teste","metadata":{}},{"cell_type":"code","source":"seed = 18051996\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=seed,stratify=y)\n\nprint(\"Número de linhas no treino= \",len(x_train))\nprint('---------------------------')\nprint('Verificando distribuição da target no treino')\nprint(y_train.value_counts()/len(y_train)*100)\nprint('---------------------------')\nprint('Verificando distribuição da target no teste')\nprint(\"Número de linhas no teste = \",len(x_test))\nprint('---------------------------')\nprint(y_test.value_counts()/len(y_test)*100)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:31:24.560232Z","iopub.execute_input":"2022-05-07T18:31:24.560476Z","iopub.status.idle":"2022-05-07T18:31:25.184152Z","shell.execute_reply.started":"2022-05-07T18:31:24.560442Z","shell.execute_reply":"2022-05-07T18:31:25.183439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Case A**\nUm falso positivo ocorre quando classificamos um cliente como insatisfeito, mas ela não se comporta como tal. Neste caso, o custo de preparar e executar uma ação de retenção é um valor fixo de 10 reais por cliente. Nada é ganho pois a ação de retenção não é capaz de mudar o comportamento do cliente. Um falso negativo ocorre quando um cliente é previsto como satisfeito, mas na verdade ele estava insatisfeito. Neste caso, nenhum dinheiro foi gasto e nada foi ganho. Um verdadeiro positivo é um cliente que estava insatisfeito e foi alvo de uma ação de retenção. O benefício neste caso é o lucro da ação (RS 100) menos os custos relacionados à ação de retenção (RS 10). Por fim, um verdadeiro negativo é um cliente insatisfeito e que não é alvo de nenhuma ação. O benefício neste caso é zero, isto é, nenhum custo, mas nenhum lucro. A primeira tarefa deste case é maximizar o lucro esperado por cliente considerando o contexto descrito no parágrafo acima.","metadata":{}},{"cell_type":"markdown","source":"### Definição da fução que devemos otimizar\n\n- Aqui por mais que estamos trabalhando com m problema de classificação, devemos ter o cuidado pois o problema possui uma série de condições que devemos tomar cuidado na modelagem que são:\n\n- **Beneficio do verdadeiro positivo (cliente que estava insatisfeito e foi alvo de ação) = 100RS - 10RS**","metadata":{}},{"cell_type":"code","source":"def lucro(modelo, x, y_true,matriz = False):\n    \n    if modelo == \"aleatorio\":\n        x['y_pred'] = 0\n        y_pred = x['y_pred']\n        x.drop(columns='y_pred',inplace=True)\n    else:\n        y_pred = modelo.predict(x)\n    \n    #Cria matriz de confusao para nos ajudar capturar os TP e FP\n    matriz_confusao = confusion_matrix(y_true, y_pred)\n      \n    #capturando dados de verdadeiro positivo e falso positivo na matriz de confusao\n    verdadeiro_positivo = matriz_confusao[1][1]\n    falso_positivo = matriz_confusao[0][1]\n\n    #Calcula o lucro com base na formula do problema\n    lucro_total = 90*verdadeiro_positivo - 10*falso_positivo \n\n    if matriz == True:\n        print('Matriz de confusão:')\n        print(matriz_confusao)      \n        print('\\nLucro obtido com a ação: R$ ' + str(lucro_total))\n        print('Porcentagem do lucro ideal: ' + str(round(lucro_total/(int(y_true.sum())*90),2)) + '%')\n\n    \n    return lucro_total","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:31:25.185489Z","iopub.execute_input":"2022-05-07T18:31:25.185747Z","iopub.status.idle":"2022-05-07T18:31:25.193279Z","shell.execute_reply.started":"2022-05-07T18:31:25.185709Z","shell.execute_reply":"2022-05-07T18:31:25.192586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modelo baseline\n\n1) Temos um modelo totalmente desbalanceado 3,95% de targte se chutarmos todo mundo como target 0, acertamos 97% da base. Qual o lucro esperado de uma previsao totalmente aleatoria como essa? Será que teremos lucro?\n\n2) E se treinarmos uma random forest sem restrições e sem tratamento na base de treino qual o lucro esperado para essa previsão?","metadata":{}},{"cell_type":"code","source":"lucro(\"aleatorio\", x_test, y_test,True)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:31:25.194589Z","iopub.execute_input":"2022-05-07T18:31:25.195031Z","iopub.status.idle":"2022-05-07T18:31:25.264082Z","shell.execute_reply.started":"2022-05-07T18:31:25.19499Z","shell.execute_reply":"2022-05-07T18:31:25.263368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline = RandomForestClassifier(random_state=seed).fit(x_train,y_train)\nlucro(baseline, x_test, y_test,True)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:31:25.265391Z","iopub.execute_input":"2022-05-07T18:31:25.265815Z","iopub.status.idle":"2022-05-07T18:31:39.18111Z","shell.execute_reply.started":"2022-05-07T18:31:25.265777Z","shell.execute_reply":"2022-05-07T18:31:39.180408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modelo otimizado","metadata":{}},{"cell_type":"code","source":"max_profit_scorer = make_scorer(lucro, greater_is_better=True)\n\ndef train_model(x_train,y_train,x_test,y_test, objetivo):\n\n    def objective(params):\n        params = {'n_estimators': int(params['n_estimators'])\n              ,'max_depth': int(params['max_depth'])\n              ,'max_features': params['max_features']\n              ,'min_samples_split': int(params['min_samples_split'])}\n        clf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', **params,random_state=seed)\n        score = cross_val_score(clf, x_train, y_train, scoring=objetivo, cv=StratifiedKFold()).mean()\n        print(\"Lucro {:.3f} params {}\".format(score, params))\n        score = -1*(score)\n        return score\n\n    space = {\n    'n_estimators': hp.quniform('n_estimators', 50, 1000, 50),\n    'max_depth': hp.quniform('max_depth', 3, 8, 1),\n    'max_features': hp.choice('max_features', ['auto', 'sqrt','log2']),\n    \"min_samples_split\" : hp.quniform('min_samples_split',10,200,10)\n    }\n\n    best = fmin(fn=objective,\n                space=space,\n                algo=tpe.suggest,\n                max_evals=10)\n\n    # Print best parameters\n    best_params = space_eval(space, best)\n\n    print(\"Start training the best model\")\n\n    model = RandomForestClassifier(n_jobs=-1,random_state=seed\n                                          , class_weight='balanced'\n                                          , max_features=best_params['max_features']\n                                          , max_depth = int(best_params['max_depth'])\n                                          , min_samples_split = int(best_params['min_samples_split'])\n                                          , n_estimators = int(best_params['n_estimators'])).fit(x_train,y_train)\n    #Scoring the best model in train dataset\n    lucro(model, x_test, y_test)\n    print(\"Scoring the train data\")\n    #Scoring the best model in train dataset\n    predict_train_entire = model.predict(x_train)\n    proba_train_entire = model.predict_proba(x_train)[:,1]\n\n    print(\"Scoring the test data\")\n    #Scoring the best model in test dataset\n    predict_test_entire = model.predict(x_test)\n    proba_test_entire = model.predict_proba(x_test)[:,1]\n\n    print(\"Getting metrics\")\n    # calculate scores\n    auc_train = roc_auc_score(y_train, proba_train_entire)\n    auc_test = roc_auc_score(y_test, proba_test_entire )\n\n    print('AUC_TRAIN', auc_train)\n    print('AUC_Test', auc_test)\n    # calculate roc curves\n    fpr_train, tpr_train, _ = roc_curve(y_train, proba_train_entire)\n    fpr_test, tpr_test, _ = roc_curve(y_test, proba_test_entire)\n\n    # plot the roc curve for the model\n    pyplot.plot(fpr_train, tpr_train, linestyle='--', label='Train')\n    pyplot.plot(fpr_test, tpr_test, linestyle='--', label='Test')\n    # axis labels\n    pyplot.xlabel('False Positive Rate')\n    pyplot.ylabel('True Positive Rate')\n    # show the legend\n    pyplot.legend()\n    # show the plot\n    return pyplot.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:35:08.446671Z","iopub.execute_input":"2022-05-07T18:35:08.447115Z","iopub.status.idle":"2022-05-07T18:35:08.464656Z","shell.execute_reply.started":"2022-05-07T18:35:08.447077Z","shell.execute_reply":"2022-05-07T18:35:08.463836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Treinando um modelo de random forest\n\n- Tuning de alguns parametros\n\nAqui já obtemos um resultado melhor do que simplemente chutarmos aleatoriamente um valor para nossa target ou se treinarmos uma random forest sem restrições\n\n**LUCRO 5686R$**","metadata":{}},{"cell_type":"code","source":"train_model(x_train,y_train,x_test,y_test, objetivo = lucro)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:35:11.230732Z","iopub.execute_input":"2022-05-07T18:35:11.231311Z","iopub.status.idle":"2022-05-07T18:36:53.754468Z","shell.execute_reply.started":"2022-05-07T18:35:11.231272Z","shell.execute_reply":"2022-05-07T18:36:53.753618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Redução do número de features\n\nCom intuito de minimizar o custo de altas dimensões, ajudando a diminuir a complexidade computacional de todos algoritmos que serão criados e diminuir a chance de problemas de sobreajuste devido ao alto numero de dimensoes iremos fazer uma análise para diminuir o espaco de possibilidades. Todo processo foi divididos nessas etapas:\n\n- Eliminação de variaveis constantes ou que possuam pouca variação (variance threshould)\n- Eliminação de multicolinearidade em variaveis continuas","metadata":{}},{"cell_type":"code","source":"def variance_threshold(df,threshold):\n    vt = VarianceThreshold(threshold=threshold)\n\n    vt.fit(df)\n\n    mask = vt.get_support()\n\n    num_vars_reduced = df.iloc[:, mask]\n    return num_vars_reduced\n\ndef correlation(df, threshold):\n    col_corr = set() # Set of all the names of deleted columns\n    corr_matrix = df.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n                colname = corr_matrix.columns[i] # getting the name of column\n                col_corr.add(colname)\n                if colname in df.columns:\n                    del df[colname] # deleting the column from the dataset\n\n    return df\n\n#Aplicando variance e correlação\nnum_vars_vt = variance_threshold(x_train.filter(num_vars),threshold = 0.01)\nnum_vars_vt_corr = correlation(x_train.filter(num_vars_vt), threshold = 0.8)\n           \n#Select important features\nx_train = x_train.filter(list(num_vars_vt_corr.columns)+list(cat_vars)+list(id_columns)).fillna(0)\nx_test  =  x_test.filter(list(num_vars_vt_corr.columns)+list(cat_vars)+list(id_columns)).fillna(0)\n\nprint('Número total de features =', len(x_train.columns))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T16:10:20.945243Z","iopub.execute_input":"2022-05-07T16:10:20.945511Z","iopub.status.idle":"2022-05-07T16:10:32.031352Z","shell.execute_reply.started":"2022-05-07T16:10:20.945473Z","shell.execute_reply":"2022-05-07T16:10:32.029785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Recursive Feature elimination (Feature selection)\n\nCom intuito de selecionar as variaveis que melhor irão nos ajudar a discriminar nosso problema iremos utilizar um método chamado recursive feature elimination. esse método funciona da seguinte forma:\n\n- 1) Treina um modelo com todas as features\n- 2) Elimina as features com feature_importances_ menores\n- 3) Retreina um novo modelo com as features restantes\n- 4) Repete passo 2 e 3\n- 5) Avalia o número de features selecionadas versus a métrica de sucesso do seu modelo\n\nAqui escolhemos o algoritmo de Random forest e a métrica de avaliação é o nosso lucro máximo","metadata":{"execution":{"iopub.status.busy":"2022-05-04T14:01:08.859806Z","iopub.status.idle":"2022-05-04T14:01:08.860114Z","shell.execute_reply.started":"2022-05-04T14:01:08.859949Z","shell.execute_reply":"2022-05-04T14:01:08.859966Z"}}},{"cell_type":"code","source":"from yellowbrick.model_selection import RFECV\nfs_model = RandomForestClassifier(max_depth=7,random_state=seed,n_jobs=-1,n_estimators=100,class_weight='balanced')\n\n# Instantiate RFECV visualizer with a linear Random forest classifier\nvisualizer = RFECV(fs_model,scoring=lucro,cv=3,step=0.1)\n\n# Fit the data to the visualizer\nvisualizer.fit(x_train, y_train)\n\n# Finalize and render the figure\nvisualizer.show()\n\nprint('Optimal number of features :', visualizer.n_features_)\nbest_features = list(x_train.columns[visualizer.support_])\nprint('features selecionadas: ', best_features)\n\nx_train = x_train[best_features]\nx_test = x_test[best_features]","metadata":{"execution":{"iopub.status.busy":"2022-05-07T16:10:32.03255Z","iopub.execute_input":"2022-05-07T16:10:32.032844Z","iopub.status.idle":"2022-05-07T16:12:13.720763Z","shell.execute_reply.started":"2022-05-07T16:10:32.032804Z","shell.execute_reply":"2022-05-07T16:12:13.720054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validation curve \n\nDado que aplicamos o feature selection e outras técnicas para reduzirmos o número de features, iremos novamente treinar um novo modelo random forest passando por toda etapa de tuning de hiperparametros. \nPara termos um **melhor direcionamento** na etapa de tuning e termos um bom balanco entre **Viés e variancia** iremos utilizar o validation curve para testarmos como nossa métrica de performance se comporta dado um intervalo de busca dos parâmetros que serão tunados na random forest.","metadata":{}},{"cell_type":"code","source":"def plot_validation_curve(x,y,modelo,parametro,param_range,metrica):\n\n    # Calculate accuracy on training and test set using range of parameter values\n    train_scores, test_scores = validation_curve(modelo, \n                                                 x, \n                                                 y, \n                                                 param_name=parametro, \n                                                 param_range=param_range,\n                                                 cv=3, \n                                                 scoring=metrica, \n                                                 n_jobs=-1)\n\n\n    # Calculate mean and standard deviation for training set scores\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n\n    # Calculate mean and standard deviation for test set scores\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    # Plot mean accuracy scores for training and test sets\n    plt.plot(param_range, train_mean, label=\"Training score\", color=\"red\")\n    plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"blue\")\n\n    # Plot accurancy bands for training and test sets\n    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"gray\")\n    plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"gray\")\n\n    # Create plot\n    plt.title(\"Validation Curve With Random Forest\")\n    plt.xlabel(\"Parameter\")\n    plt.ylabel(\"retorno\")\n    plt.tight_layout()\n    plt.ylim(ymin=0)\n    plt.legend(loc=\"best\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T16:12:13.722943Z","iopub.execute_input":"2022-05-07T16:12:13.723412Z","iopub.status.idle":"2022-05-07T16:12:13.732872Z","shell.execute_reply.started":"2022-05-07T16:12:13.723373Z","shell.execute_reply":"2022-05-07T16:12:13.732072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Max depth","metadata":{}},{"cell_type":"code","source":"plot_validation_curve(x = x_train,\n                      y = y_train.values.reshape(-1,),\n                      modelo = RandomForestClassifier(class_weight= 'balanced'),\n                      parametro = \"max_depth\",\n                      param_range = np.arange(3, 10, 1),\n                      metrica = lucro)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T16:05:15.742541Z","iopub.status.idle":"2022-05-07T16:05:15.743169Z","shell.execute_reply.started":"2022-05-07T16:05:15.742936Z","shell.execute_reply":"2022-05-07T16:05:15.74296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### N estimators\n\nIremos iterar no número máximo de árvores que nossa random forest pode receber","metadata":{}},{"cell_type":"code","source":"plot_validation_curve(x = x_train,\n                      y = y_train.values.reshape(-1,),\n                      modelo = RandomForestClassifier(class_weight= 'balanced' ,max_depth=6),\n                      parametro = \"n_estimators\",\n                      param_range = np.arange(50, 300, 50),\n                      metrica = lucro)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T16:05:15.744401Z","iopub.status.idle":"2022-05-07T16:05:15.745026Z","shell.execute_reply.started":"2022-05-07T16:05:15.744789Z","shell.execute_reply":"2022-05-07T16:05:15.744814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Class weight","metadata":{}},{"cell_type":"code","source":"plot_validation_curve(x = x_train,\n                      y = y_train.values.reshape(-1,),\n                      modelo = RandomForestClassifier(class_weight= 'balanced' ,max_depth=6),\n                      parametro = \"class_weight\",\n                      param_range = np.arange(50, 300, 50),\n                      metrica = lucro)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T16:05:15.746363Z","iopub.status.idle":"2022-05-07T16:05:15.747102Z","shell.execute_reply.started":"2022-05-07T16:05:15.746815Z","shell.execute_reply":"2022-05-07T16:05:15.746844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tuning de hiperparametros dado o validation curve","metadata":{}},{"cell_type":"code","source":"def objective(params):\n    \n    params = {'n_estimators': int(params['n_estimators'])\n              ,'max_depth': int(params['max_depth'])\n              ,'max_features': params['max_features']\n              ,'min_samples_split': int(params['min_samples_split'])}\n    \n    clf = RandomForestClassifier(n_jobs=-1\n                                 , class_weight='balanced'\n                                 , **params\n                                 ,random_state=seed)\n    \n    score = cross_val_score(clf, x_train, y_train, scoring=lucro, cv=StratifiedKFold()).mean()\n    \n    print(\"Lucro {:.3f} params {}\".format(score, params))\n    score = -1*(score)\n    return score\n\nspace = {\n    'n_estimators': hp.quniform('n_estimators', 50, 1000, 50),\n    'max_depth': hp.quniform('max_depth', 3, 8, 1),\n    'max_features': hp.choice('max_features', ['auto', 'sqrt','log2']),\n    \"min_samples_split\" : hp.quniform('min_samples_split',10,200,10)\n}\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=10)\n\n# Print best parameters\nbest_params = space_eval(space, best)\n\nprint(\"Start training the best model\")\n\nmodel = RandomForestClassifier(n_jobs=-1,random_state=seed\n                                      , class_weight='balanced'\n                                      , max_features=best_params['max_features']\n                                      , max_depth = int(best_params['max_depth'])\n                                      , min_samples_split = int(best_params['min_samples_split'])\n                                      , n_estimators = int(best_params['n_estimators'])).fit(x_train,y_train)\nprint(\"Scoring the train data\")\n#Scoring the best model in train dataset\nlucro(model, x_test, y_test)\nprint(\"Scoring the train data\")\n#Scoring the best model in train dataset\npredict_train_entire = model.predict(x_train)\nproba_train_entire = model.predict_proba(x_train)[:,1]\n\nprint(\"Scoring the test data\")\n#Scoring the best model in test dataset\npredict_test_entire = model.predict(x_test)\nproba_test_entire = model.predict_proba(x_test)[:,1]\n\nprint(\"Getting metrics\")\n# calculate scores\nauc_train = roc_auc_score(y_train, proba_train_entire)\nauc_test = roc_auc_score(y_test, proba_test_entire )\n\n# calculate roc curves\nfpr_train, tpr_train, _ = roc_curve(y_train, proba_train_entire)\nfpr_test, tpr_test, _ = roc_curve(y_test, proba_test_entire)\n\n# plot the roc curve for the model\npyplot.plot(fpr_train, tpr_train, linestyle='--', label='Train')\npyplot.plot(fpr_test, tpr_test, linestyle='--', label='Test')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()\n\n# summarize scores\nprint('ROC AUC Train =%.3f' % (auc_train))\nprint('ROC AUC Test =%.3f' % (auc_test))\n\nprint('-----------------------------------------------------')\n\nks_stat_train = binary_ks_curve(y_train, predict_train_entire)[3]\nprint('ks train =',ks_stat_train)\n\nks_stat_test = binary_ks_curve(y_test, predict_test_entire)[3]\nprint('ks test =',ks_stat_test)\n\nprint('-----------------------------------------------------')\n\nrecall_train = recall_score(y_train, predict_train_entire)\nprint('recall train =',recall_train)\n\nrecall_test = recall_score(y_test, predict_test_entire)\nprint('recall test =',recall_test)\n\nprint('-----------------------------------------------------')\nprint(\"------------------------\")\nprint(\"Calculate decil in train\")\nprint(\"------------------------\")\n\navg_tgt = y_train.sum()/len(y_train)\ndf_data = x_train.copy()\nX_data = df_data.copy()\ndf_data['Actual'] = y_train\ndf_data['Predict']= model.predict(X_data)\ny_Prob = pd.DataFrame(model.predict_proba(X_data))\ndf_data['Prob_1']=list(y_Prob[1])\ndf_data.sort_values(by=['Prob_1'],ascending=False,inplace=True)\ndf_data.reset_index(drop=True,inplace=True)\ndf_data['Decile']=pd.qcut(df_data.index,5,labels=False)\noutput_df = pd.DataFrame()\ngrouped = df_data.groupby('Decile',as_index=False)\noutput_df['Qtd']=grouped.count().Actual\noutput_df['Sum_Target']=grouped.sum().Actual\noutput_df['Per_Target'] = (output_df['Sum_Target']/output_df['Sum_Target'].sum())*100\noutput_df['Per_Acum_Target'] = output_df.Per_Target.cumsum()\noutput_df['Max_proba']=grouped.max().Prob_1\noutput_df['Min_proba']=grouped.min().Prob_1\noutput_df[\"Per_Pop\"] = (output_df[\"Qtd\"]/len(y_train))*100\noutput_df[\"Lift\"] = output_df[\"Per_Acum_Target\"]/output_df.Per_Pop.cumsum()\noutput_df= output_df.drop(columns='Per_Pop')\nprint(round(output_df,3))\n\nprint(\"------------------------\")\nprint(\"Calculate decil in test\")\nprint(\"------------------------\")\nAvg_tgt = y_test.sum()/len(y_test)\ndf_data = x_test.copy()\nX_data = df_data.copy()\ndf_data['Actual'] = y_test\ndf_data['Predict']= model.predict(X_data)\ny_Prob = pd.DataFrame(model.predict_proba(X_data))\ndf_data['Prob_1']=list(y_Prob[1])\ndf_data.sort_values(by=['Prob_1'],ascending=False,inplace=True)\ndf_data.reset_index(drop=True,inplace=True)\ndf_data['Decile']=pd.qcut(df_data.index,5,labels=False)\noutput_df = pd.DataFrame()\ngrouped = df_data.groupby('Decile',as_index=False)\noutput_df['Qtd']=grouped.count().Actual\noutput_df['Sum_Target']=grouped.sum().Actual\noutput_df['Per_Target'] = (output_df['Sum_Target']/output_df['Sum_Target'].sum())*100\noutput_df['Per_Acum_Target'] = output_df.Per_Target.cumsum()\noutput_df['Max_proba']=grouped.max().Prob_1\noutput_df['Min_proba']=grouped.min().Prob_1\noutput_df[\"Per_Pop\"] = (output_df[\"Qtd\"]/len(y_test))*100\noutput_df[\"Lift\"] = output_df[\"Per_Acum_Target\"]/output_df.Per_Pop.cumsum()\noutput_df= output_df.drop(columns='Per_Pop')\nprint(round(output_df,3))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:26:14.287269Z","iopub.execute_input":"2022-05-07T18:26:14.287533Z","iopub.status.idle":"2022-05-07T18:28:29.080981Z","shell.execute_reply.started":"2022-05-07T18:26:14.287504Z","shell.execute_reply":"2022-05-07T18:28:29.079767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Batemos 8920 RS de lucro, agora vamos procura pelo melhor threshould para obtermos um máximo lucro","metadata":{}},{"cell_type":"code","source":"import itertools\ndef plot_confusion_matrix(y_test, y_pred, title='Confusion matrix'):\n    \n    cm = confusion_matrix(y_test, y_pred)\n    classes = ['Não satisfeito', 'Satisfeito']\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues, )\n    plt.title(title, fontsize=14)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    #capturando dados de verdadeiro positivo e falso positivo na matriz de confusao\n    verdadeiro_positivo = cm[1][1]\n    falso_positivo = cm[0][1]\n\n    #Calcula o lucro com base na formula do problema\n    lucro_total = 90*verdadeiro_positivo - 10*falso_positivo \n    print(round(lucro_total,2))\n    \n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j]),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \ndef train_clf_threshold(x_train,y_train,x_test,y_test,modelo):\n    thresholds = np.arange(0.1, 1, 0.1)\n    \n            \n    model.fit(x_train, y_train)\n    y_proba = modelo.predict_proba(x_test)\n    \n    plt.figure(figsize=(10,10))\n\n    j = 1\n    for i in thresholds:\n        y_pred = y_proba[:,1] > i\n\n        plt.subplot(4, 3, j)\n        j += 1\n\n        # Compute confusion matrix\n        cnf_matrix = confusion_matrix(y_test,y_pred)\n        np.set_printoptions(precision=2)\n\n        print(f\"Threshold: {round(i, 1)} | Test Precision: {round(precision_score(y_test, y_pred), 2)} \")\n\n        # Plot non-normalized confusion matrix\n        plot_confusion_matrix(y_test, y_pred, title=f'Threshold >= {round(i, 1)}')","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:25:12.799047Z","iopub.status.idle":"2022-05-07T18:25:12.799687Z","shell.execute_reply.started":"2022-05-07T18:25:12.799449Z","shell.execute_reply":"2022-05-07T18:25:12.799474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_clf_threshold(x_train,y_train,x_test,y_test,modelo=model)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:25:12.80107Z","iopub.status.idle":"2022-05-07T18:25:12.801752Z","shell.execute_reply.started":"2022-05-07T18:25:12.801484Z","shell.execute_reply":"2022-05-07T18:25:12.80151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **B**\nA segunda tarefa consiste em dar uma nota de 1 a 5 para cada cliente da base teste, respeitando a variável ‘TARGET’, isto é, o seu nível de satisfação, sendo 1 o mais insatisfeito e 5 o mais satisfeito. Ao dar essa nota deve-se ter em mente que somente os clientes com nota 1 serão alvos de uma ação de retenção e que o objetivo dessa ação é maximizar o lucro esperado por cliente (usando os mesmos valores da primeira questão).","metadata":{}},{"cell_type":"code","source":"print(\"melhor modelo\",model)\nproba_test = model.predict_proba(x_test)[:,1]\nthreshold = 0.7\n\nscore_nps = pd.cut(proba_test, bins=list(np.linspace(0,threshold,5))+[1.0], labels=[5,4,3,2,1])\n\ntable_fim = pd.concat((pd.Series(score_nps, name='score_nps')\n           , y_test.reset_index(drop=True))\n          , axis=1, ignore_index=False)\n\ntable_fim.groupby(['TARGET','score_nps']).size().unstack('TARGET').plot.bar(stacked=True,figsize = (16,6))\nplt.xticks(rotation=0)\nplt.ylabel('Volume de clientes no conjunto teste')\nplt.xlabel('SCORE NPS')\nplt.title('Volume de clientes por nota com volume real de clientes insatisfeitos')","metadata":{"execution":{"iopub.status.busy":"2022-05-07T16:15:24.435438Z","iopub.execute_input":"2022-05-07T16:15:24.435809Z","iopub.status.idle":"2022-05-07T16:15:25.72676Z","shell.execute_reply.started":"2022-05-07T16:15:24.43577Z","shell.execute_reply":"2022-05-07T16:15:25.726074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_fim = table_fim[table_fim.score_nps==1]['score_nps']\ny_true_fim = table_fim[table_fim.score_nps==1]['TARGET']\ny_true_total = table_fim['TARGET']","metadata":{"execution":{"iopub.status.busy":"2022-05-07T16:15:25.727849Z","iopub.execute_input":"2022-05-07T16:15:25.728651Z","iopub.status.idle":"2022-05-07T16:15:25.735451Z","shell.execute_reply.started":"2022-05-07T16:15:25.72861Z","shell.execute_reply":"2022-05-07T16:15:25.734756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matriz_confusao = confusion_matrix(y_true_fim, y_pred_fim)\n      \n#capturando dados de verdadeiro positivo e falso positivo na matriz de confusao\nverdadeiro_positivo = matriz_confusao[1][1]\nfalso_positivo = matriz_confusao[0][1]\n\n#Calcula o lucro com base na formula do problema\nlucro_total = 90*verdadeiro_positivo - 10*falso_positivo \nprint('\\nLucro obtido com a ação: R$ ' + str(lucro_total))\nprint('Porcentagem do lucro ideal: ' + str(round(lucro_total/(int(y_true_total.sum())*90),2)) + '%')","metadata":{"execution":{"iopub.status.busy":"2022-05-07T16:15:25.738117Z","iopub.execute_input":"2022-05-07T16:15:25.738413Z","iopub.status.idle":"2022-05-07T16:15:25.749176Z","shell.execute_reply.started":"2022-05-07T16:15:25.738377Z","shell.execute_reply":"2022-05-07T16:15:25.748349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *C*\nTodo conjunto de dados é passível de ser dividido em grupos coesos, conhecidos como agrupamentos naturais. A terceira tarefa é encontrar os três grupos naturais que possuem os maiores lucros esperados por cliente (usando os mesmos valores da primeira questão).","metadata":{}},{"cell_type":"code","source":"def clip_outliers(df,drop_columns,q_min,q_max):\n    \n    num_vars_name = df.select_dtypes(include=['float64']).columns     \n    for i in num_vars_name:\n        \n        #get max and min quantile\n        min_value = df.loc[:,i].quantile(q_min)\n        max_value = df.loc[:,i].quantile(q_max)\n\n        #replace values with max and min quantile value\n        df.loc[:,i] = np.where(df.loc[:,i] < min_value, min_value,df.loc[:,i])\n        df.loc[:,i] = np.where(df.loc[:,i] > max_value, min_value,df.loc[:,i])\n        \n    return df\n\n#Reduce outliers for clustering analyses\nx_train = clip_outliers(x_train,drop_columns=id_columns,q_min=0.05,q_max=0.95)\nx_test = clip_outliers(x_test,drop_columns=id_columns,q_min=0.05,q_max=0.95)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T16:15:25.750727Z","iopub.execute_input":"2022-05-07T16:15:25.751155Z","iopub.status.idle":"2022-05-07T16:15:25.87847Z","shell.execute_reply.started":"2022-05-07T16:15:25.751114Z","shell.execute_reply":"2022-05-07T16:15:25.877718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min = 4\nmax = 30\nwcss = []\nsilhouette= []\n\ntrain_kmeans = x_train.select_dtypes(include=\"float64\")\n\nfor i in range(min, max):\n    \n    ##Training a kmeans model\n    model = KMeans(n_clusters = i, random_state = seed,n_init=20)\n    model.fit(train_kmeans)\n    \n    #Scoring\n    pred = model.predict(train_kmeans)\n    \n    #Get silhouette score\n    score = silhouette_score(train_kmeans, pred)\n    \n    # inertia method returns wcss for that model\n    wcss.append(model.inertia_)\n    print('Silhouette Score for k = {}: {:<.3f}'.format(i, score))\n    \nplt.figure(figsize=(10,max))\nsns.lineplot(range(min, max), wcss,marker='o',color='red')\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T16:52:21.610393Z","iopub.execute_input":"2022-05-07T16:52:21.611184Z","iopub.status.idle":"2022-05-07T17:06:36.135813Z","shell.execute_reply.started":"2022-05-07T16:52:21.611132Z","shell.execute_reply":"2022-05-07T17:06:36.135144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\n#training\nbest_k = 9\n\nmodel = KMeans(n_clusters = best_k, random_state = 42)\n# model = GaussianMixture(n_components = best_k, random_state = 42)\nmodel.fit(train_kmeans)\n\n#Scoring\ncluster = model.predict(x_test.filter(train_kmeans.columns))\n\ndf_cluster = pd.DataFrame({'cluster': cluster}).join(y_test)\ndf_cluster.columns = ['labels','true_target']\n\n# # Create crosstab: ct\ndata = pd.crosstab(df_cluster['labels'],df_cluster['true_target'])\n\ndata ['volumetria'] = data [0]+data [1]\ndata ['lucro'] = data [1]*90 - data[0]*10\ndata.sort_values(by='lucro', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T16:51:08.262027Z","iopub.execute_input":"2022-05-07T16:51:08.262528Z","iopub.status.idle":"2022-05-07T16:51:13.759304Z","shell.execute_reply.started":"2022-05-07T16:51:08.262487Z","shell.execute_reply":"2022-05-07T16:51:13.758548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Avaliação de cada cluster em relação ao lucro","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}