{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Preliminaries**","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install Boruta\n!pip install fastcluster\n!pip install factor-analyzer","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:01:56.296778Z","iopub.execute_input":"2022-05-06T05:01:56.297392Z","iopub.status.idle":"2022-05-06T05:02:31.430242Z","shell.execute_reply.started":"2022-05-06T05:01:56.29724Z","shell.execute_reply":"2022-05-06T05:02:31.429331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import libraries\nimport fastcluster\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom boruta import BorutaPy\nfrom factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo, FactorAnalyzer # the sklearn FactorAnalysis function is inadequate\nfrom scipy.cluster import hierarchy\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.cluster import FeatureAgglomeration \nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import f_classif, mutual_info_classif, RFECV\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.svm import NuSVC, SVC","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:02:31.432568Z","iopub.execute_input":"2022-05-06T05:02:31.433054Z","iopub.status.idle":"2022-05-06T05:02:32.938701Z","shell.execute_reply.started":"2022-05-06T05:02:31.433011Z","shell.execute_reply":"2022-05-06T05:02:32.937815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import the data\ntrain = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/train.csv')\nsample_sub = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:02:32.940206Z","iopub.execute_input":"2022-05-06T05:02:32.940707Z","iopub.status.idle":"2022-05-06T05:02:46.581204Z","shell.execute_reply.started":"2022-05-06T05:02:32.940664Z","shell.execute_reply":"2022-05-06T05:02:46.580412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a look at the last couple of lines from the training dataset\ntrain.tail(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a look at the last couple of lines from the testing dataset\ntest.tail(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So at first glance, it looks like we have a mix of continuous/interval, binary and ordinal/categorical variables in the mix. Let's visualise these features to confirm.","metadata":{}},{"cell_type":"code","source":"# but first we need to encode \"f_27\"\noenc = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = 999999)\n\ntrain['f_27_enc'] = oenc.fit_transform(train['f_27'].values.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:10:29.902957Z","iopub.execute_input":"2022-05-06T05:10:29.903934Z","iopub.status.idle":"2022-05-06T05:10:35.495499Z","shell.execute_reply.started":"2022-05-06T05:10:29.903898Z","shell.execute_reply":"2022-05-06T05:10:35.494779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(6, 6, figsize=(40, 20), sharex=False)\nfor i, ax in enumerate(axs.flatten()):\n    if i < len(train.columns):\n        if train.columns[i] != \"f_27\":\n            sns.histplot(data = train, x = train.columns[i], ax = ax)\n            sns.despine();\n    else:\n        ax.set_axis_off()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's drop the id column and f_27\ntrain2 = train.drop(['id', 'f_27'], axis = 1)\n\n# separate the target column out\ntarget = train2.pop(\"target\")\n# and temporarily drop the new f_27\nf_27 = train2.pop(\"f_27_enc\")\n\n# ... before reinserting it in it's proper place\ntrain2.insert(loc = 27, column = \"f_27\", value = f_27)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:10:35.497633Z","iopub.execute_input":"2022-05-06T05:10:35.497948Z","iopub.status.idle":"2022-05-06T05:10:35.666702Z","shell.execute_reply.started":"2022-05-06T05:10:35.497906Z","shell.execute_reply":"2022-05-06T05:10:35.6658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot a cluster map - can't be visualised with the entire dataset/takes a loong time to do so\nlut = dict(zip(target.unique(), \"rbg\"))\nrow_colors = target.map(lut)\n\nsns.clustermap(train2.iloc[:1000], metric = \"euclidean\", method = \"ward\", row_colors = row_colors, cmap=\"mako\", z_score = 1);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reinsert the target variable\ntrain2_incl_target = train2.copy()\ntrain2_incl_target.insert(loc = 31, column = \"target\", value = target)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:10:35.667773Z","iopub.execute_input":"2022-05-06T05:10:35.667981Z","iopub.status.idle":"2022-05-06T05:10:35.975182Z","shell.execute_reply.started":"2022-05-06T05:10:35.667955Z","shell.execute_reply":"2022-05-06T05:10:35.974404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate the correlations\ncorr = train2_incl_target.corr(method = \"pearson\")\n\nplt.figure(figsize=(40, 10))\nax = sns.heatmap(corr, vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n = 200), square=True, linewidth = 0.5)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's not a lot to work with here, but let's proceed with performing a number of different decomposition techniques to see what comes out.","metadata":{}},{"cell_type":"markdown","source":"## **PCA**","metadata":{}},{"cell_type":"code","source":"# scale the data\nsc = StandardScaler()\ntrain2_sc = sc.fit_transform(train2)\n\n# perform the PCA\npca = PCA(n_components = 5)\ntrain_pc = pca.fit_transform(train2_sc)\n\npc_df = pd.DataFrame(data = train_pc, columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])\npc_df['target'] = target\n\n# observed the amount of explained variation by each component\nprint('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n\n# extract the feature weights for each component\nweights = pca.components_\nweights_df = pd.DataFrame(data = weights.reshape(31,5), columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5'], index = train2.columns)\n#weights_df = weights_df.sort_values(by = 'PC1', axis = 0, ascending = False)\n#weights_df = weights_df.sort_values(by = 'PC2', axis = 0, ascending = False)\n#weights_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loadings plot\nplt.figure(figsize = (10,10))\np1 = sns.scatterplot(x = \"PC1\", y = \"PC2\", data = weights_df)\nplt.axvline(x = 0, color = 'red', alpha = 0.5, ls = '--')\nplt.axhline(y = 0, color = 'red', alpha = 0.5, ls = '--')\nsns.despine();\n\n# add text annotations\nfor line in range(0, weights_df.shape[0]):\n     p1.text(weights_df['PC1'][line]+0.01, weights_df['PC2'][line], \n             weights_df.index[line], horizontalalignment='left', \n             size = 'medium', color='black')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# observations plot\nplt.figure(figsize = (10,10))\nsns.scatterplot(x = \"PC1\", y = \"PC2\", hue = \"target\", data = pc_df)\nsns.despine();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly doing a horrible job at distinguishing between the two classes.","metadata":{}},{"cell_type":"markdown","source":"## **Factor Analysis**","metadata":{}},{"cell_type":"code","source":"# first test if the dataset is a good candidate for factor analysis - which it isn't but we'll proceed anyway\nchi_square_value, p_value = calculate_bartlett_sphericity(train2)\nkmo_all, kmo_model = calculate_kmo(train2)\nprint('Bartlett-sphericity chi-square: {}'.format(chi_square_value))\nprint('Bartlett-sphericity p-value: {}'.format(p_value))\nprint('KMO score: {}'.format(kmo_model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2_sc = sc.fit_transform(train2)\n\n# perform the Factor Analysis\nbase_fa = FactorAnalyzer(rotation = None)\ntrain_fa = base_fa.fit_transform(train2_sc)\n\nev, v = base_fa.get_eigenvalues()\nev_df = pd.DataFrame(ev, columns = ['eigenvalue'])\nev_df['factor'] = range(1,32)\n\n# scree plot\nplt.figure(figsize = (15,5))\nsns.lineplot(x = \"factor\", y = \"eigenvalue\", data = ev_df)\nplt.axhline(y = 1.0, color = 'red', alpha = 0.5, ls = '--')\nsns.despine();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"13 factors have an eigenvalue greater than one, so this is the number of unobserved variables.","metadata":{}},{"cell_type":"code","source":"# repeat but with promax rotation and a pre-specified number of factors\nfa = FactorAnalyzer(n_factors = 13, rotation = 'promax', method = \"ml\")\ntrain_fa = fa.fit_transform(train2_sc)\n\nfvariance = fa.get_factor_variance()\nvariance_df = pd.DataFrame(fvariance, columns = ['Factor {}'.format(i) for i in range(1, 13+1)], index = ['SS loadings', 'Proportion Var', 'Cumulative Var'])\nvariance_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get loadings\nloadings = fa.loadings_\nloadings_df = pd.DataFrame(loadings, columns = ['Factor {}'.format(i) for i in range(1, 13+1)], index = train2.columns)\nloadings_df['highest loading'] = loadings_df.idxmax(axis = 1)\nloadings_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, now this is interesting:\n- Factor 1 is loaded almost purely on f_25\n- Factor 2 is loaded almost purely on f_30\n- Factor 3 is loaded almost purely on f_03\n- Factor 4 is loaded almost purely on f_20\n- Factor 5 is loaded almost purely on f_05\n- Factor 6 - f_28\n- Factor 7 - f_19\n- Factor 8 - a high loading on f_21\n- Factor 9 to 13 - nothing high, suggesting we can drop those factors\n\nThere is also agreement amongst the various rotation methods that actually 6/7 factors is sufficient.\n\nThe final column identifies the factor with the largest loading for each variable, which could provide some direction as to what groupings of variables to investigate for interactions.","metadata":{}},{"cell_type":"code","source":"# repeat but with promax rotation and 8 factors\nfa_8 = FactorAnalyzer(n_factors = 8, rotation = 'promax', method = \"ml\")\ntrain_fa_8 = fa_8.fit_transform(train2_sc)\n\nfvariance = fa_8.get_factor_variance()\nvariance_df = pd.DataFrame(fvariance, columns = ['Factor {}'.format(i) for i in range(1, 8+1)], index = ['SS loadings', 'Proportion Var', 'Cumulative Var'])\nvariance_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get new loadings\nloadings = fa_8.loadings_\nloadings_df = pd.DataFrame(loadings, columns = ['Factor {}'.format(i) for i in range(1, 8+1)], index = train2.columns)\nloadings_df['highest loading'] = loadings_df.idxmax(axis = 1)\nloadings_df = loadings_df.sort_values(by = ['highest loading'])\nloadings_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The point of all this was to attempt to find interlinked associations. i.e. to reduce the observed variables into a few latent/unobserved variables or identify groups of interlinked variables to reveal hidden relationships.\n\nAs per the previous comment, the last column could provide some direction on what variables to investigate for interactions. For example, the loadings table indicates that f_26, f_24 and f_20 could be describing the same unobserved feature.","metadata":{}},{"cell_type":"code","source":"# plot the heatmap for the loadings\nplt.figure(figsize = (80, 5))\n\nax = sns.heatmap(loadings_df.drop('highest loading', axis=1).T, \n                 vmin = -1, vmax = 1, center = 0,\n                 cmap = sns.diverging_palette(220, 20, n = 200),\n                 square = True, annot = True, fmt = '.2f', annot_kws = {\"fontsize\":8})\nax.set_xticklabels(ax.get_xticklabels(), rotation = 45, horizontalalignment='right');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Hierachical Clustering of Variables**","metadata":{}},{"cell_type":"code","source":"# lifted verbatim from the sklearn documentation\ndef plot_dendrogram(model, **kwargs):\n    # Create linkage matrix and then plot the dendrogram\n\n    # create the counts of samples under each node\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack(\n        [model.children_, model.distances_, counts]\n    ).astype(float)\n\n    # Plot the corresponding dendrogram\n    hierarchy.dendrogram(linkage_matrix, **kwargs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit the two models to the scaled data\neuc_fc = FeatureAgglomeration(distance_threshold = 0, n_clusters = None, affinity = \"euclidean\", linkage = \"ward\")\neuc_fit = euc_fc.fit(train2_sc)\n\ncos_fc = FeatureAgglomeration(distance_threshold = 0, n_clusters = None, affinity = \"cosine\", linkage = \"complete\")\ncos_fit = cos_fc.fit(train2_sc)\n\n# and plot their respective dendograms\nplt.figure(figsize = (20, 8))\nplt.title(\"Hierarchical Clustering Dendrogram - euclidean distances and ward linkage\")\nhierarchy.set_link_color_palette(['grey', 'maroon', 'burlywood', 'darkgreen', 'deepskyblue', 'mediumslateblue'])\nplot_dendrogram(euc_fit, leaf_rotation = 0, color_threshold = 1400, labels = train2.columns)\n\nplt.figure(figsize = (20, 8))\nplt.title(\"Hierarchical Clustering Dendrogram - cosine distances and complete linkage\")\nplot_dendrogram(cos_fit, leaf_rotation = 0, color_threshold = 1.05, labels = train2.columns)\n\n# reset the colour palette\nhierarchy.set_link_color_palette(None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The cosine distance clustering is possibly slightly preferable as it gives fairly even clusters, whereas the euclidean clustering has one cluster much bigger than the rest. That being said, if you ignore the colours of the clusters (they mean absolutely nothing) and concentrate just on which variables have been placed together, you'll find remarkable consistency between the two results. ","metadata":{}},{"cell_type":"markdown","source":"## **Univariate Feature Selection**","metadata":{}},{"cell_type":"code","source":"mi_func = mutual_info_classif(train2, target)\nf_func = f_classif(train2, target)\n\nufc_df = pd.DataFrame(data = {'F stat': f_func[0], \n                              'p-value': f_func[1],\n                              'mutual info score': mi_func}, index = train2.columns)\n\nfig, axs = plt.subplots(2, 1, figsize=(20, 10), sharex = False)\n\nplot_data = ufc_df.sort_values(by = 'mutual info score', ascending = False)\np = sns.barplot(x = plot_data.index, y = \"mutual info score\", data = plot_data, ax = axs[0])\np.set_title('Feature importance based on mutual information scores')\n\nplot_data = ufc_df.sort_values(by = 'F stat', ascending = False)\np = sns.barplot(x = plot_data.index, y = 'F stat', data = plot_data, ax = axs[1])\np.set_title('Feature importance based on ANOVA F-statistics')\nsns.despine();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, according to both these methods, f_27 carries the most predictive power with respect to predicting the target class followed by f_30 or f_21 depending on your metric of choice. However, I wonder if that's purely because the encoded-values of f_27 are so much larger than any other feature. Let's find out...","metadata":{}},{"cell_type":"code","source":"mi_func = mutual_info_classif(train2_sc, target)\nf_func = f_classif(train2_sc, target)\n\nufc_df = pd.DataFrame(data = {'F stat': f_func[0], \n                              'p-value': f_func[1],\n                              'mutual info score': mi_func}, index = train2.columns)\n\nfig, axs = plt.subplots(2, 1, figsize=(20, 10), sharex = False)\n\nplot_data = ufc_df.sort_values(by = 'mutual info score', ascending = False)\np = sns.barplot(x = plot_data.index, y = \"mutual info score\", data = plot_data, ax = axs[0])\np.set_title('Feature importance based on mutual information scores and the scaled data')\n\nplot_data = ufc_df.sort_values(by = 'F stat', ascending = False)\np = sns.barplot(x = plot_data.index, y = 'F stat', data = plot_data, ax = axs[1])\np.set_title('Feature importance based on ANOVA F-statistics and the scaled data')\nsns.despine();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So it's not the case and f_27 is actually the most important predictor. However, whilst the ANOVA metric maintains that f_21 is next-most important, the mutual information metric changes its mind and puts f_29 in second place (up from way down the list) and f_30 in third.","metadata":{}},{"cell_type":"markdown","source":"## **Boruta**\n\nBoruta is an interesting package. It claims that it's an all-relevant feature selection method rather than just trying to minimise the error, which means that it's attempting to find *all* features carrying information useful for prediction.","metadata":{}},{"cell_type":"code","source":"# define the random forest classifier\nrf = RandomForestClassifier(n_jobs = -1, max_depth = 3)  ## recommended max_depth 3 to 7\n\n# define the Boruta feature selection method\n# perc = 100 - threshold for comparison between real and shadow features, i.e. the maximum\n# verbose = 2, print all output\nfeat_selector = BorutaPy(rf, n_estimators = 'auto', verbose = 2, max_iter = 100, random_state = 22)\n\n# find all relevent features\nfeat_selector.fit(train2.values, target.ravel())  ## BorutaPy only accepts numpy arrays","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the number of selected features\nprint('The number of selected features {}.'.format(feat_selector.n_features_))\n\n# check which features were selected/retained\n#print('Selected features {}'.format(feat_selector.support_))\n\n# check feature rankings\n#print('Feature rankings {}'.format(feat_selector.ranking_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Scatter Plot Matrix (SPLOM)**","metadata":{}},{"cell_type":"code","source":"# apologies for this being so small\ng = sns.PairGrid(train2_incl_target.iloc[:1000], hue = \"target\")\ng.map_diag(sns.kdeplot)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Pairwise Interaction Modelling - Perceptron**","metadata":{}},{"cell_type":"markdown","source":"A Perceptron classifier is a linear classifier suitable for large-scale learning. That is, it doesn't require a learning rate, isn't regularised and updates its model only on mistakes.","metadata":{}},{"cell_type":"code","source":"# taking an idea from another competitor - encoding each letter in f_27 as its own value\nsplit_f27 = train.f_27.str.split('', n = 0, expand = True)\n\nencoded_f27 = train.copy()\nfor col in np.arange(1, 10+1):\n    encoded_f27['f_27_' + str(col)] = split_f27[col]\nencoded_f27.drop(encoded_f27.columns[0:34], axis = 1, inplace = True)\n\n# then encoding them\noenc2 = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = 99)\n\nfor col in encoded_f27.columns:\n    encoded_f27[col] = oenc2.fit_transform(encoded_f27[col].values.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:10:49.936939Z","iopub.execute_input":"2022-05-06T05:10:49.937508Z","iopub.status.idle":"2022-05-06T05:10:59.208185Z","shell.execute_reply.started":"2022-05-06T05:10:49.937459Z","shell.execute_reply":"2022-05-06T05:10:59.207437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create all the pairwise interactions b/w f_27 (and variations thereof) and the other variables\n# this results in a performance warning regarding a fragmented data frame, but it can be safely ignored (I think)\ntrain_int = train2.copy()\ntrain_int = pd.concat([train_int, encoded_f27], axis = 1)\n    \nsize_col = train_int.columns.size\nfor i in range(0, size_col):\n    for j in range (i + 1, size_col):\n        col1 = str(train_int.columns[i])\n        col2 = str(train_int.columns[j])\n        col_name = col1 + \"_\" + col2\n        train_int[col_name] = pd.Series(train_int[col1] * train_int[col2], name = col_name)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:10:59.209954Z","iopub.execute_input":"2022-05-06T05:10:59.210433Z","iopub.status.idle":"2022-05-06T05:11:05.37824Z","shell.execute_reply.started":"2022-05-06T05:10:59.210389Z","shell.execute_reply":"2022-05-06T05:11:05.3774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split train data into training and validation sets (or development and evaluation as sklearn calls them)\n# running into memory/RAM issues so only training on a subset of the data\nX = train_int[:30000]\ny = target[:30000]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 22)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T05:11:05.379484Z","iopub.execute_input":"2022-05-06T05:11:05.379785Z","iopub.status.idle":"2022-05-06T05:11:05.752822Z","shell.execute_reply.started":"2022-05-06T05:11:05.379733Z","shell.execute_reply":"2022-05-06T05:11:05.752139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train an elastic net classifier model, using 5-fold cross-validation to identify the best combination of lambda and \n# alpha\nparams = {'l1_ratio': [0.2, 0.4, 0.6, 0.8, 1],\n          'alpha': [0.0001, 0.001, 0.01, 0.1]}\n\npmod = Perceptron(penalty = \"elasticnet\", random_state = 22, n_jobs = -1, verbose = 0, early_stopping = True)\np_clf = GridSearchCV(estimator = pmod, \n                     param_grid = params, \n                     scoring = 'roc_auc',\n                     n_jobs = -1)\np_clf.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(p_clf.best_estimator_)\nprint(p_clf.best_params_)\nprint(p_clf.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Repeated runs have indicated that an `alpha` of 0.01 and `l1 ratio` of 1 produces the highest score. This means that the model is basically a Lasso regression model, which means that it can perform feature selection by setting some coefficients in the model to zero.","metadata":{}},{"cell_type":"code","source":"p_coef_df = pd.DataFrame(data = {'feature': train_int.columns,\n                                 'coefficient': p_clf.best_estimator_.coef_[0],\n                                 'abs_coefficient': abs(p_clf.best_estimator_.coef_[0])})\np_coef_df = p_coef_df.sort_values(by = 'abs_coefficient', ascending = False)\np_coef_df = p_coef_df.loc[p_coef_df.coefficient > 0]\n\nplt.figure(figsize = (20, 5))\np1 = sns.stripplot(data = p_coef_df[:30], x = \"feature\", y = \"abs_coefficient\", size = 10, jitter = False, \n                   palette = \"mako\", linewidth = 1, edgecolor = \"w\")\np1.set_xticklabels(p1.get_xticklabels(), rotation = 30)\nsns.despine();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's a common factor here. Even if the model did no better than chance, it still thought that the interactions between f_27 and many of the other features were very important in predicting the target class. It's also interesting to note that the top 30 features in the model are all pairwise interactions...","metadata":{}},{"cell_type":"markdown","source":"... and that's all for this one folks. Thanks to everyone that has upvoted my notebook. It means a lot that my work is appreciated. There's a couple of other things I wanted to include, but they just didn't seem to want to work or were taking a crazy amount of time to run, even on a reduced dataset.","metadata":{}}]}