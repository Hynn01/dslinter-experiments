{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I answer two questions from T-1 and one from T-2.","metadata":{}},{"cell_type":"markdown","source":"# T-1 Study Question 2\n\nAnswering causal questions using data often cannot leverage perfect randomization. The most immediate cases concern non-experimental data: it is likely that, even if I were to control for all possible confounders I observe in the data, unobserved confounders exist. And even if I ran a randomized controlled trial, the randomization may not be perfect: maybe subjects refuse to comply with whatever treatment group to which they are assigned. More formally, as the notes describe, a set of latent factors $A$ could exist that affects both the treatment of interest $D$ and the outcome $Y$. One solution, however, is instrumental variables regression, in which I argue that a variable $Z$ is *directly* related to $D$ but neither to $A$ nor $Y$. Intuitively, $Z$ creates \"exogenous variation\" in $D$ that allows me to indirectly estimate the effect of $D$ on $Y$ using the relationship between $Z$ and $D$. Economist Philip Wright developed instrumental variables regression and illustrated it using path diagrams, which are equivalent to Pearl's DAGs. I can easily translate my definition of instrumental variables regression into its corresponding path diagram. These diagrams serve to illustrate an argument for an instrumental variable's validity, which in general rests on the two assumptions that (1) the instrument is related directly to the treatment variables that are endogenous and (2) the instrument itself cannot be endogenous. The second assumption specifically ensures the data become as good as random. \n\nFinally, I illustrate my discussion using Acemoglu, Johnson and Robinson (see the \"Debiased ML for Partially Linear IV Model in R\" notebook). Studying countries colonized by Europeans, Acemoglu et al. examine whether the quality of institutions affected economic performance. They circumvent unobserved confounders by leveraging \"mortality rates expected by the first European settlers in the colonies as an instrument for current institutions in these countries\" (AJR, p. 1370). I now discuss the notebook's code. ","metadata":{}},{"cell_type":"code","source":"urlPackage <- \"https://cran.r-project.org/src/contrib/Archive/randomForest/randomForest_4.6-14.tar.gz\"\ninstall.packages(urlPackage, repos=NULL, type=\"source\") \ninstall.packages(\"hdm\")\ninstall.packages(\"AER\")\n#install.packages(\"randomForest\")","metadata":{"execution":{"iopub.status.busy":"2022-05-06T18:55:01.888976Z","iopub.execute_input":"2022-05-06T18:55:01.891209Z","iopub.status.idle":"2022-05-06T18:55:46.068807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The outcome $Y$ is log GDP; the treatment $D$ is the measure of quality of institution; the instrument $Z$ is log mortality of settlers; $X$ denotes a set of geographic controls. We use a debiased machine learning approach to compare predictive performance using random forest and lasso. Random forest appears to perform better.  ","metadata":{}},{"cell_type":"code","source":"library(AER)  #applied econometrics library\nlibrary(randomForest)  #random Forest library\nlibrary(hdm) #high-dimensional econometrics library\nlibrary(glmnet) #glm net\n\n# DML for PLIVM\n\nDML2.for.PLIVM <- function(x, d, z, y, dreg, yreg, zreg, nfold=2) {\n  # this implements DML2 algorithm, where there moments are estimated via DML, before constructing\n  # the pooled estimate of theta randomly split data into folds\n  nobs <- nrow(x)\n  foldid <- rep.int(1:nfold,times = ceiling(nobs/nfold))[sample.int(nobs)]\n  I <- split(1:nobs, foldid)\n  # create residualized objects to fill\n  ytil <- dtil <- ztil<- rep(NA, nobs)\n  # obtain cross-fitted residuals\n  cat(\"fold: \")\n  for(b in 1:length(I)){\n    dfit <- dreg(x[-I[[b]],], d[-I[[b]]])  #take a fold out\n    zfit <- zreg(x[-I[[b]],], z[-I[[b]]])  #take a fold out\n    yfit <- yreg(x[-I[[b]],], y[-I[[b]]])  # take a folot out\n    dhat <- predict(dfit, x[I[[b]],], type=\"response\")  #predict the fold out\n    zhat <- predict(zfit, x[I[[b]],], type=\"response\")  #predict the fold out\n    yhat <- predict(yfit, x[I[[b]],], type=\"response\")  #predict the fold out\n    dtil[I[[b]]] <- (d[I[[b]]] - dhat) #record residual\n    ztil[I[[b]]] <- (z[I[[b]]] - zhat) #record residual\n    ytil[I[[b]]] <- (y[I[[b]]] - yhat) #record residial\n    cat(b,\" \")\n  }\n  ivfit= tsls(y=ytil,d=dtil, x=NULL, z=ztil, intercept=FALSE)\n  coef.est <-  ivfit$coef          #extract coefficient \n  se <-  ivfit$se                  #record standard error\n  cat(sprintf(\"\\ncoef (se) = %g (%g)\\n\", coef.est , se))\n  return( list(coef.est =coef.est , se=se, dtil=dtil, ytil=ytil, ztil=ztil) )\n}\n\ndata(AJR); \n\ny = AJR$GDP; \nd = AJR$Exprop; \nz = AJR$logMort\nxraw= model.matrix(~ Latitude+ Africa+Asia + Namer + Samer, data=AJR)\nx = model.matrix(~ -1 + (Latitude + Latitude2 + Africa + \n                           Asia + Namer + Samer)^2, data=AJR)\ndim(x)\n\n# DML with Random Forest\ncat(sprintf(\"\\n DML with Random Forest \\n\"))\n\ndreg <- function(x,d){ randomForest(x, d) }  #ML method=Forest\nyreg <- function(x,y){ randomForest(x, y) }  #ML method=Forest\nzreg<-  function(x,z){ randomForest(x, z)}  #ML method=Forest \n  \nset.seed(1)\nDML2.RF = DML2.for.PLIVM(xraw, d, z, y, dreg, yreg, zreg, nfold=20)\n\n# DML with PostLasso\ncat(sprintf(\"\\n DML with Post-Lasso \\n\"))\n\ndreg <- function(x,d){ rlasso(x, d) }  #ML method=lasso\nyreg <- function(x,y){ rlasso(x, y) }  #ML method=lasso\nzreg<-  function(x,z){ rlasso(x, z)}  #ML method=lasso \n\nset.seed(1)\nDML2.lasso = DML2.for.PLIVM(x, d, z, y, dreg, yreg, zreg, nfold=20)\n\n\n# Compare Forest vs Lasso\n\ncomp.tab= matrix(NA, 3, 2)\ncomp.tab[1,] = c( sqrt(mean((DML2.RF$ytil)^2)),  sqrt(mean((DML2.lasso$ytil)^2)) )\ncomp.tab[2,] = c( sqrt(mean((DML2.RF$dtil)^2)),  sqrt(mean((DML2.lasso$dtil)^2)) )\ncomp.tab[3,] = c( sqrt(mean((DML2.RF$ztil)^2)),  sqrt(mean((DML2.lasso$ztil)^2)) )\nrownames(comp.tab) = c(\"RMSE for Y:\", \"RMSE for D:\", \"RMSE for Z:\")\ncolnames(comp.tab) = c(\"RF\", \"LASSO\")\nprint(comp.tab, digits=3)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T18:56:10.670673Z","iopub.execute_input":"2022-05-06T18:56:10.704985Z","iopub.status.idle":"2022-05-06T18:56:18.961975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally we circle back to the earlier discussion by examining the validity of the instrumental variables $Z$. Since the regressions below result in magnitudes of the t-statistics below fail to be large enough, the instrumental variables are not strong enough because they fail to sufficiently meet the assumption that $D$ and $Z$ are sufficiently correlated. Econometricians would say we have \"weak instruments.\"  ","metadata":{}},{"cell_type":"code","source":"install.packages(\"lfe\")\nlibrary(lfe)\nsummary(felm(DML2.lasso$dtil~DML2.lasso$ztil), robust=T)\nsummary(felm(DML2.RF$dtil~DML2.RF$ztil), robust=T)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T18:58:59.265452Z","iopub.execute_input":"2022-05-06T18:58:59.267349Z","iopub.status.idle":"2022-05-06T18:59:21.731705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# T-1 Study Question 3\n\nI simulate the proxy controls model as in the T-1 chapter's Figure 11.4. First, below is the DAG. ","metadata":{}},{"cell_type":"code","source":"install.packages(\"dagitty\")\ninstall.packages(\"ggdag\")\n#install.packages(\"AER\")\nlibrary(dagitty)\nlibrary(ggdag)\n\ng <- dagitty('dag {\n  Q\n  S\n  D \n  Y \n  A \n  A -> Q\n  A -> S\n  A -> D\n  A -> Y\n  \n  D -> Y\n \n}')\n\nggdag(g) + theme_dag()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next I demonstrate numerically that regressing $Y$ on $D$ via OLS is inappropriate due to the unobserved confounding, i.e. $A$. Assume $A$ is a standard Normal random variable, and that each random variable's error term embedded in the above DAG is standard Normal. Given this data generating process I simulate 10000 values of all random variables. Here I find that the estimated effect of $D$ on $Y$ is about 1.4852.","metadata":{}},{"cell_type":"code","source":"set.seed(333)\nn <- 10000\na = rnorm(n)\ns = a + rnorm(n)\nq = a + rnorm(n)\nd = a + rnorm(n)\ny = d + a + rnorm(n)\ndf <- data.frame(a, s, q, d, y)\nmodel_bad <- lm(y ~ d, data = df)\nsummary(model_bad)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T18:57:01.228398Z","iopub.execute_input":"2022-05-06T18:57:01.230181Z","iopub.status.idle":"2022-05-06T18:57:01.294728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The instrumental variables (i.e. correct) approach, however, results in an estimated effect of about 1.9913. This implies that, in this simulation example, failing to control for the unobserved confounder $A$ biases the estimated effect of interest downwards.  ","metadata":{}},{"cell_type":"code","source":"model_good <- ivreg(y ~ d + s| (s + q), data = df)\nsummary(model_good)","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:10:44.87388Z","iopub.execute_input":"2022-05-06T19:10:44.875564Z","iopub.status.idle":"2022-05-06T19:10:44.926739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# T-2 Study Question 1\n\nI experiment with the \"R: Weak IV Experiments\" Notebook by varying the strength of the instrument. To assess how strong the instrument should be in order for the conventional normal approximation, based on strong approximation, to provide accurate inference, I try different `beta` values (see the first code chunk below) and determine the value at which the rejection frequency (see the final code chunk below) is first minimized. The closer this rejection frequency is to 0.05, the more accurate conventional normal approximation is. After trying many different `beta` values ranging from 0.25 to 10 thousand, I find that a `beta` value of 10 is when the rejection frequency begins to come closest. This corresponds to the strength of the instrumental variable of interest. Observe this results in an estimated slope of about 9 when we regress the treatment $D$ on the instrument $Z$. \n\nThe weak instrument problem concerns the relationship between the instrument and the treatment variable of interest. A instrument is weak if its correlation with the treatment is weak. This is one of the two assumptions of an instrumental variable that I discussed earlier. While this assumption can be verified empirically, it is often difficult to satisfy because instruments are often used to analyze non-experimental data, in which case it becomes difficult for nature to provide exogenous variation that is unquestionably strong. This assumption is important to meet because otherwise estimating the causal effect becomes more difficult. My experiment demonstrates this harsh reality: it is no wonder then that many seemingly clever instrumental variables end up insufficient due to weakness, and so their estimated causal effects of interest become more biased than hoped.  ","metadata":{}},{"cell_type":"code","source":"# Simulation Design\n\nlibrary(hdm)\nset.seed(1)\nB= 10000 # trials\nIVEst = rep(0, B)\nn=100\n#beta = .25   # .2 weak IV\n#beta = 0.5 # stronger IV\n#beta = 1   #   1 strong IV\n#beta = 2 # stronger IV\n#beta = 5 # stronger IV - rejection region 0.0571\n#beta = 7.5 # stronger IV - rejection region 0.0562\n#beta = 9 # rejection region 0.0565\nbeta = 10 # stronger IV - rejection region 0.056\n#beta = 50 # stronger IV - rejection rejection 0.056\n#beta = 100 # stronger IV - rejection region 0.0561\n#beta = 1000 # stronger IV - rejection region 0.056\n#beta = 10000 # strongest IV tried - rejection region 0.056\n\nU =  rnorm(n)  \nZ = rnorm(n)  #generate instrument\nD = beta*Z + U  #generate endogenougs variable\nY =  D+ U  # the true causal effect is 1\n\n\nsummary(lm(D~Z))  # first stage is very weak here\n\nsummary(tsls(x=NULL, d=D, y=Y, z=Z))  #","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:55:02.305112Z","iopub.execute_input":"2022-05-06T19:55:02.306854Z","iopub.status.idle":"2022-05-06T19:55:02.355415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Simulation Design\n\nset.seed(1)\nB= 10000 # trials\nIVEst = rep(0, B)\n\nfor(i in 1:B){\nU =  rnorm(n)  \nZ = rnorm(n)  #generate instrument\nD = beta*Z + U  #generate endogenougs variable\nY =  D+ U  # the true causal effect is 1\nIVEst[i] = coef(tsls(x=NULL, d=D, y=Y, z=Z))[1,1]\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:55:07.7476Z","iopub.execute_input":"2022-05-06T19:55:07.749448Z","iopub.status.idle":"2022-05-06T19:55:10.967684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot(density(IVEst-1, n=1000, from=-5, to=5),col=4, xlim= c(-5, 5),  \n     xlab= \"IV Estimator -True Effect\", main=\"Actual Distribution vs Gaussian\")\n\nval=seq(-5, 5, by=.05)\nvar = (1/beta^2)*(1/100) # theoretical variance of IV\nsd = sqrt(var)\nlines(val, dnorm(val, sd=sd), col=2, lty=2)\n\nrejection.frequency = sum(( abs(IVEst-1)/sd > 1.96))/B\n\ncat(c(\"Rejection Frequency is \", rejection.frequency, \" while we expect it to be .05\"))","metadata":{"execution":{"iopub.status.busy":"2022-05-06T19:55:14.208446Z","iopub.execute_input":"2022-05-06T19:55:14.210099Z","iopub.status.idle":"2022-05-06T19:55:14.302639Z"},"trusted":true},"execution_count":null,"outputs":[]}]}