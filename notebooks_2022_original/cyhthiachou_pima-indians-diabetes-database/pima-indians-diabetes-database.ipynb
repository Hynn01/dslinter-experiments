{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Import the modules that will do all the work","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-08T01:21:03.324187Z","iopub.execute_input":"2022-05-08T01:21:03.324565Z","iopub.status.idle":"2022-05-08T01:21:03.333196Z","shell.execute_reply.started":"2022-05-08T01:21:03.324516Z","shell.execute_reply":"2022-05-08T01:21:03.332398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Import the data","metadata":{}},{"cell_type":"markdown","source":"We load in a dataset [pima indians diabetes](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database). This dataset will allow us to predict if a pima indian will get diabetes based on their pregnancies, glucose, and a variety of other metrics. We will start from loading the data into a data frame called **df**.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:03.427586Z","iopub.execute_input":"2022-05-08T01:21:03.428043Z","iopub.status.idle":"2022-05-08T01:21:03.468926Z","shell.execute_reply.started":"2022-05-08T01:21:03.428012Z","shell.execute_reply":"2022-05-08T01:21:03.4684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Identify and deal with missing data","metadata":{}},{"cell_type":"markdown","source":"Let's start from inspecting the data by looking at the info of the dataframe.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:03.501962Z","iopub.execute_input":"2022-05-08T01:21:03.50239Z","iopub.status.idle":"2022-05-08T01:21:03.530447Z","shell.execute_reply.started":"2022-05-08T01:21:03.502363Z","shell.execute_reply":"2022-05-08T01:21:03.529599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see a bunch of the variables collected for each person in this dataset. These columns are:\n* **Pregnancies**: Number of times pregnant\n* **Glucose**: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* **BloodPressure**: Diastolic blood pressure (mm Hg)\n* **SkinThickness**: Triceps skin fold thickness (mm)\n* **Insulin**: 2-Hour serum insulin (mu U/ml)\n* **BMI**: Body mass index (weight in kg/(height in m)^2)\n* **DiabetesPedigreeFunction**: Diabetes pedigree function\n* **Age**: Ages in years\n* **Outcomes**: Class variable (0 or 1) 268 of 768 are 1, the others are 0\n\nQuestions emerged from the following table: Could Glucose, BloodPressure, SkinThickness, Insulin, BMI be zero?","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:03.558523Z","iopub.execute_input":"2022-05-08T01:21:03.559685Z","iopub.status.idle":"2022-05-08T01:21:03.597931Z","shell.execute_reply.started":"2022-05-08T01:21:03.559651Z","shell.execute_reply":"2022-05-08T01:21:03.595935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Further understand the data distribution. We can notice Insulin, DiabetesPedigreeFunction, Insulin and Ages have a right-skewed distribution pattern.","metadata":{}},{"cell_type":"code","source":"def show_hist(df):\n    return df.hist(figsize=(20,20))\nshow_hist(df)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:03.616804Z","iopub.execute_input":"2022-05-08T01:21:03.617046Z","iopub.status.idle":"2022-05-08T01:21:05.470216Z","shell.execute_reply.started":"2022-05-08T01:21:03.617023Z","shell.execute_reply":"2022-05-08T01:21:05.469219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to identify and deal with missing data, we can start from split the data into training, validating, and testing groups. In that way, we could compare whether simply drop the missing data or impute the missing data.","metadata":{}},{"cell_type":"code","source":"X_full = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ny = X_full.Outcome\nfeatures = [\"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\"]\nX=X_full[features].copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:05.471996Z","iopub.execute_input":"2022-05-08T01:21:05.472284Z","iopub.status.idle":"2022-05-08T01:21:05.483853Z","shell.execute_reply.started":"2022-05-08T01:21:05.472249Z","shell.execute_reply":"2022-05-08T01:21:05.483115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_temp,x_test,y_temp,y_test = train_test_split(X, y, train_size = 0.8, test_size=0.2,random_state=0)\nx_train,x_valid,y_train,y_valid = train_test_split(x_temp, y_temp, train_size = 0.75, test_size=0.25,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:05.485278Z","iopub.execute_input":"2022-05-08T01:21:05.485636Z","iopub.status.idle":"2022-05-08T01:21:05.498187Z","shell.execute_reply.started":"2022-05-08T01:21:05.485601Z","shell.execute_reply":"2022-05-08T01:21:05.497105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After splitting the data, we can define the way to evaluate the outcomes.","metadata":{}},{"cell_type":"code","source":"# get evaluation of the results\n\ndef confusion_matrix(pred,y):\n    #true positive, false positive, actual positive, true negative, false negative, actual negative\n    TP,FP,AP,TN,FN,AN=0,0,0,0,0,0\n    pred=list(pred)\n    y=list(y)\n    \n    for i in range(len(pred)):\n        if y[i]==1:\n            AP+=1\n            if pred[i] == 1:\n                TP+=1\n            elif pred[i] == 0: \n                FN+=1\n        elif y[i]==0:\n            AN+=1\n            if pred[i] == 1:\n                FP+=1\n            elif pred[i] == 0: \n                TN+=1\n    \n\n    #Recall rate = (true positive value) / (actual positive value)\n    recall_rate = TP/AP\n    #Specificity rate = (true negative value) / (actual negative value)\n    specificity_rate = TN/AN\n    #Accuracy rate = (true positive value + true negative value) / (total number of samples)\n    accuracy_rate = (TP+TN)/len(y)\n    #Misclassification (error) rate = (false positive value + false negative value) / (total number of samples)\n    misclassification_rate = (FP+FN)/len(y)\n    #Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. \n    precision = TP/(TP+FP)\n    #F1 score is to find the harmonic mean of recall and precision. F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n    f1_score = 2*(recall_rate*precision) / (recall_rate+precision)\n    \n    print(\"recall_rate is {}, specificity_rate is {}, accuracy_rate is {}, misclassification_rate is {}, f1_score is {}\".format(round(recall_rate,2),round(specificity_rate,2),round(accuracy_rate,2),round(misclassification_rate,2), round(f1_score,2)))\n    \n    return TP,FP,TN,FN,AP,AN,recall_rate,specificity_rate,accuracy_rate,misclassification_rate,f1_score\n\n# Define a unified function to calculate the errors\ndef score_dataset(x_train, y_train, x_valid, y_valid, model):\n    # Good practice usage scaling techniques:\n        # 1.Fit the scaler using available training data.\n        # 2.Apply the scale to training data.\n        # 3.Apply the scale to data going forward.\n    scaler = StandardScaler()\n    scaled_x_train = scaler.fit_transform(x_train)\n    model.fit(scaled_x_train, y_train.values.ravel())\n    pred = model.predict(scaler.fit_transform(x_valid))\n    return confusion_matrix(pred,y_valid)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:05.500489Z","iopub.execute_input":"2022-05-08T01:21:05.50089Z","iopub.status.idle":"2022-05-08T01:21:05.517887Z","shell.execute_reply.started":"2022-05-08T01:21:05.50085Z","shell.execute_reply":"2022-05-08T01:21:05.516913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before cleaning the data, we can calculate the error rate to understand the baseline.","metadata":{}},{"cell_type":"code","source":"# A group of models to compare\nmodel1=LogisticRegression(max_iter=1000)\nmodel2=RandomForestClassifier(n_estimators=63, random_state=0)\nmodel3=tree.DecisionTreeClassifier(max_depth = 5, random_state=0)\nmodel4 = SVC(random_state=42, C=1000, gamma=0.001)\nmodel_group = [model1,model2, model3, model4]","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:18:19.188155Z","iopub.execute_input":"2022-05-08T03:18:19.188415Z","iopub.status.idle":"2022-05-08T03:18:19.194957Z","shell.execute_reply.started":"2022-05-08T03:18:19.188387Z","shell.execute_reply":"2022-05-08T03:18:19.193851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_dataset(x_train, y_train, x_valid, y_valid, model1)\n\n# Result:\n# recall_rate is 0.53, specificity_rate is 0.87, accuracy_rate is 0.74, misclassification_rate is 0.26, f1_score is 0.6\n# Original performance without cleaning the data","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:05.544699Z","iopub.execute_input":"2022-05-08T01:21:05.545053Z","iopub.status.idle":"2022-05-08T01:21:05.579663Z","shell.execute_reply.started":"2022-05-08T01:21:05.545013Z","shell.execute_reply":"2022-05-08T01:21:05.57894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Option 1. Clean the data - Drop the zero data","metadata":{}},{"cell_type":"code","source":"# From Hist observation, following features contains non-helpful zero data: Glucose, BloodPressure, SkinThickness, Insulin, BMI, Age\nfeatures_with_zero = [\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\",\"Age\"]\n\n# Try 1st way of cleaning the zero data,\n# Drop the zero data\ndropped_x_train = x_train.drop(features_with_zero, axis=1)\ndropped_x_valid = x_valid.drop(features_with_zero, axis=1)\n#printing the score dataset for dropping the zeros\nprint(score_dataset(dropped_x_train, y_train, dropped_x_valid, y_valid, model1))\n\n# Result:\n# recall_rate is 0.23, specificity_rate is 0.92, accuracy_rate is 0.66, misclassification_rate is 0.34, f1_score is 0.33\n# (13, 8, 89, 44, 57, 97, 0.22807017543859648, 0.9175257731958762, 0.6623376623376623, 0.33766233766233766)\n# Drop the zero data actually makes the prediction less accurate.","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:05.580865Z","iopub.execute_input":"2022-05-08T01:21:05.581712Z","iopub.status.idle":"2022-05-08T01:21:05.601261Z","shell.execute_reply.started":"2022-05-08T01:21:05.581675Z","shell.execute_reply":"2022-05-08T01:21:05.600137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Option 2. Clean the data - Impute the data with Sklearn SimpleImputer","metadata":{}},{"cell_type":"code","source":"# Try 2nd way of cleaning the zero data,\n# SKLearn model replace the zero data with actual values\n\nreplaced_x_train = x_train.copy(deep=True)\nreplaced_x_valid = x_valid.copy(deep=True)\n\nreplaced_x_train[features_with_zero] = x_train[features_with_zero].replace(0,np.NaN)\nreplaced_x_valid[features_with_zero] = x_valid[features_with_zero].replace(0,np.NaN)\n\nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nimputed_x_train = pd.DataFrame(my_imputer.fit_transform(replaced_x_train))\nimputed_x_valid = pd.DataFrame(my_imputer.fit_transform(replaced_x_valid))\n\nimputed_x_train.columns = x_train.columns\nimputed_x_valid.columns = x_valid.columns\n\nprint(score_dataset(imputed_x_train, y_train, imputed_x_valid, y_valid, model1))\n\n# Original performance: recall_rate is 0.53, specificity_rate is 0.87, accuracy_rate is 0.74, misclassification_rate is 0.26,, f1_score is 0.6\n# Result:\n# recall_rate is 0.53, specificity_rate is 0.86, accuracy_rate is 0.73, misclassification_rate is 0.27, f1_score is 0.59\n# Using Simple Imputer makes it Less accurate when predicting negative results.","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:05.602523Z","iopub.execute_input":"2022-05-08T01:21:05.602828Z","iopub.status.idle":"2022-05-08T01:21:05.648227Z","shell.execute_reply.started":"2022-05-08T01:21:05.602799Z","shell.execute_reply":"2022-05-08T01:21:05.647223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Option 3. Clean the data - Manually replace the missing data","metadata":{}},{"cell_type":"code","source":"# Try 3rd way of cleaning the zero data,\n# Manually replace the zero data with actual values \n\nreplaced_x_train = x_train.copy(deep=True)\nreplaced_x_valid = x_valid.copy(deep=True)\n\ndef manual_replace(x):\n    x[\"Glucose\"].fillna(x[\"Glucose\"].mean(), inplace = True)\n    x['BloodPressure'].fillna(x['BloodPressure'].mean(), inplace = True)\n    x['SkinThickness'].fillna(x['SkinThickness'].median(), inplace = True)\n    x['Insulin'].fillna(x['Insulin'].median(), inplace = True)\n    x['BMI'].fillna(x['BMI'].median(), inplace = True)\n    return x\n\nreplaced_x_train = manual_replace(replaced_x_train)\nreplaced_x_valid = manual_replace(replaced_x_valid)\n\nprint(score_dataset(replaced_x_train, y_train, replaced_x_valid, y_valid, model1))\n\n# Original performance: recall_rate is 0.53, specificity_rate is 0.87, accuracy_rate is 0.74, misclassification_rate is 0.26,, f1_score is 0.6\n# Result:\n# recall_rate is 0.53, specificity_rate is 0.87, accuracy_rate is 0.74, misclassification_rate is 0.26, f1_score is 0.6\n# After replaceing the data with mean or median, it remains the same performance.","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:05.650609Z","iopub.execute_input":"2022-05-08T01:21:05.650899Z","iopub.status.idle":"2022-05-08T01:21:05.681386Z","shell.execute_reply.started":"2022-05-08T01:21:05.650866Z","shell.execute_reply":"2022-05-08T01:21:05.680194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After trying the three ways to clean the data, result remains the same performance. What's missing? Need more investigation.","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:05.683839Z","iopub.execute_input":"2022-05-08T01:21:05.684144Z","iopub.status.idle":"2022-05-08T01:21:05.694578Z","shell.execute_reply.started":"2022-05-08T01:21:05.684113Z","shell.execute_reply":"2022-05-08T01:21:05.693819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Choose and optimize the model","metadata":{}},{"cell_type":"markdown","source":"When we were at the step of cleaning data, we have listed several models we could try for this data set.\n* Support Vector Machines\n* Linear Regression\n* Random Forest Classifier\n* Decision Tree Classifier","metadata":{}},{"cell_type":"markdown","source":"**1.Support vector machines**","metadata":{}},{"cell_type":"code","source":"# Try Support Vector Machines\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n#     scaler = StandardScaler()\n#     scaled_x_train = scaler.fit_transform(x_train)\n#     model.fit(scaled_x_train, y_train.values.ravel())\n\nscaler = StandardScaler()\nscaled_x_train = scaler.fit_transform(x_train)\npca = PCA()\npca_x_train = pca.fit_transform(scaled_x_train)\n\nper_var = np.round(pca.explained_variance_ratio_*100, decimals=1)\nlabels = [str(x) for x in range(1,len(per_var)+1)]\n\nplt.bar(x=range(1,len(per_var)+1),height=per_var)\nplt.tick_params(\n    axis='x',\n    which='both',\n    bottom=False,\n    top=False,\n    labelbottom=False)\nplt.ylabel('Percentage of Expllained Variance')\nplt.xlabel('Principle Components')\nplt.title('Scree Plot')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:05.696851Z","iopub.execute_input":"2022-05-08T01:21:05.697155Z","iopub.status.idle":"2022-05-08T01:21:05.925632Z","shell.execute_reply.started":"2022-05-08T01:21:05.697124Z","shell.execute_reply":"2022-05-08T01:21:05.924586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The scree plot shows that the first principle component PC1 and PC2 account for a relatively large amount of variation in the raw data, which means it would be a good candidate for x-axis in the 2-dimensional graph.\n\n","metadata":{}},{"cell_type":"code","source":"# Questions: \n# what is first principle component?\n# what is explained variance?\n# why then it would be good candidates?","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:05.926769Z","iopub.execute_input":"2022-05-08T01:21:05.927636Z","iopub.status.idle":"2022-05-08T01:21:05.936574Z","shell.execute_reply.started":"2022-05-08T01:21:05.927524Z","shell.execute_reply":"2022-05-08T01:21:05.935189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pc1_coords = pca_x_train[:,0]\ntrain_pc2_coords = pca_x_train[:,1]\n\npca_train_scaled = preprocessing.scale(np.column_stack((train_pc1_coords, train_pc2_coords)))\n\nparam_grid = [\n    {'C':[1,10,100,1000],\n     'gamma':['scale',1,0.1, 0.01, 0.001, 0.0001],\n     'kernel':['rbf']\n    },\n]\n\noptimal_params = GridSearchCV(\n    SVC(),\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    verbose=0\n)\n\noptimal_params.fit(pca_train_scaled, y_train)\nprint(optimal_params.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:05.940226Z","iopub.execute_input":"2022-05-08T01:21:05.940514Z","iopub.status.idle":"2022-05-08T01:21:08.499969Z","shell.execute_reply.started":"2022-05-08T01:21:05.940483Z","shell.execute_reply":"2022-05-08T01:21:08.498758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_svm = SVC(random_state=42, C=1000, gamma=0.001)\nscore_dataset(x_train, y_train, x_valid, y_valid, clf_svm)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:53:43.594697Z","iopub.execute_input":"2022-05-08T01:53:43.594967Z","iopub.status.idle":"2022-05-08T01:53:43.641957Z","shell.execute_reply.started":"2022-05-08T01:53:43.594942Z","shell.execute_reply":"2022-05-08T01:53:43.641231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. Random Forest Classifier**\nTo find the optimal value for n_estimators for RandomForestClassifier","metadata":{}},{"cell_type":"code","source":"#decide the estimator for the random forest treee\ngroup_estimators = list(range(60,67,1))\nresults = {}\n\nfor estimator in group_estimators:\n    model2=RandomForestClassifier(n_estimators=estimator, random_state=0)\n    f1_score = score_dataset(x_train, y_train, x_valid, y_valid, model2)[10]\n    results[estimator] = f1_score\n\n%matplotlib inline\nplt.plot(list(results.keys()), list(results.values()))\nplt.show()\n# optimized RandomForestClassifier n_estimators 63","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:09.209634Z","iopub.execute_input":"2022-05-08T01:21:09.209995Z","iopub.status.idle":"2022-05-08T01:21:10.148048Z","shell.execute_reply.started":"2022-05-08T01:21:09.209959Z","shell.execute_reply":"2022-05-08T01:21:10.146639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4.Decision Tree Classifier** To find the optimal value for max_depth for DecisionTreeClassifier","metadata":{}},{"cell_type":"code","source":"leaf_dep_list = [x for x in range(20) if x >1]\n\nresults ={}\nfor leaf_dep in leaf_dep_list:\n  model3=tree.DecisionTreeClassifier(max_depth = leaf_dep, random_state=0)\n  f1_score = score_dataset(x_train, y_train,x_valid, y_valid, model3)[10]\n  results[leaf_dep]=f1_score\n\n%matplotlib inline\nplt.plot(list(results.keys()), list(results.values()))\nplt.show()\n# optimal max_depth=5","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:10.149243Z","iopub.execute_input":"2022-05-08T01:21:10.149444Z","iopub.status.idle":"2022-05-08T01:21:10.440335Z","shell.execute_reply.started":"2022-05-08T01:21:10.149419Z","shell.execute_reply":"2022-05-08T01:21:10.439527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Compare the performance among all models**","metadata":{}},{"cell_type":"code","source":"for i in model_group:\n    print(i)\n    print(score_dataset(x_train, y_train, x_valid, y_valid, i))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:18:27.46352Z","iopub.execute_input":"2022-05-08T03:18:27.46378Z","iopub.status.idle":"2022-05-08T03:18:27.63178Z","shell.execute_reply.started":"2022-05-08T03:18:27.463756Z","shell.execute_reply":"2022-05-08T03:18:27.63065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After comparing, the winning model is:\n**RandomForestClassifier**(n_estimators=63, random_state=0)\nrecall_rate is 0.63, \nspecificity_rate is 0.9, \naccuracy_rate is 0.8, \nmisclassification_rate is 0.2, f1_score is 0.7","metadata":{}},{"cell_type":"markdown","source":"# 4. Making predictions","metadata":{}},{"cell_type":"code","source":"final_model = RandomForestClassifier(n_estimators=63, random_state=0)\nfinal_model.fit(x_temp, y_temp)\npreds_test=final_model.predict(x_test)\n\n# Save predictions in format used for competition scoring\noutput = pd.DataFrame({'Id': x_test.index,\n                       'Outcome': preds_test})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:21:10.571935Z","iopub.execute_input":"2022-05-08T01:21:10.572258Z","iopub.status.idle":"2022-05-08T01:21:10.701136Z","shell.execute_reply.started":"2022-05-08T01:21:10.572232Z","shell.execute_reply":"2022-05-08T01:21:10.700065Z"},"trusted":true},"execution_count":null,"outputs":[]}]}