{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting States of Manufacturing Control Data ðŸ­","metadata":{}},{"cell_type":"markdown","source":"**Objective:** Build a powerfull GBDT Model that can provide a good estimation.\n\n**Strategy:** I think I will follow this strategy:\n\n**Level 1 Getting Started**\n\n* Quick EDA to identify potential opportunities.\n* Simple pre-processing step to encode categorical features.\n* A basic CV strategy using 90% for TRaining and 10% for Testing.\n* Looking at the feature importances.\n* Creating a submission file.\n* Submit the file to Kaggle.\n\n**Level 2 Feature Engineering**\n* Feature engineering using text information. (Massive boost in the score)\n* Cross validation loop (**Work in Progress...**)\n\n---","metadata":{}},{"cell_type":"markdown","source":"**Data Description**\n\nFor this challenge, you are given (simulated) manufacturing control data and are tasked to predict whether the machine is in state 0 or state 1. \nThe data has various feature interactions that may be important in determining the machine state.\n\nGood luck!\n\n**Files**\n* train.csv - the training data, which includes normalized continuous data and categorical data\n* test.csv - the test set; your task is to predict binary target variable which represents the state of a manufacturing process\n* sample_submission.csv - a sample submission file in the correct format","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**Notebooks Ideas and Credits**\n\nI took ideas or inspiration from the following notebooks, if you enjoy my work, please take a look to the notebooks that inspire my work.\n\n**TPSMAY22 Gradient-Boosting Quickstart:** https://www.kaggle.com/code/ambrosm/tpsmay22-gradient-boosting-quickstart/notebook\n\n\n","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 1. Loading the Requiered Libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-02T04:00:09.243126Z","iopub.execute_input":"2022-05-02T04:00:09.243441Z","iopub.status.idle":"2022-05-02T04:00:09.254993Z","shell.execute_reply.started":"2022-05-02T04:00:09.243409Z","shell.execute_reply":"2022-05-02T04:00:09.253543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n","metadata":{}},{"cell_type":"markdown","source":"# 2. Setting the Notebook","metadata":{}},{"cell_type":"code","source":"%%time\n# I like to disable my Notebook Warnings.\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:09.257162Z","iopub.execute_input":"2022-05-02T04:00:09.259807Z","iopub.status.idle":"2022-05-02T04:00:09.269373Z","shell.execute_reply.started":"2022-05-02T04:00:09.25971Z","shell.execute_reply":"2022-05-02T04:00:09.267912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Notebook Configuration...\n\n# Amount of data we want to load into the Model...\nDATA_ROWS = None\n# Dataframe, the amount of rows and cols to visualize...\nNROWS = 50\nNCOLS = 15\n# Main data location path...\nBASE_PATH = '...'","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:09.270995Z","iopub.execute_input":"2022-05-02T04:00:09.271302Z","iopub.status.idle":"2022-05-02T04:00:09.284402Z","shell.execute_reply.started":"2022-05-02T04:00:09.27124Z","shell.execute_reply":"2022-05-02T04:00:09.283195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Configure notebook display settings to only use 2 decimal places, tables look nicer.\npd.options.display.float_format = '{:,.2f}'.format\npd.set_option('display.max_columns', NCOLS) \npd.set_option('display.max_rows', NROWS)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:09.287792Z","iopub.execute_input":"2022-05-02T04:00:09.288469Z","iopub.status.idle":"2022-05-02T04:00:09.297011Z","shell.execute_reply.started":"2022-05-02T04:00:09.288409Z","shell.execute_reply":"2022-05-02T04:00:09.295609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 3. Loading the Information (CSV) Into A Dataframe","metadata":{}},{"cell_type":"code","source":"%%time\n# Load the CSV information into a Pandas DataFrame...\ntrn_data = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/train.csv')\ntst_data = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/test.csv')\n\nsub = pd.read_csv('/kaggle/input/tabular-playground-series-may-2022/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:09.298885Z","iopub.execute_input":"2022-05-02T04:00:09.29953Z","iopub.status.idle":"2022-05-02T04:00:17.309096Z","shell.execute_reply.started":"2022-05-02T04:00:09.299476Z","shell.execute_reply":"2022-05-02T04:00:17.307785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 4. Exploring the Information Available","metadata":{}},{"cell_type":"markdown","source":"## 4.1. Analysing the Trian Dataset","metadata":{}},{"cell_type":"code","source":"%%time\n# Explore the shape of the DataFrame...\ntrn_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:17.311182Z","iopub.execute_input":"2022-05-02T04:00:17.311549Z","iopub.status.idle":"2022-05-02T04:00:17.321428Z","shell.execute_reply.started":"2022-05-02T04:00:17.311502Z","shell.execute_reply":"2022-05-02T04:00:17.320183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display simple information of the variables in the dataset...\ntrn_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:17.323118Z","iopub.execute_input":"2022-05-02T04:00:17.324066Z","iopub.status.idle":"2022-05-02T04:00:17.498955Z","shell.execute_reply.started":"2022-05-02T04:00:17.324Z","shell.execute_reply":"2022-05-02T04:00:17.497719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display the first few rows of the DataFrame...\ntrn_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:17.500936Z","iopub.execute_input":"2022-05-02T04:00:17.501527Z","iopub.status.idle":"2022-05-02T04:00:17.529296Z","shell.execute_reply.started":"2022-05-02T04:00:17.50147Z","shell.execute_reply":"2022-05-02T04:00:17.5282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Generate a simple statistical summary of the DataFrame, Only Numerical...\ntrn_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:17.530759Z","iopub.execute_input":"2022-05-02T04:00:17.532213Z","iopub.status.idle":"2022-05-02T04:00:18.504046Z","shell.execute_reply.started":"2022-05-02T04:00:17.532164Z","shell.execute_reply":"2022-05-02T04:00:18.502995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Calculates the total number of missing values...\ntrn_data.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:18.508345Z","iopub.execute_input":"2022-05-02T04:00:18.508613Z","iopub.status.idle":"2022-05-02T04:00:18.654132Z","shell.execute_reply.started":"2022-05-02T04:00:18.508552Z","shell.execute_reply":"2022-05-02T04:00:18.652079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display the number of missing values by variable...\ntrn_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:18.655934Z","iopub.execute_input":"2022-05-02T04:00:18.656535Z","iopub.status.idle":"2022-05-02T04:00:18.805972Z","shell.execute_reply.started":"2022-05-02T04:00:18.656489Z","shell.execute_reply":"2022-05-02T04:00:18.804906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display the number of unique values for each variable...\ntrn_data.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:18.807576Z","iopub.execute_input":"2022-05-02T04:00:18.808447Z","iopub.status.idle":"2022-05-02T04:00:20.023189Z","shell.execute_reply.started":"2022-05-02T04:00:18.808378Z","shell.execute_reply":"2022-05-02T04:00:20.022097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the number of unique values for each variable, sorted by quantity...\ntrn_data.nunique().sort_values(ascending = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:20.024958Z","iopub.execute_input":"2022-05-02T04:00:20.025561Z","iopub.status.idle":"2022-05-02T04:00:22.102426Z","shell.execute_reply.started":"2022-05-02T04:00:20.025513Z","shell.execute_reply":"2022-05-02T04:00:22.101566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Check some of the categorical variables\ncateg_cols = ['f_29','f_30','f_13', 'f_18','f_17','f_14','f_11','f_10','f_09','f_15','f_07','f_12','f_16','f_08','f_27']\ntrn_data[categ_cols].sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:22.107404Z","iopub.execute_input":"2022-05-02T04:00:22.110018Z","iopub.status.idle":"2022-05-02T04:00:22.224129Z","shell.execute_reply.started":"2022-05-02T04:00:22.109973Z","shell.execute_reply":"2022-05-02T04:00:22.223259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Generate a quick correlation matrix to understand the dataset better\ncorrelation = trn_data.corr()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:22.229038Z","iopub.execute_input":"2022-05-02T04:00:22.231472Z","iopub.status.idle":"2022-05-02T04:00:24.612771Z","shell.execute_reply.started":"2022-05-02T04:00:22.231423Z","shell.execute_reply":"2022-05-02T04:00:24.61169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Diplay the correlation matrix\ncorrelation","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:24.614433Z","iopub.execute_input":"2022-05-02T04:00:24.615477Z","iopub.status.idle":"2022-05-02T04:00:24.647731Z","shell.execute_reply.started":"2022-05-02T04:00:24.61543Z","shell.execute_reply":"2022-05-02T04:00:24.646061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Check the most correlated variables to the target\ncorrelation['target'].sort_values(ascending = False)[:5]","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:24.649815Z","iopub.execute_input":"2022-05-02T04:00:24.65063Z","iopub.status.idle":"2022-05-02T04:00:24.66821Z","shell.execute_reply.started":"2022-05-02T04:00:24.650566Z","shell.execute_reply":"2022-05-02T04:00:24.667086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Check the least correlated variables to the target\ncorrelation['target'].sort_values(ascending = True)[:5]","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:24.669928Z","iopub.execute_input":"2022-05-02T04:00:24.671571Z","iopub.status.idle":"2022-05-02T04:00:24.692321Z","shell.execute_reply.started":"2022-05-02T04:00:24.671528Z","shell.execute_reply":"2022-05-02T04:00:24.691155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 4.2. Analysing the Trian Labels Dataset","metadata":{}},{"cell_type":"code","source":"%%time\n# Check how well balanced is the dataset\ntrn_data['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:24.693892Z","iopub.execute_input":"2022-05-02T04:00:24.694998Z","iopub.status.idle":"2022-05-02T04:00:24.715812Z","shell.execute_reply.started":"2022-05-02T04:00:24.694952Z","shell.execute_reply":"2022-05-02T04:00:24.714552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Check some statistics on the target variable\ntrn_data['target'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:24.717761Z","iopub.execute_input":"2022-05-02T04:00:24.718939Z","iopub.status.idle":"2022-05-02T04:00:24.753171Z","shell.execute_reply.started":"2022-05-02T04:00:24.718885Z","shell.execute_reply":"2022-05-02T04:00:24.752042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 5. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Text Base Features","metadata":{}},{"cell_type":"code","source":"%%time\n# The idea is to create a simple funtion to count the amount of letters on feature 27.\n# feature 27 seems quite important \n\ndef count_sequence(df, field):\n    '''\n    For each letter of the provided suquence it return new feature with the number of occurences.\n    '''\n    alphabet = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']    \n    \n    for letter in alphabet:\n        df[letter + '_count'] = df[field].str.count(letter)\n    \n    df[\"unique_characters\"] = df['f_27'].apply(lambda s: len(set(s)))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:24.755464Z","iopub.execute_input":"2022-05-02T04:00:24.755811Z","iopub.status.idle":"2022-05-02T04:00:24.765185Z","shell.execute_reply.started":"2022-05-02T04:00:24.755763Z","shell.execute_reply":"2022-05-02T04:00:24.763981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Utilizes the new created funtions to generate more features.\n# trn_data = count_sequence(trn_data, 'f_27')\n# tst_data = count_sequence(tst_data, 'f_27')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:24.767472Z","iopub.execute_input":"2022-05-02T04:00:24.768254Z","iopub.status.idle":"2022-05-02T04:00:24.781476Z","shell.execute_reply.started":"2022-05-02T04:00:24.768205Z","shell.execute_reply":"2022-05-02T04:00:24.780063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef count_chars(df, field):\n    '''\n    Describes something...\n    '''\n    \n    for i in range(10):\n        df[f'ch_{i}'] = df[field].str.get(i).apply(ord) - ord('A')\n        \n    df[\"unique_characters\"] = df[field].apply(lambda s: len(set(s)))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:24.783646Z","iopub.execute_input":"2022-05-02T04:00:24.784123Z","iopub.status.idle":"2022-05-02T04:00:24.795394Z","shell.execute_reply.started":"2022-05-02T04:00:24.784076Z","shell.execute_reply":"2022-05-02T04:00:24.794252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Utilizes the new created funtions to generate more features.\ntrn_data = count_chars(trn_data, 'f_27')\ntst_data = count_chars(tst_data, 'f_27')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:24.797488Z","iopub.execute_input":"2022-05-02T04:00:24.797926Z","iopub.status.idle":"2022-05-02T04:00:46.248781Z","shell.execute_reply.started":"2022-05-02T04:00:24.797882Z","shell.execute_reply":"2022-05-02T04:00:46.247706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 7. Pre-Processing Labels","metadata":{}},{"cell_type":"code","source":"%%time\n# Define a label encoding function\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndef encode_features(df, cols = ['f_27']):\n    for col in cols:\n        df[col + '_enc'] = encoder.fit_transform(df[col])\n    return df\n\ntrn_data = encode_features(trn_data)\ntst_data = encode_features(tst_data)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:46.250459Z","iopub.execute_input":"2022-05-02T04:00:46.251576Z","iopub.status.idle":"2022-05-02T04:00:52.098626Z","shell.execute_reply.started":"2022-05-02T04:00:46.251516Z","shell.execute_reply":"2022-05-02T04:00:52.097576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the results of the transformation\ntrn_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:52.100592Z","iopub.execute_input":"2022-05-02T04:00:52.100927Z","iopub.status.idle":"2022-05-02T04:00:52.123852Z","shell.execute_reply.started":"2022-05-02T04:00:52.100882Z","shell.execute_reply":"2022-05-02T04:00:52.122744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 8. Feature Selection for Baseline Model","metadata":{}},{"cell_type":"code","source":"%%time\n# Define what will be used in the training stage\nignore = ['id', 'target', 'f_27',  'f_27_enc'] # f_27 has been label encoded...\n\nfeatures = [feat for feat in trn_data.columns if feat not in ignore]\ntarget_feature = 'target'","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:52.131076Z","iopub.execute_input":"2022-05-02T04:00:52.131735Z","iopub.status.idle":"2022-05-02T04:00:52.138328Z","shell.execute_reply.started":"2022-05-02T04:00:52.1317Z","shell.execute_reply":"2022-05-02T04:00:52.137099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 9. Creating a Simple Train / Test Split Strategy","metadata":{}},{"cell_type":"code","source":"%%time\n# Creates a simple train split breakdown for baseline model\nfrom sklearn.model_selection import train_test_split\ntest_size_pct = 0.20\nX_train, X_valid, y_train, y_valid = train_test_split(trn_data[features], trn_data[target_feature], test_size = test_size_pct, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:52.139884Z","iopub.execute_input":"2022-05-02T04:00:52.140817Z","iopub.status.idle":"2022-05-02T04:00:52.709786Z","shell.execute_reply.started":"2022-05-02T04:00:52.140727Z","shell.execute_reply":"2022-05-02T04:00:52.708626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 10. Building a Baseline GBT Model, Simple Split","metadata":{}},{"cell_type":"markdown","source":"## 10.1 XGBoost Model","metadata":{}},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Import the model libraries\nfrom xgboost  import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:52.711611Z","iopub.execute_input":"2022-05-02T04:00:52.712202Z","iopub.status.idle":"2022-05-02T04:00:52.762649Z","shell.execute_reply.started":"2022-05-02T04:00:52.712152Z","shell.execute_reply":"2022-05-02T04:00:52.761445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Define the model parameters to get started we use default values to a certain degree\nxgb_params = {'n_estimators'     : 8192,\n              'min_child_weight' : 96,\n              #'max_depth'        : 6,\n              #'learning_rate'    : 0.15,\n              #'subsample'        : 0.95,\n              #'colsample_bytree' : 0.95,\n              #'reg_lambda'       : 1.50,\n              #'reg_alpha'        : 1.50,\n              #'gamma'            : 1.50,\n              'max_bin'          : 512,\n              'random_state'     : 46,\n              'objective'        : 'binary:logistic',\n              'tree_method'      : 'gpu_hist',\n             }","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:52.766381Z","iopub.execute_input":"2022-05-02T04:00:52.766642Z","iopub.status.idle":"2022-05-02T04:00:52.813137Z","shell.execute_reply.started":"2022-05-02T04:00:52.76661Z","shell.execute_reply":"2022-05-02T04:00:52.811745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Instanciate the XGBoost model using the previous parameters\nxgb = XGBClassifier(**xgb_params)\nxgb.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], eval_metric = ['auc'], early_stopping_rounds = 256, verbose = 250)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:52.815357Z","iopub.execute_input":"2022-05-02T04:00:52.815644Z","iopub.status.idle":"2022-05-02T04:00:52.867925Z","shell.execute_reply.started":"2022-05-02T04:00:52.815612Z","shell.execute_reply":"2022-05-02T04:00:52.866589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Check the model performance in the validation dataset\nfrom sklearn.metrics import roc_auc_score\nval_preds = xgb.predict_proba(X_valid[features])[:, 1]\nroc_auc_score(y_valid, val_preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:52.869891Z","iopub.execute_input":"2022-05-02T04:00:52.870836Z","iopub.status.idle":"2022-05-02T04:00:52.918958Z","shell.execute_reply.started":"2022-05-02T04:00:52.870801Z","shell.execute_reply":"2022-05-02T04:00:52.917731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Record some of the model results for future improvement\n# Local Score = 0.9454953628406088 First Model Run >>> LB Score = 0.93147\n# Local Score = 0.9448767329168479 First Model Run >>> LB Score = 0.93205\n\n# 0.9816418086418166","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:52.921145Z","iopub.execute_input":"2022-05-02T04:00:52.921899Z","iopub.status.idle":"2022-05-02T04:00:52.929498Z","shell.execute_reply.started":"2022-05-02T04:00:52.921846Z","shell.execute_reply":"2022-05-02T04:00:52.927336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 10.2 LGMB Model","metadata":{}},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Import the model libraries\nfrom lightgbm import LGBMClassifier","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:52.930938Z","iopub.execute_input":"2022-05-02T04:00:52.931521Z","iopub.status.idle":"2022-05-02T04:00:52.982815Z","shell.execute_reply.started":"2022-05-02T04:00:52.931472Z","shell.execute_reply":"2022-05-02T04:00:52.981644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Define the model parameters to get started we use default values to a certain degree\nlgb_params = {'n_estimators'      : 8192,\n              'min_child_samples' : 96,\n              'max_bins'          : 512,\n              'random_state'      : 46,\n             }","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:52.985892Z","iopub.execute_input":"2022-05-02T04:00:52.986309Z","iopub.status.idle":"2022-05-02T04:00:53.037754Z","shell.execute_reply.started":"2022-05-02T04:00:52.986244Z","shell.execute_reply":"2022-05-02T04:00:53.036463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Instanciate the XGBoost model using the previous parameters\nlgb = LGBMClassifier(**lgb_params)\nlgb.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], eval_metric = ['auc'], early_stopping_rounds = 256, verbose = 250)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:53.039822Z","iopub.execute_input":"2022-05-02T04:00:53.040134Z","iopub.status.idle":"2022-05-02T04:00:53.089431Z","shell.execute_reply.started":"2022-05-02T04:00:53.040092Z","shell.execute_reply":"2022-05-02T04:00:53.088331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n%%script false --no-raise-error\n# Check the model performance in the validation dataset\nfrom sklearn.metrics import roc_auc_score\nval_preds = lgb.predict_proba(X_valid[features])[:, 1]\nroc_auc_score(y_valid, val_preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:53.091937Z","iopub.execute_input":"2022-05-02T04:00:53.092332Z","iopub.status.idle":"2022-05-02T04:00:53.140012Z","shell.execute_reply.started":"2022-05-02T04:00:53.092281Z","shell.execute_reply":"2022-05-02T04:00:53.138743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 11. Building a Baseline GBT Model, Kfold Loop","metadata":{}},{"cell_type":"code","source":"%%time\nfrom lightgbm import LGBMClassifier\nfrom xgboost  import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport math","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:53.14391Z","iopub.execute_input":"2022-05-02T04:00:53.144204Z","iopub.status.idle":"2022-05-02T04:00:53.152383Z","shell.execute_reply.started":"2022-05-02T04:00:53.144171Z","shell.execute_reply":"2022-05-02T04:00:53.15114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Define the model parameters to get started we use default values to a certain degree\nlgb_params = {'n_estimators'      : 8192, # Was 8192...\n              'min_child_samples' : 96,\n              'max_bins'          : 512,\n              'random_state'      : 46,\n             }\n\nxgb_params = {'n_estimators'     : 8192,\n              'min_child_weight' : 96,\n              'max_depth'        : 6,\n              'learning_rate'    : 0.15,\n              'subsample'        : 0.95,\n              'colsample_bytree' : 0.95,\n              'reg_lambda'       : 1.50,\n              'reg_alpha'        : 1.50,\n              'gamma'            : 1.50,\n              'max_bin'          : 512,\n              'random_state'     : 46,\n              'objective'        : 'binary:logistic',\n              'tree_method'      : 'gpu_hist',\n             }","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:53.154177Z","iopub.execute_input":"2022-05-02T04:00:53.154577Z","iopub.status.idle":"2022-05-02T04:00:53.168034Z","shell.execute_reply.started":"2022-05-02T04:00:53.154531Z","shell.execute_reply":"2022-05-02T04:00:53.166602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Create empty lists to store NN information...\n\nscore_list   = []\npredictions  = [] \n# Define kfolds for training purposes...\nkf = KFold(n_splits = 5)\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(trn_data)):\n    print(f'Training Fold {fold} ...')\n    X_train, X_valid = trn_data.iloc[trn_idx][features], trn_data.iloc[val_idx][features]\n    y_train, y_valid = trn_data.iloc[trn_idx][target_feature], trn_data.iloc[val_idx][target_feature]\n    \n    # LGBM (Uncomment to use, and Comment the XGBoost Part... LGBM Takes forever)\n    # model = LGBMClassifier(**lgb_params)\n    # model.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], eval_metric = ['auc'], early_stopping_rounds = 256, verbose = 0)\n    \n    # XGBoost\n    model = XGBClassifier(**xgb_params)\n    model.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], eval_metric = ['auc'], early_stopping_rounds = 256, verbose = 0)\n    \n    y_valid_pred = model.predict_proba(X_valid.values)[:,1]\n    score = roc_auc_score(y_valid, y_valid_pred)\n\n    score_list.append(score)\n    print(f\"Fold {fold}, AUC = {score:.3f}\")\n    print((''))\n    \n    tst_pred = model.predict_proba(tst_data[features].values)[:,1]\n    predictions.append(tst_pred)\n\nprint(f'OOF AUC: {np.mean(score_list):.3f}')\nprint('.........')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:00:53.170378Z","iopub.execute_input":"2022-05-02T04:00:53.170871Z","iopub.status.idle":"2022-05-02T04:06:08.587903Z","shell.execute_reply.started":"2022-05-02T04:00:53.170766Z","shell.execute_reply":"2022-05-02T04:06:08.58675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. Undertanding Model Behavior, Feature Importance","metadata":{}},{"cell_type":"code","source":"%%time\n# Define a funtion to plot the feature importance properly\ndef plot_feature_importance(importance, names, model_type, max_features = 10):\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    fi_df = fi_df.head(max_features)\n\n    #Define size of bar plot\n    plt.figure(figsize=(8,6))\n    \n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:06:08.589624Z","iopub.execute_input":"2022-05-02T04:06:08.590091Z","iopub.status.idle":"2022-05-02T04:06:08.600535Z","shell.execute_reply.started":"2022-05-02T04:06:08.590042Z","shell.execute_reply":"2022-05-02T04:06:08.599197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Utilize the feature importance function to visualize the most valueable features\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplot_feature_importance(model.feature_importances_,X_train.columns,'LGBM ', max_features = 25)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:06:08.602294Z","iopub.execute_input":"2022-05-02T04:06:08.602807Z","iopub.status.idle":"2022-05-02T04:06:09.08213Z","shell.execute_reply.started":"2022-05-02T04:06:08.602759Z","shell.execute_reply":"2022-05-02T04:06:09.081092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 12. Baseline Model Submission File Generation","metadata":{}},{"cell_type":"code","source":"%%time\n# Review the format of the submission file\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:06:09.083993Z","iopub.execute_input":"2022-05-02T04:06:09.084336Z","iopub.status.idle":"2022-05-02T04:06:09.101194Z","shell.execute_reply.started":"2022-05-02T04:06:09.084289Z","shell.execute_reply":"2022-05-02T04:06:09.100014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Populated the prediction on the submission dataset and creates an output file\nsub['target'] = np.array(predictions).mean(axis=0)\nsub.to_csv('my_submission_043022.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:06:09.103292Z","iopub.execute_input":"2022-05-02T04:06:09.103612Z","iopub.status.idle":"2022-05-02T04:06:11.018977Z","shell.execute_reply.started":"2022-05-02T04:06:09.103567Z","shell.execute_reply":"2022-05-02T04:06:11.017931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Review the submission file as a final step to upload to Kaggle.\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:06:11.020942Z","iopub.execute_input":"2022-05-02T04:06:11.021338Z","iopub.status.idle":"2022-05-02T04:06:11.034408Z","shell.execute_reply.started":"2022-05-02T04:06:11.021289Z","shell.execute_reply":"2022-05-02T04:06:11.033159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}