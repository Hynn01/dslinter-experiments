{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torchvision import transforms,datasets\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torchvision\nimport sys\n\n\nimport matplotlib.pyplot as plt\nimport time","metadata":{"id":"1JyAhPUwg5_r","execution":{"iopub.status.busy":"2022-05-02T15:43:52.982026Z","iopub.execute_input":"2022-05-02T15:43:52.982687Z","iopub.status.idle":"2022-05-02T15:43:54.64325Z","shell.execute_reply.started":"2022-05-02T15:43:52.982581Z","shell.execute_reply":"2022-05-02T15:43:54.641817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision","metadata":{"id":"tWFxKovghBRw","execution":{"iopub.status.busy":"2022-05-02T15:43:54.644893Z","iopub.execute_input":"2022-05-02T15:43:54.645124Z","iopub.status.idle":"2022-05-02T15:43:54.656289Z","shell.execute_reply.started":"2022-05-02T15:43:54.64509Z","shell.execute_reply":"2022-05-02T15:43:54.651499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"apply_transform = transforms.Compose([transforms.Resize(32), transforms.ToTensor()])\nBatchSize = 256 # change according to system specs\nBatchSize2 = 1024 # change according to system specs\n\n\ntrainset = datasets.MNIST(root='./MNIST', train=True, download=True, transform=apply_transform)\ntrainLoader = torch.utils.data.DataLoader(trainset, batch_size=BatchSize,\n                                          shuffle=True, num_workers=4) # Creating dataloader\n\n# Validation set with random rotations in the range [-90,90]\ntestset = datasets.MNIST(root='./MNIST', train=False, download=True, transform=apply_transform)\ntestLoader = torch.utils.data.DataLoader(testset, batch_size=BatchSize,\n                                         shuffle=False, num_workers=4) # Creating dataloader\n\ntrainset2 = datasets.MNIST(root='./MNIST', train=True, download=True, transform=apply_transform)\ntrainLoader2 = torch.utils.data.DataLoader(trainset2, batch_size=BatchSize2,\n                                          shuffle=True, num_workers=4) # Creating dataloader\n\n# Validation set with random rotations in the range [-90,90]\ntestset2 = datasets.MNIST(root='./MNIST', train=False, download=True, transform=apply_transform)\ntestLoader2 = torch.utils.data.DataLoader(testset2, batch_size=BatchSize2,\n                                         shuffle=False, num_workers=4) # Creating dataloader                                         ","metadata":{"id":"jQOmc53jhLzr","outputId":"e9df1997-8f00-48c5-eeda-cea3037eb383","execution":{"iopub.status.busy":"2022-05-02T15:43:54.658019Z","iopub.execute_input":"2022-05-02T15:43:54.658636Z","iopub.status.idle":"2022-05-02T15:43:56.76877Z","shell.execute_reply.started":"2022-05-02T15:43:54.658591Z","shell.execute_reply":"2022-05-02T15:43:56.767896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Size of train and test datasets\nprint('No. of samples in train set: '+str(len(trainLoader.dataset)))\nprint('No. of samples in test set: '+str(len(testLoader.dataset)))\n# Size of train and test datasets\nprint('No. of samples in train set2: '+str(len(trainLoader2.dataset)))\nprint('No. of samples in test set2: '+str(len(testLoader2.dataset)))","metadata":{"id":"Vfft59iCkHXQ","outputId":"975e9753-f05d-4346-f551-82bdf1c815b3","execution":{"iopub.status.busy":"2022-05-02T15:43:56.770506Z","iopub.execute_input":"2022-05-02T15:43:56.77216Z","iopub.status.idle":"2022-05-02T15:43:56.780161Z","shell.execute_reply.started":"2022-05-02T15:43:56.77212Z","shell.execute_reply":"2022-05-02T15:43:56.779462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n        self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)        \n        self.fc1 = nn.Linear(400, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool1(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool2(x)\n        x = x.view(-1, 400)\n        x = F.relu(self.fc1(x)) \n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return F.log_softmax(x,dim=1)","metadata":{"id":"ykZ_LOB8kOTY","execution":{"iopub.status.busy":"2022-05-02T15:43:57.83491Z","iopub.execute_input":"2022-05-02T15:43:57.835159Z","iopub.status.idle":"2022-05-02T15:43:57.843719Z","shell.execute_reply.started":"2022-05-02T15:43:57.835132Z","shell.execute_reply":"2022-05-02T15:43:57.84307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_gpu = torch.cuda.is_available()\nnet1= LeNet()\nnet2= LeNet()\nnet3= LeNet()\n\nprint(net1)\nprint(net2)\nprint(net3)\n\nif use_gpu:\n    print('GPU is avaialble!')\n    net1 = net1.cuda()\n    net2 = net2.cuda()\n    net3 = net3.cuda()\n    ","metadata":{"id":"3qgUUTXfkltu","outputId":"ff6ab566-11f7-4580-c713-98ef5b057df8","execution":{"iopub.status.busy":"2022-05-02T15:43:58.901476Z","iopub.execute_input":"2022-05-02T15:43:58.901991Z","iopub.status.idle":"2022-05-02T15:44:01.934432Z","shell.execute_reply.started":"2022-05-02T15:43:58.901953Z","shell.execute_reply":"2022-05-02T15:44:01.933647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss() \nlearning_rate1 = 0.01\noptimizer1 = optim.Adam(net1.parameters(), lr=learning_rate1) # ADAM \nnum_epochs = 50\n\ntrain_loss = []\ntrain_acc = []\nfor epoch in range(num_epochs):\n    \n    running_loss = 0.0 \n    running_corr = 0\n        \n    for i,data in enumerate(trainLoader):\n        inputs,labels = data\n        if use_gpu:\n            inputs, labels = inputs.cuda(),labels.cuda() \n        # Initializing model gradients to zero\n        optimizer1.zero_grad() \n        # Data feed-forward through the network\n        outputs1 = net1(inputs)\n        # Predicted class is the one with maximum probability\n        preds1 = torch.argmax(outputs1,dim=1)\n        # Finding the loss\n        loss = criterion(outputs1, labels)\n        # Accumulating the loss for each batch\n        running_loss += loss \n        # Accumulate number of correct predictions\n        running_corr += torch.sum(preds1==labels)    \n        \n    totalLoss1 = running_loss/(i+1)\n    # Calculating gradients\n    totalLoss1.backward()\n    # Updating the model parameters\n    # Updating the model parameters\n    optimizer1.step()\n        \n    epoch_loss = running_loss.item()/(i+1)   #Total loss for one epoch\n    epoch_acc = running_corr.item()/60000\n    \n    \n         \n    train_loss.append(epoch_loss) #Saving the loss over epochs for plotting the graph\n    train_acc.append(epoch_acc) #Saving the accuracy over epochs for plotting the graph\n       \n        \n    print('Epoch {:.0f}/{:.0f} : Training loss: {:.4f} | Training Accuracy: {:.4f}'.format(epoch+1,num_epochs,epoch_loss,epoch_acc*100))","metadata":{"id":"TKvgsDpAkpgr","outputId":"9489d8c6-28d2-46d4-8844-98aa7fc99980","execution":{"iopub.status.busy":"2022-05-02T15:44:04.358153Z","iopub.execute_input":"2022-05-02T15:44:04.359064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=[15,5]) \nplt.subplot(121)\nplt.plot(range(num_epochs),train_loss,'r-',label='Loss/error') \nplt.legend(loc='upper right')\nplt.xlabel('Epochs')\nplt.ylabel('Training')\nplt.subplot(122)\nplt.plot(range(num_epochs),train_acc,'g-',label='Accuracy') \nplt.legend(loc='upper right')\nplt.xlabel('Epochs')\nplt.ylabel('Training')","metadata":{"id":"KJV5K495Hn-o","outputId":"24d42951-4770-4ddb-f2a2-ef8c84513769","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correct_pred1=0\nfor data in testLoader:\n    inputs,labels = data\n    if use_gpu:\n        inputs, labels = inputs.cuda(),labels.cuda()\n    # Feedforward train data batch through model\n    output = net1(inputs) \n    # Predicted class is the one with maximum probability\n    preds1 = torch.argmax(output,dim=1)\n    correct_pred1 += torch.sum(preds1==labels)\n\ntest_accuracy = correct_pred1.item()/10000.0\nprint('Testing accuracy (Batch size=256, lr=0.1 = ',test_accuracy*100) #for bathch1","metadata":{"id":"rxs1v0BcNa7e","outputId":"031fd427-0a4a-48e3-ba93-0944f296b44a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss() \nlearning_rate2 = 0.01\noptimizer2 = optim.Adam(net2.parameters(), lr=learning_rate2) # ADAM \nnum_epochs2 = 50\n\ntrain_loss2 = []\ntrain_acc2 = []\nfor epoch in range(num_epochs2):\n    \n    running_loss = 0.0 \n    running_corr = 0\n        \n    for i,data in enumerate(trainLoader2):\n        inputs,labels = data\n        if use_gpu:\n            inputs, labels = inputs.cuda(),labels.cuda() \n        # Initializing model gradients to zero\n        optimizer2.zero_grad() \n        # Data feed-forward through the network\n        outputs2 = net2(inputs)\n        # Predicted class is the one with maximum probability\n        preds2 = torch.argmax(outputs2,dim=1)\n        # Finding the loss\n        loss = criterion(outputs2, labels)\n        # Accumulating the loss for each batch\n        running_loss += loss \n        # Accumulate number of correct predictions\n        running_corr += torch.sum(preds2==labels)    \n        \n    totalLoss2 = running_loss/(i+1)\n    # Calculating gradients\n    totalLoss2.backward()\n    # Updating the model parameters\n    # Updating the model parameters\n    optimizer2.step()\n        \n    epoch_loss = running_loss.item()/(i+1)   #Total loss for one epoch\n    epoch_acc = running_corr.item()/60000\n    \n    \n         \n    train_loss2.append(epoch_loss) #Saving the loss over epochs for plotting the graph\n    train_acc2.append(epoch_acc) #Saving the accuracy over epochs for plotting the graph\n       \n        \n    print('Epoch {:.0f}/{:.0f} : Training loss: {:.4f} | Training Accuracy: {:.4f}'.format(epoch+1,num_epochs2,epoch_loss,epoch_acc*100))","metadata":{"id":"YUxl5VTtNz6-","outputId":"2df3ca41-0e0c-4d59-dab0-4ac4abba46d1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=[15,5]) \nplt.subplot(121)\nplt.plot(range(num_epochs),train_loss,'r-',label='Batch size=256') \nplt.plot(range(num_epochs2),train_loss2,'r-',color='blue',label='Batch size=1024') \n\nplt.legend(loc='upper right')\nplt.xlabel('Epochs')\nplt.ylabel('Training loss/error')\nplt.subplot(122)\nplt.plot(range(num_epochs),train_acc,'g-',label='Batch size=256') \nplt.plot(range(num_epochs2),train_acc2,'g-', color='blue',label='Batch size=1024') #learning rate in both is same\n \nplt.legend(loc='upper right')\nplt.xlabel('Epochs')\nplt.ylabel('Training Accuracy')","metadata":{"id":"W2XUgFxhs-aI","outputId":"9ec5c57a-22b3-46de-d820-bce495aec720","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correct_pred2=0\nfor data in testLoader2:\n    inputs,labels = data\n    if use_gpu:\n        inputs, labels = inputs.cuda(),labels.cuda()\n    # Feedforward train data batch through model\n    output = net2(inputs) \n    # Predicted class is the one with maximum probability\n    preds2 = torch.argmax(output,dim=1)\n    correct_pred2 += torch.sum(preds2==labels)\n\ntest_accuracy2 = correct_pred2.item()/10000.0\nprint('Testing accuracy (Batch size=1024, lr=0.1) = ',test_accuracy2*100) #for bathch1","metadata":{"id":"30vtBvaftbVB","outputId":"328ceef0-6c0a-4e55-8b1a-b7fe01246528","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss() \nlearning_rate3 = 0.001\noptimizer3 = optim.Adam(net3.parameters(), lr=learning_rate3) # ADAM \nnum_epochs3 = 50\n\ntrain_loss3 = []\ntrain_acc3 = []\nfor epoch in range(num_epochs3):\n    \n    running_loss = 0.0 \n    running_corr = 0\n        \n    for i,data in enumerate(trainLoader):\n        inputs,labels = data\n        if use_gpu:\n            inputs, labels = inputs.cuda(),labels.cuda() \n        # Initializing model gradients to zero\n        optimizer3.zero_grad() \n        # Data feed-forward through the network\n        outputs3 = net3(inputs)\n        # Predicted class is the one with maximum probability\n        preds3 = torch.argmax(outputs3,dim=1)\n        # Finding the loss\n        loss = criterion(outputs3, labels)\n        # Accumulating the loss for each batch\n        running_loss += loss \n        # Accumulate number of correct predictions\n        running_corr += torch.sum(preds3==labels)    \n        \n    totalLoss3 = running_loss/(i+1)\n    # Calculating gradients\n    totalLoss3.backward()\n    # Updating the model parameters\n    # Updating the model parameters\n    optimizer3.step()\n        \n    epoch_loss = running_loss.item()/(i+1)   #Total loss for one epoch\n    epoch_acc = running_corr.item()/60000\n    \n    \n         \n    train_loss3.append(epoch_loss) #Saving the loss over epochs for plotting the graph\n    train_acc3.append(epoch_acc) #Saving the accuracy over epochs for plotting the graph\n       \n        \n    print('Epoch {:.0f}/{:.0f} : Training loss: {:.4f} | Training Accuracy: {:.4f}'.format(epoch+1,num_epochs3,epoch_loss,epoch_acc*100))","metadata":{"id":"lLOMshOC0MtD","outputId":"8a69969c-edc7-459d-89bf-f2d9da521996","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=[15,5]) \nplt.subplot(121)\nplt.plot(range(num_epochs),train_loss,'r-',label='Batch size=256,lr=.01') \nplt.plot(range(num_epochs),train_loss3,'r-',color='blue',label='Batch size=256,lr=.001') \n\nplt.legend(loc='upper right')\nplt.xlabel('Epochs')\nplt.ylabel('Training loss/error')\nplt.subplot(122)\nplt.plot(range(num_epochs),train_acc,'g-',label='Batch size=256,lr=.01') \nplt.plot(range(num_epochs),train_acc3,'g-', color='blue',label='Batch size=256, lr=.001') \n \nplt.legend(loc='upper right')\nplt.xlabel('Epochs')\nplt.ylabel('Training Accuracy')","metadata":{"id":"vjfNJdqN5B2f","outputId":"d705e6f6-9342-40b8-8e3d-3bc82b43a699","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correct_pred3=0\nfor data in testLoader:\n    inputs,labels = data\n    if use_gpu:\n        inputs, labels = inputs.cuda(),labels.cuda()\n    # Feedforward train data batch through model\n    output = net3(inputs) \n    # Predicted class is the one with maximum probability\n    preds3 = torch.argmax(output,dim=1)\n    correct_pred3 += torch.sum(preds3==labels)\n\ntest_accuracy3 = correct_pred3.item()/10000.0\nprint('Testing accuracy (Batch size=256, lr=.01) = ',test_accuracy3*100) #for bathch1","metadata":{"id":"TCUA_nxUt5mE","outputId":"0548850c-e4c0-4ffa-b6da-742929e26ad0","trusted":true},"execution_count":null,"outputs":[]}]}