{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n# [Tabular Playground Series - May 2022][1]\n\n- The goal of this competition is to predict whether the machine is in state 0 or state 1. The data has various feature interactions that may be important in determining the machine state.\n\n---\n#### **The aim of this notebook is to**\n- **1. Conduct Exploratory Data Analysis (EDA).**\n- **2. Create unigram and bigram from 'f_27' column, and apply the 1D-CNN module to encode text features.**\n- **3. Build and train a TabTransformer model.**\n\n---\n**References:** Thanks to previous great codes and notebooks.\n- [ðŸ”¥ðŸ”¥[TensorFlow]TabTransformerðŸ”¥ðŸ”¥][2]\n- [Sachin's Blog Tensorflow Learning Rate Finder][3]\n\n---\n### **If you find this notebook useful, please do give me an upvote. It helps me keep up my motivation.**\n#### **Also, I would appreciate it if you find any mistakes and help me correct them.**\n\n---\n[1]: https://www.kaggle.com/competitions/tabular-playground-series-may-2022\n[2]: https://www.kaggle.com/code/usharengaraju/tensorflow-tabtransformer\n[3]: https://sachinruk.github.io/blog/tensorflow/learning%20rate/2021/02/15/Tensorflow-Learning-Rate-Finder.html","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>TABLE OF CONTENTS</center></h1>\n\n<ul class=\"list-group\" style=\"list-style-type:none;\">\n    <li><a href=\"#0\" class=\"list-group-item list-group-item-action\">0. Settings</a></li>\n    <li><a href=\"#1\" class=\"list-group-item list-group-item-action\">1. Data Loading</a></li>\n    <li><a href=\"#2\" class=\"list-group-item list-group-item-action\">2. Exploratory Data Analysis</a>\n        <ul class=\"list-group\" style=\"list-style-type:none;\">\n            <li><a href=\"#2.1\" class=\"list-group-item list-group-item-action\">2.1 Target Distribution</a></li>\n            <li><a href=\"#2.2\" class=\"list-group-item list-group-item-action\">2.2 Feature Distributions</a></li>\n            <li><a href=\"#2.3\" class=\"list-group-item list-group-item-action\">2.3 Exploring f_27 Feature</a></li>\n        </ul>\n    </li>\n    <li><a href=\"#3\" class=\"list-group-item list-group-item-action\">3. Model Building</a>\n        <ul class=\"list-group\" style=\"list-style-type:none;\">\n            <li><a href=\"#3.1\" class=\"list-group-item list-group-item-action\">3.1 Validation Split</a></li>\n            <li><a href=\"#3.2\" class=\"list-group-item list-group-item-action\">3.2 Dataset</a></li>\n            <li><a href=\"#3.3\" class=\"list-group-item list-group-item-action\">3.3 Preprocessing Model</a></li>\n            <li><a href=\"#3.4\" class=\"list-group-item list-group-item-action\">3.4 Training Model</a></li>\n        </ul>\n    </li>\n    <li><a href=\"#4\" class=\"list-group-item list-group-item-action\">4. Model Training</a>\n        <ul class=\"list-group\" style=\"list-style-type:none;\">\n            <li><a href=\"#4.1\" class=\"list-group-item list-group-item-action\">4.1 Learning Rate Finder</a></li>\n            <li><a href=\"#4.2\" class=\"list-group-item list-group-item-action\">4.2 Model Training</a></li>\n        </ul>\n    </li>\n    <li><a href=\"#5\" class=\"list-group-item list-group-item-action\">5. Inference</a></li>\n</ul>\n","metadata":{}},{"cell_type":"markdown","source":"<a id =\"0\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>0. Settings</center></h1>","metadata":{}},{"cell_type":"code","source":"## Import dependencies \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport os\nimport pathlib\nimport gc\nimport sys\nimport re\nimport math \nimport random\nimport time \nimport datetime as dt\nfrom tqdm import tqdm \n\nimport sklearn\nfrom sklearn.model_selection import StratifiedKFold\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('import done!')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:54:58.358832Z","iopub.execute_input":"2022-05-08T02:54:58.359076Z","iopub.status.idle":"2022-05-08T02:55:06.612522Z","shell.execute_reply.started":"2022-05-08T02:54:58.359007Z","shell.execute_reply":"2022-05-08T02:55:06.611749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For reproducible results    \ndef seed_all(s):\n    random.seed(s)\n    np.random.seed(s)\n    tf.random.set_seed(s)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['PYTHONHASHSEED'] = str(s) \n    print('Seeds setted!')\n    \nglobal_seed = 42\nseed_all(global_seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:06.614234Z","iopub.execute_input":"2022-05-08T02:55:06.614471Z","iopub.status.idle":"2022-05-08T02:55:06.622733Z","shell.execute_reply.started":"2022-05-08T02:55:06.614437Z","shell.execute_reply":"2022-05-08T02:55:06.621658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"1\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>1. Data Loading</center></h1>","metadata":{}},{"cell_type":"markdown","source":"---\n### [Files Descriptions](https://www.kaggle.com/competitions/tabular-playground-series-may-2022/data)\n\n- **train.csv** - the training data, which includes normalized continuous data and categorical data\n\n- **test.csv** - the test set; your task is to predict binary target variable which represents the state of a manufacturing process\n\n- **sample_submission.csv** -  a sample submission file in the correct format.\n\n---\n### [Submission & Evaluation](https://www.kaggle.com/competitions/tabular-playground-series-may-2022/overview/evaluation)\n\n- For each id in the test set, you must predict a probability for the target variable.\n- Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\n---","metadata":{}},{"cell_type":"code","source":"## Data Loading\ndata_config = {'train_csv_path': '../input/tabular-playground-series-may-2022/train.csv',\n               'test_csv_path': '../input/tabular-playground-series-may-2022/test.csv',\n               'sample_submission_path': '../input/tabular-playground-series-may-2022/sample_submission.csv',\n              }\n\ntrain_df = pd.read_csv(data_config['train_csv_path'])\ntest_df = pd.read_csv(data_config['test_csv_path'])\nsubmission_df = pd.read_csv(data_config['sample_submission_path'])\n\nprint(f'train_length: {len(train_df)}')\nprint(f'test_lenght: {len(test_df)}')\nprint(f'submission_length: {len(submission_df)}')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:06.623921Z","iopub.execute_input":"2022-05-08T02:55:06.624182Z","iopub.status.idle":"2022-05-08T02:55:19.367386Z","shell.execute_reply.started":"2022-05-08T02:55:06.624147Z","shell.execute_reply":"2022-05-08T02:55:19.3666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## train_df Check\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:19.369482Z","iopub.execute_input":"2022-05-08T02:55:19.369823Z","iopub.status.idle":"2022-05-08T02:55:19.405945Z","shell.execute_reply.started":"2022-05-08T02:55:19.369784Z","shell.execute_reply":"2022-05-08T02:55:19.405029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Null Value Check\nprint('train_df.info()'); print(train_df.info(), '\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:19.407395Z","iopub.execute_input":"2022-05-08T02:55:19.407744Z","iopub.status.idle":"2022-05-08T02:55:19.560646Z","shell.execute_reply.started":"2022-05-08T02:55:19.407703Z","shell.execute_reply":"2022-05-08T02:55:19.558795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"2\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>2. Exploratory Data Analysis</center></h1>","metadata":{}},{"cell_type":"markdown","source":"<a id =\"2.1\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>2.1 Target Distribution</center></h2>","metadata":{}},{"cell_type":"code","source":"## Target Distribution\ntarget_count = train_df.groupby(['target'])['id'].count()\ntarget_percent = target_count / target_count.sum()\n\n## Make Figure object\nfig = go.Figure()\n\n## Make trace (graph object)\ndata = go.Bar(x=target_count.index.astype(str).values, \n              y=target_count.values)\n\n## Add the trace to the Figure\nfig.add_trace(data)\n\n## Setting layouts\nfig.update_layout(title = dict(text='target distribution'),\n                  xaxis = dict(title='target values'),\n                  yaxis = dict(title='counts'))\n\n## Show the Figure\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:19.561887Z","iopub.execute_input":"2022-05-08T02:55:19.562126Z","iopub.status.idle":"2022-05-08T02:55:19.687855Z","shell.execute_reply.started":"2022-05-08T02:55:19.562101Z","shell.execute_reply":"2022-05-08T02:55:19.687006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Heat map of Correlation Matrix\nfig = px.imshow(train_df.drop(['id'], axis=1).corr(),\n                color_continuous_scale='RdBu_r',\n                color_continuous_midpoint=0, \n                aspect='auto')\nfig.update_layout(height=750, \n                  title = \"Heatmap\",                  \n                  showlegend=False)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:19.689433Z","iopub.execute_input":"2022-05-08T02:55:19.689696Z","iopub.status.idle":"2022-05-08T02:55:22.554485Z","shell.execute_reply.started":"2022-05-08T02:55:19.689661Z","shell.execute_reply":"2022-05-08T02:55:22.553818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"2.2\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>2.2 Feature Distributions</center></h2>","metadata":{}},{"cell_type":"code","source":"## Preparing dataframes for EDA\ntrain_pos_df = train_df.query('target==1')\ntrain_neg_df = train_df.query('target==0')\n\nnumerical_columns = ['f_00', 'f_01', 'f_02', 'f_03', 'f_04', 'f_05', 'f_06',\n                 'f_19', 'f_20', 'f_21', 'f_22', 'f_23', 'f_24', 'f_25',\n                 'f_26', 'f_28']\ncategorical_columns = ['f_07', 'f_08', 'f_09', 'f_10', 'f_11', 'f_12',\n               'f_13', 'f_14', 'f_15', 'f_16', 'f_17', 'f_18',\n               'f_29', 'f_30']\nobj_columns = ['f_27']\n\nprint(f'numerical_columns: {len(numerical_columns)},  categorical_columns: {len(categorical_columns)},  obj_columns: {len(obj_columns)}')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:22.555869Z","iopub.execute_input":"2022-05-08T02:55:22.556148Z","iopub.status.idle":"2022-05-08T02:55:22.693827Z","shell.execute_reply.started":"2022-05-08T02:55:22.556112Z","shell.execute_reply":"2022-05-08T02:55:22.69307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Numerical Features Statistics\ntrain_df[numerical_columns].describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:22.69495Z","iopub.execute_input":"2022-05-08T02:55:22.69564Z","iopub.status.idle":"2022-05-08T02:55:23.299647Z","shell.execute_reply.started":"2022-05-08T02:55:22.695601Z","shell.execute_reply":"2022-05-08T02:55:23.298899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Numerical Features Distribution\nfig = plt.figure(figsize=(16, 10))\nfor i, c in enumerate(numerical_columns):\n    ax = fig.add_subplot(4, 4, i+1)\n    ax.hist(train_pos_df[c], color='b', alpha=0.5, bins=50)\n    ax.hist(train_neg_df[c], color='r', alpha=0.5, bins=50)\n    ax.set_title(numerical_columns[i])\n    \nfig.suptitle('Distributions of Numerical Features (Blue: \"target=1\", red: \"target=0\")', fontsize=20)\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:23.30306Z","iopub.execute_input":"2022-05-08T02:55:23.30338Z","iopub.status.idle":"2022-05-08T02:55:27.961121Z","shell.execute_reply.started":"2022-05-08T02:55:23.303343Z","shell.execute_reply":"2022-05-08T02:55:27.960383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Categorical Features Statistics\ntrain_df[categorical_columns].describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:27.962496Z","iopub.execute_input":"2022-05-08T02:55:27.962762Z","iopub.status.idle":"2022-05-08T02:55:28.330278Z","shell.execute_reply.started":"2022-05-08T02:55:27.962725Z","shell.execute_reply":"2022-05-08T02:55:28.329457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Categorical Features Distribution\nfig = plt.figure(figsize=(16, 10))\nfor i, c in enumerate(categorical_columns):\n    ax = fig.add_subplot(4, 4, i+1)\n    x_range = (train_df[c].min(), train_df[c].max())\n    #bins = train_df[c].max() - train_df[c].min() + 1\n    bins = 50\n    ax.hist(train_pos_df[c], color='b', alpha=0.5, range=x_range, bins=bins)\n    ax.hist(train_neg_df[c], color='r', alpha=0.5, range=x_range, bins=bins)\n    ax.set_title(categorical_columns[i])\n    \nfig.suptitle('Distributions of Categorical Features (Blue: \"target=1\", red: \"target=0\")', fontsize=20)\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:28.331729Z","iopub.execute_input":"2022-05-08T02:55:28.332Z","iopub.status.idle":"2022-05-08T02:55:32.53184Z","shell.execute_reply.started":"2022-05-08T02:55:28.331951Z","shell.execute_reply":"2022-05-08T02:55:32.531167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"2.3\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>2.3 Exploring f_27 Feature</center></h2>","metadata":{}},{"cell_type":"code","source":"## Preparing dataframes for EDA\nf_27_df = train_df[['f_27', 'target']]\nf_27_feature_df = f_27_df.drop(['f_27'], axis=1)\nf_27_feature_df['n_char'] = f_27_df['f_27'].map(lambda x: len(x))\n\nfor i in range(65, 91): ## ASCII of A to Z\n    f_27_feature_df[chr(i)] = f_27_df['f_27'].map(lambda x: x.count(chr(i)))\n    \nf_27_feature_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:32.533186Z","iopub.execute_input":"2022-05-08T02:55:32.533429Z","iopub.status.idle":"2022-05-08T02:55:51.188631Z","shell.execute_reply.started":"2022-05-08T02:55:32.533395Z","shell.execute_reply":"2022-05-08T02:55:51.187867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n- All f_27 values have 10 characters.\n- There are no 'U', 'V', 'W', 'X', 'Y' and 'Z' in f_27.\n\n---","metadata":{}},{"cell_type":"code","source":"## Plot the number of appearance of each characters\ntmp_df = f_27_feature_df.groupby(['target']).sum()\ntmp_df = tmp_df.drop(['n_char'], axis=1)\n\nfig = make_subplots(rows=2, cols=1,\n                    subplot_titles=['target=0', 'target=1'],\n                    shared_xaxes='all',\n                    shared_yaxes='all')\nfor row in range(2):\n    for col in range(1):\n        data = go.Bar(x=tmp_df.columns.astype(str).values,\n                      y=tmp_df.query(f'target=={row}').values.squeeze())\n        fig.add_trace(data, row=row+1, col=col+1)\nfig.update_layout(title='Count of Characters',\n                  showlegend=False)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:51.189868Z","iopub.execute_input":"2022-05-08T02:55:51.190285Z","iopub.status.idle":"2022-05-08T02:55:51.443724Z","shell.execute_reply.started":"2022-05-08T02:55:51.190245Z","shell.execute_reply":"2022-05-08T02:55:51.442881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Heat map of Correlation Matrix\nf_27_feature_df = f_27_feature_df.drop(['U', 'V', 'W', 'X', 'Y', 'Z', 'n_char'], axis=1)\nfig = px.imshow(f_27_feature_df.corr(),\n                color_continuous_scale='RdBu_r',\n                color_continuous_midpoint=0,\n                aspect='auto')\nfig.update_layout(height=750, \n                  title = \"Heatmap\",                  \n                  showlegend=False)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:51.44516Z","iopub.execute_input":"2022-05-08T02:55:51.445603Z","iopub.status.idle":"2022-05-08T02:55:52.660536Z","shell.execute_reply.started":"2022-05-08T02:55:51.445567Z","shell.execute_reply":"2022-05-08T02:55:52.659797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## The Ratio between characters.\nf_27_feature_df['A_B'] = f_27_feature_df['A'] / (f_27_feature_df['B'] + 1)\nf_27_feature_df['A_C'] = f_27_feature_df['A'] / (f_27_feature_df['C'] + 1)\nf_27_feature_df['A_D'] = f_27_feature_df['A'] / (f_27_feature_df['D'] + 1)\nf_27_feature_df['B_C'] = f_27_feature_df['B'] / (f_27_feature_df['C'] + 1)\nf_27_feature_df['B_D'] = f_27_feature_df['B'] / (f_27_feature_df['D'] + 1)\nf_27_feature_df['B_E'] = f_27_feature_df['B'] / (f_27_feature_df['E'] + 1)\nf_27_feature_df['C_D'] = f_27_feature_df['C'] / (f_27_feature_df['D'] + 1)\nf_27_feature_df['C_E'] = f_27_feature_df['C'] / (f_27_feature_df['E'] + 1)\nf_27_feature_df['D_E'] = f_27_feature_df['D'] / (f_27_feature_df['E'] + 1)\n\ntmp_df = f_27_feature_df[['target', 'A_B', 'A_C', 'A_D', 'B_C', 'B_D', 'B_E', 'C_D', 'C_E', 'D_E']].groupby(['target']).sum()\nfig = make_subplots(rows=2, cols=1,\n                    subplot_titles=['target=0', 'target=1'],\n                    shared_xaxes='all',\n                    shared_yaxes='all')\nfor row in range(2):\n    for col in range(1):\n        data = go.Bar(x=tmp_df.columns.astype(str).values,\n                      y=tmp_df.query(f'target=={row}').values.squeeze())\n        fig.add_trace(data, row=row+1, col=col+1)\nfig.update_layout(title='Count of Character Raito',\n                  showlegend=False)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:52.661925Z","iopub.execute_input":"2022-05-08T02:55:52.662364Z","iopub.status.idle":"2022-05-08T02:55:52.876526Z","shell.execute_reply.started":"2022-05-08T02:55:52.662326Z","shell.execute_reply":"2022-05-08T02:55:52.875863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>3. Model Building</center></h1>","metadata":{}},{"cell_type":"markdown","source":"<a id =\"3.1\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>3.1 Validation Split</center></h2>","metadata":{}},{"cell_type":"code","source":"## Split train samples for cross-validation\nn_splits = 10\nskf = StratifiedKFold(n_splits=n_splits)\ntrain_df['k_folds'] = -1\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X=train_df, y=train_df['target'])):\n    train_df['k_folds'][valid_idx] = fold\n    \n## Check split samples\nfor i in range(n_splits):\n    print(f\"fold {i}: {len(train_df.query('k_folds == @i'))} samples\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T02:55:52.877624Z","iopub.execute_input":"2022-05-08T02:55:52.879149Z","iopub.status.idle":"2022-05-08T02:55:53.238782Z","shell.execute_reply.started":"2022-05-08T02:55:52.879109Z","shell.execute_reply":"2022-05-08T02:55:53.23806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train_df.query(f'k_folds != 0').reset_index(drop=True)\nvalid = train_df.query(f'k_folds == 0').reset_index(drop=True)\n\nprint(len(train), len(valid))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T02:55:53.239973Z","iopub.execute_input":"2022-05-08T02:55:53.24048Z","iopub.status.idle":"2022-05-08T02:55:53.453187Z","shell.execute_reply.started":"2022-05-08T02:55:53.240436Z","shell.execute_reply":"2022-05-08T02:55:53.452457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.2\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>3.2 Dataset</center></h2>","metadata":{}},{"cell_type":"code","source":"def df_to_dataset(dataframe, target=None, shuffle=False,\n                  batch_size=5, drop_remainder=False):\n    df = dataframe.copy()\n    if target is not None:\n        labels = df.pop(target)\n        df = {key: value[:, tf.newaxis] for key, value in df.items()}\n        ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n    else:\n        df = {key: value[:, tf.newaxis] for key, value in df.items()}\n        ds = tf.data.Dataset.from_tensor_slices(dict(df))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(df))\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n    ds = ds.prefetch(batch_size)\n    return ds\n\n## Create datasets\nbatch_size = 512\ntrain_ds = df_to_dataset(train,\n                         target='target',\n                         shuffle=True,\n                         batch_size=batch_size,\n                         drop_remainder=True)\nvalid_ds = df_to_dataset(valid,\n                         target='target',\n                         shuffle=False,\n                         batch_size=batch_size,\n                         drop_remainder=True)\n\n## Display a batch sample\nexample = next(iter(train_ds))[0]\nfor key in example:\n    print(f'{key}, shape:{example[key].shape}, {example[key].dtype}')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T02:55:53.454467Z","iopub.execute_input":"2022-05-08T02:55:53.454862Z","iopub.status.idle":"2022-05-08T02:55:56.335734Z","shell.execute_reply.started":"2022-05-08T02:55:53.454825Z","shell.execute_reply":"2022-05-08T02:55:56.33506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.3\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>3.3 Preprocessing Model</center></h2>","metadata":{}},{"cell_type":"code","source":"numerical_columns = ['f_00', 'f_01', 'f_02', 'f_03', 'f_04', 'f_05', 'f_06',\n                 'f_19', 'f_20', 'f_21', 'f_22', 'f_23', 'f_24', 'f_25',\n                 'f_26', 'f_28']\ncategorical_columns = ['f_07', 'f_08', 'f_09', 'f_10', 'f_11', 'f_12',\n               'f_13', 'f_14', 'f_15', 'f_16', 'f_17', 'f_18',\n               'f_29', 'f_30']\ntext_columns = ['f_27']\n\ndef create_preprocess_inputs(numerical, categorical, text):\n    preprocess_inputs = {}\n    numerical_inputs = {key: layers.Input(shape=(1, ), dtype='float64') for key in numerical}\n    categorical_inputs = {key: layers.Input(shape=(1, ), dtype='int64') for key in categorical}\n    text_inputs = {key: layers.Input(shape=(1, ), dtype='string') for key in text}\n    preprocess_inputs.update(**numerical_inputs, **categorical_inputs, **text_inputs)\n    return preprocess_inputs\n\npreprocess_inputs = create_preprocess_inputs(numerical_columns,\n                                             categorical_columns,\n                                             text_columns)\npreprocess_inputs","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T02:55:56.339641Z","iopub.execute_input":"2022-05-08T02:55:56.341833Z","iopub.status.idle":"2022-05-08T02:55:56.396377Z","shell.execute_reply.started":"2022-05-08T02:55:56.341796Z","shell.execute_reply":"2022-05-08T02:55:56.395721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Preprocess layers for numerical_features\nnormalize_layers = {}\nfor nc in numerical_columns:\n    normalize_layer = layers.Normalization(mean=train_df[nc].mean(), variance=train_df[nc].var())\n    normalize_layers[nc] = normalize_layer\nnormalize_layers","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T02:55:56.397611Z","iopub.execute_input":"2022-05-08T02:55:56.397843Z","iopub.status.idle":"2022-05-08T02:55:56.532776Z","shell.execute_reply.started":"2022-05-08T02:55:56.397811Z","shell.execute_reply":"2022-05-08T02:55:56.532062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Preprocess layers for categorical_features\nlookup_layers = {}\nfor cc in categorical_columns:\n    lookup_layer = layers.IntegerLookup(vocabulary=train_df[cc].unique(),\n                                       output_mode='int')\n    lookup_layers[cc] = lookup_layer\nlookup_layers","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T02:55:56.534246Z","iopub.execute_input":"2022-05-08T02:55:56.5345Z","iopub.status.idle":"2022-05-08T02:55:56.665612Z","shell.execute_reply.started":"2022-05-08T02:55:56.534466Z","shell.execute_reply":"2022-05-08T02:55:56.664898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Split the text_feature into unigram\ndef split_unigram(input_data):\n    s = tf.strings.regex_replace(input_data, '', ' ')\n    s = tf.strings.strip(s)\n    s = tf.strings.split(s, sep = ' ')\n    return s\n\nuni_vocabulary = [chr(i) for i in range(65, 91)] ## ASCII of A to Z\n\n## Preprocess layers for unigram\nvectorize_layers = {}\nfor tc in text_columns:\n    uni_vectorize_layer = layers.TextVectorization(standardize=None,\n                                                   split=split_unigram,\n                                                   vocabulary=uni_vocabulary)\n    vectorize_layers[f'{tc}_unigram'] = uni_vectorize_layer\nvectorize_layers","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T02:55:56.666897Z","iopub.execute_input":"2022-05-08T02:55:56.667156Z","iopub.status.idle":"2022-05-08T02:55:56.684461Z","shell.execute_reply.started":"2022-05-08T02:55:56.667121Z","shell.execute_reply":"2022-05-08T02:55:56.683638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Split the text_feature into bigram\ndef split_bigram(input_data):\n    s = tf.strings.regex_replace(input_data, '', ' ')\n    s = tf.strings.strip(s)\n    s = tf.strings.split(s, sep = ' ')\n    s = tf.strings.ngrams(s, 2, separator='')\n    return s\n\nbi_vocabulary = {uni1+uni2 for uni1 in uni_vocabulary for uni2 in uni_vocabulary}\nbi_vocabulary = list(bi_vocabulary)\nbi_vocabulary.sort()\n\n## Preprocess layers for bigram\nfor tc in text_columns:\n    bi_vectorize_layer = layers.TextVectorization(standardize=None,\n                                                  split=split_bigram,\n                                                  vocabulary=bi_vocabulary)\n    vectorize_layers[f'{tc}_bigram'] = bi_vectorize_layer\nvectorize_layers","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T02:55:56.68555Z","iopub.execute_input":"2022-05-08T02:55:56.68576Z","iopub.status.idle":"2022-05-08T02:55:56.707182Z","shell.execute_reply.started":"2022-05-08T02:55:56.685735Z","shell.execute_reply":"2022-05-08T02:55:56.706473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess_outputs = {}\nfor key in preprocess_inputs:\n    if key in normalize_layers:\n        output = normalize_layers[key](preprocess_inputs[key])\n        normalize_layers[key]\n        preprocess_outputs[key] = output\n    elif key in lookup_layers:\n        output = lookup_layers[key](preprocess_inputs[key])\n        preprocess_outputs[key] = output\n    else:\n        uni_output = vectorize_layers[f'{key}_unigram'](preprocess_inputs[key])\n        bi_output = vectorize_layers[f'{key}_bigram'](preprocess_inputs[key])\n        preprocess_outputs[f'{key}_unigram'] = uni_output\n        preprocess_outputs[f'{key}_bigram'] = bi_output\n        \npreprocess_outputs","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T02:55:56.708141Z","iopub.execute_input":"2022-05-08T02:55:56.708345Z","iopub.status.idle":"2022-05-08T02:55:56.899515Z","shell.execute_reply.started":"2022-05-08T02:55:56.708314Z","shell.execute_reply":"2022-05-08T02:55:56.898841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create the preprocessing model\npreprocessing_model = tf.keras.Model(preprocess_inputs,\n                                     preprocess_outputs)\n\n## Apply the preprocessing model in tf.data.Dataset.map\ntrain_ds = train_ds.map(lambda x, y: (preprocessing_model(x), y),\n                        num_parallel_calls=tf.data.AUTOTUNE)\nvalid_ds = valid_ds.map(lambda x, y: (preprocessing_model(x), y),\n                        num_parallel_calls=tf.data.AUTOTUNE)\n\n## Display a preprocessed input sample\nexample = next(train_ds.take(1).as_numpy_iterator())\nfor key in example[0]:\n    print(f'{key}, shape:{example[0][key].shape}, {example[0][key].dtype}')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T02:55:56.90056Z","iopub.execute_input":"2022-05-08T02:55:56.900813Z","iopub.status.idle":"2022-05-08T02:55:57.68338Z","shell.execute_reply.started":"2022-05-08T02:55:56.900778Z","shell.execute_reply":"2022-05-08T02:55:57.682664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.4\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>3.4 Training Model</center></h2>","metadata":{}},{"cell_type":"code","source":"## Training model inputs\ndef create_model_inputs(numerical, categorical, unigram_keys, bigram_keys):\n    model_inputs = {}\n    \n    normalized_inputs = {key: layers.Input(shape=(1, ), dtype='float64') for key in numerical}\n    lookup_inputs = {key: layers.Input(shape=(1, ), dtype='int64') for key in categorical}\n    #vectorized_inputs = {key: layers.Input(shape=(10, ), dtype='int64') for key in text}\n    unigram_inputs = {key: layers.Input(shape=(10, ), dtype='int64') for key in unigram_keys}\n    bigram_inputs = {key: layers.Input(shape=(9, ), dtype='int64') for key in bigram_keys}\n    \n    model_inputs.update(**normalized_inputs, **lookup_inputs, **unigram_inputs, **bigram_inputs)\n    return model_inputs\n\nunigram_keys = ['f_27_unigram']\nbigram_keys = ['f_27_bigram']\nmodel_inputs = create_model_inputs(numerical_columns,\n                                   categorical_columns,\n                                   unigram_keys,\n                                   bigram_keys)\nmodel_inputs","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T02:55:57.684676Z","iopub.execute_input":"2022-05-08T02:55:57.684933Z","iopub.status.idle":"2022-05-08T02:55:57.726556Z","shell.execute_reply.started":"2022-05-08T02:55:57.684896Z","shell.execute_reply":"2022-05-08T02:55:57.72591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create Embedding Layers\ncat_embedding_dim = 16\nunigram_embedding_dim = 16\nbigram_embedding_dim = 32\n\nnumerical_feature_list = []\nencoded_categorical_feature_list = []\nencoded_text_feature_list = []\nfor key in model_inputs:\n    if key in numerical_columns:\n        numerical_feature_list.append(model_inputs[key])\n    elif key in categorical_columns:\n        embedding = layers.Embedding(input_dim=lookup_layers[key].vocabulary_size(),\n                                     output_dim=cat_embedding_dim)\n        encoded_categorical_feature = embedding(model_inputs[key])\n        encoded_categorical_feature_list.append(encoded_categorical_feature)\n    elif key in unigram_keys:\n        embedding = layers.Embedding(input_dim=vectorize_layers[key].vocabulary_size(),\n                                   output_dim=unigram_embedding_dim)\n        encoded_text_feature = embedding(model_inputs[key])\n        encoded_text_feature_list.append(encoded_text_feature)\n    elif key in bigram_keys:\n        embedding = layers.Embedding(input_dim=vectorize_layers[key].vocabulary_size(),\n                                   output_dim=bigram_embedding_dim)\n        encoded_text_feature = embedding(model_inputs[key])\n        encoded_text_feature_list.append(encoded_text_feature)\n\nencoded_categorical_features = tf.concat(encoded_categorical_feature_list, axis=1)\nencoded_categorical_features.shape","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T02:55:57.730357Z","iopub.execute_input":"2022-05-08T02:55:57.730553Z","iopub.status.idle":"2022-05-08T02:55:57.829495Z","shell.execute_reply.started":"2022-05-08T02:55:57.730524Z","shell.execute_reply":"2022-05-08T02:55:57.828824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1D-CNN for unigram and bigram features","metadata":{}},{"cell_type":"code","source":"## 1D-CNN for N-gram features\ndef create_conv1d(n_filters, kernels):\n    conv1d = tf.keras.models.Sequential([\n        layers.Conv1D(filters=n_filters[0], kernel_size=kernels[0], strides=1, use_bias=False), \n        layers.BatchNormalization(), \n        layers.ReLU(),\n        layers.Conv1D(filters=n_filters[1], kernel_size=kernels[1], strides=1, use_bias=False), \n        layers.BatchNormalization(), \n        layers.ReLU(),\n        layers.Conv1D(filters=n_filters[2], kernel_size=kernels[2], strides=1, use_bias=False), \n        layers.BatchNormalization(), \n        layers.ReLU(),\n        layers.GlobalAveragePooling1D()\n    ])\n    return conv1d\n    \nfor encoded_text_feature in encoded_text_feature_list:\n    if encoded_text_feature.shape[1] == 10: ##unigram\n        conv1d_uni = create_conv1d([32, 64, 128], [4, 4, 4])\n        uni_numerical = conv1d_uni(encoded_text_feature)\n        numerical_feature_list.append(uni_numerical)\n    elif encoded_text_feature.shape[1] == 9: ##bigram\n        conv1d_bi = create_conv1d([64, 128, 256], [4, 4, 3])\n        bi_numerical = conv1d_bi(encoded_text_feature)\n        numerical_feature_list.append(bi_numerical)\n        \nnumerical_features = layers.concatenate(numerical_feature_list)\nnumerical_features.shape","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T02:55:57.830635Z","iopub.execute_input":"2022-05-08T02:55:57.830874Z","iopub.status.idle":"2022-05-08T02:55:58.049933Z","shell.execute_reply.started":"2022-05-08T02:55:57.830843Z","shell.execute_reply":"2022-05-08T02:55:58.049222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tab Transformer\n\nThe TabTransformer architecture works as follows:\n\n- All the categorical features are encoded as embeddings, using the same embedding_dims. This means that each value in each categorical feature will have its own embedding vector.\n\n- A column embedding, one embedding vector for each categorical feature, is added (point-wise) to the categorical feature embedding.\n\n- The embedded categorical features are fed into a stack of Transformer blocks. Each Transformer block consists of a multi-head self-attention layer followed by a feed-forward layer.\n\n- The outputs of the final Transformer layer, which are the contextual embeddings of the categorical features, are concatenated with the input numerical features, and fed into a final MLP block.\n\n<img src=\"https://raw.githubusercontent.com/keras-team/keras-io/master/examples/structured_data/img/tabtransformer/tabtransformer.png\" width=\"500\"/>\n","metadata":{}},{"cell_type":"code","source":"def create_mlp(hidden_units, dropout_rate, \n               activation, normalization_layer,\n               name=None):\n    mlp_layers = []\n    for units in hidden_units:\n        mlp_layers.append(normalization_layer),\n        mlp_layers.append(layers.Dense(units,\n                                       activation=activation))\n        mlp_layers.append(layers.Dropout(dropout_rate))\n    return keras.Sequential(mlp_layers, name=name)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:58.051086Z","iopub.execute_input":"2022-05-08T02:55:58.051489Z","iopub.status.idle":"2022-05-08T02:55:58.058101Z","shell.execute_reply.started":"2022-05-08T02:55:58.051451Z","shell.execute_reply":"2022-05-08T02:55:58.057324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create TabTransformer model\nnum_transformer_blocks = 4\nnum_heads = 4\ndropout_rate = 0.2\nmlp_hidden_units_factors = [2, 1] \n\nfor block_idx in range(num_transformer_blocks):\n    ## Create a multi-head attention layer\n    attention_output = layers.MultiHeadAttention(\n        num_heads=num_heads,\n        key_dim=cat_embedding_dim,\n        dropout=dropout_rate,\n        name=f'multi-head_attention_{block_idx}'\n    )(encoded_categorical_features, encoded_categorical_features)\n    ## Skip connection 1\n    x = layers.Add(\n        name=f'skip_connection1_{block_idx}'\n    )([attention_output, encoded_categorical_features])\n    ## Layer normalization 1\n    x = layers.LayerNormalization(\n        name=f'layer_norm1_{block_idx}', epsilon=1e-6\n    )(x)\n    ## Feedforward\n    feedforward_output = keras.Sequential([\n        layers.Dense(cat_embedding_dim, activation=keras.activations.gelu),\n        layers.Dropout(dropout_rate),\n    ], name=f'feedforward_{block_idx}'\n    )(x)\n    ## Skip connection 2\n    x = layers.Add(\n        name=f'skip_connection2_{block_idx}'\n    )([feedforward_output, x])\n    ## Layer normalization 2\n    encoded_categorical_features = layers.LayerNormalization(\n        name=f'layer_norm2_{block_idx}', epsilon=1e-6\n    )(x)\n    \ncontextualized_categorical_features = layers.Flatten(\n)(encoded_categorical_features)\n    \n## Numerical features\nnumerical_features = layers.LayerNormalization(\n    name='numerical_norm', epsilon=1e-6\n)(numerical_features)\n\n## Concatenate categorical features with numerical features\nfeatures = layers.Concatenate()([\n    contextualized_categorical_features,\n    numerical_features])\n\n## Final MLP\nmlp_hidden_units = [\n    factor * features.shape[-1] for factor in mlp_hidden_units_factors\n]\n\nfeatures = create_mlp(\n    hidden_units=mlp_hidden_units, \n    dropout_rate=dropout_rate, \n    activation=keras.activations.selu, \n    normalization_layer=layers.BatchNormalization(),\n    name='MLP'\n)(features)\n\n# Add a sigmoid to cap the output from 0 to 1\nmodel_outputs = layers.Dense(\n    units=1, \n    activation='sigmoid', \n    name='sigmoid'\n)(features)\n\ntraining_model = keras.Model(inputs=model_inputs,\n                             outputs=model_outputs)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:58.061037Z","iopub.execute_input":"2022-05-08T02:55:58.061269Z","iopub.status.idle":"2022-05-08T02:55:58.487662Z","shell.execute_reply.started":"2022-05-08T02:55:58.061228Z","shell.execute_reply":"2022-05-08T02:55:58.48695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LEARNING_RATE = 1e-3\nWEIGHT_DECAY = 0.0001\n\noptimizer = tfa.optimizers.AdamW(\n    learning_rate=LEARNING_RATE, \n    weight_decay=WEIGHT_DECAY)\n\nloss_fn = keras.losses.BinaryCrossentropy(\n    from_logits=False)\n\ntraining_model.compile(optimizer=optimizer,\n                       loss=loss_fn,\n                       metrics=[keras.metrics.AUC()])\n\ntraining_model.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T02:55:58.488941Z","iopub.execute_input":"2022-05-08T02:55:58.489182Z","iopub.status.idle":"2022-05-08T02:55:58.576227Z","shell.execute_reply.started":"2022-05-08T02:55:58.489151Z","shell.execute_reply":"2022-05-08T02:55:58.564482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"4\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>4. Model Training</center></h1>","metadata":{}},{"cell_type":"markdown","source":"<a id =\"4.1\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>4.1 Learning Rate Finder</center></h2>","metadata":{}},{"cell_type":"code","source":"class LRFind(tf.keras.callbacks.Callback):\n    def __init__(self, min_lr, max_lr, n_rounds):\n        self.min_lr = tf.constant(min_lr)\n        self.max_lr = tf.constant(max_lr)\n        self.step_up = tf.constant((max_lr / min_lr) ** (1 / n_rounds))\n        self.lrs = []\n        self.losses = []\n\n    def on_train_begin(self, logs=None):\n        self.weights= self.model.get_weights()\n        self.model.optimizer.lr = self.min_lr \n\n    def on_train_batch_end(self, batch, logs=None):\n        self.lrs.append(self.model.optimizer.lr.numpy())\n        self.losses.append(logs['loss'])\n        self.model.optimizer.lr = self.model.optimizer.lr * self.step_up \n        if self.model.optimizer.lr > self.max_lr:\n            self.model.stop_training = True \n\n    def on_train_end(self, logs=None):\n        self.model.set_weights(self.weights)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:58.577629Z","iopub.execute_input":"2022-05-08T02:55:58.577864Z","iopub.status.idle":"2022-05-08T02:55:58.601263Z","shell.execute_reply.started":"2022-05-08T02:55:58.577829Z","shell.execute_reply":"2022-05-08T02:55:58.599813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_find_epochs = 1\nlr_finder_steps = 500 \nlr_find = LRFind(1e-6, 1e1, lr_finder_steps)\n\nlr_find_batch_size = 512\nlr_find_ds = df_to_dataset(train_df, target='target', batch_size=lr_find_batch_size) \nlr_find_ds = lr_find_ds.map(lambda x, y: (preprocessing_model(x), y),\n                        num_parallel_calls=tf.data.AUTOTUNE)\n\ntraining_model.fit(lr_find_ds,\n                   steps_per_epoch=lr_finder_steps,\n                   epochs=lr_find_epochs,\n                   callbacks=[lr_find])\n\nplt.plot(lr_find.lrs, lr_find.losses)\nplt.xscale('log')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:55:58.602809Z","iopub.execute_input":"2022-05-08T02:55:58.603107Z","iopub.status.idle":"2022-05-08T02:56:48.398192Z","shell.execute_reply.started":"2022-05-08T02:55:58.603069Z","shell.execute_reply":"2022-05-08T02:56:48.397491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"4.2\"></a><h2 style=\"background:#d9afed; border:0; border-radius: 8px; color:black\"><center>4.2 Model Training</center></h2>","metadata":{}},{"cell_type":"code","source":"epochs = 10\nsteps_per_epoch = len(train)//batch_size\n\n## Re-construct the model\nmodel_config = training_model.get_config()\ntraining_model = tf.keras.Model.from_config(model_config)\n\n## Model compile\nlearning_schedule = tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=1e-2,\n    decay_steps=epochs*steps_per_epoch,\n    alpha=0.0)\n\nweight_decay = 0.0001\n\noptimizer = tfa.optimizers.AdamW(\n        learning_rate=learning_schedule,\n        weight_decay=weight_decay)\n\nloss_fn = keras.losses.BinaryCrossentropy(\n    from_logits=False)\n\ntraining_model.compile(optimizer=optimizer,\n                       loss=loss_fn,\n                       metrics=[keras.metrics.AUC()])\n\n## Checkpoint callback\ncheckpoint_filepath = './tmp/model/exp_ckpt'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath, \n    save_weights_only=True,\n    monitor='val_loss', \n    mode='min', \n    save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:56:48.399658Z","iopub.execute_input":"2022-05-08T02:56:48.400071Z","iopub.status.idle":"2022-05-08T02:56:49.179007Z","shell.execute_reply.started":"2022-05-08T02:56:48.400033Z","shell.execute_reply":"2022-05-08T02:56:49.178227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_model.fit(train_ds, epochs=epochs, shuffle=True,\n                   validation_data=valid_ds,\n                   callbacks=[model_checkpoint_callback])\n\ntraining_model.load_weights(checkpoint_filepath)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:56:49.18026Z","iopub.execute_input":"2022-05-08T02:56:49.180493Z","iopub.status.idle":"2022-05-08T02:59:23.066476Z","shell.execute_reply.started":"2022-05-08T02:56:49.180461Z","shell.execute_reply":"2022-05-08T02:59:23.064133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"5\"></a><h1 style=\"background:#a1a8f0; border:0; border-radius: 10px; color:black\"><center>5. Inference  </center></h1>","metadata":{}},{"cell_type":"code","source":"## Inference model = preprocessing model + training model\ninference_inputs = preprocessing_model.input\ninference_outputs = training_model(preprocessing_model(inference_inputs))\ninference_model = tf.keras.Model(inputs=inference_inputs,\n                                 outputs=inference_outputs)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:59:50.881843Z","iopub.execute_input":"2022-05-08T02:59:50.882229Z","iopub.status.idle":"2022-05-08T02:59:51.366218Z","shell.execute_reply.started":"2022-05-08T02:59:50.882193Z","shell.execute_reply":"2022-05-08T02:59:51.365485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Test Dataset\ntest = test_df\ntest_ds = df_to_dataset(test,\n                        target=None,\n                        shuffle=False,\n                        batch_size=batch_size,\n                        drop_remainder=False)\n\n## Display a test sample\nexample = next(test_ds.take(1).as_numpy_iterator())\nfor key in example:\n    print(f'{key}, shape:{example[key].shape}, {example[key].dtype}')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T02:59:52.458482Z","iopub.execute_input":"2022-05-08T02:59:52.458743Z","iopub.status.idle":"2022-05-08T02:59:52.912959Z","shell.execute_reply.started":"2022-05-08T02:59:52.458715Z","shell.execute_reply":"2022-05-08T02:59:52.91228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Inference and submission\npreds = inference_model.predict(test_ds)\npreds = np.squeeze(preds)\nsubmission_df['target'] = preds\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T03:00:02.123948Z","iopub.execute_input":"2022-05-08T03:00:02.124736Z","iopub.status.idle":"2022-05-08T03:00:32.403586Z","shell.execute_reply.started":"2022-05-08T03:00:02.124687Z","shell.execute_reply":"2022-05-08T03:00:32.402839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}