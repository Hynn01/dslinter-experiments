{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install sweetviz\n#!pip install pandas-profiling\n#!pip install imbalanced-learn\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport math\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfrom collections import Counter\nfrom imblearn.under_sampling import RandomUnderSampler\nimport plotly.express as px\nfrom datetime import datetime\nimport time\nimport matplotlib.cm as cm\n\n# Importamos algunas librerías para análisis de datos\nimport sweetviz as sv\n\nfrom sklearn.tree import DecisionTreeClassifier\n#from sklearn import cluster\nfrom sklearn.cluster import KMeans, MiniBatchKMeans, AffinityPropagation, MeanShift, AgglomerativeClustering, SpectralClustering\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import silhouette_samples, silhouette_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Directory and Version Specification\n","metadata":{}},{"cell_type":"code","source":"MODEL_TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d__%H-%M-%S\")\n\nDATA_PATH = 'data/'\n\n# Resolución de imágenes\nresolution = 300\n\nMODEL_TIMESTAMP","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def silhoutte_analysis(model):\n    num_clusters = range(2,10)\n    for k in num_clusters:\n        # Create a subplot with 1 row and 2 columns\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig.set_size_inches(18, 7)\n\n        # The 1st subplot is the silhouette plot\n        # The silhouette coefficient can range from -1, 1 but in this example all\n        # lie within [-0.1, 1]\n        ax1.set_xlim([-0.1, 1])\n        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n        # plots of individual clusters, to demarcate them clearly.\n        ax1.set_ylim([0, len(cluster_data) + (k + 1) * 10])\n\n        # Initialize the clusterer with n_clusters value and a random generator\n        # seed of 10 for reproducibility.\n        clusterer = model(n_clusters=k, random_state=10) #KMeans(n_clusters=k, random_state=10)\n        cluster_labels = clusterer.fit_predict(cluster_data)\n\n        # The silhouette_score gives the average value for all the samples.\n        # This gives a perspective into the density and separation of the formed\n        # clusters\n        silhouette_avg = silhouette_score(cluster_data, cluster_labels)\n        print(\"For n_clusters =\", k,\n              \"The average silhouette_score is :\", silhouette_avg)\n\n        # Compute the silhouette scores for each sample\n        sample_silhouette_values = silhouette_samples(cluster_data, cluster_labels)\n\n        y_lower = 10\n        for i in range(k):\n            # Aggregate the silhouette scores for samples belonging to\n            # cluster i, and sort them\n            ith_cluster_silhouette_values = \\\n                sample_silhouette_values[cluster_labels == i]\n\n            ith_cluster_silhouette_values.sort()\n\n            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n            y_upper = y_lower + size_cluster_i\n\n            color = cm.nipy_spectral(float(i) / k)\n            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                              0, ith_cluster_silhouette_values,\n                              facecolor=color, edgecolor=color, alpha=0.7)\n\n            # Label the silhouette plots with their cluster numbers at the middle\n            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n            # Compute the new y_lower for next plot\n            y_lower = y_upper + 10  # 10 for the 0 samples\n\n        ax1.set_title(\"The silhouette plot for the various clusters.\")\n        ax1.set_xlabel(\"The silhouette coefficient values\")\n        ax1.set_ylabel(\"Cluster label\")\n\n        # The vertical line for average silhouette score of all the values\n        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n        # 2nd Plot showing the actual clusters formed\n        colors = cm.nipy_spectral(cluster_labels.astype(float) / k)\n        ax2.scatter(np.array(cluster_data)[:, 0], np.array(cluster_data)[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                    c=colors, edgecolor='k')\n\n        # Labeling the clusters\n        centers = clusterer.cluster_centers_\n        # Draw white circles at cluster centers\n        ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                    c=\"white\", alpha=1, s=200, edgecolor='k')\n\n        for i, c in enumerate(centers):\n            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                        s=50, edgecolor='k')\n\n        ax2.set_title(\"The visualization of the clustered data.\")\n        ax2.set_xlabel(\"Feature space for the 1st feature\")\n        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                      \"with n_clusters = %d\" % k),\n                     fontsize=14, fontweight='bold')\n\n    plt.show()\n\ndef elbow_method(cluster_data, _model):\n    sum_of_squared_distances = []\n    num_clusters = range(1,10)\n\n    for i in num_clusters:\n        model = _model(n_clusters = i)#, random_state = 42)\n        model.fit(cluster_data)\n        sum_of_squared_distances.append(model.inertia_)\n\n    plt.plot(num_clusters, sum_of_squared_distances, 'bx-')\n    plt.xlabel('Valor de k (número de clusters)')\n    plt.ylabel('Suma de las distancias al cuadrado')\n    plt.title('Método del codo para buscar una k optima')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Aquí definimos la función para tratar los datos del clustering.","metadata":{}},{"cell_type":"code","source":"def handle_non_numerical_data(df):\n    \n    # handling non-numerical data: must convert.\n    columns = df.columns.values\n\n    for column in columns:\n        text_digit_vals = {}\n        def convert_to_int(val):\n            return text_digit_vals[val]\n\n        #print(column,df[column].dtype)\n        if df[column].dtype != np.int64 and df[column].dtype != np.float64:\n            \n            column_contents = df[column].values.tolist()\n            #finding just the uniques\n            unique_elements = set(column_contents)\n            # great, found them. \n            x = 0\n            for unique in unique_elements:\n                if unique not in text_digit_vals:\n                    # creating dict that contains new\n                    # id per unique string\n                    text_digit_vals[unique] = x\n                    x+=1\n            # now we map the new \"id\" vlaue\n            # to replace the string. \n            df[column] = list(map(convert_to_int,df[column]))\n\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"atp = pd.DataFrame()\n\nyears_index_20_22 = range(0,22)\n\nfor index in years_index_20_22:\n    index_str = str(index)\n\n    if len(index_str) == 1:\n        index_str = '0' + index_str\n\n    FILE_NAME = \"atp_matches_20{}.csv\".format(index_str)\n\n    data_frame_iter = pd.read_csv(DATA_PATH + FILE_NAME)\n    atp = pd.concat([atp, data_frame_iter])\n\n# Mostramos todas las columnas, con este comando evitamos que se oculten cuando son muchas.\npd.set_option('display.max_columns', None) \natp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('=================================================================')\nprint(atp.info())\nprint('=================================================================')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clean Dataset\n> Realizamos la limpieza de algunos datos, realizamos algunas transformaciones en algunas variables categóricas.\n> ","metadata":{}},{"cell_type":"code","source":"df_clustering = atp.copy()\nround_replace = {'R128': 128,\n                  'R64': 64,\n                  'R32': 32,\n                  'R16': 16,\n                  'QF': 4,\n                  'SF': 2,\n                  'F': 1\n}\n\n# Eliminamos las Round Robin (RR y ER)\ndf_clustering['round'].replace(round_replace, inplace = True)\ndf_clustering\n\ndf_pca = atp.copy()\ndf_pca['round'].replace(round_replace, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accident_type_replace = {}\nfor index,accident_type in enumerate(df_clustering.tourney_name.unique()):\n    if not pd.isna(accident_type): accident_type_replace[accident_type] = int(index)\n    \ndf_clustering['tourney_name'].replace(accident_type_replace, inplace = True)\ndf_clustering\n\ndf_pca['tourney_name'].replace(accident_type_replace, inplace = True)\ndf_pca.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COLUMNS_TO_GET = [\n                  \"surface\",\n                  \"minutes\",\n                  \"winner_ht\", \"loser_ht\",\n                  \"w_ace\", \"l_ace\",\n                  \"w_svpt\", \"l_svpt\", # service points\n                  \"w_1stWon\", \"l_1stWon\",\n                  \"w_2ndWon\", \"l_2ndWon\",\n                  \"w_bpSaved\", \"l_bpSaved\",\n                  \"w_bpFaced\", \"l_bpFaced\",\n                  \"w_SvGms\", \"l_SvGms\", # service games won\n                  \"winner_rank_points\", \"loser_rank_points\",\n                  \"round\",\n                 ]\n# Parámetros de los winners y losers con los que se realizan cálculos y se debn hacer drop.\nUNNECESSARY_ATTR = ['tourney_id', 'tourney_name', 'winner_name', 'loser_name', 'winner_entry', 'winner_seed', 'loser_entry', 'loser_seed','tourney_date', 'winner_id', 'loser_id', 'score']\n\nWL_DROP = [ 'w_1stIn', 'w_1stWon', 'w_2ndWon', 'w_SvGms',  'l_1stIn', 'l_1stWon', 'l_2ndWon', 'l_SvGms']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clustering = df_clustering[df_clustering['round'] != 'RR']\ndf_clustering = df_clustering[df_clustering['round'] != 'ER']\n\ndf_clustering = df_clustering.drop(UNNECESSARY_ATTR, axis = 1) \ndf_clustering = df_clustering.dropna()\ndf_clustering = df_clustering.drop_duplicates()\n\n# Crearemos dos formulas para calculos del ganador y el perdedor para evitar la correlación de estas variables, tambien haremos un drop de estas variables.\ndf_clustering['w_calculation'] = df_clustering['w_svpt'] + df_clustering['w_1stIn'] + df_clustering['w_1stWon'] + df_clustering['w_2ndWon'] + df_clustering['w_SvGms']\ndf_clustering['l_calculation'] = df_clustering['l_svpt'] + df_clustering['l_1stIn'] + df_clustering['l_1stWon'] + df_clustering['l_2ndWon'] + df_clustering['l_SvGms']\ndf_clustering = df_clustering.drop(WL_DROP, axis = 1)\n\ndf_clustering = handle_non_numerical_data(df_clustering)\n\n# Eliminamos los outlier para minutes\ndf_clustering = df_clustering[df_clustering['minutes'] < 400]\n\nCounter(df_clustering['surface'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_clustering[df_clustering['match'] > 400]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Resampling\n> Vamos a realizar un undersampling de la variable categorica a predecir **surface** vamos a reducir las muestras a la categoría minoritaria.","metadata":{}},{"cell_type":"code","source":"# Podemos observar que hay un desbalanceo en las varaibles a predecir.\ndf_without_under_sampling = df_clustering\nprint(Counter(df_clustering['surface']))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clustering.head()\n\nX = df_clustering.drop('surface', axis=1)\ny = df_clustering['surface']\n\n# Hacemos undersampling a la categoría con menor cantidad de datos.\nundersample = RandomUnderSampler(sampling_strategy='not minority')\n\nX_over, y_over = undersample.fit_resample(X, y)\nprint(Counter(y_over))\n\ndf_clustering = pd.concat([X_over, y_over], axis=1)\n\n# Eliminamos los outlier para minutes, se investigó los partidos más extensos de la historia del tennis y ninguno duró mas de \n# 660 minutos según el record mundial de los partidos de tennis más largos.\ndf_clustering = df_clustering[df_clustering['minutes'] < 400]\n\nprint(df_clustering.head())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"atp_report = sv.analyze(df_clustering)\natp_report.show_html()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clustering","metadata":{}},{"cell_type":"markdown","source":"## Principal Components Analysis **(PCA)** y KMeans\n\n> Realizaremos un análisis de componentes principales y los graficaremos para ver si es posible ver algún patrón en los datos, esto se realizará previamente a la aplicación de técnicas de clustering.","metadata":{}},{"cell_type":"code","source":"# Graficaremos PCA con 3 componentes\nscaler = StandardScaler()\ndf_clustering_pca = df_clustering.copy()\nX = df_clustering_pca\ny = df_clustering_pca['minutes'].copy()\n\npca = PCA(n_components = 3) \ncomponents = pca.fit_transform(X)#,y)\n\ntotal_var = pca.explained_variance_ratio_.sum() * 100\n\nfig = px.scatter_3d(\n    components, x=0, y=1, color=y, z=2,\n    title=f'Varianza Total Explicada: {total_var:.2f}%',\n    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n)\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wcss = []\nfor i in range(1, 21):\n    kmeans_pca = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans_pca.fit(components)\n    wcss.append(kmeans_pca.inertia_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nplt.plot(range(1,21), wcss, marker = 'o', linestyle = '--')\nplt.xlabel('Valor de k (número de clusters)')\nplt.ylabel('Suma de las distancias al cuadrado')\nplt.title('Método del Codo')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans_pca = KMeans(n_clusters = 2, init = 'k-means++', random_state = 42)\nkmeans_pca.fit(components)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pca_kmeans = pd.concat([df_clustering_pca.reset_index(drop = True), pd.DataFrame(components)], axis = 1)\ndf_pca_kmeans.columns.values[-3: ] = ['Componente 1', 'Componente 2', 'Componente 3']\ndf_pca_kmeans['ATP KMeans PCA'] = kmeans_pca.labels_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pca_kmeans","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_pca_kmeans['clase'] = df_pca_kmeans['ATP KMeans PCA'].map({0: 'Primero',\n#                                                                  1: 'Segundo',\n#                                                                  2: 'Tercero',\n#                                                                  3: 'Cuarto'})\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x_axis = df_pca_kmeans['Componente 2']\n# y_axis = df_pca_kmeans['Componente 1']\n# plt.figure(figsize = (10, 8))\n# sns.scatterplot(x_axis, y_axis, hue = df_pca_kmeans['clase'], palette = ['g', 'r', 'c', 'm'])\n# plt.title('Cluster Utilizando Análisis de Componentes Principales')\n# plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Podemos observar que aplicando el análisis de componentes principales PCA utilizando 3 componentes principales se puede explicar el 99.44 de la varianza de los datos. Esto podría ser un muy buen indicador ya que nos permitiría utilizar los datos filtrados para entrenar nuestros modelos, incluso PCA es una muy buena opción, aunque se pierda información del resto de las componentes.\n\n> Pero podemos notar que al graficar las 3 componentes principales, no somos capaces de ver clústeres de datos.\n","metadata":{}},{"cell_type":"markdown","source":"## KMeans","metadata":{}},{"cell_type":"code","source":"loser_rank_points = df_clustering.loser_rank_points.to_list()\nw_calculation = df_clustering.w_calculation.to_list()\nw_bpSaved = df_clustering.w_bpSaved.to_list()\ncluster_data = list(zip(loser_rank_points, w_calculation , w_bpSaved))\n\nelbow_method(cluster_data, KMeans)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Haciendo un análsisi de la `Suma de las distancias acummuladas` o método del codo, podemos observar que `k = 2` y `k = 3` son buenas opciones, pero cómo podemos verificar cúal es la mejor? Para esto utilizaremos el análisis de silueta que se encuentra en `sklearn`.","metadata":{}},{"cell_type":"markdown","source":"### Silhoutte Analysis\n\n[Silhouette Analysis](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#:~:text=Silhouette%20analysis%20can%20be%20used,like%20number%20of%20clusters%20visually)\n","metadata":{"tags":[]}},{"cell_type":"code","source":"silhoutte_analysis(KMeans)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Del análisis de silueta del cluster utilizando KMeans, podemos identificar que el mejor score es para `k = 2`, también es una buenas opción `k = 3`.","metadata":{}},{"cell_type":"code","source":"fig = px.scatter_3d(\n    df_clustering, x = 'loser_rank_points', y = 'w_calculation', z = 'w_bpSaved',\n    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n)\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MiniBatchKMeans","metadata":{}},{"cell_type":"code","source":"loser_rank_points = df_clustering.loser_rank_points.to_list()\nw_calculation = df_clustering.w_calculation.to_list()\nw_bpSaved = df_clustering.w_bpSaved.to_list()\ncluster_data = list(zip(loser_rank_points, w_calculation , w_bpSaved))\n\nelbow_method(cluster_data, MiniBatchKMeans)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Silhoutte Analysis","metadata":{}},{"cell_type":"code","source":"silhoutte_analysis(MiniBatchKMeans)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Agglomerative Clustering","metadata":{}},{"cell_type":"code","source":"# X = np.array(df_clustering.drop('surface', 1).astype(float))\n# X = preprocessing.scale(X)\n# y = np.array(df_clustering['surface'])\n\n# clf = AffinityPropagation()\n# clf.fit(X)\n# cluster_centers_indices = clf.cluster_centers_indices_\n\n# labels = clf.labels_\n\n# n_clusters_ = len(cluster_centers_indices)\n\n# print(\"Estimated number of clusters: %d\" % n_clusters_)\n# print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels))\n# print(\"Completeness: %0.3f\" % metrics.completeness_score(y, labels))\n# print(\"V-measure: %0.3f\" % metrics.v_measure_score(y, labels))\n# print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(y, labels))\n# # print(\n# #     \"Adjusted Mutual Information: %0.3f\"\n# #     % metrics.adjusted_mutual_info_score(y, labels)\n# # )\n# # print(\n# #     \"Silhouette Coefficient: %0.3f\"\n# #     % metrics.silhouette_score(X, labels, metric=\"sqeuclidean\")\n# # )\n\nloser_rank_points = df_clustering.loser_rank_points.to_list()\nw_calculation = df_clustering.w_calculation.to_list()\nw_bpSaved = df_clustering.w_bpSaved.to_list()\ncluster_data = list(zip(loser_rank_points, w_calculation , w_bpSaved))\n\nelbow_method(cluster_data, SpectralClustering)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mean Shift","metadata":{}},{"cell_type":"code","source":"# original_df = pd.DataFrame.copy(df_clustering)\n\n# df_ms = handle_non_numerical_data(df_clustering.copy())\n# #df_ms.drop(['ticket','home.dest'], 1, inplace=True)\n\n# X = np.array(df_ms.drop(['surface'], 1).astype(float))\n# X = preprocessing.scale(X)\n# y = np.array(df_ms['surface'])\n\n# clf = MeanShift()\n# clf.fit(X)\n\n# labels = clf.labels_\n# cluster_centers = clf.cluster_centers_\n\n# original_df['cluster_group']=np.nan\n\n# for i in range(len(X)):\n#     original_df['cluster_group'].iloc[i] = labels[i]\n    \n# n_clusters_ = len(np.unique(labels))\n# surface_rates = {}\n\n# for i in range(n_clusters_):\n#     temp_df = original_df[ (original_df['cluster_group']==float(i)) ]\n#     #print(temp_df.head())\n\n#     surface_cluster = temp_df[  (temp_df['surface'] == 1) ]\n\n#     surface_rate = len(surface_cluster) / len(temp_df)\n#     #print(i,survival_rate)\n#     surface_rates[i] = surface_rate\n    \n# print(surface_rates)\n\n# print(original_df[ (original_df['cluster_group']==1) ])\n\n# print(original_df[ (original_df['cluster_group']==0) ].describe())\n\n# print(original_df[ (original_df['cluster_group']==2) ].describe())\n\n# cluster_0 = (original_df[ (original_df['cluster_group']==0) ])\n\n# #cluster_0_fc = (cluster_0[ (cluster_0['pclass']==1) ])\n\n# #print(cluster_0_fc.describe())\n\nloser_rank_points = df_clustering.loser_rank_points.to_list()\nw_calculation = df_clustering.w_calculation.to_list()\nw_bpSaved = df_clustering.w_bpSaved.to_list()\ncluster_data = list(zip(loser_rank_points, w_calculation , w_bpSaved))\n\nelbow_method(cluster_data, MeanShift)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bibliografía\n\nhttps://medium.com/analytics-vidhya/clustering-on-mixed-data-types-in-python-7c22b3898086\n\nhttps://pythonprogramming.net/mean-shift-titanic-dataset-machine-learning-tutorial/?completed=/hierarchical-clustering-mean-shift-machine-learning-tutorial/\n\n\n","metadata":{}}]}