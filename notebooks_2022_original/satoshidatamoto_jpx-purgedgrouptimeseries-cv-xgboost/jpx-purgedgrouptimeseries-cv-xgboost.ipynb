{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":">### PurgedGroupTimeSeries CV - XGBoost Version\n>This is a simple starter notebook for Kaggle's JPX Comp showing purged group timeseries KFold with extra data. Purged Times Series is explained [here][1]. There are many configuration variables below to allow you to experiment. Use either CPU or GPU. You can control which years are loaded, which type of models are used, and whether to use feature engineering. You can experiment with different data preprocessing, model hyperparameters, loss, and number of seeds to ensemble. The extra datasets contain the full history of the assets at the same format of the competition, so you can input that into your model too.\n>\n>**NOTE:** this notebook lets you run a different experiment in each fold if you want to run lots of experiments. (Then it is like running multiple holdout validation experiments but in that case note that the overall CV score is meaningless because LB will be much different when the multiple experiments are ensembled to predict test). **If you want a proper CV with a reliable overall CV score you need to choose the same configuration for each fold.**\n>\n\n[1]: TBD","metadata":{"_cell_guid":"8af163ff-915a-4fae-aefe-6447e64952e5","_uuid":"b328cc9e-a536-4347-beed-d033e9f5ac6a","papermill":{"duration":0.028626,"end_time":"2021-11-27T14:04:21.90705","exception":false,"start_time":"2021-11-27T14:04:21.878424","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<center><img src=\"https://i.ibb.co/8bvJY8B/xgboost-logo.png\" height=250 width=250></center>\n<hr>\n<center>XGBoost = üå≥ + üßìüèª + üí™üèª</center>","metadata":{"papermill":{"duration":0.029649,"end_time":"2021-11-27T14:04:22.422995","exception":false,"start_time":"2021-11-27T14:04:22.393346","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"XGBoost is a favorite choice on kaggle and it doesn't look like it is going anywhere! \nIt is basiclly the a version of gradient boosting machines framework that made the approach so popular.\n\n**It is usually included in winning ensembles on Kaggle when solving a tabular problem**\n\nXGBoost algorithm provides large range of hyperparameters. In order to get the best performance out of it, we need to know to tune them.\n\n><h4>TL;DR: What makes XGBoost great:</h4>\n>\n>1. XGBoost was the first wide-spread GBM framework so it has \"more mileage\" then all other frameworks.\n>2. Easy to use \n>3. When using GPU it is usually faster than nearly all other gradient boosting algorithms that use GPU.\n>4. A very powerful gradient boosting. \n\n<h4>Leaf growth in XGBoost</h4>\n\nXGboost splits up to the specified max_depth hyperparameter and then starts pruning the tree backwards and removes splits beyond which there is no positive gain. It uses this approach since sometimes a split of no loss reduction may be followed by a split with loss reduction. XGBoost can also perform leaf-wise tree growth (as LightGBM).\n\nNormally it is impossible to enumerate all the possible tree structures q. A greedy algorithm that starts from a single leaf and iteratively adds branches to the tree is used instead. Assume that I_L and I_R are the instance sets of left and right nodes after the split. Then the loss reduction after the split is given by,\n\n![](https://i.imgur.com/jzyLh81.png)\n\n<h4>XGBoost vs LightGBM</h4>\n\nLightGBM uses a novel technique of Gradient-based One-Side Sampling (GOSS) to filter out the data instances for finding a split value while XGBoost uses pre-sorted algorithm & Histogram-based algorithm for computing the best split. Here instances mean observations/samples.\n\nLet's see how pre-sorting splitting works-\n\n- For each node, enumerate over all features\n\n- For each feature, sort the instances by feature value\n\n- Use a linear scan to decide the best split along that feature basis information gain\n\n- Take the best split solution along all the features\n\nIn simple terms, Histogram-based algorithm splits all the data points for a feature into discrete bins and uses these bins to find the split value of histogram. While, it is efficient than pre-sorted algorithm in training speed which enumerates all possible split points on the pre-sorted feature values, it is still behind GOSS in terms of speed.\n\n<h4>XGBoost Model Parameters</h4>\n\n>For an exhaustive overview of all parameters [see here](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n\n**objective [default=reg:linear]**\n\nThis defines the loss function to be minimized. Mostly used values are:\n\n- binary:logistic ‚Äìlogistic regression for binary classification, returns predicted probability (not class)\n\n- multi:softmax ‚Äìmulticlass classification using the softmax objective, returns predicted class (not probabilities)\nyou also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n\n- multi:softprob ‚Äìsame as softmax, but returns predicted probability of each data point belonging to each class.\n\n**eval_metric [ default according to objective ]**\n\nThe metric to be used for validation data. The default values are rmse for regression and error for classification.\nTypical values are:\n\n- rmse ‚Äì root mean square error\n\n- mae ‚Äì mean absolute error\n\n- logloss ‚Äì negative log-likelihood\n\n- error ‚Äì Binary classification error rate (0.5 threshold)\n\n- merror ‚Äì Multiclass classification error rate\n\n- mlogloss ‚Äì Multiclass logloss\n\n- auc: Area under the curve\n\n\n**eta [default=0.3]**\n\n- Analogous to learning rate in GBM.\n- Makes the model more robust by shrinking the weights on each step.\n- Typical final values to be used: 0.01-0.2\n\n**colsample_bytree**:  We can create a random sample of the features (or columns) to use prior to creating each decision tree in the boosted model. That is, tuning Column Sub-sampling in XGBoost By Tree. This is controlled by the colsample_bytree parameter. The default value is 1.0 meaning that all columns are used in each decision tree. A fraction (e.g. 0.6) means a fraction of columns to be subsampled. We can evaluate values for colsample_bytree between 0.1 and 1.0 incrementing by 0.1.\n\n<h3>Regularization in XGBoost</h3>\n\nXGBoost adds built-in regularization to achieve accuracy gains beyond gradient boosting. Regularization is the process of adding information to reduce variance and prevent overfitting.\n\nAlthough data may be regularized through hyperparameter fine-tuning, regularized algorithms may also be attempted. For example, Ridge and Lasso are regularized machine learning alternatives to LinearRegression.\n\nXGBoost includes regularization as part of the learning objective, as contrasted with gradient boosting and random forests. The regularized parameters penalize complexity and smooth out the final weights to prevent overfitting. XGBoost is a regularized version of gradient boosting.\n\nMathematically, XGBoost's learning objective may be defined as follows:\n\n$$obj(Œ∏) = l(Œ∏) + Œ© (Œ∏)$$\n\nHere, **l(Œ∏)**  is the loss function, which is the Mean Squared Error (MSE) for regression, or the log loss for classification, and **Œ© (Œ∏)** is the regularization function, a penalty term to prevent over-fitting. Including a regularization term as part of the objective function distinguishes XGBoost from most tree ensembles.\n\nThe learning objective for the th boosted tree can now be rewritten as follows:\n\n![img](https://i.imgur.com/IRNCrvM.png)\n\n**reg_alpha and reg_lambda** : First note the loss function is defined as\n\n![img](https://i.imgur.com/aw1Hod9.png)\n\n>So the above is how the regularized objective function looks like if you want to allow for the inclusion of a L1 and a L2 parameter in the same model\n\n`reg_alpha` and `reg_lambda` control the L1 and L2 regularization terms, which in this case limit how extreme the weights at the leaves can become. Higher values of alpha mean more L1 regularization. See the documentation [here](http://xgboost.readthedocs.io/en/latest///parameter.html#parameters-for-tree-booster).\n\nSince L1 regularization in GBDTs is applied to leaf scores rather than directly to features as in logistic regression, it actually serves to reduce the depth of trees. This in turn will tend to reduce the impact of less-predictive features. We might think of L1 regularization as more aggressive against less-predictive features than L2 regularization.\n\nThese two regularization terms have different effects on the weights; L2 regularization (controlled by the lambda term) encourages the weights to be small, whereas L1 regularization (controlled by the alpha term) encourages sparsity ‚Äî so it encourages weights to go to 0. This is helpful in models such as logistic regression, where you want some feature selection, but in decision trees we‚Äôve already selected our features, so zeroing their weights isn‚Äôt super helpful. For this reason, I found setting a high lambda value and a low (or 0) alpha value to be the most effective when regularizing.\n\n>[From this Paper](https://arxiv.org/pdf/1603.02754.pdf)\n\n\n<div class=\"alert alert-block alert-info\">\n<b>Read More:</b>\n<ul>\n    <li><a href = \"https://github.com/dmlc/xgboost\">XGBoost Github Documentation</a></li>\n    <li><a href = \"https://xgboost.readthedocs.io/en/stable/parameter.html\">XGBoost Parameters</a></li>\n    <li><a href = \"https://xgboost.readthedocs.io/en/stable/\">Official Documentation</a></li>    \n</ul>\n</div>\n\n____\n\n<h3>All Parameters Overview</h3>\n\n____\n\nBefore diving into the actual parameters of XGBoost, Let's define three types of parameters: General Parameters, Booster Parameters and Task Parameters.\n\n1. **General parameters**  Relate to chosing which booster algorithm we will be using, usually tree or linear model.\n\n2. **Booster parameters**  Are the actual parameters of the booster you have chosen.\n\n3. **Task parameters**  Tells the framework what problem are we trying to solve. For example, regression tasks may use different parameters with ranking tasks.\n\n____\n\n\n<h3>How to tune XGBoost like a boss?</h3>\n\nHyperparameters tuning guide:\n\n<h4>General Parameters</h4>\n\n1. **booster**  [default= gbtree ] \n  - Which booster to use. Can be gbtree, gblinear or dart; \n  - gbtree and dart use tree based models while gblinear uses linear functions.\n\n2. **verbosity** [default=1]\n  - Verbosity of printing messages. Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug). \n  - Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message. \n  - If there‚Äôs unexpected behaviour, please try to increase value of verbosity.\n\n3. **nthread**  [default to maximum number of threads available if not set]\n  - Number of parallel threads used to run XGBoost. When choosing it, please keep thread contention and hyperthreading in mind.\n\n____\n\n\n<h4>Tree Booster Parameters</h4>\n\n1. **eta [default=0.3, ]**\n  - alias: learning_rate\n  - Step size shrinkage used in update to prevents overfitting.\n  - After each boosting step, we can directly get the weights of new features\n  - It makes the model more robust by shrinking the weights on each step.\n  - range: [0,1]\n\n2. **gamma [default=0]**\n  - Minimum loss reduction required to make a further partition on a leaf node of the tree. \n  - The larger gamma is, the more conservative the algorithm will be.\n  - range: [0,‚àû]\n\n3. **max_depth [default=6]**\n\n  - Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. \n  - 0 is only accepted in lossguided growing policy when tree_method is set as hist or gpu_hist and it indicates no limit on depth. \n  - Beware that XGBoost aggressively consumes memory when training a deep tree.\n  - range: [0,‚àû] )\n\n4. **min_child_weight [default=1]**\n\n  - its Minimum sum of instance weight (hessian) needed in a child. I\n  - if the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. \n  - In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. \n  - The larger min_child_weight is, the more conservative the algorithm will be.\n  - range: [0,‚àû]\n\n5. **max_delta_step [default=0]**\n\n  - Maximum delta step we allow each leaf output to be. \n  - If the value is set to 0, it means there is no constraint. \n  - If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update.\n  - range: [0,‚àû]\n\n6. **subsample [default=1]**\n\n  - It denotes the fraction of observations to be randomly samples for each tree.\n  - Subsample ratio of the training instances.\n  - Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. - This will prevent overfitting.\n  - Subsampling will occur once in every boosting iteration.\n  - Lower values make the algorithm more conservative and prevents overfitting but too small alues might lead to under-fitting.\n  - typical values: 0.5-1\n  - range: (0,1]\n\n7. **sampling_method [default= uniform]**\n\n  - The method to use to sample the training instances.\n  - **uniform:** each training instance has an equal probability of being selected. Typically set subsample >= 0.5 for good results.\n  - **gradient_based:**  the selection probability for each training instance is proportional to the regularized absolute value of gradients \n\n8. **colsample_bytree, colsample_bylevel, colsample_bynode [default=1]**\n\n><h4>This is a family of parameters for subsampling of columns.</h4>\n>\n>**All colsample_by**  parameters have a range of (0, 1], the default value of 1, and specify the fraction of columns to be subsampled.\n>\n>**lsample_bytree**s the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n>\n>**colsample_bylevel**  is the subsample ratio of columns for each level. Subsampling occurs once for every new depth level reached in a tree. Columns are subsampled from the set of columns chosen for the current tree.\n>\n>**colsample_bynode**  is the subsample ratio of columns for each node (split). Subsampling occurs once every time a new split is evaluated. Columns are subsampled from the set of columns chosen for the current level.\n>\n>**colsample_by**  parameters work cumulatively. For instance, the combination **{'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5}** with 64 features will leave 8 features to choose from at each split.\n>\n\n9. **lambda [default=1]**\n  - alias: reg_lambda\n  - L2 regularization term on weights. \n  - Increasing this value will make model more conservative.\n\n10. **alpha [default=0]**\n  - alias: reg_alpha\n  - L1 regularization term on weights.\n  - Increasing this value will make model more conservative.\n\n11. **grow_policy [default= depthwise]**\n  - Controls a way new nodes are added to the tree.\n  - Currently supported only if tree_method is set to hist or gpu_hist.\n  - **Choices:*  - depthwise, lossguide\n  - **depthwise:*  - split at nodes closest to the root.\n  - **lossguide:*  - split at nodes with highest loss change.\n\n12. **max_leaves [default=0]**\n  - Maximum number of nodes to be added. \n  - Only relevant when grow_policy=lossguide is set.\n\n[Read More](https://xgboost.readthedocs.io/en/latest/parameter.html)\n\n____\n\n\n\n<h4>Task Parameters</h4>\n\n1. **objective [default=reg:squarederror]**\n\nIt defines the loss function to be minimized. Most commonly used values are given below -\n\n  - reg:squarederror : regression with squared loss.\n\n  - reg:squaredlogerror: regression with squared log loss 1/2[log(pred+1)‚àílog(label+1)]2. - All input labels are required to be greater than -1.\n\n  - reg:logistic : logistic regression\n\n  - binary:logistic : logistic regression for binary classification, output probability\n\n  - binary:logitraw: logistic regression for binary classification, output score before logistic transformation\n\n  - binary:hinge : hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.\n\n  - multi:softmax : set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes)\n\n  - multi:softprob : same as softmax, but output a vector of ndata nclass, which can be further reshaped to ndata nclass matrix. The result contains predicted probability of each data point belonging to each class.\n\n2. **eval_metric [default according to objective]**\n  - The metric to be used for validation data.\n  - The default values are rmse for regression, error for classification and mean average precision for ranking.\n  - We can add multiple evaluation metrics.\n  - Python users must pass the metrices as list of parameters pairs instead of map.\n  - The most common values are given below -\n\n   - rmse : root mean square error\n   - mae : mean absolute error\n   - logloss : negative log-likelihood\n   - error : Binary classification error rate (0.5 threshold). \n   - merror : Multiclass classification error rate.\n   - mlogloss : Multiclass logloss\n   - auc: Area under the curve\n   - aucpr : Area under the PR curve\n   \n   \n\n<div class=\"alert alert-block alert-warning\">\n<b>Introduction Credits:</b>\n<ul>\n    <li><a href = \"https://www.kaggle.com/shivansh002/a-gentle-guide-on-xgboost-hyperparameters\">A Gentle Guide on XGBoost hyperparameters</a> By @shivansh002. Thank you @shivansh002 for a great introduction! </li>\n    <li><a href = \"https://www.kaggle.com/paulrohan2020/tutorial-lightgbm-xgboost-catboost-top-11\">Tutorial LightGBM + XGBoost + CatBoost</a> By @paulrohan2020. Thank you @paulrohan2020 for a great tutorial! </li>    \n</ul>\n</div>   ","metadata":{"papermill":{"duration":0.029595,"end_time":"2021-11-27T14:04:22.482565","exception":false,"start_time":"2021-11-27T14:04:22.45297","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Code starts here ‚¨á","metadata":{}},{"cell_type":"code","source":"import os\nimport traceback\nimport seaborn as sns\nfrom scipy.stats import pearsonr\nimport pandas as pd, numpy as np\nfrom xgboost import XGBRegressor\nimport jpx_tokyo_market_prediction\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error","metadata":{"papermill":{"duration":1.399019,"end_time":"2021-11-27T14:04:24.030312","exception":false,"start_time":"2021-11-27T14:04:22.631293","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:15:37.059858Z","iopub.execute_input":"2022-04-23T10:15:37.060337Z","iopub.status.idle":"2022-04-23T10:15:38.649807Z","shell.execute_reply.started":"2022-04-23T10:15:37.060186Z","shell.execute_reply":"2022-04-23T10:15:38.649065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"config\">Configuration üéöÔ∏è</span>\n<hr >\n\nIn order to be a proper cross validation with a meaningful overall CV score, **you need to choose the same** `INC2022`, `INC2021`, `INC2020`, `INC2019`, `INC2018`, `INC2017`, `INCCOMP`, `INCSUPP`, and `NUM_LEAVES`, `MAX_DEPTH` **for each fold**. If your goal is to just run lots of experiments, then you can choose to have a different experiment in each fold. Then each fold is like a holdout validation experiment. When you find a configuration you like, you can use that configuration for all folds.\n* DEVICE - is CPU or GPU\n* SEED - a different seed produces a different triple stratified kfold split.\n* FOLDS - number of folds. Best set to 3, 5, or 15 but can be any number between 2 and 15\n* LOAD_STRICT - This controls whether to load strict at proposed [here](https://www.kaggle.com/julian3833/proposal-for-a-meaningful-lb-strict-lgbm)\n* INC2022 - This controls whether to include the extra historical prices during 2022.\n* INC2021 - This controls whether to include the extra historical prices during 2021.\n* INC2020 - This controls whether to include the extra historical prices during 2020.\n* INC2019 - This controls whether to include the extra historical prices during 2019.\n* INC2018 - This controls whether to include the extra historical prices during 2018.\n* INC2017 - This controls whether to include the extra historical prices during 2017.\n* INCSUPP - This controls whether to include the supplemented train data that was released with the competition.\n* N_ESTIMATORS - is a list of length FOLDS. These are n_estimators for each fold. For maximum speed, it is best to use the smallest number of estimators as your GPU or CPU allows.\n* MAX_DEPTH - is a list of length FOLDS. These are max_depth for each fold. For maximum speed, it is best to use the smallest number of estimators as your GPU or CPU allows.\n* LEARNING_RATE - is a list of length FOLDS. These are max_depth for each fold. For maximum speed, it is best to use the smallest number of estimators as your GPU or CPU allows.","metadata":{"papermill":{"duration":0.029679,"end_time":"2021-11-27T14:04:24.092084","exception":false,"start_time":"2021-11-27T14:04:24.062405","status":"completed"},"tags":[]}},{"cell_type":"code","source":"DEVICE = \"CPU\" #or \"GPU\"\n\nSEED = 42\n\n# CV PARAMS\nFOLDS = 5\nGROUP_GAP = 130\nMAX_TEST_GROUP_SIZE = 180\nMAX_TRAIN_GROUP_SIZE = 280\n\n# WHICH YEARS TO INCLUDE? YES=1 NO=0\nINC2022 = 1\nINC2021 = 1\nINC2020 = 1\nINC2019 = 1\nINC2018 = 1\nINC2017 = 1\nINCSUPP = 1\n\n# HYPER PARAMETERS\nLEARNING_RATE = [0.09, 0.09, 0.09, 0.09, 0.09]\nN_ESTIMATORS = [1000, 1000, 1000, 1000, 1000]\nMAX_DEPTH = [10, 10, 10, 10, 10]","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","papermill":{"duration":0.040744,"end_time":"2021-11-27T14:04:24.223321","exception":false,"start_time":"2021-11-27T14:04:24.182577","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:15:38.651118Z","iopub.execute_input":"2022-04-23T10:15:38.651757Z","iopub.status.idle":"2022-04-23T10:15:38.659873Z","shell.execute_reply.started":"2022-04-23T10:15:38.651713Z","shell.execute_reply":"2022-04-23T10:15:38.658971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"loading\">Data Loading üóÉÔ∏è</span>\n<hr>\n\nThe data organisation has already been done and saved to Kaggle datasets. Here we choose which years to load. We can use either 2017, 2018, 2019, 2020, 2021, Original, Supplement by changing the `INC2021`, `INC2020`, `INC2019`, `INC2018`, `INC2017`, `INCCOMP`, `INCSUPP` variables in the preceeding code section. These datasets are discussed [here][1].\n\n[1]: https://www.kaggle.com/c/g-research-crypto-forecasting/discussion/285726","metadata":{"papermill":{"duration":0.029654,"end_time":"2021-11-27T14:04:24.2836","exception":false,"start_time":"2021-11-27T14:04:24.253946","status":"completed"},"tags":[]}},{"cell_type":"code","source":"stock_list = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv\")\nprices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\nstock_list = stock_list.loc[stock_list['SecuritiesCode'].isin(prices['SecuritiesCode'].unique())]\nstock_name_dict = {stock_list['SecuritiesCode'].tolist()[idx]: stock_list['Name'].tolist()[idx] for idx in range(len(stock_list))}\n\ndef load_training_data(asset_id = None):\n    prices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\n    supplemental_prices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv\")\n    df_train = pd.concat([prices, supplemental_prices]) if INCSUPP else prices\n    df_train = pd.merge(df_train, stock_list[['SecuritiesCode', 'Name']], left_on = 'SecuritiesCode', right_on = 'SecuritiesCode', how = 'left')\n    df_train['date'] = pd.to_datetime(df_train['Date'])\n    df_train['year'] = df_train['date'].dt.year\n    if not INC2022: df_train = df_train.loc[df_train['year'] != 2022]\n    if not INC2021: df_train = df_train.loc[df_train['year'] != 2021]\n    if not INC2020: df_train = df_train.loc[df_train['year'] != 2020]\n    if not INC2019: df_train = df_train.loc[df_train['year'] != 2019]\n    if not INC2018: df_train = df_train.loc[df_train['year'] != 2018]\n    if not INC2017: df_train = df_train.loc[df_train['year'] != 2017]\n    # asset_id = 1301 # Remove before flight\n    if asset_id is not None: df_train = df_train.loc[df_train['SecuritiesCode'] == asset_id]\n    # df_train = df_train[:1000] # Remove before flight\n    return df_train","metadata":{"_kg_hide-input":true,"papermill":{"duration":23.471948,"end_time":"2021-11-27T14:04:47.785458","exception":false,"start_time":"2021-11-27T14:04:24.31351","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:15:38.661337Z","iopub.execute_input":"2022-04-23T10:15:38.662078Z","iopub.status.idle":"2022-04-23T10:15:44.764789Z","shell.execute_reply.started":"2022-04-23T10:15:38.662034Z","shell.execute_reply":"2022-04-23T10:15:44.76417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"features\">Feature Engineering üî¨</span>\n<hr>\n\nThis notebook uses upper_shadow, lower_shadow, high_div_low, open_sub_close, seasonality/datetime features first shown in this notebook [here][1] and successfully used by julian3833 [here][2].\n\nAdditionally we can decide to use external data by changing the variables `INC2021`, `INC2020`, `INC2019`, `INC2018`, `INC2017`, `INCCOMP`, `INCSUPP` in the preceeding code section. These variables respectively indicate whether to load last year 2021 data and/or year 2020, 2019, 2018, 2017, the original, supplemented data. These datasets are discussed [here][3]\n\nConsider experimenting with different feature engineering and/or external data. The code to extract features out of the dataset is taken from julian3833' notebook [here][2]. Thank you julian3833, this is great work.\n\n[1]: https://www.kaggle.com/cstein06/tutorial-to-the-g-research-crypto-competition\n[2]: https://www.kaggle.com/julian3833\n[3]: TBD","metadata":{"papermill":{"duration":0.031226,"end_time":"2021-11-27T14:04:47.848547","exception":false,"start_time":"2021-11-27T14:04:47.817321","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from pandas import concat\nfrom pandas import DataFrame\nfrom scipy.stats import pearsonr\nfrom sklearn.model_selection import train_test_split\n\ndef upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\ndef lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']\n\ndef get_features(df):\n    df_feat = df[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n    df_feat['upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['lower_Shadow'] = lower_shadow(df_feat)\n    df_feat[\"high_div_low\"] = df_feat[\"High\"] / df_feat[\"Low\"]\n    df_feat[\"open_sub_close\"] = df_feat[\"Open\"] - df_feat[\"Close\"]\n    return df_feat","metadata":{"papermill":{"duration":0.042512,"end_time":"2021-11-27T14:04:47.922212","exception":false,"start_time":"2021-11-27T14:04:47.8797","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:15:44.766866Z","iopub.execute_input":"2022-04-23T10:15:44.767326Z","iopub.status.idle":"2022-04-23T10:15:44.774964Z","shell.execute_reply.started":"2022-04-23T10:15:44.767283Z","shell.execute_reply":"2022-04-23T10:15:44.77437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"modelconf\">Configure the model ‚öôÔ∏è</span>\n<hr>\n\nThis is a simple model with simple set of hyperparameters. Consider experimenting with different models, parameters, ensembles and so on.","metadata":{"papermill":{"duration":0.030323,"end_time":"2021-11-27T14:04:47.983055","exception":false,"start_time":"2021-11-27T14:04:47.952732","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**The Model**","metadata":{"papermill":{"duration":0.029976,"end_time":"2021-11-27T14:04:48.176997","exception":false,"start_time":"2021-11-27T14:04:48.147021","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def build_model(fold):\n    \n    # Do feel free to experiment with different models here!\n    model = XGBRegressor(n_estimators = N_ESTIMATORS[fold], max_depth = MAX_DEPTH[fold], learning_rate = LEARNING_RATE[fold], tree_method = 'hist' if DEVICE == 'CPU' else 'gpu_hist')\n\n    return model","metadata":{"papermill":{"duration":0.038534,"end_time":"2021-11-27T14:04:48.245699","exception":false,"start_time":"2021-11-27T14:04:48.207165","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:15:44.776424Z","iopub.execute_input":"2022-04-23T10:15:44.776997Z","iopub.status.idle":"2022-04-23T10:15:44.797626Z","shell.execute_reply.started":"2022-04-23T10:15:44.776953Z","shell.execute_reply":"2022-04-23T10:15:44.796839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time Series Cross Validation\n\n> \"There are many different ways one can do cross-validation, and **it is the most critical step when building a good machine learning model** which is generalizable when it comes to unseen data.\"\n-- **Approaching (Almost) Any Machine Learning Problem**, by Abhishek Thakur\n\nCV is the **first** step, but very few notebooks are talking about this. Here we look at \"purged rolling time series CV\" and actually apply it in hyperparameter tuning for a basic estimator. This notebook owes a debt of gratitude to the notebook [\"Found the Holy Grail GroupTimeSeriesSplit\"](https://www.kaggle.com/jorijnsmit/found-the-holy-grail-grouptimeseriessplit). That notebook is excellent and this solution is an extention of the quoted pending sklearn estimator. I modify that estimator to make it more suitable for the task at hand in this competition. The changes are\n\n- you can specify a **gap** between each train and validation split. This is important because even though the **group** aspect keeps whole days together, we suspect that the anonymized features have some kind of lag or window calculations in them (which would be standard for financial features). By introducing a gap, we mitigate the risk that we leak information from train into validation\n- we can specify the size of the train and validation splits in terms of **number of days**. The ability to specify a validation set size is new and the the ability to specify days, as opposed to samples, is new.\n\nThe code for `PurgedTimeSeriesSplit` is below. I've hidden it because it is really meant to act as an imported class. If you want to see the code and copy for your work, click on the \"Code\" box.","metadata":{"papermill":{"duration":0.031644,"end_time":"2021-11-27T14:04:48.310018","exception":false,"start_time":"2021-11-27T14:04:48.278374","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\nclass GroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_size : int, default=None\n        Maximum size for a single training set.\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\\\n                           'b', 'b', 'b', 'b', 'b',\\\n                           'c', 'c', 'c', 'c',\\\n                           'd', 'd', 'd'])\n    >>> gtss = GroupTimeSeriesSplit(n_splits=3)\n    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):\n    ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n    ...     print(\"TRAIN GROUP:\", groups[train_idx],\\\n                  \"TEST GROUP:\", groups[test_idx])\n    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\\\n    TEST GROUP: ['b' 'b' 'b' 'b' 'b']\n    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\\\n    TEST GROUP: ['c' 'c' 'c' 'c']\n    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\\\n    TEST: [15, 16, 17]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\\\n    TEST GROUP: ['d' 'd' 'd']\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_size=None\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_size = max_train_size\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n        group_test_size = n_groups // n_folds\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n            for train_group_idx in unique_groups[:group_test_start]:\n                train_array_tmp = group_dict[train_group_idx]\n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n            train_end = train_array.size\n            if self.max_train_size and self.max_train_size < train_end:\n                train_array = train_array[train_end -\n                                          self.max_train_size:train_end]\n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n            yield [int(i) for i in train_array], [int(i) for i in test_array]\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# modified code for group gaps; source\n# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\nclass PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Allows for a gap in groups to avoid potentially leaking info from\n    train into test if the model has windowed or lag features.\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 verbose=False\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n        self.verbose = verbose\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n\n        group_test_size = min(n_groups // n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n\n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n\n            train_end = train_array.size\n\n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n\n            test_array  = test_array[group_gap:]\n\n\n            if self.verbose > 0:\n                    pass\n\n            yield [int(i) for i in train_array], [int(i) for i in test_array]","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":0.071869,"end_time":"2021-11-27T14:04:48.412786","exception":false,"start_time":"2021-11-27T14:04:48.340917","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:15:44.799739Z","iopub.execute_input":"2022-04-23T10:15:44.800458Z","iopub.status.idle":"2022-04-23T10:15:44.833224Z","shell.execute_reply.started":"2022-04-23T10:15:44.800407Z","shell.execute_reply":"2022-04-23T10:15:44.832242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"training\">Training üèãÔ∏è</span>\n<hr>\nOur model will be trained for the number of FOLDS and ESTIMATORS you chose in the configuration above. Each fold the model with lowest validation loss will be saved and used to predict OOF and test. Adjust the variable `VERBOSE`. The variable `VERBOSE=1 or 2` will display the training and validation loss for each iteration as text.","metadata":{"papermill":{"duration":0.030435,"end_time":"2021-11-27T14:04:48.473675","exception":false,"start_time":"2021-11-27T14:04:48.44324","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**Let's take a look at our CV**","metadata":{"papermill":{"duration":0.029731,"end_time":"2021-11-27T14:04:48.533651","exception":false,"start_time":"2021-11-27T14:04:48.50392","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\ndef plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n    cmap_cv = plt.cm.coolwarm\n    jet = plt.cm.get_cmap('jet', 256)\n    seq = np.linspace(0, 1, 256)\n    _ = np.random.shuffle(seq)   # inplace\n    cmap_data = ListedColormap(jet(seq))    \n    for ii, (tr, tt) in enumerate(list(cv.split(X=X, y=y, groups=group))):\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0        \n        ax.scatter(range(len(indices)), [ii + .5] * len(indices), c=indices, marker='_', lw=lw, cmap=cmap_cv, vmin=-.2, vmax=1.2)\n    ax.scatter(range(len(X)), [ii + 1.5] * len(X), c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n    ax.scatter(range(len(X)), [ii + 2.5] * len(X), c=group, marker='_', lw=lw, cmap=cmap_data)\n    yticklabels = list(range(n_splits)) + ['target', 'day']\n    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels, xlabel='Sample index', ylabel=\"CV iteration\", ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n    return ax\n\ndef plot_importance(importances, features_names, PLOT_TOP_N = 20, figsize=(12, 20)):\n    try: plt.close()\n    except: pass\n    importance_df = pd.DataFrame(data=importances, columns=features_names)\n    sorted_indices = importance_df.median(axis=0).sort_values(ascending=False).index\n    sorted_importance_df = importance_df.loc[:, sorted_indices]\n    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n    _, ax = plt.subplots(figsize=figsize)\n    ax.grid()\n    ax.set_xscale('log')\n    ax.set_ylabel('Feature')\n    ax.set_xlabel('Importance')\n    plt.title('Feature Importances')\n    sns.boxplot(data=sorted_importance_df[plot_cols], orient='h', ax=ax)\n    plt.show()\n\nasset_id = 1301\ndf = load_training_data(asset_id)\ndf_proc = get_features(df)\ndf_proc['date'] = df['date'].copy()\ndf_proc['y'] = df['Target']\ndf_proc = df_proc.dropna(how=\"any\")\nX = df_proc.drop(\"y\", axis=1)\ny = df_proc[\"y\"]\ngroups = pd.factorize(X['date'].dt.day.astype(str) + '_' + X['date'].dt.month.astype(str) + '_' + X['date'].dt.year.astype(str))[0]\nX = X.drop(columns = 'date')\n\nfig, ax = plt.subplots(figsize = (12, 6))\ncv = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap = GROUP_GAP, max_train_group_size=MAX_TRAIN_GROUP_SIZE, max_test_group_size=MAX_TEST_GROUP_SIZE)\nplot_cv_indices(cv, X, y, groups, ax, FOLDS, lw=20)","metadata":{"papermill":{"duration":166.841586,"end_time":"2021-11-27T14:07:35.405381","exception":false,"start_time":"2021-11-27T14:04:48.563795","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:15:44.836041Z","iopub.execute_input":"2022-04-23T10:15:44.836302Z","iopub.status.idle":"2022-04-23T10:15:50.760843Z","shell.execute_reply.started":"2022-04-23T10:15:44.836264Z","shell.execute_reply":"2022-04-23T10:15:50.759916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Main Training Function**","metadata":{"papermill":{"duration":0.032621,"end_time":"2021-11-27T14:07:35.472169","exception":false,"start_time":"2021-11-27T14:07:35.439548","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit\nVERBOSE = 0\n\ndef get_Xy_and_model():\n    df = load_training_data()\n    orig_close = df['Close'].copy()\n    orig_sec_code = df['SecuritiesCode'].copy()\n    df_proc = get_features(df)\n    df_proc['date'] = df['date'].copy()\n    df_proc['y'] = df['Target']\n    df_proc = df_proc.dropna(how=\"any\")\n    X = df_proc.drop(\"y\", axis=1)\n    y = df_proc[\"y\"]\n    groups = pd.factorize(X['date'].dt.day.astype(str) + '_' + X['date'].dt.month.astype(str) + '_' + X['date'].dt.year.astype(str))[0]\n    X = X.drop(columns = 'date')\n    oof_preds = np.zeros(len(X))\n    importances, scores, models = [], [], []\n    for fold, (train_idx, val_idx) in enumerate(PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap = GROUP_GAP).split(X, y, groups)):\n        # GET TRAINING, VALIDATION SET\n        x_train, x_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        # DISPLAY FOLD INFO\n        print('#' * 25); print('#### FOLD', fold + 1)\n        print('#### Training N_ESTIMATORS %s | MAX_DEPTH %s | LEARNING_RATE %s' % (N_ESTIMATORS[fold], MAX_DEPTH[fold], LEARNING_RATE[fold]))\n\n        model = build_model(fold)\n\n        # TRAIN\n        model.fit( x_train, y_train, eval_set = [(x_val, y_val)], verbose = VERBOSE)\n\n        # PREDICT OOF\n        pred = model.predict(x_val)\n        models.append(model)\n\n        # REPORT RESULTS\n        try: mse = mean_squared_error(np.nan_to_num(y_val), np.nan_to_num(pred))\n        except: mse = 0.0\n        scores.append(mse)\n        print('#### FOLD %i OOF MSE %.3f' % (fold + 1, mse))\n\n        oof_preds[val_idx] = pred\n        importances.append(model.feature_importances_)\n\n    df_proc['SecuritiesCode'] = orig_sec_code\n    df = df_proc    \n    df['oof_preds'] = np.nan_to_num(oof_preds)\n    df['Close'] = orig_close\n    print('\\n\\n' + ('-' * 80) + '\\n' + 'Finished trainings. Results:')\n    print('Model: r2_score: %s | pearsonr: %s ' % (r2_score(df['y'], df['oof_preds']), pearsonr(df['y'], df['oof_preds'])[0]))\n    print('Predictions std: %s | Target std: %s' % (df['oof_preds'].std(), df['y'].std()))\n    \n    try: plt.close()\n    except: pass\n    df2 = df.reset_index().set_index('date')\n    df2 = df2.loc[df2['SecuritiesCode'] == 1301] # For demonstration purpose only.\n    fig = plt.figure(figsize = (12, 6))\n    # fig, ax_left = plt.subplots(figsize = (12, 6))\n    ax_left = fig.add_subplot(111)\n    ax_left.set_facecolor('azure')\n    ax_right = ax_left.twinx()\n    ax_left.plot(df2['y'].rolling(3 * 30 * 24 * 60).corr(df2['oof_preds']).iloc[::24 * 60], color = 'crimson', label = \"Corr\")\n    ax_right.plot(df2['Close'].iloc[::24 * 60], color = 'darkgrey', label = \"%s Close\" % stock_name_dict[asset_id])\n    plt.legend()\n    plt.grid()\n    plt.xlabel('Time')\n    plt.title('3 month rolling pearsonr for %s' % (stock_name_dict[asset_id]))\n    plt.show()\n\n    plot_importance(np.array(importances), list(X.columns), PLOT_TOP_N = 20)\n    \n    return scores, oof_preds, models, y\n\n\nprint(f\"Training model\")\ncur_scores, cur_oof_preds, cur_models, cur_targets = get_Xy_and_model()\nscores, oof_preds, models, targets = cur_scores, cur_oof_preds, cur_models, cur_targets","metadata":{"papermill":{"duration":4194.335227,"end_time":"2021-11-27T15:17:29.839296","exception":false,"start_time":"2021-11-27T14:07:35.504069","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:19:25.50424Z","iopub.execute_input":"2022-04-23T10:19:25.504552Z","iopub.status.idle":"2022-04-23T10:27:33.944458Z","shell.execute_reply.started":"2022-04-23T10:19:25.504521Z","shell.execute_reply":"2022-04-23T10:27:33.943814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"codebook\">Calculate OOF MSE</span>\nThe OOF (out of fold) predictions are saved to disk. If you wish to ensemble multiple models, use the OOF to determine what are the best weights to blend your models with. Choose weights that maximize OOF CV score when used to blend OOF. Then use those same weights to blend your test predictions.","metadata":{"papermill":{"duration":0.144313,"end_time":"2021-11-27T15:17:30.119235","exception":false,"start_time":"2021-11-27T15:17:29.974922","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print('Overall MEAN OOF MSE %s' % np.mean(list(scores)))","metadata":{"papermill":{"duration":162.099743,"end_time":"2021-11-27T15:20:12.346189","exception":false,"start_time":"2021-11-27T15:17:30.246446","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:27:33.946045Z","iopub.execute_input":"2022-04-23T10:27:33.946725Z","iopub.status.idle":"2022-04-23T10:27:33.952978Z","shell.execute_reply.started":"2022-04-23T10:27:33.946692Z","shell.execute_reply":"2022-04-23T10:27:33.951851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"submit\">Submit To Kaggle üá∞</span>\n<hr>","metadata":{"papermill":{"duration":0.131515,"end_time":"2021-11-27T15:20:12.609997","exception":false,"start_time":"2021-11-27T15:20:12.478482","status":"completed"},"tags":[]}},{"cell_type":"code","source":"env = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()\n\nfor (df_test, options, financials, trades, secondary_prices, df_pred) in iter_test:\n    x_test = get_features(df_test)\n    y_pred = np.mean(np.concatenate([np.expand_dims(model.predict(x_test), axis = 0) for model in models], axis = 0), axis = 0)\n    df_pred['Target'] = y_pred\n    df_pred = df_pred.sort_values(by = \"Target\", ascending = False)\n    df_pred['Rank'] = np.arange(0,2000)\n    df_pred = df_pred.sort_values(by = \"SecuritiesCode\", ascending = True)\n    df_pred.drop([\"Target\"], axis = 1)\n    submission = df_pred[[\"Date\", \"SecuritiesCode\", \"Rank\"]]\n    env.predict(submission)","metadata":{"papermill":{"duration":2.23887,"end_time":"2021-11-27T15:20:14.980611","exception":false,"start_time":"2021-11-27T15:20:12.741741","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:27:33.954622Z","iopub.execute_input":"2022-04-23T10:27:33.954959Z","iopub.status.idle":"2022-04-23T10:27:34.796279Z","shell.execute_reply.started":"2022-04-23T10:27:33.954917Z","shell.execute_reply":"2022-04-23T10:27:34.795335Z"},"trusted":true},"execution_count":null,"outputs":[]}]}