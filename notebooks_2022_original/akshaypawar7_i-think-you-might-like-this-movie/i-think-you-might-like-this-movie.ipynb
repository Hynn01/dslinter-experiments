{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Recommender system...ðŸ¤”\n\nHow to design a recommendation system is a popular question appearing in the most recent system design interviews for big tech companies. The reason is simple. Recommendation systems have become very important over the past few years. Amazon, Netflix, YouTube, Hulu, Instagram and several other companies have their own recommendation systems to make different kinds of suggestions to their users. Different types of algorithms are used to track patterns in the kind of data utilized by the users on the application, to suggest the most relevant material to each user from a large store of content.\n\nLetâ€™s discuss how a recommendation system, such as the Movies Recommender, is designed.\n\n\n![](https://miro.medium.com/max/875/1*6gDIv9mWyc8vkdGerBKiXw.jpeg)\n\n\n\n# Overview\nRecommender systems usually make use of either or both collaborative filtering and content-based filtering, as well as other systems such as knowledge-based systems.\n* Collaborative filtering approaches build a model from a user's past behavior (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that the user may have an interest in.\n* Content-based filtering approaches utilize a series of discrete, pre-tagged characteristics of an item in order to recommend additional items with similar properties.\n\n# Approaches\n\n## Content-based filtering\nContent-based filtering, also called classification based or item-item collaborative filtering, is a machine learning technique that establishes a correlation between the products to make decisions.\n\n\n![content_based](https://miro.medium.com/max/500/1*O278BoepckGzdtcQlnJM_g.png)\n\nConsider a database of movies. Content-based filtering will process the features or characteristics of these movies. Some of the properties that the algorithm considers can be:\n* Whether the movie is animated or not?\n* Is the movie based on Marvel comics?\n* Whether the movie is a thriller or not?\n* Is the movie rated PG-13?\n* Is Emma Watson starring in it?\n\nSuppose our recommendation system holds 6 movies. The table below matches the movies with 5 features that are important to our application:\n\n\n![](https://miro.medium.com/max/875/0*2bPoe1jFswpwlTIC)\n\n\nNow if a user has watched Movie 2, which has features 2, 3, and 5 in it, the content-based filtering algorithm finds other movies that share the most features with Movie 2. \n\nSince Movie 3 and Movie 5 share the most features with Movie 2 among all the movies in the database, the user is suggested Movie 3 and Movie 5.\n\nContent-based filtering does not take into account other userâ€™s activities when recommending content to a user.\n\n\n\n## Collaborative filtering\n\nCollaborative filtering or user-user collaborative model is based on the assumption that users will like products that are consumed by other users with similar taste. Instead of assigning features to content and then recommending the content to the users based on their feature preferences, as was the case with content-based filtering, this approach attempts at detecting patterns between the data of different users.\n\n\n![](https://miro.medium.com/max/875/0*DYPV_AtSVsXAjuwg)\n\nWhile collaborative filtering is a complex machine learning approach and can be employed in multiple forms, letâ€™s discuss the most basic solution.\nSo instead of storing a table of features against movies, letâ€™s store a table of movies against users.\n\n![](https://miro.medium.com/max/875/0*oCb8XandSEcGSOeW)\n\nThe table above shows the available data of 5 users as to whether or not they have watched 4 different movies. Using this data, our collaborative filtering approach can identify which movie to suggest to User 5.\n\nConsider ticks as a representation that the user has watched that particular movie and crosses to tell that the user has not watched that movie. Now we know that User 5 has watched Movie 1. Our algorithm considers User 1 and User 3 as similar users since they have also watched Movie 1 but Users 2 and 4 have not. Now looking at the data for User 1 and 3 (similar users), you can see from the table that both have watched Movie 3, while only User 1 has watched Movie 2 and Movie 4.\n\n![](https://miro.medium.com/max/500/1*E2MqFWv1nz2uhWMSk3L9Ag.png)\n\nOur algorithm gives Movie 3 two votes, while it gives Movie 2 and Movie 4 a single vote each as shown in the diagram above. Since Movie 3 has the most votes, the recommendation system will suggest Movie 3 to User 5.\n\n## that's the basic things, now we can build one.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom skimage import io\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T02:15:06.68594Z","iopub.execute_input":"2022-05-05T02:15:06.686761Z","iopub.status.idle":"2022-05-05T02:15:08.388338Z","shell.execute_reply.started":"2022-05-05T02:15:06.68656Z","shell.execute_reply":"2022-05-05T02:15:08.386498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/millions-of-movies/movies.csv')\ndf.drop('imdb_id',axis = 1, inplace =True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T03:16:59.455755Z","iopub.execute_input":"2022-05-05T03:16:59.456182Z","iopub.status.idle":"2022-05-05T03:17:03.886412Z","shell.execute_reply.started":"2022-05-05T03:16:59.456143Z","shell.execute_reply":"2022-05-05T03:17:03.885114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature extraction\nWe will compute pairwise similarity scores for all movies based on their overview and recommend movies based on that similarity score.\n\nFor any of you who has done even a bit of text processing before knows we need to convert the word vector of each overview. Now we'll compute Term Frequency-Inverse Document Frequency (TF-IDF) vectors for each overview.\n\nNow if you are wondering what is term frequency , it is the relative frequency of a word in a document and is given as (term instances/total instances). Inverse Document Frequency is the relative count of documents containing the term is given as log(number of documents/documents with term) The overall importance of each word to the documents in which they appear is equal to TF * IDF\n\nThis will give you a matrix where each column represents a word in the overview vocabulary (all the words that appear in at least one document) and each row represents a movie, as before.This is done to reduce the importance of words that occur frequently in plot overviews and therefore, their significance in computing the final similarity score.\n\nFortunately, scikit-learn gives you a built-in TfIdfVectorizer class that produces the TF-IDF matrix in a couple of lines. That's great, isn't it?","metadata":{}},{"cell_type":"code","source":"def lammitization():\n    import nltk\n    nltk.download('punkt')\n    nltk.download('averaged_perceptron_tagger')\n    nltk.download('wordnet')\n    from nltk.corpus import wordnet, stopwords\n    from nltk.stem import WordNetLemmatizer\n    from nltk.tokenize import word_tokenize\n    df['overview']= df['overview'].fillna('')\n\n    # Tokenizing the tweet base texts.\n\n    df['overview']=df['overview'].str.lower().apply(word_tokenize).apply(nltk.tag.pos_tag) # Applying part of speech tags.\n\n\n    # Converting part of speeches to wordnet format.\n\n    def get_wordnet_pos(tag):\n        if tag.startswith('J'):\n            return wordnet.ADJ\n        elif tag.startswith('V'):\n            return wordnet.VERB\n        elif tag.startswith('N'):\n            return wordnet.NOUN\n        elif tag.startswith('R'):\n            return wordnet.ADV\n        else:\n            return wordnet.NOUN\n\n\n    df['overview']= df['overview'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n\n    # Applying word lemmatizer.\n\n    wnl = WordNetLemmatizer()\n\n    df['overview']= df['overview'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n\n\n    df['overview']= df['overview'].apply(lambda x: ' '.join(x))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T02:15:18.537866Z","iopub.execute_input":"2022-05-05T02:15:18.538202Z","iopub.status.idle":"2022-05-05T02:15:18.552281Z","shell.execute_reply.started":"2022-05-05T02:15:18.538156Z","shell.execute_reply":"2022-05-05T02:15:18.551172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop_duplicates(inplace=True, ignore_index=True)\n#df = df.groupby('title').first().reset_index()\ndf.fillna(value={i: ' ' for i in ['overview', 'genres', 'keywords', 'credits']}, inplace=True)\n\n# lambda func for str split join\nstrOp= lambda x: ' '.join(x.split('-'))\n\ndf.overview = df.overview + df.keywords.apply(strOp) + df.genres.apply(strOp) + df.credits.apply(lambda x: ' '.join(x.replace(' ', '').split('-')[:3]))\n \n#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\ntfidf = TfidfVectorizer(stop_words='english')\n\n#Construct the required TF-IDF matrix by fitting and transforming the data\ntfidf_matrix = tfidf.fit_transform(df['overview'])\n\ndisplay(pd.DataFrame(\n    tfidf_matrix[:10, 7000:7070].toarray(),\n    columns= tfidf.get_feature_names_out()[7000:7070],\n    index = df.title[:10]).round())\n\nprint(tfidf_matrix.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T03:17:03.888234Z","iopub.execute_input":"2022-05-05T03:17:03.888475Z","iopub.status.idle":"2022-05-05T03:17:25.45166Z","shell.execute_reply.started":"2022-05-05T03:17:03.888443Z","shell.execute_reply":"2022-05-05T03:17:25.450841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that over 460000 different words were used to describe the 230160 movies in our dataset.\n\nWith this matrix in hand, we can now compute a similarity score.\n\nWe will be using the cosine similarity to calculate a numeric quantity that denotes the similarity between two movies. We use the cosine similarity score since it is independent of magnitude and is relatively easy and fast to calculate.\n\n# Finally The Recommender Function","metadata":{}},{"cell_type":"code","source":"\n# Function that takes in movie title as input and outputs most similar movies\ndef get_recommendations(title):\n    # Get the index of the movie that matches the title\n    idx = df.index[df['title'] == title][0]\n    # show given movie poster\n    try:\n        a = io.imread(f'https://image.tmdb.org/t/p/w500/{df.loc[idx, \"poster_path\"]}')\n        plt.imshow(a)\n        plt.axis('off')\n        plt.title(title)\n        plt.show()\n    except:pass\n    \n    print('Recommendations\\n')\n\n\n    # Get the pairwsie similarity scores of all movies with that movie\n    sim_scores = list(enumerate(\n        cosine_similarity(\n            tfidf_matrix,\n            tfidf_matrix[idx])))\n\n    # Sort the movies based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    # Get the scores of the 10 most similar movies\n    sim_scores = sim_scores[1:10]\n\n    # Get the movie indices\n    movie_indices = [i[0] for i in sim_scores]\n\n    # Return the top 10 most similar movies\n    result = df.iloc[movie_indices]\n    \n    # show reco. movie posters\n    fig, ax = plt.subplots(3, 3, figsize=(15,20))\n    ax=ax.flatten()\n    for i, j in enumerate(result.poster_path):\n        try:\n            ax[i].axis('off')\n            ax[i].set_title(result.iloc[i].title,fontsize=22)\n            a = io.imread(f'https://image.tmdb.org/t/p/w500/{j}')\n            ax[i].imshow(a)\n        except: pass\n    fig.tight_layout()\n    fig.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T03:17:51.091153Z","iopub.execute_input":"2022-05-05T03:17:51.091538Z","iopub.status.idle":"2022-05-05T03:17:51.106108Z","shell.execute_reply.started":"2022-05-05T03:17:51.091495Z","shell.execute_reply":"2022-05-05T03:17:51.104925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_recommendations(\"Avatar\")","metadata":{"execution":{"iopub.status.busy":"2022-05-05T03:20:34.681151Z","iopub.execute_input":"2022-05-05T03:20:34.681661Z","iopub.status.idle":"2022-05-05T03:20:42.828364Z","shell.execute_reply.started":"2022-05-05T03:20:34.681626Z","shell.execute_reply":"2022-05-05T03:20:42.827006Z"},"trusted":true},"execution_count":null,"outputs":[]}]}