{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:100%;text-align:center\">IMPORT</p></div>","metadata":{}},{"cell_type":"code","source":"print(\"\\n... IMPORTS STARTING ...\\n\")\n\nprint(\"\\n\\tVERSION INFORMATION\")\n# Machine Learning and Data Science Imports\nimport tensorflow as tf; print(f\"\\t\\t– TENSORFLOW VERSION: {tf.__version__}\");\nimport tensorflow_hub as tfhub; print(f\"\\t\\t– TENSORFLOW HUB VERSION: {tfhub.__version__}\");\nimport tensorflow_addons as tfa; print(f\"\\t\\t– TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np; print(f\"\\t\\t– NUMPY VERSION: {np.__version__}\");\nimport sklearn; print(f\"\\t\\t– SKLEARN VERSION: {sklearn.__version__}\");\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures\nfrom pandarallel import pandarallel; pandarallel.initialize();\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nfrom scipy.spatial import cKDTree\n\n# # RAPIDS\n# import cudf, cupy, cuml\n# from cuml.neighbors import NearestNeighbors\n# from cuml.manifold import TSNE, UMAP\n\n# Built In Imports\nfrom kaggle_datasets import KaggleDatasets\nfrom collections import Counter\nfrom datetime import datetime\nfrom glob import glob\nimport warnings\nimport requests\nimport hashlib\nimport imageio\nimport IPython\nimport sklearn\nimport urllib\nimport zipfile\nimport pickle\nimport random\nimport shutil\nimport string\nimport json\nimport math\nimport time\nimport gzip\nimport ast\nimport sys\nimport io\nimport os\nimport gc\nimport re\n\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.patches import Rectangle\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm; tqdm.pandas();\nimport plotly.express as px\nimport seaborn as sns\nfrom PIL import Image, ImageEnhance\nimport matplotlib; print(f\"\\t\\t– MATPLOTLIB VERSION: {matplotlib.__version__}\");\nfrom matplotlib import animation, rc, gridspec \nrc('animation', html='jshtml')\nimport plotly\nimport PIL\nimport cv2\n\nimport plotly.io as pio\nprint(pio.renderers)\n\ndef seed_it_all(seed=7):\n    \"\"\" Attempt to be Reproducible \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    \nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-24T03:30:34.340557Z","iopub.execute_input":"2022-04-24T03:30:34.340934Z","iopub.status.idle":"2022-04-24T03:30:43.563242Z","shell.execute_reply.started":"2022-04-24T03:30:34.340841Z","shell.execute_reply":"2022-04-24T03:30:43.562338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:100%;text-align:center\">SETUP</p></div>\n****","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">ACCELERATOR DETECTION</p></div>","metadata":{}},{"cell_type":"code","source":"print(f\"\\n... ACCELERATOR SETUP STARTING ...\\n\")\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  \nexcept ValueError:\n    TPU = None\n\nif TPU:\n    print(f\"\\n... RUNNING ON TPU - {TPU.master()}...\")\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    print(f\"\\n... RUNNING ON CPU/GPU ...\")\n    # Yield the default distribution strategy in Tensorflow\n    #   --> Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy() \n\n# What Is a Replica?\n#    --> A single Cloud TPU device consists of FOUR chips, each of which has TWO TPU cores. \n#    --> Therefore, for efficient utilization of Cloud TPU, a program should make use of each of the EIGHT (4x2) cores. \n#    --> Each replica is essentially a copy of the training graph that is run on each core and \n#        trains a mini-batch containing 1/8th of the overall batch size\nN_REPLICAS = strategy.num_replicas_in_sync\n    \nprint(f\"... # OF REPLICAS: {N_REPLICAS} ...\\n\")\n\nprint(f\"\\n... ACCELERATOR SETUP COMPLTED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:43.565313Z","iopub.execute_input":"2022-04-24T03:30:43.565572Z","iopub.status.idle":"2022-04-24T03:30:43.582367Z","shell.execute_reply.started":"2022-04-24T03:30:43.565541Z","shell.execute_reply":"2022-04-24T03:30:43.581433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">LEVERAGING XLA OPTIMIZATIONS</p></div>","metadata":{}},{"cell_type":"code","source":"print(f\"\\n... XLA OPTIMIZATIONS STARTING ...\\n\")\n\nprint(f\"\\n... CONFIGURE JIT (JUST IN TIME) COMPILATION ...\\n\")\n# enable XLA optmizations (10% speedup when using @tf.function calls)\ntf.config.optimizer.set_jit(True)\n\nprint(f\"\\n... XLA OPTIMIZATIONS COMPLETED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:43.583685Z","iopub.execute_input":"2022-04-24T03:30:43.584175Z","iopub.status.idle":"2022-04-24T03:30:43.600981Z","shell.execute_reply.started":"2022-04-24T03:30:43.584142Z","shell.execute_reply":"2022-04-24T03:30:43.599928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:100%;text-align:center\">CONFIGURATION and HANDING FUNCTIONS</p></div>\n****","metadata":{}},{"cell_type":"code","source":"class CFG:\n    EPOCHS = 30\n    DEMO_ID = \"case123_day20_slice_0082\"\n    DEMO_CASE = 134\n    NPY_DIR = \"/kaggle/working/npy_files\"","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:43.603318Z","iopub.execute_input":"2022-04-24T03:30:43.604281Z","iopub.status.idle":"2022-04-24T03:30:43.612077Z","shell.execute_reply.started":"2022-04-24T03:30:43.604209Z","shell.execute_reply":"2022-04-24T03:30:43.611267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing_df(df, globbed_file_list, is_test=False):\n    # 1. Get Case-ID as a column (str and int)\n    df[\"case_id_str\"] = df.id.apply(lambda x : x.split(\"_\",2)[0])\n    df[\"case_id\"] = df.case_id_str.apply(lambda x : int(x.replace(\"case\",\"\")))\n\n    # 2. Get Date as a column\n    df[\"day_num_str\"] = df.id.apply(lambda x : x.split(\"_\",2)[1])\n    df[\"day_num\"] = df.day_num_str.apply(lambda x : int(x.replace(\"day\",\"\")))\n\n    # 3. Get Slide Identifier as a column\n    df[\"slice_id\"] = df.id.apply(lambda x : x.split(\"_\",2)[2])\n\n    # 4. Get full file paths for the representative scans\n    df[\"_partial_ident\"] = (globbed_file_list[0].rsplit(\"/\",4)[0]+\"/\"+ # /kaggle/input/uw-madison-gi-tract-image-segmentation/train/\n                                 df[\"case_id_str\"]+\"/\"+ # .../case###/\n                                 df[\"case_id_str\"]+\"_\"+ df[\"day_num_str\"]+ # .../case###_day##/\n                                 \"/scans/\"+\n                                 df[\"slice_id\"]) # .../slice_#### \n    _tmp_merge_df = pd.DataFrame({\"_partial_ident\":[x.rsplit(\"_\",4)[0] for x in globbed_file_list], \"f_path\":globbed_file_list})\n    df = df.merge(_tmp_merge_df, on=\"_partial_ident\").drop(columns=[\"_partial_ident\"])\n\n    # 5. Get slice dimensions from filepath (int in pixels)\n    df[\"slice_h\"] = df.f_path.apply(lambda x: int(x[:-4].split(\"_\")[3]))\n    df[\"slice_w\"] = df.f_path.apply(lambda x: int(x[:-4].split(\"_\")[4]))\n\n    # 6. Pixel spacing from filepath (float in mm)\n    df[\"px_spacing_h\"] = df.f_path.apply(lambda x: float(x[:-4].split(\"_\")[5]))\n    df[\"px_spacing_w\"] = df.f_path.apply(lambda x: float(x[:-4].split(\"_\")[6]))\n    if not is_test:\n        # 7. Merge 3 Rows Into A Single Row (As This/Segmentation-RLE Is The Only Unique Information Across Those Rows)\n        l_bowel_df = df[df[\"class\"]==\"large_bowel\"][[\"id\",\"segmentation\"]].rename(columns={\"segmentation\":\"lb_seg_rle\"})\n        s_bowel_df = df[df[\"class\"]==\"small_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"sb_seg_rle\"})\n        stomach_df = df[df[\"class\"]==\"stomach\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"st_seg_rle\"})\n\n        df = df.merge(l_bowel_df, on=\"id\", how=\"left\")\n        df = df.merge(s_bowel_df, on=\"id\", how=\"left\")\n        df = df.merge(stomach_df, on=\"id\", how=\"left\")\n\n        df = df.drop_duplicates(subset=[\"id\",]).reset_index(drop=True)\n\n        df[\"lb_seg_flag\"] = df.lb_seg_rle.apply(lambda x: not pd.isna(x))\n        df[\"sb_seg_flag\"] = df.sb_seg_rle.apply(lambda x: not pd.isna(x))\n        df[\"st_seg_flag\"] = df.st_seg_rle.apply(lambda x: not pd.isna(x))\n\n        df[\"n_segs\"] = df.lb_seg_flag.astype(int)+df.sb_seg_flag.astype(int)+df.st_seg_flag.astype(int)\n    # 8. Reorder columns to the a new ordering (drops class and segmentation as no longer necessary)\n    new_col_order = [\"id\", \"f_path\", \"n_segs\",\n                     \"lb_seg_rle\", \"lb_seg_flag\",\n                     \"sb_seg_rle\", \"sb_seg_flag\", \n                     \"st_seg_rle\", \"st_seg_flag\",\n                     \"slice_h\", \"slice_w\", \"px_spacing_h\", \n                     \"px_spacing_w\", \"case_id_str\", \"case_id\", \n                     \"day_num_str\", \"day_num\", \"slice_id\",]\n    if is_test: new_col_order.insert(1, \"class\")\n    new_col_order = [_c for _c in new_col_order if _c in df.columns]\n    df = df[new_col_order]\n    # 9. Display update dataframe\n    display(df)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:43.613682Z","iopub.execute_input":"2022-04-24T03:30:43.614022Z","iopub.status.idle":"2022-04-24T03:30:43.637781Z","shell.execute_reply.started":"2022-04-24T03:30:43.613975Z","shell.execute_reply":"2022-04-24T03:30:43.636844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n# modified from: https://www.kaggle.com/inversion/run-length-decoding-quick-start\ndef rle_decode(mask_rle, shape, color=1):\n    \"\"\" TBD\n    \n    Args:\n        mask_rle (str): run-length as string formated (start length)\n        shape (tuple of ints): (height,width) of array to return \n    \n    Returns: \n        Mask (np.array)\n            - 1 indicating mask\n            - 0 indicating background\n\n    \"\"\"\n    # Split the string by space, then convert it into a integer array\n    s = np.array(mask_rle.split(), dtype=int)\n\n    # Every even value is the start, every odd value is the \"run\" length\n    starts = s[0::2] - 1\n    lengths = s[1::2]\n    ends = starts + lengths\n\n    # The image image is actually flattened since RLE is a 1D \"run\"\n    if len(shape)==3:\n        h, w, d = shape\n        img = np.zeros((h * w, d), dtype=np.float32)\n    else:\n        h, w = shape\n        img = np.zeros((h * w,), dtype=np.float32)\n\n    # The color here is actually just any integer you want!\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n        \n    # Don't forget to change the image back to the original shape\n    return img.reshape(shape)\n\n# https://www.kaggle.com/namgalielei/which-reshape-is-used-in-rle\ndef rle_decode_top_to_bot_first(mask_rle, shape):\n    \"\"\" TBD\n    \n    Args:\n        mask_rle (str): run-length as string formated (start length)\n        shape (tuple of ints): (height,width) of array to return \n    \n    Returns:\n        Mask (np.array)\n            - 1 indicating mask\n            - 0 indicating background\n\n    \"\"\"\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape((shape[1], shape[0]), order='F').T  # Reshape from top -> bottom first\n\n# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_encode(img):\n    \"\"\" TBD\n    \n    Args:\n        img (np.array): \n            - 1 indicating mask\n            - 0 indicating background\n    \n    Returns: \n        run length as string formated\n    \"\"\"\n    \n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef flatten_l_o_l(nested_list):\n    \"\"\" Flatten a list of lists \"\"\"\n    return [item for sublist in nested_list for item in sublist]\n\ndef load_json_to_dict(json_path):\n    \"\"\" tbd \"\"\"\n    with open(json_path) as json_file:\n        data = json.load(json_file)\n    return data\n\ndef tf_load_png(img_path):\n    return tf.image.decode_png(tf.io.read_file(img_path), channels=3)\n\ndef open_gray16(_path, normalize=True, to_rgb=False):\n    \"\"\" Helper to open files \"\"\"\n    if normalize:\n        if to_rgb:\n            return np.tile(np.expand_dims(cv2.imread(_path, cv2.IMREAD_ANYDEPTH)/65535., axis=-1), 3)\n        else:\n            return cv2.imread(_path, cv2.IMREAD_ANYDEPTH)/65535.\n    else:\n        if to_rgb:\n            return np.tile(np.expand_dims(cv2.imread(_path, cv2.IMREAD_ANYDEPTH), axis=-1), 3)\n        else:\n            return cv2.imread(_path, cv2.IMREAD_ANYDEPTH)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:43.639479Z","iopub.execute_input":"2022-04-24T03:30:43.640106Z","iopub.status.idle":"2022-04-24T03:30:43.862019Z","shell.execute_reply.started":"2022-04-24T03:30:43.640065Z","shell.execute_reply":"2022-04-24T03:30:43.860884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_overlay(img_path, rle_strs, img_shape, _alpha=0.999, _beta=0.35, _gamma=0):\n    _img = open_gray16(img_path, to_rgb=True)\n    _img = ((_img-_img.min())/(_img.max()-_img.min())).astype(np.float32)\n    _seg_rgb = np.stack([rle_decode(rle_str, shape=img_shape, color=1) \\\n                         if (rle_str is not None and not pd.isna(rle_str)) \\\n                         else np.zeros(img_shape, dtype=np.float32) for rle_str in rle_strs], axis=-1).astype(np.float32)\n    seg_overlay = cv2.addWeighted(src1=_img, alpha=_alpha, \n                                  src2=_seg_rgb, beta=_beta, gamma=_gamma)\n    return seg_overlay\n\ndef examine_id(ex_id, df, plot_overlay=True, print_meta=False, plot_grayscale=False, plot_binary_segmentation=False):\n    \"\"\" Wrapper function to allow for easy visual exploration of an example \"\"\"\n    print(f\"\\n... ID ({ex_id}) EXPLORATION STARTED ...\\n\\n\")\n    demo_ex = df[df.id==ex_id].squeeze()\n\n    if print_meta:\n        print(f\"\\n... WITH DEMO_ID=`{DEMO_ID}` WE HAVE THE FOLLOWING DEMO EXAMPLE TO WORK FROM... \\n\\n\")\n        display(demo_ex.to_frame())\n\n    if plot_grayscale:\n        print(f\"\\n\\n... GRAYSCALE IMAGE PLOT ...\\n\")\n        plt.figure(figsize=(12,12))\n        plt.imshow(open_gray16(demo_ex.f_path), cmap=\"gray\")\n        plt.title(f\"Original Grayscale Image For ID: {demo_ex.id}\", fontweight=\"bold\")\n        plt.axis(False)\n        plt.show()\n\n    if plot_binary_segmentation:\n        print(f\"\\n\\n... BINARY SEGMENTATION MASKS ...\\n\")\n        plt.figure(figsize=(20,10))\n        for i, _seg_type in enumerate([\"lb\", \"sb\", \"st\"]):\n            if pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]): continue\n            plt.subplot(1,3,i+1)\n            plt.imshow(rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1))\n            plt.title(f\"RLE Encoding For {SF2LF[_seg_type]} Segmentation\", fontweight=\"bold\")\n            plt.axis(False)\n        plt.tight_layout()\n        plt.show()\n\n    if plot_overlay:\n        print(f\"\\n\\n... IMAGE WITH RGB SEGMENTATION MASK OVERLAY ...\\n\")\n        # We need to normalize the loaded image values to be between 0 and 1 or else our plot will look weird\n        # _img = open_gray16(demo_ex.f_path, to_rgb=True)\n        #_img = ((_img-_img.min())/(_img.max()-_img.min())).astype(np.float32)\n        #_seg_rgb = np.stack([rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1) if not pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]) else np.zeros((demo_ex.slice_w, demo_ex.slice_h)) for _seg_type in [\"lb\", \"sb\", \"st\"]], axis=-1).astype(np.float32)\n        #seg_overlay = cv2.addWeighted(src1=_img, alpha=0.99, \n                                      #src2=_seg_rgb, beta=0.33, gamma=0)\n        _rle_strs = [demo_ex[f\"{_seg_type}_seg_rle\"] if not pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]) else None for _seg_type in [\"lb\", \"sb\", \"st\"]]\n        seg_overlay = get_overlay(demo_ex.f_path, _rle_strs, img_shape=(demo_ex.slice_w, demo_ex.slice_h))\n\n        plt.figure(figsize=(12,12))\n        plt.imshow(seg_overlay)\n        plt.title(f\"Segmentation Overlay For ID: {demo_ex.id}\", fontweight=\"bold\")\n        handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\n        labels = [\"Large Bowel Segmentation Map\", \"Small Bowel Segmentation Map\", \"Stomach Segmentation Map\"]\n        plt.legend(handles,labels)\n        plt.axis(False)\n        plt.show()\n\n    print(\"\\n\\n... SINGLE ID EXPLORATION FINISHED ...\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:08:00.545039Z","iopub.execute_input":"2022-04-24T04:08:00.545569Z","iopub.status.idle":"2022-04-24T04:08:00.565876Z","shell.execute_reply.started":"2022-04-24T04:08:00.545511Z","shell.execute_reply":"2022-04-24T04:08:00.564862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_seg_combo_str(row):\n    seg_str_list = []\n    if row[\"lb_seg_flag\"]: seg_str_list.append(\"Large Bowel\")\n    if row[\"sb_seg_flag\"]: seg_str_list.append(\"Small Bowel\")\n    if row[\"st_seg_flag\"]: seg_str_list.append(\"Stomach\")\n    if len(seg_str_list)>0:\n        return \", \".join(seg_str_list)\n    else:\n        return \"No Mask\"","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:43.887816Z","iopub.execute_input":"2022-04-24T03:30:43.888374Z","iopub.status.idle":"2022-04-24T03:30:43.902984Z","shell.execute_reply.started":"2022-04-24T03:30:43.888321Z","shell.execute_reply":"2022-04-24T03:30:43.902189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_case(case_id, df, day=None, _figsize=(20,30), n_cols=16):\n    # Initialize\n    case_df = df[df.case_id==case_id]\n    \n    if day is not None:\n        _case_df = case_df[(case_df.day_num==day) | (case_df.day_num_str==str(day))]\n        if len(_case_df)>0:\n            approx_shrink = len(_case_df)/len(case_df)\n            case_df=_case_df\n            _figsize = (_figsize[0], int(np.ceil(1.25*_figsize[1]*approx_shrink)))\n        else:\n            print(\"There are no valid samples for the passed `day`. Reverting to all days in case.\")\n        del _case_df\n    \n    n_ex = len(case_df)\n    \n    print(\"...Preparing...\")\n    # Get relevant data\n    case_paths = case_df[\"f_path\"].tolist()\n    case_rles = [[_rle if not pd.isna(_rle) else None for _rle in _rles] for _rles in case_df[[\"lb_seg_rle\", \"sb_seg_rle\", \"st_seg_rle\"]].values.tolist()]\n    case_img_shapes = [(_w,_h) for _w,_h in zip(case_df[\"slice_w\"].tolist(), case_df[\"slice_h\"].tolist())]\n    all_overlays = [get_overlay(img_path, rle_strs, img_shape) for img_path, rle_strs, img_shape in zip(case_paths, case_rles, case_img_shapes)]\n    \n    print(\"...Plotting...\")    \n    # Plot\n    plt.figure(figsize=_figsize)\n    n_rows = int(np.ceil(n_ex/n_cols))\n    \n    gs = gridspec.GridSpec(n_rows, n_cols,\n         wspace=0.0, hspace=0.0, \n         top=1.-0.5/(n_rows+1), bottom=0.5/(n_rows+1), \n         left=0.5/(n_cols+1), right=1-0.5/(n_cols+1))\n    \n    for i in range(n_rows):\n        if len(all_overlays)==0: break\n        for j in range(n_cols):\n            if len(all_overlays)==0: break\n            ax=plt.subplot(gs[i,j])\n            ax.imshow(all_overlays.pop())\n            ax.axis(False)\n            ax.set_xticklabels([])\n            ax.set_yticklabels([])\n        \n    print(\"...Displaying...\")    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:11:29.142007Z","iopub.execute_input":"2022-04-24T04:11:29.142508Z","iopub.status.idle":"2022-04-24T04:11:29.15849Z","shell.execute_reply.started":"2022-04-24T04:11:29.14246Z","shell.execute_reply":"2022-04-24T04:11:29.157411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_mask_area(rle):\n    return sum([int(x) for x in rle.split()[1::2]])","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:43.922777Z","iopub.execute_input":"2022-04-24T03:30:43.923033Z","iopub.status.idle":"2022-04-24T03:30:43.937347Z","shell.execute_reply.started":"2022-04-24T03:30:43.923Z","shell.execute_reply":"2022-04-24T03:30:43.936438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_overlap(_arr):\n    return _arr.sum(axis=-1).max()>1\n    \ndef make_seg_mask(row, output_dir=\"/kaggle/working/npy_files\", check_overlap=False):\n    slice_shape = (row.slice_w, row.slice_h)\n    if not pd.isna(row.lb_seg_rle):\n        lb_mask = rle_decode(row.lb_seg_rle, slice_shape, )\n    else:\n        lb_mask = np.zeros(slice_shape)\n    if not pd.isna(row.sb_seg_rle):\n        sb_mask = rle_decode(row.sb_seg_rle, slice_shape)\n    else:\n        sb_mask = np.zeros(slice_shape)\n    if not pd.isna(row.st_seg_rle):\n        st_mask = rle_decode(row.st_seg_rle, slice_shape)\n    else:\n        st_mask = np.zeros(slice_shape)\n    mask_arr = np.stack([lb_mask, sb_mask, st_mask], axis=-1).astype(np.uint8)\n    np.save(f\"./npy_files/{row.id}_mask\", mask_arr)\n    \n    if check_overlap: \n        if is_overlap(mask_arr): \n            return np.where(mask_arr.sum(axis=-1)>1, 1, 0).sum()\n        else:\n            return 0","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:43.94125Z","iopub.execute_input":"2022-04-24T03:30:43.941945Z","iopub.status.idle":"2022-04-24T03:30:43.952311Z","shell.execute_reply.started":"2022-04-24T03:30:43.941908Z","shell.execute_reply":"2022-04-24T03:30:43.951293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_vals(row):\n    _img = cv2.imread(row.f_path, -1)\n    _nonzero_px_count = np.count_nonzero(_img)\n    \n    row[\"nonzero_num_pxs\"] = _nonzero_px_count\n    row[\"max_px_value\"] = _img.max()\n    row[\"min_px_value\"] = _img.min()\n    row[\"mean_px_value\"] = _img.mean()\n    row[\"nonzero_mean_px_value\"] = _img.sum()/_nonzero_px_count\n    \n    return row","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:43.953796Z","iopub.execute_input":"2022-04-24T03:30:43.954514Z","iopub.status.idle":"2022-04-24T03:30:43.971516Z","shell.execute_reply.started":"2022-04-24T03:30:43.954472Z","shell.execute_reply":"2022-04-24T03:30:43.970466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_animation(case_id, day_num, df):\n    \n    sub_df = df[(df.case_id==case_id) & (df.day_num==day_num)]\n    \n    f_paths  = sub_df.f_path.tolist()\n    lb_rles  = sub_df.lb_seg_rle.tolist()\n    sb_rles  = sub_df.sb_seg_rle.tolist()\n    st_rles  = sub_df.st_seg_rle.tolist()\n    slice_ws = sub_df.slice_w.tolist()\n    slice_hs = sub_df.slice_h.tolist()\n    \n    animation_arr = np.stack([get_overlay(img_path=_f, \n                                          rle_strs=(_lb, _sb, _st), \n                                          img_shape=(_w, _h)) \\\n                                for _f, _lb, _sb, _st, _w, _h in zip(f_paths, lb_rles, sb_rles, st_rles, slice_ws, slice_hs)\n                                ], axis=0)\n    \n    fig = plt.figure(figsize=(8,8))\n    \n    plt.axis('off')\n    im = plt.imshow(animation_arr[0])\n    plt.title(f\"3D Animation for Case {case_id} on Day {day_num}\", fontweight=\"bold\")\n    \n    def animate_func(i):\n        im.set_array(animation_arr[i])\n        return [im]\n    plt.close()\n    \n    return animation.FuncAnimation(fig, animate_func, frames = animation_arr.shape[0], interval = 1000//12)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:12:04.74871Z","iopub.execute_input":"2022-04-24T04:12:04.749092Z","iopub.status.idle":"2022-04-24T04:12:04.760584Z","shell.execute_reply.started":"2022-04-24T04:12:04.749036Z","shell.execute_reply":"2022-04-24T04:12:04.759816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:100%;text-align:center\">DATASET</p></div>","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">COMPETITION DATA ACCESS</p></div>","metadata":{}},{"cell_type":"code","source":"print(\"\\n... DATA ACCESS SETUP STARTED ...\\n\")\n\nif TPU:\n    # Google Cloud Dataset path to training and validation images\n    DATA_DIR = KaggleDatasets().get_gcs_path('uw-madison-gi-tract-image-segmentation')\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\nelse:\n    # Local path to training and validation images\n    DATA_DIR = \"/kaggle/input/uw-madison-gi-tract-image-segmentation\"\n    save_locally = None\n    load_locally = None\n\nprint(f\"\\n... DATA DIRECTORY PATH IS:\\n\\t--> {DATA_DIR}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF DATA DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(\"\\n\\n... DATA ACCESS SETUP COMPLETED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:43.973006Z","iopub.execute_input":"2022-04-24T03:30:43.974042Z","iopub.status.idle":"2022-04-24T03:30:43.992514Z","shell.execute_reply.started":"2022-04-24T03:30:43.974006Z","shell.execute_reply":"2022-04-24T03:30:43.991448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">BASIC DATA DEFINITIONS & INITIALIZATIONS</p></div> ","metadata":{}},{"cell_type":"code","source":"print(\"\\n... BASIC DATA SETUP STARTING ...\\n\\n\")\nTRAIN_DIR = os.path.join(DATA_DIR, \"train\")\nTRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\ntrain_full = pd.read_csv(TRAIN_CSV)\n# Get all training images\nall_train_images = glob(os.path.join(TRAIN_DIR, \"**\", \"*.png\"), recursive=True)\n\nprint(\"\\n... ORIGINAL TRAINING DATAFRAME... \\n\")\ndisplay(train_full)\n\n\nTEST_DIR = os.path.join(DATA_DIR, \"test\")\nSS_CSV   = os.path.join(DATA_DIR, \"sample_submission.csv\")\nss_df = pd.read_csv(SS_CSV)\n# Get all testing images if there are any\nall_test_images = glob(os.path.join(TEST_DIR, \"**\", \"*.png\"), recursive=True)\n\nprint(\"\\n\\n\\n... ORIGINAL SUBMISSION DATAFRAME... \\n\")\ndisplay(ss_df)\n\n# For debugging purposes when the test set hasn't been substituted we will know\nDEBUG=len(ss_df)==0\n\nif DEBUG:\n    TEST_DIR = TRAIN_DIR\n    all_test_images = all_train_images\n    ss_df = train_full.iloc[:10]\n    ss_df = ss_df[[\"id\", \"class\"]]\n    ss_df[\"predicted\"] = \"\"\n    \n    print(\"\\n\\n\\n... DEBUG SUBMISSION DATAFRAME... \\n\")\n    display(ss_df)\n\nSF2LF = {\"lb\":\"Large Bowel\",\"sb\":\"Small Bowel\",\"st\":\"Stomach\"}\nLF2SF = {v:k for k,v in SF2LF.items()}\n\nprint(\"\\n... BASIC DATA SETUP FINISHED ...\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:43.993862Z","iopub.execute_input":"2022-04-24T03:30:43.994302Z","iopub.status.idle":"2022-04-24T03:30:48.513236Z","shell.execute_reply.started":"2022-04-24T03:30:43.99425Z","shell.execute_reply":"2022-04-24T03:30:48.512258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">UPDATE DATAFRAMES WITH ACCESSIBLE EXTRA INFORMATION</p></div>  \n****\nI have changed the column identifiers as follows for the sake of brevity:\n\n* large_bowel --> lb\n* small_bowel --> sb\n* stomach --> st","metadata":{}},{"cell_type":"code","source":"train_df = train_full.copy()\nprint(\"\\n... UPDATED TRAINING DATAFRAME... \\n\")\ntrain_df = preprocessing_df(train_df, all_train_images)\nprint(\"\\n\\n\\n... UPDATED SUBMISSION DATAFRAME... \\n\")\nss_df = preprocessing_df(ss_df, all_test_images, is_test=True)\nprint(\"\\n... UPDATING DATAFRAMES WITH ACCESSIBLE INFORMATION FINISHED ...\\n\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:48.514595Z","iopub.execute_input":"2022-04-24T03:30:48.515034Z","iopub.status.idle":"2022-04-24T03:30:50.514724Z","shell.execute_reply.started":"2022-04-24T03:30:48.515001Z","shell.execute_reply":"2022-04-24T03:30:50.513649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:100%;text-align:center\">DATASET EXPLORATION</p></div>","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">LOOK AT A SINGLE EXAMPLE PRIOR TO INVESTIGATION</p></div>  ","metadata":{}},{"cell_type":"code","source":"print(\"\\n... SINGLE ID EXPLORATION STARTED ...\\n\\n\")\ndemo_ex = train_df[train_df.id == CFG.DEMO_ID].squeeze()\nprint(f\"\\n... WITH DEMO_ID=`{CFG.DEMO_ID}` WE HAVE THE FOLLOWING DEMO EXAMPLE TO WORK FROM... \\n\\n\")\ndisplay(demo_ex.to_frame())","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:50.516408Z","iopub.execute_input":"2022-04-24T03:30:50.517352Z","iopub.status.idle":"2022-04-24T03:30:50.536898Z","shell.execute_reply.started":"2022-04-24T03:30:50.517315Z","shell.execute_reply":"2022-04-24T03:30:50.53595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\\n\\n... LET'S PLOT THE IMAGE FIRST ...\\n\")\nplt.figure(figsize=(12,12))\nplt.imshow(open_gray16(demo_ex.f_path), cmap=\"gray\")\nplt.title(f\"Original Grayscale Image For ID: {demo_ex.id}\", fontweight=\"bold\")\nplt.axis(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:50.538296Z","iopub.execute_input":"2022-04-24T03:30:50.539017Z","iopub.status.idle":"2022-04-24T03:30:50.902045Z","shell.execute_reply.started":"2022-04-24T03:30:50.538985Z","shell.execute_reply":"2022-04-24T03:30:50.901317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\\n\\n... LET'S PLOT THE 3 SEGMENTATION MASKS ...\\n\")\n\nplt.figure(figsize=(20,10))\nfor i, _seg_type in enumerate([\"lb\", \"sb\", \"st\"]):\n    if pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]): continue\n    plt.subplot(1,3,i+1)\n    plt.imshow(rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1))\n    plt.title(f\"RLE Encoding For {SF2LF[_seg_type]} Segmentation\", fontweight=\"bold\")\n    plt.axis(False)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:50.903508Z","iopub.execute_input":"2022-04-24T03:30:50.903975Z","iopub.status.idle":"2022-04-24T03:30:51.298336Z","shell.execute_reply.started":"2022-04-24T03:30:50.903925Z","shell.execute_reply":"2022-04-24T03:30:51.29728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\\n\\n... LET'S PLOT THE IMAGE WITH AN RGB SEGMENTATION MASK OVERLAY ...\\n\")\n\n# We need to normalize the loaded image values to be between 0 and 1 or else our plot will look weird\n_img = open_gray16(demo_ex.f_path, to_rgb=True)\n_img = ((_img-_img.min())/(_img.max()-_img.min())).astype(np.float32)\n_seg_rgb = np.stack([rle_decode(demo_ex[f\"{_seg_type}_seg_rle\"], shape=(demo_ex.slice_w, demo_ex.slice_h), color=1) if not pd.isna(demo_ex[f\"{_seg_type}_seg_rle\"]) else np.zeros((demo_ex.slice_w, demo_ex.slice_h)) for _seg_type in [\"lb\", \"sb\", \"st\"]], axis=-1).astype(np.float32)\nseg_overlay = cv2.addWeighted(src1=_img, alpha=0.99, \n                              src2=_seg_rgb, beta=0.33, gamma=0.0)\n\nplt.figure(figsize=(12,12))\nplt.imshow(seg_overlay)\nplt.title(f\"Segmentation Overlay For ID: {demo_ex.id}\", fontweight=\"bold\")\nhandles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\nlabels = [\"Large Bowel Segmentation Map\", \"Small Bowel Segmentation Map\", \"Stomach Segmentation Map\"]\nplt.legend(handles,labels)\nplt.axis(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:51.300102Z","iopub.execute_input":"2022-04-24T03:30:51.300487Z","iopub.status.idle":"2022-04-24T03:30:51.677315Z","shell.execute_reply.started":"2022-04-24T03:30:51.300429Z","shell.execute_reply":"2022-04-24T03:30:51.676294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">INVESTIGATE THE OCCURENCE SEGMENTATION MAP TYPES</p></div>  ","metadata":{}},{"cell_type":"code","source":"train_df[\"seg_combo_str\"] = train_df.progress_apply(get_seg_combo_str, axis=1)\n\nfig = px.histogram(train_df, \n                   train_df[\"n_segs\"].astype(str), \n                   color=\"seg_combo_str\", \n                   title=\"<b>Number of Segmentation Masks Per Image</b>\", \n                   labels={\"x\":\"Number of Segmentation Masks Per Image\", \n                           \"seg_combo_str\":\"<b>Segmentation Masks Present</b>\"})\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:51.67885Z","iopub.execute_input":"2022-04-24T03:30:51.679178Z","iopub.status.idle":"2022-04-24T03:30:54.274144Z","shell.execute_reply.started":"2022-04-24T03:30:51.679135Z","shell.execute_reply":"2022-04-24T03:30:54.273169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">INVESTIGATE THE IMAGE SIZES</p></div>  ","metadata":{}},{"cell_type":"code","source":"fig = px.scatter(train_df.drop_duplicates(subset=[\"slice_w\", \"slice_h\"]), \n                 x=\"slice_w\", \n                 y=\"slice_h\", \n                 size=train_df.groupby([\"slice_w\", \"slice_h\"])[\"id\"].transform(\"count\").iloc[train_df.drop_duplicates(subset=[\"slice_w\", \"slice_h\"]).index], \n                 color=\"(\"+train_df.drop_duplicates(subset=[\"slice_w\", \"slice_h\"])[\"slice_w\"].astype(str)+\",\"+train_df.drop_duplicates(subset=[\"slice_w\", \"slice_h\"])[\"slice_h\"].astype(str)+\")\", \n                 title=\"<b>Bubble Chart Showing The Various Image Sizes</b>\",\n                 labels={\"color\":\"<b>Size Legend</b>\", \n                         \"size\":\"<b>Number Of Observations</b>\",\n                         \"slice_h\":\"<b>Image Slice Height (pixels)</b>\",\n                         \"slice_w\":\"<b>Image Slice Width (pixels)</b>\"},\n                 size_max=160)\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:54.275573Z","iopub.execute_input":"2022-04-24T03:30:54.276535Z","iopub.status.idle":"2022-04-24T03:30:54.429091Z","shell.execute_reply.started":"2022-04-24T03:30:54.276489Z","shell.execute_reply":"2022-04-24T03:30:54.428109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">INVESTIGATE THE PIXEL SPACING</p></div> ","metadata":{}},{"cell_type":"code","source":"fig = px.scatter(train_df.drop_duplicates(subset=[\"px_spacing_w\", \"px_spacing_h\"]), \n                 x=\"px_spacing_w\", \n                 y=\"px_spacing_h\", \n                 size=train_df.groupby([\"px_spacing_w\", \"px_spacing_h\"])[\"id\"].transform(\"count\").iloc[train_df.drop_duplicates(subset=[\"px_spacing_w\", \"px_spacing_h\"]).index], \n                 color=\"(\"+train_df.drop_duplicates(subset=[\"px_spacing_w\", \"px_spacing_h\"])[\"px_spacing_w\"].astype(str)+\",\"+train_df.drop_duplicates(subset=[\"px_spacing_w\", \"px_spacing_h\"])[\"px_spacing_h\"].astype(str)+\")\", \n                 title=\"<b>Bubble Chart Showing The Various Pixel Spacings</b>\",\n                 labels={\"color\":\"<b>Pixel Spacing Sets Legend</b>\", \n                         \"size\":\"<b>Number Of Observations</b>\",\n                         \"px_spacing_h\":\"<b>Pixel Spacing Height (mm)</b>\",\n                         \"px_spacing_w\":\"<b>Pixel Spacing Width (mm)</b>\"},\n                 size_max=160)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:54.430839Z","iopub.execute_input":"2022-04-24T03:30:54.431323Z","iopub.status.idle":"2022-04-24T03:30:54.52996Z","shell.execute_reply.started":"2022-04-24T03:30:54.431286Z","shell.execute_reply":"2022-04-24T03:30:54.528997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">INVESTIGATE CASE IDS</p></div>  ","metadata":{}},{"cell_type":"code","source":"fig = px.histogram(train_df, \n                   train_df.case_id.astype(str), \n                   color=\"day_num_str\", \n                   title=\"<b>Distribution Of Images Per Case ID</b>\", \n                   labels={\"x\":\"<b>Case ID</b>\", \"day_num_str\": \"<b>The Day The Scan Took Place</b>\"}, \n                   text_auto=True, \n                   width=2000)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:30:54.531726Z","iopub.execute_input":"2022-04-24T03:30:54.532172Z","iopub.status.idle":"2022-04-24T03:30:55.34621Z","shell.execute_reply.started":"2022-04-24T03:30:54.532123Z","shell.execute_reply":"2022-04-24T03:30:55.345305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">VISUALIZE A SIMPLE CASE</p></div>  ","metadata":{}},{"cell_type":"code","source":"print(f\"\\n\\n... PLOTTING DEMO CASE ID #{CFG.DEMO_CASE} ...\\n\\n\")\nplot_case(CFG.DEMO_CASE, df=train_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:11:29.15978Z","iopub.execute_input":"2022-04-24T04:11:29.16062Z","iopub.status.idle":"2022-04-24T04:12:04.746162Z","shell.execute_reply.started":"2022-04-24T04:11:29.160581Z","shell.execute_reply":"2022-04-24T04:12:04.744413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">MASK SIZES/AREAS</p></div>  ","metadata":{}},{"cell_type":"code","source":"train_df[\"lb_seg_area\"] = train_df.lb_seg_rle.apply(lambda x: None if pd.isna(x) else get_mask_area(x))\ntrain_df[\"sb_seg_area\"] = train_df.sb_seg_rle.apply(lambda x: None if pd.isna(x) else get_mask_area(x))\ntrain_df[\"st_seg_area\"] = train_df.st_seg_rle.apply(lambda x: None if pd.isna(x) else get_mask_area(x))\n\nfig = px.histogram(train_df, \n                   [\"lb_seg_area\", \"sb_seg_area\", \"st_seg_area\"], \n                   title=\"<b>Mask Areas Overlaid</b>\", \n                   barmode=\"overlay\",\n                   labels={\"value\":\"<b>Mask Area</b>\"})\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:31:28.434625Z","iopub.execute_input":"2022-04-24T03:31:28.434975Z","iopub.status.idle":"2022-04-24T03:31:31.137547Z","shell.execute_reply.started":"2022-04-24T03:31:28.434933Z","shell.execute_reply":"2022-04-24T03:31:31.13637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">MASK DATASET CREATION, CLASS OVERLAP & MASK HEATMAP</p></div>  ","metadata":{}},{"cell_type":"code","source":"if not os.path.isdir(CFG.NPY_DIR): os.makedirs(CFG.NPY_DIR, exist_ok=True)\ntrain_df[\"seg_overlap_area\"] = train_df.progress_apply(lambda x: make_seg_mask(x, output_dir=CFG.NPY_DIR, check_overlap=True), axis=1)\n\nprint(\"\\n... LET'S EXAMINE THE IMAGE WITH THE HIGHEST AMOUNT OF OVERLAP ...\\n\")\n\nexamine_id(train_df[train_df.seg_overlap_area==train_df.seg_overlap_area.max()].id.values[0], train_df)\n\nfig = px.histogram(train_df[train_df.seg_overlap_area>0], \n                   \"seg_overlap_area\", \n                   color=\"seg_combo_str\", \n                   nbins=50,\n                   log_y=True, \n                   title=\"<b>Distribution of Non-Zero Segmentation Overlaps <sub>(Count Is Logarithmic)</sub></b>\",  \n                   labels={\"seg_overlap_area\":\"<b>Area of Mask Overlap</b>\", \n                           \"seg_combo_str\":\"<b>Segmentation Masks In Image</b>\"})\nfig.update_layout(legend=dict(yanchor=\"top\",\n                              y=0.99,\n                              xanchor=\"right\",\n                              x=0.995\n                             )\n                 )\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:08:33.624556Z","iopub.execute_input":"2022-04-24T04:08:33.624895Z","iopub.status.idle":"2022-04-24T04:11:29.139877Z","shell.execute_reply.started":"2022-04-24T04:08:33.624857Z","shell.execute_reply":"2022-04-24T04:11:29.138619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">HEATMAP for SEGMENTATION MASK</p></div>  ","metadata":{}},{"cell_type":"code","source":"heatmap = np.zeros((256,256,3), dtype=np.float32)\nfor _, _row in tqdm(train_df.iterrows(), total=len(train_df)):\n    if (_row.lb_seg_flag or _row.sb_seg_flag or _row.st_seg_flag):\n        _mask = cv2.resize(np.load(f\"./npy_files/{_row.id}_mask.npy\"), (256,256), interpolation=cv2.INTER_NEAREST)\n        heatmap+=_mask","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:34:03.548885Z","iopub.execute_input":"2022-04-24T03:34:03.550006Z","iopub.status.idle":"2022-04-24T03:34:26.105491Z","shell.execute_reply.started":"2022-04-24T03:34:03.549958Z","shell.execute_reply":"2022-04-24T03:34:26.104348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">PIXEL VALUES</p></div>  \n****\nKnowing the range of Pixel values to normalize the data to convert it into the format that is expected in ML. \n\nOBSERVATIONS: \n\nThe maximum value in the dataset(15865.0) is equiavlent to less than half of an int16 or a quarter of a uint16(16384).\n\n","metadata":{}},{"cell_type":"code","source":"train_df = train_df.progress_apply(get_image_vals, axis=1)\n\nprint(f\"\\n\\n\\n... UPDATED TRAIN DATAFRAME ...\\n\")\ndisplay(train_df.head())\nprint(\"\\n\\n\")\n\nfor _c in [\"nonzero_num_pxs\", \"max_px_value\", \"min_px_value\", \"mean_px_value\", \"nonzero_mean_px_value\"]:\n    print(f\"\\n... STATS FOR COLUMN --> `{_c}`...\")\n    print(f\"\\t--> MIN  VAL: {train_df[_c].min():.1f}\")\n    print(f\"\\t--> MEAN VAL: {train_df[_c].mean():.1f}\")\n    print(f\"\\t--> MAX  VAL: {train_df[_c].max():.1f}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:34:26.107311Z","iopub.execute_input":"2022-04-24T03:34:26.107916Z","iopub.status.idle":"2022-04-24T03:43:34.97533Z","shell.execute_reply.started":"2022-04-24T03:34:26.107865Z","shell.execute_reply":"2022-04-24T03:43:34.974195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">IDENTIFY ANY HEURISTICS OR RULES REGARDING SEGMENTATION</p></div>  \n****\nFor a given case-id and day number there are two different amounts of scans present\n\n* 144 slices --> 259 instances\n* 80 slices ---> 15 instances","metadata":{}},{"cell_type":"code","source":"train_df[\"slice_count\"] = train_df.id.apply(lambda x: int(x.rsplit(\"_\", 1)[-1]))\nprint(\"\\n... CASE-ID/DAY-NUM SLICE INFORMATION ...\\n\")\ntrain_df.groupby([\"case_id\", \"day_num\"])[\"slice_count\"].max().value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:43:34.977256Z","iopub.execute_input":"2022-04-24T03:43:34.97752Z","iopub.status.idle":"2022-04-24T03:43:35.039259Z","shell.execute_reply.started":"2022-04-24T03:43:34.977488Z","shell.execute_reply":"2022-04-24T03:43:35.038269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"slice_to_occurence_df = train_df.groupby(\"slice_count\")[[\"lb_seg_flag\", \"sb_seg_flag\", \"st_seg_flag\"]].sum().reset_index()\nfig = px.bar(slice_to_occurence_df, \n             x=\"slice_count\", \n             y=[\"lb_seg_flag\", \"sb_seg_flag\", \"st_seg_flag\"],\n             orientation=\"v\", \n             labels={\"slice_count\":\"<b>Slice Number</b>\", \"value\":\"<b>Number Of Examples</b>\",}, \n             title=\"<b>Number of Examples Per Example For Our 3 Organs</b>\")\nfig.update_layout(legend_title=\"<b>Organ Type Legend</b>\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:43:35.040574Z","iopub.execute_input":"2022-04-24T03:43:35.041471Z","iopub.status.idle":"2022-04-24T03:43:35.097254Z","shell.execute_reply.started":"2022-04-24T03:43:35.04142Z","shell.execute_reply":"2022-04-24T03:43:35.096511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n... WHICH SLICES ARE ALWAYS BLANK (NO SEG) BY LABEL ...\\n\")\nkeep_slice_blank_map = {_sh_lbl:slice_to_occurence_df[slice_to_occurence_df[f\"{_sh_lbl}_seg_flag\"]==0].slice_count.to_list() for _sh_lbl in [\"lb\", \"sb\", \"st\"]}\nkeep_slice_blank_map","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:48:15.388378Z","iopub.execute_input":"2022-04-24T03:48:15.388716Z","iopub.status.idle":"2022-04-24T03:48:15.405419Z","shell.execute_reply.started":"2022-04-24T03:48:15.388679Z","shell.execute_reply":"2022-04-24T03:48:15.404073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:70%;text-align:left\">CREATE 3D GIF FOR CASE>DAY GROUPS OF SLICES (WITH MASK)</p></div> ","metadata":{}},{"cell_type":"code","source":"case_id=115\nday_num=0\ndf=train_df\nsub_df = df[(df.case_id==case_id) & (df.day_num==day_num)]\n    \nf_paths  = sub_df.f_path.tolist()\nlb_rles  = sub_df.lb_seg_rle.tolist()\nsb_rles  = sub_df.sb_seg_rle.tolist()\nst_rles  = sub_df.st_seg_rle.tolist()\nslice_ws = sub_df.slice_w.tolist()\nslice_hs = sub_df.slice_h.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T03:54:56.03023Z","iopub.execute_input":"2022-04-24T03:54:56.030598Z","iopub.status.idle":"2022-04-24T03:54:56.041459Z","shell.execute_reply.started":"2022-04-24T03:54:56.030558Z","shell.execute_reply":"2022-04-24T03:54:56.040445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_animation(case_id=115, \n                 day_num=0, \n                 df=train_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T04:12:13.848298Z","iopub.execute_input":"2022-04-24T04:12:13.84877Z","iopub.status.idle":"2022-04-24T04:12:34.135728Z","shell.execute_reply.started":"2022-04-24T04:12:13.848717Z","shell.execute_reply":"2022-04-24T04:12:34.134253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:100%;text-align:center\">REFERENCE</p></div>\n****\nhttps://www.kaggle.com/code/dschettler8845/uwm-gi-tract-image-segmentation-eda","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"color:white;display:fill;border-radius:5px;background-color:#75B7BF;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:20px;color:white;overflow:hidden;margin:0;font-size:100%;text-align:center\">To be Continued</p></div>","metadata":{}}]}