{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":">### PurgedGroupTimeSeries CV - LGBM Version\n>This is a simple starter notebook for Kaggle's JPX Comp showing purged group timeseries KFold with extra data. Purged Times Series is explained [here][1]. There are many configuration variables below to allow you to experiment. Use either CPU or GPU. You can control which years are loaded, which type of models are used, and whether to use feature engineering. You can experiment with different data preprocessing, model hyperparameters, loss, and number of seeds to ensemble. The extra datasets contain the full history of the assets at the same format of the competition, so you can input that into your model too.\n>\n>**NOTE:** this notebook lets you run a different experiment in each fold if you want to run lots of experiments. (Then it is like running multiple holdout validation experiments but in that case note that the overall CV score is meaningless because LB will be much different when the multiple experiments are ensembled to predict test). **If you want a proper CV with a reliable overall CV score you need to choose the same configuration for each fold.**\n>\n\n[1]: TBD","metadata":{"_cell_guid":"8af163ff-915a-4fae-aefe-6447e64952e5","_uuid":"b328cc9e-a536-4347-beed-d033e9f5ac6a","papermill":{"duration":0.029501,"end_time":"2021-11-29T20:05:53.836047","exception":false,"start_time":"2021-11-29T20:05:53.806546","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<center><img src=\"https://lightgbm.readthedocs.io/en/latest/_images/LightGBM_logo_black_text.svg\" height=250 width=250></center>\n<hr>\n<center>LightGBM = üå≥ + üöÄ + ‚ò¢Ô∏è</center>","metadata":{"papermill":{"duration":0.030287,"end_time":"2021-11-29T20:05:54.455816","exception":false,"start_time":"2021-11-29T20:05:54.425529","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"LightGBM is the current \"Meta\" on kaggle and it doesn't look like it is going to get Nerfed anytime soon! \nIt is basiclly a \"light\" version of gradient boosting machines framework that aims to increases efficiency and reduces memory usage.\n\n**It is usually THE Algorithm everyone on Kaggle try when facing a tabular dataset**\n\n><h4>TL;DR: What makes LightGBM so great:</h4>\n>\n>1. LGBM was developed and maintained by Microsoft themselves so it gets constant maintenance and support.\n>2. Easy to use \n>3. Faster than nearly all other gradient boosting algorithms.\n>4. Usually the most powerful gradient boosting. \n\n\nIt is a **gradient boosting** model that makes use of tree based learning algorithms. It is considered to be a fast processing algorithm.\n\nWhile other algorithms trees grow horizontally, LightGBM algorithm grows vertically, meaning it grows leaf-wise and other algorithms grow level-wise. LightGBM chooses the leaf with large loss to grow. It can lower down more loss than a level wise algorithm when growing the same leaf.\n\n![img](https://i.imgur.com/pzOP2Lb.png)\n\n[Source of Image](https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/)\n\nLight GBM is prefixed as Light because of its high speed. Light GBM can handle the large size of data and takes lower memory to run.\n\nAnother reason why Light GBM is so popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development.\n\n<h4>Leaf growth technique in LightGBM</h4>\n\nLightGBM uses leaf-wise (best-first) tree growth. It chooses to grow the leaf that minimizes the loss, allowing a growth of an imbalanced tree. Because it doesn‚Äôt grow level-wise, but leaf-wise, over-fitting can happen when data is small. In these cases, it is important to control the tree depth.\n\n<h4>LightGBM vs XGBoost</h4>\n\nbase learner of almost all of the competitions that have structured datasets right now. This is mostly because of LightGBM's implementation; it doesn't do exact searches for optimal splits like XGBoost does in it's default setting but rather through histogram approximations (XGBoost now has this functionality as well but it's still not as fast as LightGBM). \n\nThis results in slight decrease of predictive performance buy much larger increase of speed. This means more opportunity for feature engineering/experimentation/model tuning which inevitably yields larger increases in predictive performance. (Feature engineering are the key to winning most Kaggle competitions)\n\n\n<h4>LightGBM vs Catboost</h4>\n\nCatBoost is not used as much, mostly because it tends to be much slower than LightGBM and XGBoost. That being said, CatBoost is very different when it comes to the implementation of gradient boosting. This can give slightly more accurate predictions, in particular if you have large amounts of categorical features. Because rapid experimentation is vital in Kaggle competitions, LightGBM tends to be the go-to algorithm when first creating strong base learners.\n\nIn general, it is important to note that a large amount of approaches involves combining all three boosting algorithms in an ensemble. LightGBM, CatBoost, and XGBoost might be thrown together in a mix to create a strong ensemble. This is done to really squeeze spots on the leaderboard and it usually works.\n\n<div class=\"alert alert-block alert-info\">\n<b>Read More:</b>\n<ul>\n    <li><a href = \"https://github.com/microsoft/LightGBM/tree/master/python-package\">LightGBM Github Documentation</a></li>\n    <li><a href = \"https://lightgbm.readthedocs.io/en/latest/Features.html\">All features of LightGBM</a></li>\n    <li><a href = \"https://lightgbm.readthedocs.io/en/latest/index.html\">Official Documentation</a></li>    \n</ul>\n</div>\n\n____\n\n<h3>Hyper-Parameter Tuning in LightGBM</h3>\n\n____\n    \nParameter Tuning is an important part that is usually done by data scientists to achieve a good accuracy, fast result and to deal with overfitting. Let us see quickly some of the parameter tuning you can do for better results.\nWhile, LightGBM has more than 100 parameters that are given in the [documentation of LightGBM](https://github.com/microsoft/LightGBM), we are going to check the most important ones.\n\n**num_leaves**: This parameter is responsible for the complexity of the model. I normally start by trying values in the range [10,100]. But if you have a solid heuristic to choose tree depth you can always use it and set num_leaves to 2^tree_depth - 1\n\n[LightGBM Documentation](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html) says in respect -\nThis is the main parameter to control the complexity of the tree model. Theoretically, we can set num_leaves = 2^(max_depth) to obtain the same number of leaves as depth-wise tree. However, this simple conversion is not good in practice. The reason is that a leaf-wise tree is typically much deeper than a depth-wise tree for a fixed number of leaves. Unconstrained depth can induce over-fitting. Thus, when trying to tune the num_leaves, we should let it be smaller than 2^(max_depth). For example, when the max_depth=7 the depth-wise tree can get good accuracy, but setting num_leaves to 127 may cause over-fitting, and setting it to 70 or 80 may get better accuracy than depth-wise.\n\n**Min_data_in_leaf**: Assigning bigger value to this parameter can result in underfitting of the model. Giving it a value of 100 or 1000 is sufficient for a large dataset.\n\n**Max_depth**: Controls the depth of the individual trees. Typical values range from a depth of 3‚Äì8 but it is not uncommon to see a tree depth of 1. Smaller depth trees are computationally efficient (but require more trees); however, higher depth trees allow the algorithm to capture unique interactions but also increase the risk of over-fitting. Larger training data sets are more tolerable to deeper trees.\n\n**num_iterations**: Num_iterations specifies the number of boosting iterations (trees to build). The more trees you build the more accurate your model can be at the cost of:\n    - Longer training time\n    - Higher chance of over-fitting\nSo typically start with a lower number of trees to build a baseline and increase it later when you want to squeeze the last % out of your model.\n\nIt is recommended to use smaller `learning_rate` with larger `num_iterations`. Also, we should use `early_stopping_rounds` if we go for higher `num_iterations` to stop your training when it is not learning anything useful.\n\n**early_stopping_rounds** - \"early stopping\" refers to stopping the training process if the model's performance on a given validation set does not improve for several consecutive iterations. This parameter will stop training if the validation metric is not improving after the last early stopping round. It should be defined in pair with a number of iterations. If we set it too large we increase the chance of over-fitting. **The rule of thumb is to have it at 10% of your `num_iterations`**.\n\n____\n    \n<h3>Other Parameters Overview</h3>\n\n____\n    \n**Parameters that control the trees of LightGBM**\n\n- num_leaves: controls the number of decision leaves in a single tree. there will be multiple trees in pool.\n- min_data_in_leaf: the minimum number of data/sample/count per leaf (default is 20; lower min_data_in_leaf means less conservative/control, potentially overfitting).\n- max_depth: this the height of a decision tree. if its more possibility of overfitting but too low may underfit.\n>**NOTE:** max_depth directly impacts:\n>1. The best value for the num_leaves parameter\n>2. Model Performance\n>3. Training Time\n\n____\n\n**Parameters For Better Accuracy**\n\n- Use large max_bin (may be slower)\n\n Use small learning_rate with large num_iterations\n\n- Use large num_leaves (may cause over-fitting)\n\n- Use bigger training data\n\n- Try dart\n\n____\n\n**Parameters for Dealing with Over-fitting**\n\n- Use small max_bin\n\n- Use small num_leaves\n\n- Use min_data_in_leaf and min_sum_hessian_in_leaf\n\n- Use bagging by set bagging_fraction and bagging_freq\n\n- Use feature sub-sampling by set feature_fraction\n\n- Use bigger training data\n\n- Try lambda_l1, lambda_l2 and min_gain_to_split for regularization\n\n- Try max_depth to avoid growing deep tree\n\n- Try extra_trees\n\n- Try increasing path_smooth\n\n____\n\n\n<h3>How to tune LightGBM like a boss?</h3>\n\nHyperparameters tuning guide:\n\n**objective**\n * When you change it affects other parameters\tSpecify the type of ML model\n * default- value regression\n * aliases- Objective_type\n\n**boosting**\n * If you set it RF, that would be a bagging approach\n * default- gbdt\n * Range- [gbdt, rf, dart, goss]\n * aliases- boosting_type\n\n**lambda_l1**\n * regularization parameter\n * default- 0.0\n * Range- [0, ‚àû]\n * aliases- reg_alpha\n * constraints- lambda_l1 >= 0.0\n\n**bagging_fraction**\n * randomly select part of data without resampling\n * default-1.0\n * range- [0, 1]\n * aliases- Subsample\n * constarints- 0.0 < bagging_fraction <= 1.0\n\n**bagging_freq**\n * default- 0.0\n * range- [0, ‚àû]\n * aliases- subsample_freq\n * bagging_fraction should be set to value smaller than 1.0 as well 0 means disable bagging\n\n**num_leaves**\n * max number of leaves in one tree\n * default- 31\n * Range- [1, ‚àû]\n * Note- 1 < num_leaves <= 131072\n\n**feature_fraction**\n * if you set it to 0.8, LightGBM will select 80% of features\n * default- 1.0\n * Range- [0, 1]\n * aliases- sub_feature\n * constarint- 0.0 < feature_fraction <= 1.0\n\n**max_depth**\n * default- [-1]\n * range- [-1, ‚àû]m\n * Larger is usually better, but overfitting speed increases.\n * limit the max depth Forr tree model\n\n**max_bin**\n * deal with over-fitting\n * default- 255\n * range- [2, ‚àû]\n * aliases- Histogram Binning\n * max_bin > 1\n\n**num_iterations**\n * number of boosting iterations\n * default- 100\n * range- [1, ‚àû]\n * AKA- Num_boost_round, n_iter\n * constarints- num_iterations >= 0\n\n**learning_rate**\n * default- 0.1\n * range- [0 1]\n * aliases- eta\n * general values- learning_rate > 0.0Typical: 0.05.\n\n**early_stopping_round**\n * will stop training if validation doesn‚Äôt improve in last early_stopping_round\n * Model Performance, Number of Iterations, Training Time\n * default- 0\n * Range- [0, ‚àû]\n\n**categorical_feature** \n * to sepecify or Handle categorical features\n * i.e LGBM automatically handels categorical variable we dont need to one hot encode them.\n\n**bagging_freq**\n * default-0.0\n * Range-[0, ‚àû]\n * aliases- subsample_freq\n * note- 0 means disable bagging; k means perform bagging at every k iteration\n * enable    bagging, bagging_fraction should be set to value smaller than 1.0 as well\n\n**verbosity**\n * default- 0\n * range- [-‚àû, ‚àû]\n * aliases- verbose\n * constraints- {< 0: Fatal, = 0: Error (Warning), = 1: Info, > 1}\n\n**min_data_in_leaf**\n * Can be used to deal with over-fitting:\n * default- 20\n * constarint-min_data_in_leaf >= 0      \n\n\n<div class=\"alert alert-block alert-warning\">\n<b>Introduction Credits:</b>\n<ul>\n    <li><a href = \"https://www.kaggle.com/shivansh002/your-friendly-neighbour-lightgbm\">Your Friendly Neighbour LightGBM</a> By @shivansh002. Thank you @shivansh002 for a great introduction! </li>\n    <li><a href = \"https://www.kaggle.com/paulrohan2020/tutorial-lightgbm-xgboost-catboost-top-11\">Tutorial LightGBM + XGBoost + CatBoost</a> By @paulrohan2020. Thank you @paulrohan2020 for a great tutorial! </li>\n</ul>\n</div>","metadata":{"papermill":{"duration":0.030511,"end_time":"2021-11-29T20:05:54.517188","exception":false,"start_time":"2021-11-29T20:05:54.486677","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Code starts here ‚¨á","metadata":{}},{"cell_type":"code","source":"import os\nimport traceback\nimport seaborn as sns\nfrom scipy.stats import pearsonr\nimport pandas as pd, numpy as np\nimport jpx_tokyo_market_prediction\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error","metadata":{"papermill":{"duration":2.072505,"end_time":"2021-11-29T20:05:56.744057","exception":false,"start_time":"2021-11-29T20:05:54.671552","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:17:00.689178Z","iopub.execute_input":"2022-04-23T10:17:00.689827Z","iopub.status.idle":"2022-04-23T10:17:02.820656Z","shell.execute_reply.started":"2022-04-23T10:17:00.689723Z","shell.execute_reply":"2022-04-23T10:17:02.819597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"config\">Configuration üéöÔ∏è</span>\n<hr >\n\nIn order to be a proper cross validation with a meaningful overall CV score, **you need to choose the same** `INC2022`, `INC2021`, `INC2020`, `INC2019`, `INC2018`, `INC2017`, `INCCOMP`, `INCSUPP`, and `NUM_LEAVES`, `MAX_DEPTH` **for each fold**. If your goal is to just run lots of experiments, then you can choose to have a different experiment in each fold. Then each fold is like a holdout validation experiment. When you find a configuration you like, you can use that configuration for all folds.\n* DEVICE - is CPU or GPU\n* SEED - a different seed produces a different triple stratified kfold split.\n* FOLDS - number of folds. Best set to 3, 5, or 15 but can be any number between 2 and 15\n* LOAD_STRICT - This controls whether to load strict at proposed [here](https://www.kaggle.com/julian3833/proposal-for-a-meaningful-lb-strict-lgbm)\n* INC2022 - This controls whether to include the extra historical prices during 2022.\n* INC2021 - This controls whether to include the extra historical prices during 2021.\n* INC2020 - This controls whether to include the extra historical prices during 2020.\n* INC2019 - This controls whether to include the extra historical prices during 2019.\n* INC2018 - This controls whether to include the extra historical prices during 2018.\n* INC2017 - This controls whether to include the extra historical prices during 2017.\n* INCSUPP - This controls whether to include the supplemented train data that was released with the competition.\n* N_ESTIMATORS - is a list of length FOLDS. These are n_estimators for each fold. For maximum speed, it is best to use the smallest number of estimators as your GPU or CPU allows.\n* NUM_LEAVES - is a list of length FOLDS. These are num_leaves for each fold. For maximum speed, it is best to use the smallest number of estimators as your GPU or CPU allows.\n* MAX_DEPTH - is a list of length FOLDS. These are max_depth for each fold. For maximum speed, it is best to use the smallest number of estimators as your GPU or CPU allows.\n* LEARNING_RATE - is a list of length FOLDS. These are max_depth for each fold. For maximum speed, it is best to use the smallest number of estimators as your GPU or CPU allows.","metadata":{"papermill":{"duration":0.030988,"end_time":"2021-11-29T20:05:56.806489","exception":false,"start_time":"2021-11-29T20:05:56.775501","status":"completed"},"tags":[]}},{"cell_type":"code","source":"DEVICE = \"CPU\" #or \"GPU\"\n\nSEED = 42\n\n# CV PARAMS\nFOLDS = 5\nGROUP_GAP = 130\nMAX_TEST_GROUP_SIZE = 180\nMAX_TRAIN_GROUP_SIZE = 280\n\n# WHICH YEARS TO INCLUDE? YES=1 NO=0\nINC2022 = 1\nINC2021 = 1\nINC2020 = 1\nINC2019 = 1\nINC2018 = 1\nINC2017 = 1\nINCSUPP = 1\n\n# HYPER PARAMETERS\nLEARNING_RATE = [0.09, 0.09, 0.09, 0.09, 0.09]\nN_ESTIMATORS = [1000, 1000, 1000, 1000, 1000]\nNUM_LEAVES = [500, 500, 500, 500, 500]\nMAX_DEPTH = [10, 10, 10, 10, 10]","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","papermill":{"duration":0.042029,"end_time":"2021-11-29T20:05:56.879687","exception":false,"start_time":"2021-11-29T20:05:56.837658","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:17:02.822083Z","iopub.execute_input":"2022-04-23T10:17:02.822325Z","iopub.status.idle":"2022-04-23T10:17:02.82981Z","shell.execute_reply.started":"2022-04-23T10:17:02.822299Z","shell.execute_reply":"2022-04-23T10:17:02.82828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"loading\">Data Loading üóÉÔ∏è</span>\n<hr>\n\nHere we choose which years to load. We can use either 2017, 2018, 2019, 2020, 2021, Original, Supplement by changing the `INC2021`, `INC2020`, `INC2019`, `INC2018`, `INC2017`, `INCSUPP` variables in the preceeding code section. These datasets are discussed [here][1].\n\n[1]: TBD\n","metadata":{"papermill":{"duration":0.030688,"end_time":"2021-11-29T20:05:57.012548","exception":false,"start_time":"2021-11-29T20:05:56.98186","status":"completed"},"tags":[]}},{"cell_type":"code","source":"stock_list = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv\")\nprices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\nstock_list = stock_list.loc[stock_list['SecuritiesCode'].isin(prices['SecuritiesCode'].unique())]\nstock_name_dict = {stock_list['SecuritiesCode'].tolist()[idx]: stock_list['Name'].tolist()[idx] for idx in range(len(stock_list))}\n\ndef load_training_data(asset_id = None):\n    prices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\n    supplemental_prices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv\")\n    df_train = pd.concat([prices, supplemental_prices]) if INCSUPP else prices\n    df_train = pd.merge(df_train, stock_list[['SecuritiesCode', 'Name']], left_on = 'SecuritiesCode', right_on = 'SecuritiesCode', how = 'left')\n    df_train['date'] = pd.to_datetime(df_train['Date'])\n    df_train['year'] = df_train['date'].dt.year\n    if not INC2022: df_train = df_train.loc[df_train['year'] != 2022]\n    if not INC2021: df_train = df_train.loc[df_train['year'] != 2021]\n    if not INC2020: df_train = df_train.loc[df_train['year'] != 2020]\n    if not INC2019: df_train = df_train.loc[df_train['year'] != 2019]\n    if not INC2018: df_train = df_train.loc[df_train['year'] != 2018]\n    if not INC2017: df_train = df_train.loc[df_train['year'] != 2017]\n    # asset_id = 1301 # Remove before flight\n    if asset_id is not None: df_train = df_train.loc[df_train['SecuritiesCode'] == asset_id]\n    # df_train = df_train[:1000] # Remove before flight\n    return df_train","metadata":{"_kg_hide-input":true,"papermill":{"duration":22.881486,"end_time":"2021-11-29T20:06:19.925396","exception":false,"start_time":"2021-11-29T20:05:57.04391","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:17:02.83254Z","iopub.execute_input":"2022-04-23T10:17:02.833498Z","iopub.status.idle":"2022-04-23T10:17:09.991284Z","shell.execute_reply.started":"2022-04-23T10:17:02.833434Z","shell.execute_reply":"2022-04-23T10:17:09.990426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"features\">Feature Engineering üî¨</span>\n<hr>\n\nThis notebook uses upper_shadow, lower_shadow, high_div_low, open_sub_close, seasonality/datetime features first shown in this notebook [here][1] and successfully used by julian3833 [here][2].\n\nAdditionally we can decide to use external data by changing the variables `INC2021`, `INC2020`, `INC2019`, `INC2018`, `INC2017`, `INCCOMP` in the preceeding code section. These variables respectively indicate whether to load last year 2021 data and/or year 2020, 2019, 2018, 2017, the original, supplemented data. These datasets are discussed [here][3]\n\nConsider experimenting with different feature engineering and/or external data. The code to extract features out of the dataset is taken from julian3833' notebook [here][2]. Thank you julian3833, this is great work.\n\n[1]: https://www.kaggle.com/cstein06/tutorial-to-the-g-research-crypto-competition\n[2]: https://www.kaggle.com/julian3833\n[3]: TBD","metadata":{"papermill":{"duration":0.030035,"end_time":"2021-11-29T20:06:19.986688","exception":false,"start_time":"2021-11-29T20:06:19.956653","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from pandas import concat\nfrom pandas import DataFrame\nfrom scipy.stats import pearsonr\nfrom sklearn.model_selection import train_test_split\n\ndef upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\ndef lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']\n\ndef get_features(df):\n    df_feat = df[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n    df_feat['upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['lower_Shadow'] = lower_shadow(df_feat)\n    df_feat[\"high_div_low\"] = df_feat[\"High\"] / df_feat[\"Low\"]\n    df_feat[\"open_sub_close\"] = df_feat[\"Open\"] - df_feat[\"Close\"]\n    return df_feat","metadata":{"papermill":{"duration":0.041949,"end_time":"2021-11-29T20:06:20.059338","exception":false,"start_time":"2021-11-29T20:06:20.017389","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:17:09.993001Z","iopub.execute_input":"2022-04-23T10:17:09.99323Z","iopub.status.idle":"2022-04-23T10:17:10.001687Z","shell.execute_reply.started":"2022-04-23T10:17:09.993204Z","shell.execute_reply":"2022-04-23T10:17:10.000638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"modelconf\">Configure the model ‚öôÔ∏è</span>\n<hr>\n\nThis is a simple model with simple set of hyperparameters. Consider experimenting with different models, parameters, ensembles and so on.","metadata":{"papermill":{"duration":0.030233,"end_time":"2021-11-29T20:06:20.11976","exception":false,"start_time":"2021-11-29T20:06:20.089527","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**The Model**","metadata":{"papermill":{"duration":0.030343,"end_time":"2021-11-29T20:06:20.311281","exception":false,"start_time":"2021-11-29T20:06:20.280938","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def build_model(fold):\n\n    # Do feel free to experiment with different models here!\n    model = LGBMRegressor(n_estimators = N_ESTIMATORS[fold], num_leaves = NUM_LEAVES[fold], max_depth = MAX_DEPTH[fold], learning_rate = LEARNING_RATE[fold])\n\n    return model","metadata":{"papermill":{"duration":0.038312,"end_time":"2021-11-29T20:06:20.379895","exception":false,"start_time":"2021-11-29T20:06:20.341583","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:17:10.003611Z","iopub.execute_input":"2022-04-23T10:17:10.003933Z","iopub.status.idle":"2022-04-23T10:17:10.018668Z","shell.execute_reply.started":"2022-04-23T10:17:10.003893Z","shell.execute_reply":"2022-04-23T10:17:10.017867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time Series Cross Validation\n\n> \"There are many different ways one can do cross-validation, and **it is the most critical step when building a good machine learning model** which is generalizable when it comes to unseen data.\"\n-- **Approaching (Almost) Any Machine Learning Problem**, by Abhishek Thakur\n\nCV is the **first** step, but very few notebooks are talking about this. Here we look at \"purged rolling time series CV\" and actually apply it in hyperparameter tuning for a basic estimator. This notebook owes a debt of gratitude to the notebook [\"Found the Holy Grail GroupTimeSeriesSplit\"](https://www.kaggle.com/jorijnsmit/found-the-holy-grail-grouptimeseriessplit). That notebook is excellent and this solution is an extention of the quoted pending sklearn estimator. I modify that estimator to make it more suitable for the task at hand in this competition. The changes are\n\n- you can specify a **gap** between each train and validation split. This is important because even though the **group** aspect keeps whole days together, we suspect that the anonymized features have some kind of lag or window calculations in them (which would be standard for financial features). By introducing a gap, we mitigate the risk that we leak information from train into validation\n- we can specify the size of the train and validation splits in terms of **number of days**. The ability to specify a validation set size is new and the the ability to specify days, as opposed to samples, is new.\n\nThe code for `PurgedTimeSeriesSplit` is below. I've hidden it because it is really meant to act as an imported class. If you want to see the code and copy for your work, click on the \"Code\" box.","metadata":{"papermill":{"duration":0.03025,"end_time":"2021-11-29T20:06:20.441","exception":false,"start_time":"2021-11-29T20:06:20.41075","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\nclass GroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_size : int, default=None\n        Maximum size for a single training set.\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\\\n                           'b', 'b', 'b', 'b', 'b',\\\n                           'c', 'c', 'c', 'c',\\\n                           'd', 'd', 'd'])\n    >>> gtss = GroupTimeSeriesSplit(n_splits=3)\n    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):\n    ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n    ...     print(\"TRAIN GROUP:\", groups[train_idx],\\\n                  \"TEST GROUP:\", groups[test_idx])\n    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\\\n    TEST GROUP: ['b' 'b' 'b' 'b' 'b']\n    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\\\n    TEST GROUP: ['c' 'c' 'c' 'c']\n    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\\\n    TEST: [15, 16, 17]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\\\n    TEST GROUP: ['d' 'd' 'd']\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_size=None\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_size = max_train_size\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n        group_test_size = n_groups // n_folds\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n            for train_group_idx in unique_groups[:group_test_start]:\n                train_array_tmp = group_dict[train_group_idx]\n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n            train_end = train_array.size\n            if self.max_train_size and self.max_train_size < train_end:\n                train_array = train_array[train_end -\n                                          self.max_train_size:train_end]\n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n            yield [int(i) for i in train_array], [int(i) for i in test_array]\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# modified code for group gaps; source\n# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\nclass PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Allows for a gap in groups to avoid potentially leaking info from\n    train into test if the model has windowed or lag features.\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 verbose=False\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n        self.verbose = verbose\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n\n        group_test_size = min(n_groups // n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n\n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n\n            train_end = train_array.size\n\n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n\n            test_array  = test_array[group_gap:]\n\n\n            if self.verbose > 0:\n                    pass\n\n            yield [int(i) for i in train_array], [int(i) for i in test_array]","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":0.068379,"end_time":"2021-11-29T20:06:20.539899","exception":false,"start_time":"2021-11-29T20:06:20.47152","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:17:10.019957Z","iopub.execute_input":"2022-04-23T10:17:10.020532Z","iopub.status.idle":"2022-04-23T10:17:10.060793Z","shell.execute_reply.started":"2022-04-23T10:17:10.02049Z","shell.execute_reply":"2022-04-23T10:17:10.06016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"training\">Training üèãÔ∏è</span>\n<hr>\nOur model will be trained for the number of FOLDS and ESTIMATORS you chose in the configuration above. Each fold the model with lowest validation loss will be saved and used to predict OOF and test. Adjust the variable `VERBOSE`. The variable `VERBOSE=1 or 2` will display the training and validation loss for each iteration as text.","metadata":{"papermill":{"duration":0.030152,"end_time":"2021-11-29T20:06:20.599967","exception":false,"start_time":"2021-11-29T20:06:20.569815","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**Let's take a look at our CV**","metadata":{"papermill":{"duration":0.031012,"end_time":"2021-11-29T20:06:20.661609","exception":false,"start_time":"2021-11-29T20:06:20.630597","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\ndef plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n    cmap_cv = plt.cm.coolwarm\n    jet = plt.cm.get_cmap('jet', 256)\n    seq = np.linspace(0, 1, 256)\n    _ = np.random.shuffle(seq)   # inplace\n    cmap_data = ListedColormap(jet(seq))    \n    for ii, (tr, tt) in enumerate(list(cv.split(X=X, y=y, groups=group))):\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0        \n        ax.scatter(range(len(indices)), [ii + .5] * len(indices), c=indices, marker='_', lw=lw, cmap=cmap_cv, vmin=-.2, vmax=1.2)\n    ax.scatter(range(len(X)), [ii + 1.5] * len(X), c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n    ax.scatter(range(len(X)), [ii + 2.5] * len(X), c=group, marker='_', lw=lw, cmap=cmap_data)\n    yticklabels = list(range(n_splits)) + ['target', 'day']\n    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels, xlabel='Sample index', ylabel=\"CV iteration\", ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n    return ax\n\ndef plot_importance(importances, features_names, PLOT_TOP_N = 20, figsize=(12, 20)):\n    try: plt.close()\n    except: pass\n    importance_df = pd.DataFrame(data=importances, columns=features_names)\n    sorted_indices = importance_df.median(axis=0).sort_values(ascending=False).index\n    sorted_importance_df = importance_df.loc[:, sorted_indices]\n    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n    _, ax = plt.subplots(figsize=figsize)\n    ax.grid()\n    ax.set_xscale('log')\n    ax.set_ylabel('Feature')\n    ax.set_xlabel('Importance')\n    plt.title('Feature Importances')\n    sns.boxplot(data=sorted_importance_df[plot_cols], orient='h', ax=ax)\n    plt.show()\n\nasset_id = 1301\ndf = load_training_data(asset_id)\ndf_proc = get_features(df)\ndf_proc['date'] = df['date'].copy()\ndf_proc['y'] = df['Target']\ndf_proc = df_proc.dropna(how=\"any\")\nX = df_proc.drop(\"y\", axis=1)\ny = df_proc[\"y\"]\ngroups = pd.factorize(X['date'].dt.day.astype(str) + '_' + X['date'].dt.month.astype(str) + '_' + X['date'].dt.year.astype(str))[0]\nX = X.drop(columns = 'date')\n\nfig, ax = plt.subplots(figsize = (12, 6))\ncv = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap = GROUP_GAP, max_train_group_size=MAX_TRAIN_GROUP_SIZE, max_test_group_size=MAX_TEST_GROUP_SIZE)\nplot_cv_indices(cv, X, y, groups, ax, FOLDS, lw=20)","metadata":{"papermill":{"duration":164.493548,"end_time":"2021-11-29T20:09:05.186137","exception":false,"start_time":"2021-11-29T20:06:20.692589","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:17:10.062088Z","iopub.execute_input":"2022-04-23T10:17:10.062844Z","iopub.status.idle":"2022-04-23T10:17:17.271471Z","shell.execute_reply.started":"2022-04-23T10:17:10.062791Z","shell.execute_reply":"2022-04-23T10:17:17.270598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Main Training Function**","metadata":{"papermill":{"duration":0.031993,"end_time":"2021-11-29T20:09:05.250334","exception":false,"start_time":"2021-11-29T20:09:05.218341","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit\nVERBOSE = 0\n\ndef get_Xy_and_model():\n    df = load_training_data()\n    orig_close = df['Close'].copy()\n    orig_sec_code = df['SecuritiesCode'].copy()\n    df_proc = get_features(df)\n    df_proc['date'] = df['date'].copy()\n    df_proc['y'] = df['Target'].values\n    df_proc = df_proc.dropna(how=\"any\")\n    X = df_proc.drop(\"y\", axis=1)\n    y = df_proc[\"y\"]\n    groups = pd.factorize(X['date'].dt.day.astype(str) + '_' + X['date'].dt.month.astype(str) + '_' + X['date'].dt.year.astype(str))[0]\n    X = X.drop(columns = 'date')\n    oof_preds = np.zeros(len(X))\n    importances, scores, models = [], [], []\n    for fold, (train_idx, val_idx) in enumerate(PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap = GROUP_GAP, max_train_group_size = MAX_TRAIN_GROUP_SIZE, max_test_group_size = MAX_TEST_GROUP_SIZE).split(X, y, groups)):\n        # GET TRAINING, VALIDATION SET\n        x_train, x_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        # DISPLAY FOLD INFO\n        print('#' * 25); print('#### FOLD', fold + 1)\n        model = build_model(fold)\n\n        # TRAIN\n        model.fit( x_train, y_train, eval_set = [(x_val, y_val)], verbose = VERBOSE )\n        \n        # PREDICT OOF\n        pred = model.predict(x_val)\n        models.append(model)\n\n        # REPORT RESULTS\n        try: mse = mean_squared_error(np.nan_to_num(y_val), np.nan_to_num(pred))\n        except: mse = 0.0\n        scores.append(mse)\n        print('#### FOLD %i OOF MSE %.3f' % (fold + 1, mse))\n\n        oof_preds[val_idx] = pred\n        importances.append(model.feature_importances_)\n\n    df_proc['SecuritiesCode'] = orig_sec_code\n    df = df_proc\n    df['oof_preds'] = np.nan_to_num(oof_preds)\n    df['Close'] = orig_close\n    print('\\n\\n' + ('-' * 80) + '\\n' + 'Finished trainings. Results:')\n    print('Model: r2_score: %s | pearsonr: %s ' % (r2_score(df['y'], df['oof_preds']), pearsonr(df['y'], df['oof_preds'])[0]))\n    print('Predictions std: %s | Target std: %s' % (df['oof_preds'].std(), df['y'].std()))\n\n    try: plt.close()\n    except: pass   \n    df2 = df.reset_index().set_index('date')\n    df2 = df2.loc[df2['SecuritiesCode'] == 1301] # For demonstration purpose only.\n    fig = plt.figure(figsize = (12, 6))\n    # fig, ax_left = plt.subplots(figsize = (12, 6))\n    ax_left = fig.add_subplot(111)\n    ax_left.set_facecolor('azure')\n    ax_right = ax_left.twinx()\n    ax_left.plot(df2['y'].rolling(3 * 30 * 24 * 60).corr(df2['oof_preds']).iloc[::24 * 60], color = 'crimson', label = \"Corr\")\n    ax_right.plot(df2['Close'].iloc[::24 * 60], color = 'darkgrey', label = \"%s Close\" % stock_name_dict[asset_id])\n    plt.legend()\n    plt.grid()\n    plt.xlabel('Time')\n    plt.title('3 month rolling pearsonr for %s' % (stock_name_dict[asset_id]))\n    plt.show()\n\n    plot_importance(np.array(importances), list(X.columns), PLOT_TOP_N = 20)\n    \n    return scores, oof_preds, models, y\n\nprint(f\"Training model\")\ncur_scores, cur_oof_preds, cur_models, cur_targets = get_Xy_and_model()\nscores, oof_preds, models, targets = cur_scores, cur_oof_preds, cur_models, cur_targets","metadata":{"papermill":{"duration":3998.186989,"end_time":"2021-11-29T21:15:43.470016","exception":false,"start_time":"2021-11-29T20:09:05.283027","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:17:17.27282Z","iopub.execute_input":"2022-04-23T10:17:17.273041Z","iopub.status.idle":"2022-04-23T10:22:35.978935Z","shell.execute_reply.started":"2022-04-23T10:17:17.273016Z","shell.execute_reply":"2022-04-23T10:22:35.977914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"codebook\">Calculate OOF MSE</span>\nThe OOF (out of fold) predictions are saved to disk. If you wish to ensemble multiple models, use the OOF to determine what are the best weights to blend your models with. Choose weights that maximize OOF CV score when used to blend OOF. Then use those same weights to blend your test predictions.","metadata":{"papermill":{"duration":0.13112,"end_time":"2021-11-29T21:15:43.726463","exception":false,"start_time":"2021-11-29T21:15:43.595343","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print('Overall MEAN OOF MSE %s' % np.mean(list(scores)))","metadata":{"papermill":{"duration":160.247471,"end_time":"2021-11-29T21:18:24.104968","exception":false,"start_time":"2021-11-29T21:15:43.857497","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:22:35.979964Z","iopub.execute_input":"2022-04-23T10:22:35.980376Z","iopub.status.idle":"2022-04-23T10:22:35.986778Z","shell.execute_reply.started":"2022-04-23T10:22:35.980344Z","shell.execute_reply":"2022-04-23T10:22:35.985287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"submit\">Submit To Kaggle üá∞</span>\n<hr>","metadata":{"papermill":{"duration":0.130926,"end_time":"2021-11-29T21:18:24.366128","exception":false,"start_time":"2021-11-29T21:18:24.235202","status":"completed"},"tags":[]}},{"cell_type":"code","source":"env = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()\n\nfor (df_test, options, financials, trades, secondary_prices, df_pred) in iter_test:\n    x_test = get_features(df_test)\n    y_pred = np.mean(np.concatenate([np.expand_dims(model.predict(x_test), axis = 0) for model in models], axis = 0), axis = 0)\n    df_pred['Target'] = y_pred\n    df_pred = df_pred.sort_values(by = \"Target\", ascending = False)\n    df_pred['Rank'] = np.arange(0,2000)\n    df_pred = df_pred.sort_values(by = \"SecuritiesCode\", ascending = True)\n    df_pred.drop([\"Target\"], axis = 1)\n    submission = df_pred[[\"Date\", \"SecuritiesCode\", \"Rank\"]]\n    env.predict(submission)","metadata":{"papermill":{"duration":1.038598,"end_time":"2021-11-29T21:18:25.5404","exception":false,"start_time":"2021-11-29T21:18:24.501802","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-04-23T10:22:35.98898Z","iopub.execute_input":"2022-04-23T10:22:35.98926Z","iopub.status.idle":"2022-04-23T10:22:37.746851Z","shell.execute_reply.started":"2022-04-23T10:22:35.989221Z","shell.execute_reply":"2022-04-23T10:22:37.745805Z"},"trusted":true},"execution_count":null,"outputs":[]}]}